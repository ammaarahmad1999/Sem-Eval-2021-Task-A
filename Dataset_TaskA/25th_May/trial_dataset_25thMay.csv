topic,paper_ID,text,main_heading,sub_heading,label,pos1,pos2,pos3,citation,head,ofs1,ofs2,ofs3
machine-translation,0,Learning Phrase Representations using RNN Encoder - Decoder for Statistical Machine Translation,title,title,1,2,1,1,0,title : title,0.0091324200913242,1.0,1.0
machine-translation,0,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.0136986301369863,0.16666666666666666,0.16666666666666666
machine-translation,0,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.0182648401826484,0.3333333333333333,0.3333333333333333
machine-translation,0,"One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.0228310502283105,0.5,0.5
machine-translation,0,The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.0273972602739726,0.6666666666666666,0.6666666666666666
machine-translation,0,The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.0319634703196347,0.8333333333333334,0.8333333333333334
machine-translation,0,"Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.0365296803652968,1.0,1.0
machine-translation,0,Introduction,introduction,introduction,0,9,1,1,0,introduction : introduction,0.0410958904109589,0.037037037037037035,0.037037037037037035
machine-translation,0,"Deep neural networks have shown great success in various applications such as objection recognition ( see , e.g. , ) and speech recognition ( see , e.g. , ) .",introduction,introduction,0,10,2,2,0,introduction : introduction,0.045662100456621,0.07407407407407407,0.07407407407407407
machine-translation,0,"Furthermore , many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing ( NLP ) .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.0502283105022831,0.1111111111111111,0.1111111111111111
machine-translation,0,"These include , but are not limited to , language modeling , paraphrase detection and word embedding extraction .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.0547945205479452,0.14814814814814814,0.14814814814814814
machine-translation,0,"In the field of statistical machine translation ( SMT ) , deep neural networks have begun to show promising results .",introduction,introduction,0,13,5,5,0,introduction : introduction,0.0593607305936073,0.18518518518518517,0.18518518518518517
machine-translation,0,summarizes a successful usage of feedforward neural networks in the framework of phrase - based SMT system .,introduction,introduction,0,14,6,6,0,introduction : introduction,0.0639269406392694,0.2222222222222222,0.2222222222222222
machine-translation,0,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .",introduction,introduction,1,15,7,7,0,introduction : introduction,0.0684931506849315,0.25925925925925924,0.25925925925925924
machine-translation,0,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .",introduction,introduction,1,16,8,8,0,introduction : introduction,0.0730593607305936,0.2962962962962963,0.2962962962962963
machine-translation,0,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .",introduction,introduction,1,17,9,9,0,introduction : introduction,0.0776255707762557,0.3333333333333333,0.3333333333333333
machine-translation,0,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,introduction,introduction,1,18,10,10,0,introduction : introduction,0.0821917808219178,0.37037037037037035,0.37037037037037035
machine-translation,0,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .",introduction,introduction,1,19,11,11,0,introduction : introduction,0.0867579908675799,0.4074074074074074,0.4074074074074074
machine-translation,0,The proposed RNN Encoder - Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French .,introduction,introduction,0,20,12,12,0,introduction : introduction,0.091324200913242,0.4444444444444444,0.4444444444444444
machine-translation,0,We train the model to learn the translation probability of an English phrase to a corresponding French phrase .,introduction,introduction,0,21,13,13,0,introduction : introduction,0.0958904109589041,0.48148148148148145,0.48148148148148145
machine-translation,0,The model is then used as apart of a standard phrase - based SMT system by scoring each phrase pair in the phrase table .,introduction,introduction,0,22,14,14,0,introduction : introduction,0.1004566210045662,0.5185185185185185,0.5185185185185185
machine-translation,0,The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder - Decoder improves the translation performance .,introduction,introduction,0,23,15,15,0,introduction : introduction,0.1050228310502283,0.5555555555555556,0.5555555555555556
machine-translation,0,We qualitatively analyze the trained RNN Encoder - Decoder by comparing its phrase scores with those given by the existing translation model .,introduction,introduction,0,24,16,16,0,introduction : introduction,0.1095890410958904,0.5925925925925926,0.5925925925925926
machine-translation,0,"The qualitative analysis shows that the RNN Encoder - Decoder is better at capturing the linguistic regularities in the phrase table , indirectly explaining the quantitative improvements in the over all translation performance .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.1141552511415525,0.6296296296296297,0.6296296296296297
machine-translation,0,The further analysis of the model reveals that the RNN Encoder - Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase .,introduction,introduction,0,26,18,18,0,introduction : introduction,0.1187214611872146,0.6666666666666666,0.6666666666666666
machine-translation,0,"recurrent neural network ( RNN ) is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = ( x 1 , . . . , x T ) .",introduction,introduction,0,27,19,19,0,introduction : introduction,0.1232876712328767,0.7037037037037037,0.7037037037037037
machine-translation,0,"At each time step t , the hidden state ht of the RNN is updated by",introduction,introduction,0,28,20,20,0,introduction : introduction,0.1278538812785388,0.7407407407407407,0.7407407407407407
machine-translation,0,where f is a non-linear activation function .,introduction,introduction,0,29,21,21,0,introduction : introduction,0.1324200913242009,0.7777777777777778,0.7777777777777778
machine-translation,0,maybe as simple as an elementwise logistic sigmoid function and as complex as along short - term memory ( LSTM ) unit ) .,introduction,introduction,0,30,22,22,0,introduction : introduction,0.136986301369863,0.8148148148148148,0.8148148148148148
machine-translation,0,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence .,introduction,introduction,0,31,23,23,0,introduction : introduction,0.1415525114155251,0.8518518518518519,0.8518518518518519
machine-translation,0,"In that case , the output at each timestep t is the conditional distribution p ( x t | x t?1 , . . . , x 1 ) .",introduction,introduction,0,32,24,24,0,introduction : introduction,0.1461187214611872,0.8888888888888888,0.8888888888888888
machine-translation,0,"For example , a multinomial distribution ( 1 - of - K coding ) can be output using a softmax activation function",introduction,introduction,0,33,25,25,0,introduction : introduction,0.1506849315068493,0.9259259259259259,0.9259259259259259
machine-translation,0,"for all possible symbols j = 1 , . . . , K , where w j are the rows of a weight matrix W. By combining these probabilities , we can compute the probability of the sequence x using",introduction,introduction,0,34,26,26,0,introduction : introduction,0.1552511415525114,0.9629629629629629,0.9629629629629629
machine-translation,0,"From this learned distribution , it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step .",introduction,introduction,0,35,27,27,0,introduction : introduction,0.1598173515981735,1.0,1.0
machine-translation,0,RNN Encoder - Decoder,system description,RNN Encoder-Decoder,0,36,1,1,0,system description : RNN Encoder-Decoder,0.1643835616438356,0.012195121951219513,0.08333333333333333
machine-translation,0,The RNN Encoder - Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder .,system description,RNN Encoder-Decoder,0,37,2,2,0,system description : RNN Encoder-Decoder,0.1689497716894977,0.024390243902439025,0.16666666666666666
machine-translation,0,"The input matrix between each input symbol x t and the hidden unit is approximated with two lower - rank matrices , and the output matrix is approximated similarly .",system description,RNN Encoder-Decoder,0,38,3,3,0,system description : RNN Encoder-Decoder,0.1735159817351598,0.036585365853658534,0.25
machine-translation,0,"We used rank - 100 matrices , equivalent to learning an embedding of dimension 100 for each word .",system description,RNN Encoder-Decoder,0,39,4,4,0,system description : RNN Encoder-Decoder,0.1780821917808219,0.04878048780487805,0.3333333333333333
machine-translation,0,The activation function used forh in Eq. ( 8 ) is a hyperbolic tangent function .,system description,RNN Encoder-Decoder,0,40,5,5,0,system description : RNN Encoder-Decoder,0.182648401826484,0.06097560975609756,0.4166666666666667
machine-translation,0,"The computation from the hidden state in the decoder to the output is implemented as a deep neural network ( Pascanu et al. , 2014 ) with a single intermediate layer having 500 maxout units each pooling 2 inputs .",system description,RNN Encoder-Decoder,0,41,6,6,0,system description : RNN Encoder-Decoder,0.1872146118721461,0.07317073170731707,0.5
machine-translation,0,"All the weight parameters in the RNN Encoder - Decoder were initialized by sampling from an isotropic zero-mean ( white ) Gaussian distribution with its standard deviation fixed to 0.01 , except for the recurrent weight parameters .",system description,RNN Encoder-Decoder,0,42,7,7,0,system description : RNN Encoder-Decoder,0.1917808219178082,0.08536585365853659,0.5833333333333334
machine-translation,0,"For the recurrent weight matrices , we first sampled from a white Gaussian distribution and used its left singular vectors matrix , following .",system description,RNN Encoder-Decoder,0,43,8,8,0,system description : RNN Encoder-Decoder,0.1963470319634703,0.0975609756097561,0.6666666666666666
machine-translation,0,"We used Adadelta and stochastic gradient descent to train the RNN Encoder - Decoder with hyperparameters = 10 ?6 and ? = 0.95 ( Zeiler , 2012 ) .",system description,RNN Encoder-Decoder,0,44,9,9,0,system description : RNN Encoder-Decoder,0.2009132420091324,0.10975609756097561,0.75
machine-translation,0,"At each update , we used 64 randomly selected phrase pairs from a phrase table ( which was created from 348 M words ) .",system description,RNN Encoder-Decoder,0,45,10,10,0,system description : RNN Encoder-Decoder,0.2054794520547945,0.12195121951219512,0.8333333333333334
machine-translation,0,The model was trained for approximately three days .,system description,RNN Encoder-Decoder,0,46,11,11,0,system description : RNN Encoder-Decoder,0.2100456621004566,0.13414634146341464,0.9166666666666666
machine-translation,0,Details of the architecture used in the experiments are explained in more depth in the supplementary material .,system description,RNN Encoder-Decoder,0,47,12,12,0,system description : RNN Encoder-Decoder,0.2146118721461187,0.14634146341463414,1.0
machine-translation,0,Hidden Unit that Adaptively Remembers and Forgets,system description,Hidden Unit that Adaptively Remembers and Forgets,0,48,13,1,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2191780821917808,0.15853658536585366,0.043478260869565216
machine-translation,0,"In addition to a novel model architecture , we also propose a new type of hidden unit ( f in Eq .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,49,14,2,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2237442922374429,0.17073170731707318,0.08695652173913043
machine-translation,0,1 ) ) that has been motivated by the LSTM unit but is much simpler to compute and implement .,system description,Hidden Unit that Adaptively Remembers and Forgets,0,50,15,3,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.228310502283105,0.18292682926829268,0.13043478260869565
machine-translation,0,shows the graphical depiction of the proposed hidden unit .,system description,Hidden Unit that Adaptively Remembers and Forgets,0,51,16,4,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2328767123287671,0.1951219512195122,0.17391304347826086
machine-translation,0,Let us describe how the activation of the j - th hidden unit is computed .,system description,Hidden Unit that Adaptively Remembers and Forgets,0,52,17,5,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2374429223744292,0.2073170731707317,0.21739130434782608
machine-translation,0,"First , the reset gate r j is computed by",system description,Hidden Unit that Adaptively Remembers and Forgets,0,53,18,6,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2420091324200913,0.21951219512195122,0.2608695652173913
machine-translation,0,"where ? is the logistic sigmoid function , and [. ] j denotes the j - th element of a vector .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,54,19,7,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2465753424657534,0.23170731707317074,0.30434782608695654
machine-translation,0,"where ? is the logistic sigmoid function , and [. ] j denotes the j - th element of a vector .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,55,20,8,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2511415525114155,0.24390243902439024,0.34782608695652173
machine-translation,0,"and h t?1 are the input and the previous hidden state , respectively .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,56,21,9,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2557077625570776,0.25609756097560976,0.391304347826087
machine-translation,0,rand Ur are weight matrices which are learned .,system description,Hidden Unit that Adaptively Remembers and Forgets,0,57,22,10,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2602739726027397,0.2682926829268293,0.43478260869565216
machine-translation,0,"Similarly , the update gate z j is computed by",system description,Hidden Unit that Adaptively Remembers and Forgets,0,58,23,11,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2648401826484018,0.2804878048780488,0.4782608695652174
machine-translation,0,The actual activation of the proposed unit h j is then computed by,system description,Hidden Unit that Adaptively Remembers and Forgets,0,59,24,12,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2694063926940639,0.2926829268292683,0.5217391304347826
machine-translation,0,wher ? h,system description,Hidden Unit that Adaptively Remembers and Forgets,0,60,25,13,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.273972602739726,0.3048780487804878,0.5652173913043478
machine-translation,0,wher ? h,system description,Hidden Unit that Adaptively Remembers and Forgets,0,61,26,14,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2785388127853881,0.3170731707317073,0.6086956521739131
machine-translation,0,"In this formulation , when the reset gate is close to 0 , the hidden state is forced to ignore the previous hidden state and reset with the current input only .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,62,27,15,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2831050228310502,0.32926829268292684,0.6521739130434783
machine-translation,0,"This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future , thus , allowing a more compact representation .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,63,28,16,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2876712328767123,0.34146341463414637,0.6956521739130435
machine-translation,0,"On the other hand , the update gate controls how much information from the previous hidden state will carryover to the current hidden state .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,64,29,17,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2922374429223744,0.35365853658536583,0.7391304347826086
machine-translation,0,This acts similarly to the memory cell in the LSTM network and helps the RNN to remember longterm information .,system description,Hidden Unit that Adaptively Remembers and Forgets,0,65,30,18,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2968036529680365,0.36585365853658536,0.782608695652174
machine-translation,0,"Furthermore , this maybe considered an adaptive variant of a leaky - integration unit .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,66,31,19,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.3013698630136986,0.3780487804878049,0.8260869565217391
machine-translation,0,"As each hidden unit has separate reset and update gates , each hidden unit will learn to capture dependencies over different time scales .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,67,32,20,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.3059360730593607,0.3902439024390244,0.8695652173913043
machine-translation,0,"Those units that learn to capture short - term dependencies will tend to have reset gates thatare frequently active , but those that capture longer - term dependencies will have update gates thatare mostly active .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,68,33,21,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.3105022831050228,0.4024390243902439,0.9130434782608695
machine-translation,0,"In our preliminary experiments , we found that it is crucial to use this new unit with gating units .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,69,34,22,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.3150684931506849,0.4146341463414634,0.9565217391304348
machine-translation,0,We were notable to get meaningful result with an oft - used tanh unit without any gating .,system description,Hidden Unit that Adaptively Remembers and Forgets,0,70,35,23,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.319634703196347,0.4268292682926829,1.0
machine-translation,0,Statistical Machine Translation,system description,Statistical Machine Translation,0,71,36,1,0,system description : Statistical Machine Translation,0.3242009132420091,0.43902439024390244,0.07692307692307693
machine-translation,0,"In a commonly used statistical machine translation system ( SMT ) , the goal of the system ( decoder , specifically ) is to find a translation f given a source sentence e , which maximizes",system description,Statistical Machine Translation,0,72,37,2,0,system description : Statistical Machine Translation,0.3287671232876712,0.45121951219512196,0.15384615384615385
machine-translation,0,"where the first term at the right hand side is called translation model and the latter language model ( see , e.g. , ) .",system description,Statistical Machine Translation,0,73,38,3,0,system description : Statistical Machine Translation,0.3333333333333333,0.4634146341463415,0.23076923076923078
machine-translation,0,"In practice , however , most SMT systems model log p ( f | e ) as a loglinear model with additional features and corre - sponding weights : where f n and w n are the n - th feature and weight , respectively .",system description,Statistical Machine Translation,0,74,39,4,0,system description : Statistical Machine Translation,0.3378995433789954,0.47560975609756095,0.3076923076923077
machine-translation,0,( e ) is a normalization constant that does not depend on the weights .,system description,Statistical Machine Translation,0,75,40,5,0,system description : Statistical Machine Translation,0.3424657534246575,0.4878048780487805,0.38461538461538464
machine-translation,0,The weights are often optimized to maximize the BLEU score on a development set .,system description,Statistical Machine Translation,0,76,41,6,0,system description : Statistical Machine Translation,0.3470319634703196,0.5,0.46153846153846156
machine-translation,0,"In the phrase - based SMT framework introduced in and , the translation model log p ( e | f ) is factorized into the translation probabilities of matching phrases in the source and target sentences .",system description,Statistical Machine Translation,0,77,42,7,0,system description : Statistical Machine Translation,0.3515981735159817,0.5121951219512195,0.5384615384615384
machine-translation,0,2,system description,Statistical Machine Translation,0,78,43,8,0,system description : Statistical Machine Translation,0.3561643835616438,0.524390243902439,0.6153846153846154
machine-translation,0,These probabilities are once again considered additional features in the log - linear model ( see Eq. ) and are weighted accordingly to maximize the BLEU score .,system description,Statistical Machine Translation,0,79,44,9,0,system description : Statistical Machine Translation,0.3607305936073059,0.5365853658536586,0.6923076923076923
machine-translation,0,"Since the neural net language model was proposed in , neural networks have been used widely in SMT systems .",system description,Statistical Machine Translation,0,80,45,10,0,system description : Statistical Machine Translation,0.365296803652968,0.5487804878048781,0.7692307692307693
machine-translation,0,"In many cases , neural networks have been used to rescore translation hypotheses ( n- best lists ) ( see , e.g. , ) .",system description,Statistical Machine Translation,0,81,46,11,0,system description : Statistical Machine Translation,0.3698630136986301,0.5609756097560976,0.8461538461538461
machine-translation,0,"Recently , however , there has been interest in training neural networks to score the translated sentence ( or phrase pairs ) using a representation of the source sentence as an additional input .",system description,Statistical Machine Translation,0,82,47,12,0,system description : Statistical Machine Translation,0.3744292237442922,0.573170731707317,0.9230769230769231
machine-translation,0,"See , e.g. , , and .",system description,Statistical Machine Translation,0,83,48,13,0,system description : Statistical Machine Translation,0.3789954337899543,0.5853658536585366,1.0
machine-translation,0,Scoring Phrase Pairs with RNN Encoder - Decoder,system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,84,49,1,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.3835616438356164,0.5975609756097561,0.08333333333333333
machine-translation,0,Here we propose to train the RNN Encoder - Decoder ( see Sec. 2.2 ) on a table of phrase pairs and use its scores as additional features in the loglinear model in Eq. ( 9 ) when tuning the SMT decoder .,system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,85,50,2,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.3881278538812785,0.6097560975609756,0.16666666666666666
machine-translation,0,"When we train the RNN Encoder - Decoder , we ignore the ( normalized ) frequencies of each phrase pair in the original corpora .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,86,51,3,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.3926940639269406,0.6219512195121951,0.25
machine-translation,0,This measure was taken in order ( 1 ) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and ( 2 ) to ensure that the RNN Encoder - Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences .,system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,87,52,4,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.3972602739726027,0.6341463414634146,0.3333333333333333
machine-translation,0,One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase pairs in the original corpus .,system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,88,53,5,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4018264840182648,0.6463414634146342,0.4166666666666667
machine-translation,0,"With a fixed capacity of the RNN Encoder - Decoder , we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities , i.e. , distinguishing between plausible and implausible translations , or learning the "" manifold "" ( region of probability concentration ) of plausible translations .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,89,54,6,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4063926940639269,0.6585365853658537,0.5
machine-translation,0,"Once the RNN Encoder - Decoder is trained , we add a new score for each phrase pair to the existing phrase table .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,90,55,7,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.410958904109589,0.6707317073170732,0.5833333333333334
machine-translation,0,This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation .,system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,91,56,8,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4155251141552511,0.6829268292682927,0.6666666666666666
machine-translation,0,"As Schwenk pointed out in , it is possible to completely replace the existing phrase table with the proposed RNN Encoder - Decoder .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,92,57,9,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4200913242009132,0.6951219512195121,0.75
machine-translation,0,"In that case , for a given source phrase , the RNN Encoder - Decoder will need to generate a list of ( good ) target phrases .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,93,58,10,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4246575342465753,0.7073170731707317,0.8333333333333334
machine-translation,0,"This requires , however , an expensive sampling procedure to be performed repeatedly .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,94,59,11,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4292237442922374,0.7195121951219512,0.9166666666666666
machine-translation,0,"In this paper , thus , we only consider rescoring the phrase pairs in the phrase table .",system description,Scoring Phrase Pairs with RNN Encoder-Decoder,0,95,60,12,0,system description : Scoring Phrase Pairs with RNN Encoder-Decoder,0.4337899543378995,0.7317073170731707,1.0
machine-translation,0,Related Approaches : Neural Networks in Machine Translation,system description,Related Approaches: Neural Networks in Machine Translation,0,96,61,1,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4383561643835616,0.7439024390243902,0.045454545454545456
machine-translation,0,"Before presenting the empirical results , we discuss a number of recent works that have proposed to use neural networks in the context of SMT .",system description,Related Approaches: Neural Networks in Machine Translation,0,97,62,2,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4429223744292237,0.7560975609756098,0.09090909090909091
machine-translation,0,Schwenk in proposed a similar approach of scoring phrase pairs .,system description,Related Approaches: Neural Networks in Machine Translation,0,98,63,3,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4474885844748858,0.7682926829268293,0.13636363636363635
machine-translation,0,"Instead of the RNN - based neural network , he used a feedforward neural network that has fixed - size inputs ( 7 words in his case , with zero - padding for shorter phrases ) and fixed - size outputs ( 7 words in the target language ) .",system description,Related Approaches: Neural Networks in Machine Translation,0,99,64,4,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4520547945205479,0.7804878048780488,0.18181818181818182
machine-translation,0,"When it is used specifically for scoring phrases for the SMT system , the maximum phrase length is often chosen to be small .",system description,Related Approaches: Neural Networks in Machine Translation,0,100,65,5,0,system description : Related Approaches: Neural Networks in Machine Translation,0.45662100456621,0.7926829268292683,0.22727272727272727
machine-translation,0,"However , as the length of phrases increases or as we apply neural networks to other variable - length sequence data , it is important that the neural network can handle variable - length input and output .",system description,Related Approaches: Neural Networks in Machine Translation,0,101,66,6,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4611872146118721,0.8048780487804879,0.2727272727272727
machine-translation,0,The proposed RNN Encoder - Decoder is well - suited for these applications .,system description,Related Approaches: Neural Networks in Machine Translation,0,102,67,7,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4657534246575342,0.8170731707317073,0.3181818181818182
machine-translation,0,"Similar to , Devlin et al. proposed to use a feedforward neural network to model a translation model , however , by predicting one word in a target phrase at a time .",system description,Related Approaches: Neural Networks in Machine Translation,0,103,68,8,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4703196347031963,0.8292682926829268,0.36363636363636365
machine-translation,0,"They reported an impressive improvement , but their approach still requires the maximum length of the input phrase ( or context words ) to be fixed a priori .",system description,Related Approaches: Neural Networks in Machine Translation,0,104,69,9,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4748858447488584,0.8414634146341463,0.4090909090909091
machine-translation,0,"Although it is not exactly a neural network they train , the authors of proposed to learn a bilingual embedding of words / phrases .",system description,Related Approaches: Neural Networks in Machine Translation,0,105,70,10,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4794520547945205,0.8536585365853658,0.45454545454545453
machine-translation,0,They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system .,system description,Related Approaches: Neural Networks in Machine Translation,0,106,71,11,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4840182648401826,0.8658536585365854,0.5
machine-translation,0,"In , a feedforward neural network was trained to learn a mapping from a bag - of - words representation of an input phrase to an output phrase .",system description,Related Approaches: Neural Networks in Machine Translation,0,107,72,12,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4885844748858447,0.8780487804878049,0.5454545454545454
machine-translation,0,"This is closely related to both the proposed RNN Encoder - Decoder and the model proposed in , except that their input representation of a phrase is a bag - of - words .",system description,Related Approaches: Neural Networks in Machine Translation,0,108,73,13,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4931506849315068,0.8902439024390244,0.5909090909090909
machine-translation,0,similar approach of using bag - of - words representations was proposed in as well .,system description,Related Approaches: Neural Networks in Machine Translation,0,109,74,14,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4977168949771689,0.9024390243902439,0.6363636363636364
machine-translation,0,"Earlier , a similar encoder - decoder model using two recursive neural networks was proposed in ) , but their model was restricted to a monolingual setting , i.e. the model reconstructs an input sentence .",system description,Related Approaches: Neural Networks in Machine Translation,0,110,75,15,0,system description : Related Approaches: Neural Networks in Machine Translation,0.502283105022831,0.9146341463414634,0.6818181818181818
machine-translation,0,"More recently , another encoder - decoder model using an RNN was proposed in , where the decoder is conditioned on a representation of either a source sentence or a source context .",system description,Related Approaches: Neural Networks in Machine Translation,0,111,76,16,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5068493150684932,0.926829268292683,0.7272727272727273
machine-translation,0,One important difference between the proposed RNN Encoder - Decoder and the approaches in and is that the order of the words in source and target phrases is taken into account .,system description,Related Approaches: Neural Networks in Machine Translation,0,112,77,17,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5114155251141552,0.9390243902439024,0.7727272727272727
machine-translation,0,"The RNN Encoder - Decoder naturally distinguishes between sequences that have the same words but in a different order , whereas the aforementioned approaches effectively ignore order information .",system description,Related Approaches: Neural Networks in Machine Translation,0,113,78,18,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5159817351598174,0.9512195121951219,0.8181818181818182
machine-translation,0,The closest approach related to the proposed RNN Encoder - Decoder is the Recurrent Continuous Translation Model ( Model 2 ) proposed in .,system description,Related Approaches: Neural Networks in Machine Translation,0,114,79,19,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5205479452054794,0.9634146341463414,0.8636363636363636
machine-translation,0,"In their paper , they proposed a similar model that consists of an encoder and decoder .",system description,Related Approaches: Neural Networks in Machine Translation,0,115,80,20,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5251141552511416,0.975609756097561,0.9090909090909091
machine-translation,0,The difference with our model is that they used a convolutional n-gram model ( CGM ) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder .,system description,Related Approaches: Neural Networks in Machine Translation,0,116,81,21,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5296803652968036,0.9878048780487805,0.9545454545454546
machine-translation,0,"They , however , evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations .",system description,Related Approaches: Neural Networks in Machine Translation,0,117,82,22,0,system description : Related Approaches: Neural Networks in Machine Translation,0.5342465753424658,1.0,1.0
machine-translation,0,Experiments,experiment,Experiments,0,118,1,1,0,experiment : Experiments,0.5388127853881278,0.5,0.5
machine-translation,0,We evaluate our approach on the English / French translation task of the WMT ' 14 workshop .,experiment,Experiments,0,119,2,2,0,experiment : Experiments,0.54337899543379,1.0,1.0
machine-translation,0,Data and Baseline System,baseline,Data and Baseline System,0,120,1,1,0,baseline : Data and Baseline System,0.547945205479452,0.058823529411764705,0.058823529411764705
machine-translation,0,Large amounts of resources are available to build an English / French SMT system in the framework of the WMT ' 14 translation task .,baseline,Data and Baseline System,0,121,2,2,0,baseline : Data and Baseline System,0.5525114155251142,0.11764705882352941,0.11764705882352941
machine-translation,0,"The bilingual corpora include Europarl ( 61M words ) , news commentary ( 5.5 M ) , UN ( 421 M ) , and two crawled corpora of 90 M and 780M words respectively .",baseline,Data and Baseline System,0,122,3,3,0,baseline : Data and Baseline System,0.5570776255707762,0.17647058823529413,0.17647058823529413
machine-translation,0,The last two corpora are quite noisy .,baseline,Data and Baseline System,0,123,4,4,0,baseline : Data and Baseline System,0.5616438356164384,0.23529411764705882,0.23529411764705882
machine-translation,0,"To train the French language model , about 712M words of crawled newspaper material is available in addition to the target side of the bitexts .",baseline,Data and Baseline System,0,124,5,5,0,baseline : Data and Baseline System,0.5662100456621004,0.29411764705882354,0.29411764705882354
machine-translation,0,All the word counts refer to French words after tokenization .,baseline,Data and Baseline System,0,125,6,6,0,baseline : Data and Baseline System,0.5707762557077626,0.35294117647058826,0.35294117647058826
machine-translation,0,"It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .",baseline,Data and Baseline System,0,126,7,7,0,baseline : Data and Baseline System,0.5753424657534246,0.4117647058823529,0.4117647058823529
machine-translation,0,"Instead , one should focus on the most relevant subset of the data for a given task .",baseline,Data and Baseline System,0,127,8,8,0,baseline : Data and Baseline System,0.5799086757990868,0.47058823529411764,0.47058823529411764
machine-translation,0,"We have done so by applying the data selection method proposed in , and its extension to bitexts .",baseline,Data and Baseline System,0,128,9,9,0,baseline : Data and Baseline System,0.5844748858447488,0.5294117647058824,0.5294117647058824
machine-translation,0,By these means we selected a subset of 418 M words out of more than 2G words for language modeling and a subset of 348 M out of 850 M words for training the RNN Encoder - Decoder .,baseline,Data and Baseline System,0,129,10,10,0,baseline : Data and Baseline System,0.589041095890411,0.5882352941176471,0.5882352941176471
machine-translation,0,"We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT , and newstest2014 as our test set .",baseline,Data and Baseline System,0,130,11,11,0,baseline : Data and Baseline System,0.593607305936073,0.6470588235294118,0.6470588235294118
machine-translation,0,Each set has more than 70 thousand words and a single reference translation .,baseline,Data and Baseline System,0,131,12,12,0,baseline : Data and Baseline System,0.5981735159817352,0.7058823529411765,0.7058823529411765
machine-translation,0,"For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",baseline,Data and Baseline System,0,132,13,13,0,baseline : Data and Baseline System,0.6027397260273972,0.7647058823529411,0.7647058823529411
machine-translation,0,This covers approximately 93 % of the dataset .,baseline,Data and Baseline System,0,133,14,14,0,baseline : Data and Baseline System,0.6073059360730594,0.8235294117647058,0.8235294117647058
machine-translation,0,All the out - of - vocabulary words were mapped to a special token ( [ UNK ] ) .,baseline,Data and Baseline System,0,134,15,15,0,baseline : Data and Baseline System,0.6118721461187214,0.8823529411764706,0.8823529411764706
machine-translation,0,The baseline phrase - based SMT system was built using Moses with default settings .,baseline,Data and Baseline System,1,135,16,16,0,baseline : Data and Baseline System,0.6164383561643836,0.9411764705882353,0.9411764705882353
machine-translation,0,"This system achieves a BLEU score of 30.64 and 33.3 on the development and test sets , respectively ( see Table 1 ) .",baseline,Data and Baseline System,0,136,17,17,0,baseline : Data and Baseline System,0.6210045662100456,1.0,1.0
machine-translation,0,Neural Language Model,model,Neural Language Model,0,137,1,1,0,model : Neural Language Model,0.6255707762557078,0.07142857142857142,0.07142857142857142
machine-translation,0,"In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder - Decoder , we also tried a more traditional approach of using a neural network for learning a target language model ( CSLM ) .",model,Neural Language Model,0,138,2,2,0,model : Neural Language Model,0.6301369863013698,0.14285714285714285,0.14285714285714285
machine-translation,0,"Especially , the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder - Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys - tem add up or are redundant .",model,Neural Language Model,0,139,3,3,0,model : Neural Language Model,0.634703196347032,0.21428571428571427,0.21428571428571427
machine-translation,0,We trained the CSLM model on 7 - grams from the target corpus .,model,Neural Language Model,0,140,4,4,0,model : Neural Language Model,0.639269406392694,0.2857142857142857,0.2857142857142857
machine-translation,0,"Each input word was projected into the embedding space R 512 , and they were concatenated to form a 3072 dimensional vector .",model,Neural Language Model,0,141,5,5,0,model : Neural Language Model,0.6438356164383562,0.35714285714285715,0.35714285714285715
machine-translation,0,The concatenated vector was fed through two rectified layers ( of size 1536 and 1024 ) .,model,Neural Language Model,0,142,6,6,0,model : Neural Language Model,0.6484018264840182,0.42857142857142855,0.42857142857142855
machine-translation,0,The output layer was a simple softmax layer ( see Eq. ) .,model,Neural Language Model,0,143,7,7,0,model : Neural Language Model,0.6529680365296804,0.5,0.5
machine-translation,0,"All the weight parameters were initialized uniformly between ? 0.01 and 0.01 , and the model was trained until the validation perplexity did not improve for 10 epochs .",model,Neural Language Model,0,144,8,8,0,model : Neural Language Model,0.6575342465753424,0.5714285714285714,0.5714285714285714
machine-translation,0,"After training , the language model achieved a perplexity of 45.80 .",model,Neural Language Model,0,145,9,9,0,model : Neural Language Model,0.6621004566210046,0.6428571428571429,0.6428571428571429
machine-translation,0,The validation set was a random selection of 0.1 % of the corpus .,model,Neural Language Model,0,146,10,10,0,model : Neural Language Model,0.6666666666666666,0.7142857142857143,0.7142857142857143
machine-translation,0,"The model was used to score partial translations during the decoding process , which generally leads to higher gains in BLEU score than n-best list rescoring .",model,Neural Language Model,0,147,11,11,0,model : Neural Language Model,0.6712328767123288,0.7857142857142857,0.7857142857142857
machine-translation,0,To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder .,model,Neural Language Model,0,148,12,12,0,model : Neural Language Model,0.6757990867579908,0.8571428571428571,0.8571428571428571
machine-translation,0,"Only when the buffer is full , or a stack is about to be pruned , the n-grams are scored by the CSLM .",model,Neural Language Model,0,149,13,13,0,model : Neural Language Model,0.680365296803653,0.9285714285714286,0.9285714285714286
machine-translation,0,This allows us to perform fast matrixmatrix multiplication on GPU using Theano .,model,Neural Language Model,0,150,14,14,0,model : Neural Language Model,0.684931506849315,1.0,1.0
machine-translation,0,Quantitative Analysis,analysis,Quantitative Analysis,0,151,1,1,0,analysis : Quantitative Analysis,0.6894977168949772,0.017857142857142856,0.047619047619047616
machine-translation,0,We tried the following combinations : :,analysis,Quantitative Analysis,0,152,2,2,0,analysis : Quantitative Analysis,0.6940639269406392,0.03571428571428571,0.09523809523809523
machine-translation,0,The top scoring target phrases for a small set of source phrases according to the translation model ( direct translation probability ) and by the RNN Encoder - Decoder .,analysis,Quantitative Analysis,0,153,3,3,0,analysis : Quantitative Analysis,0.6986301369863014,0.05357142857142857,0.14285714285714285
machine-translation,0,Source phrases were randomly selected from phrases with 4 or more words .,analysis,Quantitative Analysis,0,154,4,4,0,analysis : Quantitative Analysis,0.7031963470319634,0.07142857142857142,0.19047619047619047
machine-translation,0,denotes an incomplete ( partial ) character .,analysis,Quantitative Analysis,0,155,5,5,0,analysis : Quantitative Analysis,0.7077625570776256,0.08928571428571429,0.23809523809523808
machine-translation,0,denotes an incomplete ( partial ) character .,analysis,Quantitative Analysis,0,156,6,6,0,analysis : Quantitative Analysis,0.7123287671232876,0.10714285714285714,0.2857142857142857
machine-translation,0,is a Cyrillic letter ghe .,analysis,Quantitative Analysis,0,157,7,7,0,analysis : Quantitative Analysis,0.7168949771689498,0.125,0.3333333333333333
machine-translation,0,The results are presented in .,analysis,Quantitative Analysis,0,158,8,8,0,analysis : Quantitative Analysis,0.7214611872146118,0.14285714285714285,0.38095238095238093
machine-translation,0,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .",analysis,Quantitative Analysis,1,159,9,9,0,analysis : Quantitative Analysis,0.726027397260274,0.16071428571428573,0.42857142857142855
machine-translation,0,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,analysis,Quantitative Analysis,1,160,10,10,0,analysis : Quantitative Analysis,0.730593607305936,0.17857142857142858,0.47619047619047616
machine-translation,0,This suggests that the contributions of the CSLM and the RNN Encoder - Decoder are not too correlated and that one can expect better results by improving each method independently .,analysis,Quantitative Analysis,0,161,11,11,0,analysis : Quantitative Analysis,0.7351598173515982,0.19642857142857142,0.5238095238095238
machine-translation,0,"Furthermore , we tried penalizing the number of words thatare unknown to the neural networks ( i.e. words which are not in the shortlist ) .",analysis,Quantitative Analysis,0,162,12,12,0,analysis : Quantitative Analysis,0.7397260273972602,0.21428571428571427,0.5714285714285714
machine-translation,0,We do so by simply adding the number of unknown words as an additional feature the loglinear model in Eq. ( 9 ) .,analysis,Quantitative Analysis,0,163,13,13,0,analysis : Quantitative Analysis,0.7442922374429224,0.23214285714285715,0.6190476190476191
machine-translation,0,3,analysis,Quantitative Analysis,0,164,14,14,0,analysis : Quantitative Analysis,0.7488584474885844,0.25,0.6666666666666666
machine-translation,0,"However , in this case we 3 To understand the effect of the penalty , consider the set of all words in the 15,000 large shortlist , SL .",analysis,Quantitative Analysis,0,165,15,15,0,analysis : Quantitative Analysis,0.7534246575342466,0.26785714285714285,0.7142857142857143
machine-translation,0,All words x i / ? SL are replaced by a special token [ UNK ] before being scored by the neural networks .,analysis,Quantitative Analysis,0,166,16,16,0,analysis : Quantitative Analysis,0.7579908675799086,0.2857142857142857,0.7619047619047619
machine-translation,0,All words x i / ? SL are replaced by a special token [ UNK ] before being scored by the neural networks .,analysis,Quantitative Analysis,0,167,17,17,0,analysis : Quantitative Analysis,0.7625570776255708,0.30357142857142855,0.8095238095238095
machine-translation,0,"Hence , the conditional probability of any xi t / ? SL is actually given by the model as",analysis,Quantitative Analysis,0,168,18,18,0,analysis : Quantitative Analysis,0.7671232876712328,0.32142857142857145,0.8571428571428571
machine-translation,0,"Hence , the conditional probability of any xi t / ? SL is actually given by the model as",analysis,Quantitative Analysis,0,169,19,19,0,analysis : Quantitative Analysis,0.771689497716895,0.3392857142857143,0.9047619047619048
machine-translation,0,"where x <t is a shorthand notation for xt ? 1 , . . . , x 1 .",analysis,Quantitative Analysis,0,170,20,20,0,analysis : Quantitative Analysis,0.776255707762557,0.35714285714285715,0.9523809523809523
machine-translation,0,"were notable to achieve better performance on the test set , but only on the development set .",analysis,Quantitative Analysis,0,171,21,21,0,analysis : Quantitative Analysis,0.7808219178082192,0.375,1.0
machine-translation,0,Qualitative Analysis,analysis,Qualitative Analysis,0,172,22,1,0,analysis : Qualitative Analysis,0.7853881278538812,0.39285714285714285,0.047619047619047616
machine-translation,0,"In order to understand where the performance improvement comes from , we analyze the phrase pair scores computed by the RNN Encoder - Decoder against the corresponding p ( f | e ) from the translation model .",analysis,Qualitative Analysis,0,173,23,2,0,analysis : Qualitative Analysis,0.7899543378995434,0.4107142857142857,0.09523809523809523
machine-translation,0,"Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus , we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases .",analysis,Qualitative Analysis,0,174,24,3,0,analysis : Qualitative Analysis,0.7945205479452054,0.42857142857142855,0.14285714285714285
machine-translation,0,"Also , as we mentioned earlier in Sec. 3.1 , we further expect the RNN Encoder - Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus .",analysis,Qualitative Analysis,0,175,25,4,0,analysis : Qualitative Analysis,0.7990867579908676,0.44642857142857145,0.19047619047619047
machine-translation,0,We focus on those pairs whose source phrase is long ( more than 3 words per source phrase ) and,analysis,Qualitative Analysis,0,176,26,5,0,analysis : Qualitative Analysis,0.8036529680365296,0.4642857142857143,0.23809523809523808
machine-translation,0,"As a result , the probability of words not in the shortlist is always overestimated .",analysis,Qualitative Analysis,0,177,27,6,0,analysis : Qualitative Analysis,0.8082191780821918,0.48214285714285715,0.2857142857142857
machine-translation,0,"It is possible to address this issue by backing off to an existing model that contain non-shortlisted words ( see ) In this paper , however , we opt for introducing a word penalty instead , which counteracts the word probability overestimation .",analysis,Qualitative Analysis,0,178,28,7,0,analysis : Qualitative Analysis,0.8127853881278538,0.5,0.3333333333333333
machine-translation,0,frequent .,analysis,Qualitative Analysis,0,179,29,8,0,analysis : Qualitative Analysis,0.817351598173516,0.5178571428571429,0.38095238095238093
machine-translation,0,"For each such source phrase , we look at the target phrases that have been scored high either by the translation probability p ( f | e ) or by the RNN Encoder - Decoder .",analysis,Qualitative Analysis,0,180,30,9,0,analysis : Qualitative Analysis,0.821917808219178,0.5357142857142857,0.42857142857142855
machine-translation,0,"Similarly , we perform the same procedure with those pairs whose source phrase is long but rare in the corpus .",analysis,Qualitative Analysis,0,181,31,10,0,analysis : Qualitative Analysis,0.8264840182648402,0.5535714285714286,0.47619047619047616
machine-translation,0,lists the top - 3 target phrases per source phrase favored either by the translation model or by the RNN Encoder - Decoder .,analysis,Qualitative Analysis,0,182,32,11,0,analysis : Qualitative Analysis,0.8310502283105022,0.5714285714285714,0.5238095238095238
machine-translation,0,The source phrases were randomly chosen among long ones having more than 4 or 5 words .,analysis,Qualitative Analysis,0,183,33,12,0,analysis : Qualitative Analysis,0.8356164383561644,0.5892857142857143,0.5714285714285714
machine-translation,0,"In most cases , the choices of the target phrases by the RNN Encoder - Decoder are closer to actual or literal translations .",analysis,Qualitative Analysis,0,184,34,13,0,analysis : Qualitative Analysis,0.8401826484018264,0.6071428571428571,0.6190476190476191
machine-translation,0,We can observe that the RNN Encoder - Decoder prefers shorter phrases in general .,analysis,Qualitative Analysis,0,185,35,14,0,analysis : Qualitative Analysis,0.8447488584474886,0.625,0.6666666666666666
machine-translation,0,"Interestingly , many phrase pairs were scored similarly by both the translation model and the RNN Encoder - Decoder , but there were as many other phrase pairs that were scored radically different ( see ) .",analysis,Qualitative Analysis,0,186,36,15,0,analysis : Qualitative Analysis,0.8493150684931506,0.6428571428571429,0.7142857142857143
machine-translation,0,"This could arise from the proposed approach of training the RNN Encoder - Decoder on a set of unique phrase pairs , discouraging the RNN Encoder - Decoder from learning simply the frequencies of the phrase pairs from the corpus , as explained earlier . , we show for each of the source phrases in , the generated samples from the RNN Encoder - Decoder .",analysis,Qualitative Analysis,0,187,37,16,0,analysis : Qualitative Analysis,0.8538812785388128,0.6607142857142857,0.7619047619047619
machine-translation,0,"For each source phrase , we generated 50 samples and show the top - five phrases accordingly to their scores .",analysis,Qualitative Analysis,0,188,38,17,0,analysis : Qualitative Analysis,0.8584474885844748,0.6785714285714286,0.8095238095238095
machine-translation,0,We can see that the RNN Encoder - Decoder is able to propose well - formed target phrases without looking at the actual phrase table .,analysis,Qualitative Analysis,0,189,39,18,0,analysis : Qualitative Analysis,0.863013698630137,0.6964285714285714,0.8571428571428571
machine-translation,0,"Importantly , the generated phrases do not overlap completely with the target phrases from the phrase table .",analysis,Qualitative Analysis,0,190,40,19,0,analysis : Qualitative Analysis,0.867579908675799,0.7142857142857143,0.9047619047619048
machine-translation,0,This encourages us to further investigate the possibility of replacing the whole or apart of the phrase table with the proposed RNN Encoder - Decoder in the future .,analysis,Qualitative Analysis,0,191,41,20,0,analysis : Qualitative Analysis,0.8721461187214612,0.7321428571428571,0.9523809523809523
machine-translation,0,"Furthermore , in",analysis,Qualitative Analysis,0,192,42,21,0,analysis : Qualitative Analysis,0.8767123287671232,0.75,1.0
machine-translation,0,Word and Phrase Representations,analysis,Word and Phrase Representations,0,193,43,1,0,analysis : Word and Phrase Representations,0.8812785388127854,0.7678571428571429,0.07142857142857142
machine-translation,0,"Since the proposed RNN Encoder - Decoder is not specifically designed only for the task of machine translation , here we briefly look at the properties of the trained model .",analysis,Word and Phrase Representations,0,194,44,2,0,analysis : Word and Phrase Representations,0.8858447488584474,0.7857142857142857,0.14285714285714285
machine-translation,0,"It has been known for sometime that continuous space language models using neural networks are able to learn semantically meaningful embeddings ( See , e.g. , ) .",analysis,Word and Phrase Representations,0,195,45,3,0,analysis : Word and Phrase Representations,0.8904109589041096,0.8035714285714286,0.21428571428571427
machine-translation,0,"Since the proposed RNN Encoder - Decoder also projects to and maps back from a sequence of words into a continuous space vector , we expect to see a similar property with the proposed model as well .",analysis,Word and Phrase Representations,0,196,46,4,0,analysis : Word and Phrase Representations,0.8949771689497716,0.8214285714285714,0.2857142857142857
machine-translation,0,The left plot in shows the 2 - D embedding of the words using the word embedding matrix learned by the RNN Encoder - Decoder .,analysis,Word and Phrase Representations,0,197,47,5,0,analysis : Word and Phrase Representations,0.8995433789954338,0.8392857142857143,0.35714285714285715
machine-translation,0,The projection was done by the recently proposed Barnes - Hut - SNE .,analysis,Word and Phrase Representations,0,198,48,6,0,analysis : Word and Phrase Representations,0.9041095890410958,0.8571428571428571,0.42857142857142855
machine-translation,0,We can clearly see that semantically similar words are clustered with each other ( see the zoomed - in plots in .,analysis,Word and Phrase Representations,0,199,49,7,0,analysis : Word and Phrase Representations,0.908675799086758,0.875,0.5
machine-translation,0,The proposed RNN Encoder - Decoder naturally generates a continuous - space representation of a phrase .,analysis,Word and Phrase Representations,0,200,50,8,0,analysis : Word and Phrase Representations,0.91324200913242,0.8928571428571429,0.5714285714285714
machine-translation,0,The representation ( c in ) in this case is a 1000 - dimensional vector .,analysis,Word and Phrase Representations,0,201,51,9,0,analysis : Word and Phrase Representations,0.9178082191780822,0.9107142857142857,0.6428571428571429
machine-translation,0,"Similarly to the word representations , we visualize the representations of the phrases that consists of four or more words using the Barnes - Hut - SNE in .",analysis,Word and Phrase Representations,0,202,52,10,0,analysis : Word and Phrase Representations,0.9223744292237442,0.9285714285714286,0.7142857142857143
machine-translation,0,"From the visualization , it is clear that the RNN Encoder - Decoder captures both semantic and syntactic structures of the phrases .",analysis,Word and Phrase Representations,0,203,53,11,0,analysis : Word and Phrase Representations,0.9269406392694064,0.9464285714285714,0.7857142857142857
machine-translation,0,"For instance , in the bottom - left plot , most of the phrases are about the duration of time , while those phrases thatare syntactically similar are clustered together .",analysis,Word and Phrase Representations,0,204,54,12,0,analysis : Word and Phrase Representations,0.9315068493150684,0.9642857142857143,0.8571428571428571
machine-translation,0,The bottom - right plot shows the cluster of phrases thatare semantically similar ( countries or regions ) .,analysis,Word and Phrase Representations,0,205,55,13,0,analysis : Word and Phrase Representations,0.9360730593607306,0.9821428571428571,0.9285714285714286
machine-translation,0,"On the other hand , the top - right plot shows the phrases thatare syntactically similar .",analysis,Word and Phrase Representations,0,206,56,14,0,analysis : Word and Phrase Representations,0.9406392694063926,1.0,1.0
machine-translation,0,Conclusion,conclusion,Conclusion,0,207,1,1,0,conclusion : Conclusion,0.9452054794520548,0.07692307692307693,0.07692307692307693
machine-translation,0,"In this paper , we proposed a new neural network architecture , called an RNN Encoder - Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence , possibly from a different set , of an arbitrary length .",conclusion,Conclusion,0,208,2,2,0,conclusion : Conclusion,0.9497716894977168,0.15384615384615385,0.15384615384615385
machine-translation,0,The proposed RNN Encoder - Decoder is able to either score a pair of sequences ( in terms of a conditional probability ) or generate a target sequence given a source sequence .,conclusion,Conclusion,0,209,3,3,0,conclusion : Conclusion,0.954337899543379,0.23076923076923078,0.23076923076923078
machine-translation,0,"Along with the new architecture , we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading / generating a sequence .",conclusion,Conclusion,0,210,4,4,0,conclusion : Conclusion,0.958904109589041,0.3076923076923077,0.3076923076923077
machine-translation,0,"We evaluated the proposed model with the task of statistical machine translation , where we used the RNN Encoder - Decoder to score each phrase pair in the phrase table .",conclusion,Conclusion,0,211,5,5,0,conclusion : Conclusion,0.9634703196347032,0.38461538461538464,0.38461538461538464
machine-translation,0,"Qualitatively , we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder - Decoder is able to propose well - formed target phrases .",conclusion,Conclusion,0,212,6,6,0,conclusion : Conclusion,0.9680365296803652,0.46153846153846156,0.46153846153846156
machine-translation,0,The scores by the RNN Encoder - Decoder were found to improve the over all translation performance in terms of BLEU scores .,conclusion,Conclusion,0,213,7,7,0,conclusion : Conclusion,0.9726027397260274,0.5384615384615384,0.5384615384615384
machine-translation,0,"Also , we found that the contribution by the RNN Encoder - Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system , so that we can improve further the performance by using , for instance , the RNN Encoder - Decoder and the neural net language model together .",conclusion,Conclusion,0,214,8,8,0,conclusion : Conclusion,0.9771689497716894,0.6153846153846154,0.6153846153846154
machine-translation,0,Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level .,conclusion,Conclusion,0,215,9,9,0,conclusion : Conclusion,0.9817351598173516,0.6923076923076923,0.6923076923076923
machine-translation,0,This suggests that there maybe more natural language related applications that may benefit from the proposed RNN Encoder - Decoder .,conclusion,Conclusion,0,216,10,10,0,conclusion : Conclusion,0.9863013698630136,0.7692307692307693,0.7692307692307693
machine-translation,0,The proposed architecture has large potential for further improvement and analysis .,conclusion,Conclusion,0,217,11,11,0,conclusion : Conclusion,0.9908675799086758,0.8461538461538461,0.8461538461538461
machine-translation,0,"One approach that was not investigated here is to replace the whole , or apart of the phrase table by letting the RNN Encoder - Decoder propose target phrases .",conclusion,Conclusion,0,218,12,12,0,conclusion : Conclusion,0.9954337899543378,0.9230769230769231,0.9230769230769231
machine-translation,0,"Also , noting that the proposed model is not limited to being used with written language , it will bean important future research to apply the proposed architecture to other applications such as speech transcription .",conclusion,Conclusion,0,219,13,13,0,conclusion : Conclusion,1.0,1.0,1.0
machine-translation,1,Neural Machine Translation in Linear Time,title,title,1,2,1,1,0,title : title,0.009950248756218905,1.0,1.0
machine-translation,1,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.014925373134328358,0.1,0.1
machine-translation,1,We present a novel neural network for processing sequences .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.01990049751243781,0.2,0.2
machine-translation,1,"The ByteNet is a one-dimensional convolutional neural network that is composed of two parts , one to encode the source sequence and the other to decode the target sequence .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.024875621890547265,0.3,0.3
machine-translation,1,The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.029850746268656716,0.4,0.4
machine-translation,1,"To address the differing lengths of the source and the target , we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03482587064676617,0.5,0.5
machine-translation,1,The ByteNet uses dilation in the convolutional layers to increase its receptive field .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03980099502487562,0.6,0.6
machine-translation,1,The resulting network has two core properties : it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.04477611940298507,0.7,0.7
machine-translation,1,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,abstract,abstract,1,10,8,8,0,abstract : abstract,0.04975124378109453,0.8,0.8
machine-translation,1,"The ByteNet also achieves state - of - the - art performance on character - to - character machine translation on the English - to - German WMT translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time .",abstract,abstract,1,11,9,9,0,abstract : abstract,0.05472636815920398,0.9,0.9
machine-translation,1,We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.05970149253731343,1.0,1.0
machine-translation,1,Introduction,introduction,introduction,0,13,1,1,0,introduction : introduction,0.06467661691542288,0.02631578947368421,0.02631578947368421
machine-translation,1,"In neural language modelling , a neural network estimates a distribution over sequences of words or characters that belong to a given language .",introduction,introduction,1,14,2,2,0,introduction : introduction,0.06965174129353234,0.05263157894736842,0.05263157894736842
machine-translation,1,"In neural machine translation , the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language .",introduction,introduction,1,15,3,3,0,introduction : introduction,0.07462686567164178,0.07894736842105263,0.07894736842105263
machine-translation,1,The network can bethought of as composed of two parts : a source network ( the encoder ) that encodes the source sequence into a representation and a target network ( the decoder ) that uses the representation of the source encoder to generate the target sequence .,introduction,introduction,0,16,4,4,0,introduction : introduction,0.07960199004975124,0.10526315789473684,0.10526315789473684
machine-translation,1,"Recurrent neural networks ( RNN ) are powerful sequence models and are widely used in language modelling ) , yet they have a potential drawback .",introduction,introduction,0,17,5,5,0,introduction : introduction,0.0845771144278607,0.13157894736842105,0.13157894736842105
machine-translation,1,RNNs have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation .,introduction,introduction,0,18,6,6,0,introduction : introduction,0.08955223880597014,0.15789473684210525,0.15789473684210525
machine-translation,1,Forward and backward signals in a RNN also need to traverse the full distance of the serial path to reach from one token in the sequence to another .,introduction,introduction,0,19,7,7,0,introduction : introduction,0.0945273631840796,0.18421052631578946,0.18421052631578946
machine-translation,1,"The larger the distance , the harder it is to learn the dependencies between the tokens .",introduction,introduction,0,20,8,8,0,introduction : introduction,0.09950248756218906,0.21052631578947367,0.21052631578947367
machine-translation,1,"number of neural architectures have been proposed for modelling translation , such as encoder - decoder networks , networks with attentional pooling and twodimensional networks .",introduction,introduction,0,21,9,9,0,introduction : introduction,0.1044776119402985,0.23684210526315788,0.23684210526315788
machine-translation,1,"Despite the generally good performance , the proposed models ar Xiv : 1610.10099v2 [ cs. CL ] 15 Mar 2017 EOS EOS EOS | s | | t | | t| .",introduction,introduction,0,22,10,10,0,introduction : introduction,0.10945273631840796,0.2631578947368421,0.2631578947368421
machine-translation,1,Dynamic unfolding in the ByteNet architecture .,introduction,introduction,0,23,11,11,0,introduction : introduction,0.11442786069651742,0.2894736842105263,0.2894736842105263
machine-translation,1,"At each step the decoder is conditioned on the source representation produced by the encoder for that step , or simply on no representation for steps beyond the extended length | t | .",introduction,introduction,0,24,12,12,0,introduction : introduction,0.11940298507462686,0.3157894736842105,0.3157894736842105
machine-translation,1,The decoding ends when the target network produces an end - of - sequence ( EOS ) symbol .,introduction,introduction,0,25,13,13,0,introduction : introduction,0.12437810945273632,0.34210526315789475,0.34210526315789475
machine-translation,1,"either have running time that is super - linear in the length of the source and target sequences , or they process the source sequence into a constant size representation , burdening the model with a memorization step .",introduction,introduction,0,26,14,14,0,introduction : introduction,0.12935323383084577,0.3684210526315789,0.3684210526315789
machine-translation,1,Both of these drawbacks grow more severe as the length of the sequences increases .,introduction,introduction,0,27,15,15,0,introduction : introduction,0.13432835820895522,0.39473684210526316,0.39473684210526316
machine-translation,1,We present a family of encoder - decoder neural networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above .,introduction,introduction,0,28,16,16,0,introduction : introduction,0.13930348258706468,0.42105263157894735,0.42105263157894735
machine-translation,1,The first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal resolution of the sequences ; this is in contrast with architectures that encode the source into a fixed - size representation .,introduction,introduction,0,29,17,17,0,introduction : introduction,0.14427860696517414,0.4473684210526316,0.4473684210526316
machine-translation,1,The second mechanism is the dynamic unfolding mechanism that allows the network to process in a simple and efficient way source and target sequences of different lengths ( Sect. 3.2 ) .,introduction,introduction,0,30,18,18,0,introduction : introduction,0.14925373134328357,0.47368421052631576,0.47368421052631576
machine-translation,1,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,introduction,introduction,1,31,19,19,0,introduction : introduction,0.15422885572139303,0.5,0.5
machine-translation,1,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,introduction,introduction,1,32,20,20,0,introduction : introduction,0.15920398009950248,0.5263157894736842,0.5263157894736842
machine-translation,1,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,introduction,introduction,1,33,21,21,0,introduction : introduction,0.16417910447761194,0.5526315789473685,0.5526315789473685
machine-translation,1,The network has beneficial computational and learning properties .,introduction,introduction,1,34,22,22,0,introduction : introduction,0.1691542288557214,0.5789473684210527,0.5789473684210527
machine-translation,1,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?",introduction,introduction,1,35,23,23,0,introduction : introduction,0.17412935323383086,0.6052631578947368,0.6052631578947368
machine-translation,1,log d where d is the size of the desired dependency field ) .,introduction,introduction,0,36,24,24,0,introduction : introduction,0.1791044776119403,0.631578947368421,0.631578947368421
machine-translation,1,The computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences ( Sect. 2 ) .,introduction,introduction,0,37,25,25,0,introduction : introduction,0.18407960199004975,0.6578947368421053,0.6578947368421053
machine-translation,1,"From a learning perspective , the representation of the source sequence in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder .",introduction,introduction,1,38,26,26,0,introduction : introduction,0.1890547263681592,0.6842105263157895,0.6842105263157895
machine-translation,1,"In addition , the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis - tance between the tokens .",introduction,introduction,0,39,27,27,0,introduction : introduction,0.19402985074626866,0.7105263157894737,0.7105263157894737
machine-translation,1,Dependencies overlarge distances are connected by short paths and can be learnt more easily .,introduction,introduction,0,40,28,28,0,introduction : introduction,0.19900497512437812,0.7368421052631579,0.7368421052631579
machine-translation,1,We apply the ByteNet model to strings of characters for character - level language modelling and character - tocharacter machine translation .,introduction,introduction,0,41,29,29,0,introduction : introduction,0.20398009950248755,0.7631578947368421,0.7631578947368421
machine-translation,1,We evaluate the decoder network on the Hutter Prize Wikipedia task where it achieves the state - of - the - art performance of 1.31 bits / character .,introduction,introduction,0,42,30,30,0,introduction : introduction,0.208955223880597,0.7894736842105263,0.7894736842105263
machine-translation,1,"We further evaluate the encoderdecoder network on character - to - character machine translation on the English - to - German WMT benchmark where it achieves a state - of - the - art BLEU score of 22.85 ( 0.380 bits / character ) and 25.53 ( 0.389 bits / character ) on the 2014 and 2015 test sets , respectively .",introduction,introduction,0,43,31,31,0,introduction : introduction,0.21393034825870647,0.8157894736842105,0.8157894736842105
machine-translation,1,"On the character - level machine translation task , ByteNet betters a comparable version of GNMT that is a state - of - the - art system .",introduction,introduction,0,44,32,32,0,introduction : introduction,0.21890547263681592,0.8421052631578947,0.8421052631578947
machine-translation,1,"These results show that deep CNNs are simple , scalable and effective architectures for challenging linguistic processing tasks .",introduction,introduction,0,45,33,33,0,introduction : introduction,0.22388059701492538,0.868421052631579,0.868421052631579
machine-translation,1,The paper is organized as follows .,introduction,introduction,0,46,34,34,0,introduction : introduction,0.22885572139303484,0.8947368421052632,0.8947368421052632
machine-translation,1,Section 2 lays out the background and some desiderata for neural architectures underlying translation models .,introduction,introduction,0,47,35,35,0,introduction : introduction,0.23383084577114427,0.9210526315789473,0.9210526315789473
machine-translation,1,Section 3 defines the proposed family of architectures and the specific convolutional instance ( ByteNet ) used in the experiments .,introduction,introduction,0,48,36,36,0,introduction : introduction,0.23880597014925373,0.9473684210526315,0.9473684210526315
machine-translation,1,Section 4 analyses ByteNet as well as existing neural translation models based on the desiderata set out in Section 2 .,introduction,introduction,0,49,37,37,0,introduction : introduction,0.24378109452736318,0.9736842105263158,0.9736842105263158
machine-translation,1,Section 5 reports the experiments on language modelling and Section 6 reports the experiments on character - to - character machine translation .,introduction,introduction,0,50,38,38,0,introduction : introduction,0.24875621890547264,1.0,1.0
machine-translation,1,Neural Translation Model,model,Neural Translation Model,0,51,1,1,0,model : Neural Translation Model,0.2537313432835821,0.01639344262295082,0.09090909090909091
machine-translation,1,"Given a string s from a source language , a neural translation model estimates a distribution p ( t |s ) over strings t of a target language .",model,Neural Translation Model,0,52,2,2,0,model : Neural Translation Model,0.25870646766169153,0.03278688524590164,0.18181818181818182
machine-translation,1,The distribution indicates the probability of a string t being a translation of s .,model,Neural Translation Model,0,53,3,3,0,model : Neural Translation Model,0.263681592039801,0.04918032786885246,0.2727272727272727
machine-translation,1,"product of conditionals over the tokens in the target t = t 0 , ... , t N leads to a tractable formulation of the distribution :",model,Neural Translation Model,0,54,4,4,0,model : Neural Translation Model,0.26865671641791045,0.06557377049180328,0.36363636363636365
machine-translation,1,Each conditional factor expresses complex and long - range dependencies among the source and target tokens .,model,Neural Translation Model,0,55,5,5,0,model : Neural Translation Model,0.2736318407960199,0.08196721311475409,0.45454545454545453
machine-translation,1,"The strings are usually sentences of the respective languages ; the tokens are words or , as in the our case , characters .",model,Neural Translation Model,0,56,6,6,0,model : Neural Translation Model,0.27860696517412936,0.09836065573770492,0.5454545454545454
machine-translation,1,The network that models p ( t | s ) is composed of two parts : a source network ( the encoder ) that processes the source string into a representation and a target network ( the decoder ) that uses the source representation to generate the target string .,model,Neural Translation Model,0,57,7,7,0,model : Neural Translation Model,0.2835820895522388,0.11475409836065574,0.6363636363636364
machine-translation,1,The decoder functions as a language model for the target language .,model,Neural Translation Model,0,58,8,8,0,model : Neural Translation Model,0.2885572139303483,0.13114754098360656,0.7272727272727273
machine-translation,1,neural translation model has some basic properties .,model,Neural Translation Model,0,59,9,9,0,model : Neural Translation Model,0.2935323383084577,0.14754098360655737,0.8181818181818182
machine-translation,1,The decoder is autoregressive in the target tokens and the model is sensitive to the ordering of the tokens in the source and target strings .,model,Neural Translation Model,0,60,10,10,0,model : Neural Translation Model,0.29850746268656714,0.16393442622950818,0.9090909090909091
machine-translation,1,It is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary .,model,Neural Translation Model,0,61,11,11,0,model : Neural Translation Model,0.3034825870646766,0.18032786885245902,1.0
machine-translation,1,Desiderata,model,Desiderata,0,62,12,1,0,model : Desiderata,0.30845771144278605,0.19672131147540983,0.02
machine-translation,1,"Beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture , so we aim at identifying some desiderata .",model,Desiderata,0,63,13,2,0,model : Desiderata,0.31343283582089554,0.21311475409836064,0.04
machine-translation,1,"First , the running time of the network should be linear in the length of the source and target strings .",model,Desiderata,0,64,14,3,0,model : Desiderata,0.31840796019900497,0.22950819672131148,0.06
machine-translation,1,"This ensures that the model is scalable to longer strings , which is the case when using characters as tokens .",model,Desiderata,0,65,15,4,0,model : Desiderata,0.32338308457711445,0.2459016393442623,0.08
machine-translation,1,The use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time .,model,Desiderata,0,66,16,5,0,model : Desiderata,0.3283582089552239,0.26229508196721313,0.1
machine-translation,1,"Second , the size of the source representation should be linear in the length of the source string , i.e. it should be resolution preserving , and not have constant size .",model,Desiderata,0,67,17,6,0,model : Desiderata,0.3333333333333333,0.2786885245901639,0.12
machine-translation,1,This is to avoid burdening the model with an additional memorization step before translation .,model,Desiderata,0,68,18,7,0,model : Desiderata,0.3383084577114428,0.29508196721311475,0.14
machine-translation,1,"In more general terms , the size of a representation should be proportional to the amount of information it represents or predicts .",model,Desiderata,0,69,19,8,0,model : Desiderata,0.34328358208955223,0.3114754098360656,0.16
machine-translation,1,"Third , the path traversed by forward and backward signals in the network ( between input and ouput tokens ) should be short .",model,Desiderata,0,70,20,9,0,model : Desiderata,0.3482587064676617,0.32786885245901637,0.18
machine-translation,1,Shorter paths whose length is largely decoupled from the sequence distance between the two tokens have the potential to better propagate the signals and to let the network learn long - range dependencies more easily .,model,Desiderata,0,71,21,10,0,model : Desiderata,0.35323383084577115,0.3442622950819672,0.2
machine-translation,1,Byte Net,model,Desiderata,0,72,22,11,0,model : Desiderata,0.3582089552238806,0.36065573770491804,0.22
machine-translation,1,We aim at building neural language and translation models that capture the desiderata set out in Sect. 2.1 .,model,Desiderata,0,73,23,12,0,model : Desiderata,0.36318407960199006,0.3770491803278688,0.24
machine-translation,1,The proposed ByteNet architecture is composed of a decoder that is stacked on an encoder ( Sect. 3.1 ) and generates variable - length outputs via dynamic unfolding ( Sect. 3.2 ) .,model,Desiderata,0,74,24,13,0,model : Desiderata,0.3681592039800995,0.39344262295081966,0.26
machine-translation,1,The decoder is a language model that is formed of one - dimensional convolutional layers that are masked ( Sect. 3.4 ) and use dilation ( Sect. 3.5 ) .,model,Desiderata,0,75,25,14,0,model : Desiderata,0.373134328358209,0.4098360655737705,0.28
machine-translation,1,The encoder processes the source string into a representation and is formed of one - dimensional convolutional layers that use dilation but are not masked .,model,Desiderata,0,76,26,15,0,model : Desiderata,0.3781094527363184,0.4262295081967213,0.3
machine-translation,1,depicts the two networks and their combination .,model,Desiderata,0,77,27,16,0,model : Desiderata,0.38308457711442784,0.4426229508196721,0.32
machine-translation,1,Encoder - Decoder Stacking,model,Desiderata,0,78,28,17,0,model : Desiderata,0.3880597014925373,0.45901639344262296,0.34
machine-translation,1,notable feature of the proposed family of architectures is the way the encoder and the decoder are connected .,model,Desiderata,0,79,29,18,0,model : Desiderata,0.39303482587064675,0.47540983606557374,0.36
machine-translation,1,"To maximize the representational bandwidth between the encoder and the decoder , we place the decoder on top of the representation computed by the encoder .",model,Desiderata,0,80,30,19,0,model : Desiderata,0.39800995024875624,0.4918032786885246,0.38
machine-translation,1,This is in contrast to models that compress the source representation into a fixed - size vector or that pool over the source representation with a mechanism such as attentional pooling .,model,Desiderata,0,81,31,20,0,model : Desiderata,0.40298507462686567,0.5081967213114754,0.4
machine-translation,1,Dynamic Unfolding,model,Desiderata,0,82,32,21,0,model : Desiderata,0.4079601990049751,0.5245901639344263,0.42
machine-translation,1,An encoder and a decoder network that process sequences of different lengths can not be directly connected due to the different sizes of the computed representations .,model,Desiderata,0,83,33,22,0,model : Desiderata,0.4129353233830846,0.5409836065573771,0.44
machine-translation,1,"We circumvent this issue via a mechanism which we call dynamic unfolding , which works as follows .",model,Desiderata,0,84,34,23,0,model : Desiderata,0.417910447761194,0.5573770491803278,0.46
machine-translation,1,"Given source and target sequences sand t with respective lengths | s | and | t| , one first chooses a sufficiently tight upper bound | t| on the target length | t | as a linear function of the source length | s | : |",model,Desiderata,0,85,35,24,0,model : Desiderata,0.4228855721393035,0.5737704918032787,0.48
machine-translation,1,"The tight upper bound | t| is chosen in such away that , on the one hand , it is greater than the actual length | t | in almost all cases and , on the other hand , it does not increase excessively the amount of computation that is required .",model,Desiderata,0,86,36,25,0,model : Desiderata,0.42786069651741293,0.5901639344262295,0.5
machine-translation,1,"Once a linear relationship is chosen , one designs the source encoder so that , given a source sequence of length | s | , the encoder outputs a representation of the established lengt ? | t| .",model,Desiderata,0,87,37,26,0,model : Desiderata,0.43283582089552236,0.6065573770491803,0.52
machine-translation,1,"Once a linear relationship is chosen , one designs the source encoder so that , given a source sequence of length | s | , the encoder outputs a representation of the established lengt ? | t| .",model,Desiderata,0,88,38,27,0,model : Desiderata,0.43781094527363185,0.6229508196721312,0.54
machine-translation,1,"In our case , we let a = 1.20 and b = 0 when translating from English into German , as German sentences tend to be somewhat longer than their English counterparts .",model,Desiderata,0,89,39,28,0,model : Desiderata,0.4427860696517413,0.639344262295082,0.56
machine-translation,1,"In this manner the representation produced by the encoder can be efficiently computed , while maintaining high bandwidth and being resolution - preserving .",model,Desiderata,0,90,40,29,0,model : Desiderata,0.44776119402985076,0.6557377049180327,0.58
machine-translation,1,"Once the encoder representation is computed , we let the decoder unfold stepby - step over the encoder representation until the decoder itself outputs an end - of - sequence symbol ; the unfolding process may freely proceed beyond the estimated length | t| of the encoder representation .",model,Desiderata,0,91,41,30,0,model : Desiderata,0.4527363184079602,0.6721311475409836,0.6
machine-translation,1,gives an example of dynamic unfolding .,model,Desiderata,0,92,42,31,0,model : Desiderata,0.4577114427860697,0.6885245901639344,0.62
machine-translation,1,Input Embedding Tensor,model,Desiderata,0,93,43,32,0,model : Desiderata,0.4626865671641791,0.7049180327868853,0.64
machine-translation,1,"Given the target sequence t = t 0 , ... , tn the ByteNet decoder embeds each of the first n tokens t 0 , ... , t n?1 via a look - up table ( the n tokens t 1 , ... , tn serve as targets for the predictions ) .",model,Desiderata,0,94,44,33,0,model : Desiderata,0.46766169154228854,0.7213114754098361,0.66
machine-translation,1,The resulting embeddings are concatenated into a tensor of size n 2 d where d is the number of inner channels in the network .,model,Desiderata,0,95,45,34,0,model : Desiderata,0.472636815920398,0.7377049180327869,0.68
machine-translation,1,Masked One-dimensional,model,Desiderata,0,96,46,35,0,model : Desiderata,0.47761194029850745,0.7540983606557377,0.7
machine-translation,1,Convolutions,model,Desiderata,0,97,47,36,0,model : Desiderata,0.48258706467661694,0.7704918032786885,0.72
machine-translation,1,The decoder applies masked one - dimensional convolutions ( van den to the input embedding tensor that have a masked kernel of size k.,model,Desiderata,0,98,48,37,0,model : Desiderata,0.48756218905472637,0.7868852459016393,0.74
machine-translation,1,The masking ensures that information from future tokens does not affect the prediction of the current token .,model,Desiderata,0,99,49,38,0,model : Desiderata,0.4925373134328358,0.8032786885245902,0.76
machine-translation,1,The operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2 k ? 1 or by padding the input map .,model,Desiderata,0,100,50,39,0,model : Desiderata,0.4975124378109453,0.819672131147541,0.78
machine-translation,1,The operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2 k ? 1 or by padding the input map .,model,Desiderata,0,101,51,40,0,model : Desiderata,0.5024875621890548,0.8360655737704918,0.8
machine-translation,1,Dilation,model,Desiderata,0,102,52,41,0,model : Desiderata,0.5074626865671642,0.8524590163934426,0.82
machine-translation,1,The masked convolutions use dilation to increase the receptive field of the target network .,model,Desiderata,0,103,53,42,0,model : Desiderata,0.5124378109452736,0.8688524590163934,0.84
machine-translation,1,"Dilation makes the receptive field grow exponentially in terms of the depth of the networks , as opposed to linearly .",model,Desiderata,0,104,54,43,0,model : Desiderata,0.5174129353233831,0.8852459016393442,0.86
machine-translation,1,We use a dilation scheme whereby the dilation rates are doubled every layer up to a maximum rater ( for our experiments r = 16 ) .,model,Desiderata,0,105,55,44,0,model : Desiderata,0.5223880597014925,0.9016393442622951,0.88
machine-translation,1,The scheme is repeated multiple times in the network always starting from a dilation rate of 1 ( van den .,model,Desiderata,0,106,56,45,0,model : Desiderata,0.527363184079602,0.9180327868852459,0.9
machine-translation,1,Residual Blocks,model,Desiderata,0,107,57,46,0,model : Desiderata,0.5323383084577115,0.9344262295081968,0.92
machine-translation,1,"Each layer is wrapped in a residual block that contains additional convolutional layers with filters of size 1 1 . We adopt two variants of the residual blocks : one with ReLUs , which is used in the machine translation experiments , and one with Multiplicative Units , which is used in the language modelling experiments .",model,Desiderata,0,108,58,47,0,model : Desiderata,0.5373134328358209,0.9508196721311475,0.94
machine-translation,1,diagrams the two variants of the blocks .,model,Desiderata,0,109,59,48,0,model : Desiderata,0.5422885572139303,0.9672131147540983,0.96
machine-translation,1,"In both cases , we use layer normalization before the activation function , as it is well suited to sequence processing where computing the activation statistics over the following future tokens ( as would be done by batch normalization ) must be avoided .",model,Desiderata,0,110,60,49,0,model : Desiderata,0.5472636815920398,0.9836065573770492,0.98
machine-translation,1,"After a series of residual blocks of increased dilation , the network applies one more convolution and ReLU followed by a convolution and a final softmax layer .",model,Desiderata,0,111,61,50,0,model : Desiderata,0.5522388059701493,1.0,1.0
machine-translation,1,Model Comparison,model,model,0,112,1,1,0,model : model,0.5572139303482587,0.030303030303030304,0.030303030303030304
machine-translation,1,In this section we analyze the properties of various previously introduced neural translation models as well as the ByteNet family of models .,model,model,0,113,2,2,0,model : model,0.5621890547263682,0.06060606060606061,0.06060606060606061
machine-translation,1,"For the sake of a more complete analysis , we include two recurrent ByteNet variants ( which we do not evaluate in the experiments ) .",model,model,0,114,3,3,0,model : model,0.5671641791044776,0.09090909090909091,0.09090909090909091
machine-translation,1,Recurrent ByteNets,model,model,0,115,4,4,0,model : model,0.572139303482587,0.12121212121212122,0.12121212121212122
machine-translation,1,The ByteNet is composed of two stacked encoder and decoder networks where the decoder network dynamically adapts to the output length .,model,model,0,116,5,5,0,model : model,0.5771144278606966,0.15151515151515152,0.15151515151515152
machine-translation,1,This way of combining the networks is not tied to the networks being strictly convolutional .,model,model,0,117,6,6,0,model : model,0.582089552238806,0.18181818181818182,0.18181818181818182
machine-translation,1,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks ( see ) .,model,model,0,118,7,7,0,model : model,0.5870646766169154,0.21212121212121213,0.21212121212121213
machine-translation,1,The first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded .,model,model,0,119,8,8,0,model : model,0.5920398009950248,0.24242424242424243,0.24242424242424243
machine-translation,1,"The second variant also replaces the convolutional encoder with a recurrent encoder , e.g. a bidirectional RNN .",model,model,0,120,9,9,0,model : model,0.5970149253731343,0.2727272727272727,0.2727272727272727
machine-translation,1,The target RNN is then placed on top of the source RNN .,model,model,0,121,10,10,0,model : model,0.6019900497512438,0.30303030303030304,0.30303030303030304
machine-translation,1,"Considering the latter Recurrent ByteNet , we can see that the RNN Enc - Dec network ) is a Recurrent ByteNet where all connections between source and target - except for the first one that connects s 0 and t 0 - have been severed .",model,model,0,122,11,11,0,model : model,0.6069651741293532,0.3333333333333333,0.3333333333333333
machine-translation,1,"The Recurrent ByteNet is a generalization of the RNN Enc - Dec and , modulo the type of weight - sharing scheme , so is the convolutional ByteNet .",model,model,0,123,12,12,0,model : model,0.6119402985074627,0.36363636363636365,0.36363636363636365
machine-translation,1,Comparison of Properties,model,model,0,124,13,13,0,model : model,0.6169154228855721,0.3939393939393939,0.3939393939393939
machine-translation,1,In our comparison we consider the following neural translation models : the Recurrent Continuous Translation Model ( RCTM ) 1 and 2 ; the RNN Enc - Dec ; the RNN Enc - Dec Att with the attentional pooling mechanism of which there are a few variations ; the Grid LSTM translation model ) that uses a multi-dimensional architecture ; the Extended Neural GPU model ) that has a convolutional RNN architecture ; the ByteNet and the two Recurrent ByteNet variants .,model,model,0,125,14,14,0,model : model,0.6218905472636815,0.42424242424242425,0.42424242424242425
machine-translation,1,Our comparison criteria reflect the desiderata set out in Sect. 2.1 .,model,model,0,126,15,15,0,model : model,0.6268656716417911,0.45454545454545453,0.45454545454545453
machine-translation,1,We separate the first ( computation time ) desider - atum into three columns .,model,model,0,127,16,16,0,model : model,0.6318407960199005,0.48484848484848486,0.48484848484848486
machine-translation,1,The first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by Time .,model,model,0,128,17,17,0,model : model,0.6368159203980099,0.5151515151515151,0.5151515151515151
machine-translation,1,"The other two columns Net Sand Net T indicate , respectively , whether the source and the target network use a convolutional structure ( CNN ) or a recurrent one ( RNN ) ; a CNN structure has the advantage that it can be run in parallel along the length of the sequence .",model,model,0,129,18,18,0,model : model,0.6417910447761194,0.5454545454545454,0.5454545454545454
machine-translation,1,"The second ( resolution preservation ) desideratum corresponds to the RP column , which indicates whether the source representation in the network is resolution preserving .",model,model,0,130,19,19,0,model : model,0.6467661691542289,0.5757575757575758,0.5757575757575758
machine-translation,1,"Finally , the third desideratum ( short forward and backward flow paths ) is reflected by two columns .",model,model,0,131,20,20,0,model : model,0.6517412935323383,0.6060606060606061,0.6060606060606061
machine-translation,1,The Path S column corresponds to the length in layer steps of the shortest path between a source token and any output target token .,model,model,0,132,21,21,0,model : model,0.6567164179104478,0.6363636363636364,0.6363636363636364
machine-translation,1,"Similarly , the Path T column corresponds to the length of the shortest path between an input target token and any output target token .",model,model,0,133,22,22,0,model : model,0.6616915422885572,0.6666666666666666,0.6666666666666666
machine-translation,1,Shorter paths lead to better forward and backward signal propagation .,model,model,0,134,23,23,0,model : model,0.6666666666666666,0.696969696969697,0.696969696969697
machine-translation,1,summarizes the properties of the models .,model,model,0,135,24,24,0,model : model,0.6716417910447762,0.7272727272727273,0.7272727272727273
machine-translation,1,"The ByteNet , the Recurrent ByteNets and the RNN Enc - Dec are the only networks that have linear running time ( up to the constant c ) .",model,model,0,136,25,25,0,model : model,0.6766169154228856,0.7575757575757576,0.7575757575757576
machine-translation,1,"The RNN Enc - Dec , however , does not preserve the source sequence resolution , a feature that aggravates learning for long sequences such as those that appear in character - to - character machine translation .",model,model,0,137,26,26,0,model : model,0.681592039800995,0.7878787878787878,0.7878787878787878
machine-translation,1,"The RCTM 2 , the RNN Enc - Dec Att , the Grid LSTM and the Extended Neural GPU do preserve the resolution , but at a cost of a quadratic running time .",model,model,0,138,27,27,0,model : model,0.6865671641791045,0.8181818181818182,0.8181818181818182
machine-translation,1,The ByteNet stands out also for its Path properties .,model,model,0,139,28,28,0,model : model,0.6915422885572139,0.8484848484848485,0.8484848484848485
machine-translation,1,The dilated structure of the convolutions connects any two source or target tokens in the sequences byway of a small number of network layers corresponding to the depth of the source or target networks .,model,model,0,140,29,29,0,model : model,0.6965174129353234,0.8787878787878788,0.8787878787878788
machine-translation,1,"For character sequences where learning long - range dependencies is important , paths that are sublinear in the distance are advantageous .",model,model,0,141,30,30,0,model : model,0.7014925373134329,0.9090909090909091,0.9090909090909091
machine-translation,1,Phrase Based MT phrases phrases 20.7 24.0 RNN Enc - Dec words words 11.3 Reverse RNN Enc - Dec words words 14.0 RNN Enc - Dec,model,model,0,142,31,31,0,model : model,0.7064676616915423,0.9393939393939394,0.9393939393939394
machine-translation,1,Att words words 20.6 RNN Enc - Dec Att words words 20.9 GNMT ( RNN Enc - Dec Att ) word - pieces word - pieces 24.61,model,model,0,143,32,32,0,model : model,0.7114427860696517,0.9696969696969697,0.9696969696969697
machine-translation,1,RNN,model,model,0,144,33,33,0,model : model,0.7164179104477612,1.0,1.0
machine-translation,1,Model Test,model,model,0,145,1,1,0,model : model,0.7213930348258707,0.02040816326530612,0.02040816326530612
machine-translation,1,Stacked LSTM,model,model,0,146,2,2,0,model : model,0.7263681592039801,0.04081632653061224,0.04081632653061224
machine-translation,1,GF - LSTM 1.58 Grid- LSTM 1.47 Layer - normalized LSTM 1.46 MI- LSTM 1.44 Recurrent Memory Array Structures,model,model,0,147,3,3,0,model : model,0.7313432835820896,0.061224489795918366,0.061224489795918366
machine-translation,1,HM- LSTM 1.40 Layer,model,model,0,148,4,4,0,model : model,0.736318407960199,0.08163265306122448,0.08163265306122448
machine-translation,1,Norm HyperLSTM 1.38 Large Layer,model,model,0,149,5,5,0,model : model,0.7412935323383084,0.10204081632653061,0.10204081632653061
machine-translation,1,Norm HyperLSTM 1.34 Recurrent Highway Networks 1.32 Byte,model,model,0,150,6,6,0,model : model,0.746268656716418,0.12244897959183673,0.12244897959183673
machine-translation,1,Net Decoder 1.31 . Negative log- likelihood results in bits / byte on the Hutter Prize Wikipedia benchmark .,model,model,0,151,7,7,0,model : model,0.7512437810945274,0.14285714285714285,0.14285714285714285
machine-translation,1,Character Prediction,model,model,1,152,8,8,0,model : model,0.7562189054726368,0.16326530612244897,0.16326530612244897
machine-translation,1,We first evaluate the ByteNet Decoder separately on a character - level language modelling benchmark .,model,model,0,153,9,9,0,model : model,0.7611940298507462,0.1836734693877551,0.1836734693877551
machine-translation,1,"We use the Hutter Prize version of the Wikipedia dataset and follow the standard split where the first 90 million bytes are used for training , the next 5 million bytes are used for validation and the last 5 million bytes are used for testing .",model,model,0,154,10,10,0,model : model,0.7661691542288557,0.20408163265306123,0.20408163265306123
machine-translation,1,The total number of characters in the vocabulary is 205 .,model,model,0,155,11,11,0,model : model,0.7711442786069652,0.22448979591836735,0.22448979591836735
machine-translation,1,"The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 .",model,model,1,156,12,12,0,model : model,0.7761194029850746,0.24489795918367346,0.24489795918367346
machine-translation,1,The masked kernel has size 3 .,model,model,1,157,13,13,0,model : model,0.7810945273631841,0.2653061224489796,0.2653061224489796
machine-translation,1,This gives a receptive field of 315 characters .,model,model,0,158,14,14,0,model : model,0.7860696517412935,0.2857142857142857,0.2857142857142857
machine-translation,1,The number of hidden units dis 512 .,model,model,1,159,15,15,0,model : model,0.7910447761194029,0.30612244897959184,0.30612244897959184
machine-translation,1,For this task we use residual multiplicative blocks .,model,model,0,160,16,16,0,model : model,0.7960199004975125,0.32653061224489793,0.32653061224489793
machine-translation,1,For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 .,model,model,1,161,17,17,0,model : model,0.8009950248756219,0.3469387755102041,0.3469387755102041
machine-translation,1,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,model,model,1,162,18,18,0,model : model,0.8059701492537313,0.3673469387755102,0.3673469387755102
machine-translation,1,We do not reduce the learning rate during training .,model,model,0,163,19,19,0,model : model,0.8109452736318408,0.3877551020408163,0.3877551020408163
machine-translation,1,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .",model,model,1,164,20,20,0,model : model,0.8159203980099502,0.40816326530612246,0.40816326530612246
machine-translation,1,lists recent results of various neural sequence models on the Wikipedia dataset .,model,model,0,165,21,21,0,model : model,0.8208955223880597,0.42857142857142855,0.42857142857142855
machine-translation,1,All the results except for the ByteNet result are obtained using some variant of the LSTM recurrent neural network .,model,model,0,166,22,22,0,model : model,0.8258706467661692,0.4489795918367347,0.4489795918367347
machine-translation,1,The ByteNet decoder achieves 1.31 bits / character on the test set .,model,model,1,167,23,23,0,model : model,0.8308457711442786,0.46938775510204084,0.46938775510204084
machine-translation,1,Character - Level Machine Translation,model,model,1,168,24,24,0,model : model,0.835820895522388,0.4897959183673469,0.4897959183673469
machine-translation,1,We evaluate the full ByteNet on the WMT English to German translation task .,model,model,0,169,25,25,0,model : model,0.8407960199004975,0.5102040816326531,0.5102040816326531
machine-translation,1,We use NewsTest 2013 for validation and NewsTest 2014 and 2015 for testing .,model,model,0,170,26,26,0,model : model,0.845771144278607,0.5306122448979592,0.5306122448979592
machine-translation,1,The English and German strings are encoded as sequences of characters ; no explicit segmentation into words or morphemes is applied to the strings .,model,model,0,171,27,27,0,model : model,0.8507462686567164,0.5510204081632653,0.5510204081632653
machine-translation,1,The outputs of the network are strings of characters in the target language .,model,model,0,172,28,28,0,model : model,0.8557213930348259,0.5714285714285714,0.5714285714285714
machine-translation,1,We keep 323 characters in the German vocabulary and 296 in the English vocabulary .,model,model,0,173,29,29,0,model : model,0.8606965174129353,0.5918367346938775,0.5918367346938775
machine-translation,1,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,model,model,1,174,30,30,0,model : model,0.8656716417910447,0.6122448979591837,0.6122448979591837
machine-translation,1,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .",model,model,1,175,31,31,0,model : model,0.8706467661691543,0.6326530612244898,0.6326530612244898
machine-translation,1,For this task we use the residual blocks with ReLUs ( .,model,model,1,176,32,32,0,model : model,0.8756218905472637,0.6530612244897959,0.6530612244897959
machine-translation,1,The number of hidden units dis 800 .,model,model,1,177,33,33,0,model : model,0.8805970149253731,0.673469387755102,0.673469387755102
machine-translation,1,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .",model,model,1,178,34,34,0,model : model,0.8855721393034826,0.6938775510204082,0.6938775510204082
machine-translation,1,For the optimization we use Adam with a learning rate of 0.0003 .,model,model,1,179,35,35,0,model : model,0.8905472636815921,0.7142857142857143,0.7142857142857143
machine-translation,1,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,model,model,1,180,36,36,0,model : model,0.8955223880597015,0.7346938775510204,0.7346938775510204
machine-translation,1,Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training .,model,model,1,181,37,37,0,model : model,0.900497512437811,0.7551020408163265,0.7551020408163265
machine-translation,1,We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token .,model,model,1,182,38,38,0,model : model,0.9054726368159204,0.7755102040816326,0.7755102040816326
machine-translation,1,We use a beam of size 12 .,model,model,1,183,39,39,0,model : model,0.9104477611940298,0.7959183673469388,0.7959183673469388
machine-translation,1,"We do not use length normalization , nor do we keep score of which parts of the source sentence have been translated .",model,model,0,184,40,40,0,model : model,0.9154228855721394,0.8163265306122449,0.8163265306122449
machine-translation,1,and contain the results of the experiments .,model,model,0,185,41,41,0,model : model,0.9203980099502488,0.8367346938775511,0.8367346938775511
machine-translation,1,"On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .",model,model,1,186,42,42,0,model : model,0.9253731343283582,0.8571428571428571,0.8571428571428571
machine-translation,1,"On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date .",model,model,1,187,43,43,0,model : model,0.9303482587064676,0.8775510204081632,0.8775510204081632
machine-translation,1,contains some of the unaltered generated translations from the ByteNet that highlight reordering and other phenomena such as transliteration .,model,model,0,188,44,44,0,model : model,0.9353233830845771,0.8979591836734694,0.8979591836734694
machine-translation,1,The character - level aspect of the model makes post -processing unnecessary in principle .,model,model,0,189,45,45,0,model : model,0.9402985074626866,0.9183673469387755,0.9183673469387755
machine-translation,1,We further visualize the sensitivity of the ByteNet 's predictions to specific source and target inputs using gradient - based visualization .,model,model,0,190,46,46,0,model : model,0.945273631840796,0.9387755102040817,0.9387755102040817
machine-translation,1,represents a heatmap of the magnitude of the gradients of the generated outputs with respect to the source and target inputs .,model,model,0,191,47,47,0,model : model,0.9502487562189055,0.9591836734693877,0.9591836734693877
machine-translation,1,"For visual clarity , we sum the gradients for all the characters that makeup each word and normalize the values along each column .",model,model,0,192,48,48,0,model : model,0.9552238805970149,0.9795918367346939,0.9795918367346939
machine-translation,1,"In contrast with the attentional pooling mechanism , this general technique allows us to inspect not just dependencies of the outputs on the source inputs , but also dependencies of the outputs on previous target inputs , or on any other neural network layers .",model,model,0,193,49,49,0,model : model,0.9601990049751243,1.0,1.0
machine-translation,1,Conclusion,conclusion,conclusion,0,194,1,1,0,conclusion : conclusion,0.9651741293532339,0.125,0.125
machine-translation,1,"We have introduced the ByteNet , a neural translation model that has linear running time , decouples translation from memorization and has short signal propagation paths for tokens in sequences .",conclusion,conclusion,0,195,2,2,0,conclusion : conclusion,0.9701492537313433,0.25,0.25
machine-translation,1,We have shown that the ByteNet decoder is a state - of - the - art character - level language model based on a convolutional neural network that outperforms recurrent neural language models .,conclusion,conclusion,0,196,3,3,0,conclusion : conclusion,0.9751243781094527,0.375,0.375
machine-translation,1,"We have also shown that the ByteNet generalizes the RNN Enc - Dec architecture and achieves state - of - the - art results for character - to - character machine translation and excellent results in general , while maintaining linear running time complexity .",conclusion,conclusion,0,197,4,4,0,conclusion : conclusion,0.9800995024875622,0.5,0.5
machine-translation,1,We have revealed the latent structure learnt by the ByteNet and found it to mirror the expected alignment between the tokens in the sentences ..,conclusion,conclusion,0,198,5,5,0,conclusion : conclusion,0.9850746268656716,0.625,0.625
machine-translation,1,Magnitude of gradients of the predicted outputs with respect to the source and target inputs .,conclusion,conclusion,0,199,6,6,0,conclusion : conclusion,0.9900497512437811,0.75,0.75
machine-translation,1,The gradients are summed for all the characters in a given word .,conclusion,conclusion,0,200,7,7,0,conclusion : conclusion,0.9950248756218906,0.875,0.875
machine-translation,1,"In the bottom heatmap the magnitudes are nonzero on the diagonal , since the prediction of a target character depends highly on the preceding target character in the same word .",conclusion,conclusion,0,201,8,8,0,conclusion : conclusion,1.0,1.0,1.0
machine-translation,2,Attention Is All You Need,title,title,0,2,1,1,0,title : title,0.008928571428571428,1.0,1.0
machine-translation,2,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.013392857142857142,0.058823529411764705,0.058823529411764705
machine-translation,2,The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.017857142857142856,0.11764705882352941,0.11764705882352941
machine-translation,2,The best performing models also connect the encoder and decoder through an attention mechanism .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.022321428571428572,0.17647058823529413,0.17647058823529413
machine-translation,2,"We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.026785714285714284,0.23529411764705882,0.23529411764705882
machine-translation,2,Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.03125,0.29411764705882354,0.29411764705882354
machine-translation,2,"Our model achieves 28.4 BLEU on the WMT 2014 Englishto - German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03571428571428571,0.35294117647058826,0.35294117647058826
machine-translation,2,"On the WMT 2014 English - to - French translation task , our model establishes a new single - model state - of - the - art BLEU score of 41.8 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.04017857142857143,0.4117647058823529,0.4117647058823529
machine-translation,2,We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.044642857142857144,0.47058823529411764,0.47058823529411764
machine-translation,2,Equal contribution .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.049107142857142856,0.5294117647058824,0.5294117647058824
machine-translation,2,Listing order is random .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.05357142857142857,0.5882352941176471,0.5882352941176471
machine-translation,2,Jakob proposed replacing RNNs with self - attention and started the effort to evaluate this idea .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.05803571428571429,0.6470588235294118,0.6470588235294118
machine-translation,2,"Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .",abstract,abstract,0,14,12,12,0,abstract : abstract,0.0625,0.7058823529411765,0.7058823529411765
machine-translation,2,"Noam proposed scaled dot-product attention , multi-head attention and the parameter - free position representation and became the other person involved in nearly every detail .",abstract,abstract,0,15,13,13,0,abstract : abstract,0.06696428571428571,0.7647058823529411,0.7647058823529411
machine-translation,2,"Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .",abstract,abstract,0,16,14,14,0,abstract : abstract,0.07142857142857142,0.8235294117647058,0.8235294117647058
machine-translation,2,"Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .",abstract,abstract,0,17,15,15,0,abstract : abstract,0.07589285714285714,0.8823529411764706,0.8823529411764706
machine-translation,2,"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .",abstract,abstract,0,18,16,16,0,abstract : abstract,0.08035714285714286,0.9411764705882353,0.9411764705882353
machine-translation,2,Work performed while at Google Brain .,abstract,abstract,0,19,17,17,0,abstract : abstract,0.08482142857142858,1.0,1.0
machine-translation,2,Introduction,introduction,introduction,0,20,1,1,0,introduction : introduction,0.08928571428571429,0.08333333333333333,0.08333333333333333
machine-translation,2,"Recurrent neural networks , long short - term memory and gated recurrent neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation .",introduction,introduction,0,21,2,2,0,introduction : introduction,0.09375,0.16666666666666666,0.16666666666666666
machine-translation,2,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,introduction,introduction,1,22,3,3,0,introduction : introduction,0.09821428571428571,0.25,0.25
machine-translation,2,Recurrent models typically factor computation along the symbol positions of the input and output sequences .,introduction,introduction,0,23,4,4,0,introduction : introduction,0.10267857142857142,0.3333333333333333,0.3333333333333333
machine-translation,2,"Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t.",introduction,introduction,0,24,5,5,0,introduction : introduction,0.10714285714285714,0.4166666666666667,0.4166666666666667
machine-translation,2,"This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .",introduction,introduction,0,25,6,6,0,introduction : introduction,0.11160714285714286,0.5,0.5
machine-translation,2,"Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter .",introduction,introduction,0,26,7,7,0,introduction : introduction,0.11607142857142858,0.5833333333333334,0.5833333333333334
machine-translation,2,"The fundamental constraint of sequential computation , however , remains .",introduction,introduction,0,27,8,8,0,introduction : introduction,0.12053571428571429,0.6666666666666666,0.6666666666666666
machine-translation,2,"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences .",introduction,introduction,0,28,9,9,0,introduction : introduction,0.125,0.75,0.75
machine-translation,2,"In all but a few cases , however , such attention mechanisms are used in conjunction with a recurrent network .",introduction,introduction,0,29,10,10,0,introduction : introduction,0.12946428571428573,0.8333333333333334,0.8333333333333334
machine-translation,2,"In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .",introduction,introduction,1,30,11,11,0,introduction : introduction,0.13392857142857142,0.9166666666666666,0.9166666666666666
machine-translation,2,The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .,introduction,introduction,0,31,12,12,0,introduction : introduction,0.13839285714285715,1.0,1.0
machine-translation,2,Background,background,Background,0,32,1,1,0,background : Background,0.14285714285714285,0.1,0.1
machine-translation,2,"The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU , ByteNet and ConvS2S , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .",background,Background,0,33,2,2,0,background : Background,0.14732142857142858,0.2,0.2
machine-translation,2,"In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .",background,Background,0,34,3,3,0,background : Background,0.15178571428571427,0.3,0.3
machine-translation,2,This makes it more difficult to learn dependencies between distant positions .,background,Background,0,35,4,4,0,background : Background,0.15625,0.4,0.4
machine-translation,2,"In the Transformer this is reduced to a constant number of operations , albeit at the cost of reduced effective resolution due to averaging attention - weighted positions , an effect we counteract with Multi - Head Attention as described in section 3.2 .",background,Background,0,36,5,5,0,background : Background,0.16071428571428573,0.5,0.5
machine-translation,2,"Self - attention , sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .",background,Background,0,37,6,6,0,background : Background,0.16517857142857142,0.6,0.6
machine-translation,2,"Self - attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task - independent sentence representations .",background,Background,0,38,7,7,0,background : Background,0.16964285714285715,0.7,0.7
machine-translation,2,End - to - end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple - language question answering and language modeling tasks .,background,Background,0,39,8,8,0,background : Background,0.17410714285714285,0.8,0.8
machine-translation,2,"To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self - attention to compute representations of its input and output without using sequencealigned RNNs or convolution .",background,Background,0,40,9,9,0,background : Background,0.17857142857142858,0.9,0.9
machine-translation,2,"In the following sections , we will describe the Transformer , motivate self - attention and discuss its advantages over models such as and .",background,Background,0,41,10,10,0,background : Background,0.18303571428571427,1.0,1.0
machine-translation,2,Model Architecture,model,Model Architecture,0,42,1,1,0,model : Model Architecture,0.1875,0.009174311926605505,0.16666666666666666
machine-translation,2,Most competitive neural sequence transduction models have an encoder - decoder structure .,model,Model Architecture,0,43,2,2,0,model : Model Architecture,0.19196428571428573,0.01834862385321101,0.3333333333333333
machine-translation,2,"Here , the encoder maps an input sequence of symbol representations ( x 1 , ... , x n ) to a sequence of continuous representations z = ( z 1 , ... , z n ) .",model,Model Architecture,0,44,3,3,0,model : Model Architecture,0.19642857142857142,0.027522935779816515,0.5
machine-translation,2,"Given z , the decoder then generates an output sequence ( y 1 , ... , y m ) of symbols one element at a time .",model,Model Architecture,0,45,4,4,0,model : Model Architecture,0.20089285714285715,0.03669724770642202,0.6666666666666666
machine-translation,2,"At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next .",model,Model Architecture,0,46,5,5,0,model : Model Architecture,0.20535714285714285,0.045871559633027525,0.8333333333333334
machine-translation,2,"The Transformer follows this over all architecture using stacked self - attention and point - wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of",model,Model Architecture,0,47,6,6,0,model : Model Architecture,0.20982142857142858,0.05504587155963303,1.0
machine-translation,2,Encoder and Decoder Stacks,model,Encoder and Decoder Stacks,1,48,7,1,0,model : Encoder and Decoder Stacks,0.21428571428571427,0.06422018348623854,0.07142857142857142
machine-translation,2,Encoder :,model,Encoder and Decoder Stacks,1,49,8,2,0,model : Encoder and Decoder Stacks,0.21875,0.07339449541284404,0.14285714285714285
machine-translation,2,The encoder is composed of a stack of N = 6 identical layers .,model,Encoder and Decoder Stacks,1,50,9,3,0,model : Encoder and Decoder Stacks,0.22321428571428573,0.08256880733944955,0.21428571428571427
machine-translation,2,Each layer has two sub-layers .,model,Encoder and Decoder Stacks,1,51,10,4,0,model : Encoder and Decoder Stacks,0.22767857142857142,0.09174311926605505,0.2857142857142857
machine-translation,2,"The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .",model,Encoder and Decoder Stacks,1,52,11,5,0,model : Encoder and Decoder Stacks,0.23214285714285715,0.10091743119266056,0.35714285714285715
machine-translation,2,"We employ a residual connection around each of the two sub-layers , followed by layer normalization .",model,Encoder and Decoder Stacks,1,53,12,6,0,model : Encoder and Decoder Stacks,0.23660714285714285,0.11009174311926606,0.42857142857142855
machine-translation,2,"That is , the output of each sub - layer is LayerNorm ( x + Sublayer ( x ) ) , where Sublayer ( x ) is the function implemented by the sub - layer itself .",model,Encoder and Decoder Stacks,0,54,13,7,0,model : Encoder and Decoder Stacks,0.24107142857142858,0.11926605504587157,0.5
machine-translation,2,"To facilitate these residual connections , all sub- layers in the model , as well as the embedding layers , produce outputs of dimension d model = 512 .",model,Encoder and Decoder Stacks,0,55,14,8,0,model : Encoder and Decoder Stacks,0.24553571428571427,0.12844036697247707,0.5714285714285714
machine-translation,2,Decoder :,model,Encoder and Decoder Stacks,1,56,15,9,0,model : Encoder and Decoder Stacks,0.25,0.13761467889908258,0.6428571428571429
machine-translation,2,The decoder is also composed of a stack of N = 6 identical layers .,model,Encoder and Decoder Stacks,1,57,16,10,0,model : Encoder and Decoder Stacks,0.2544642857142857,0.14678899082568808,0.7142857142857143
machine-translation,2,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .",model,Encoder and Decoder Stacks,1,58,17,11,0,model : Encoder and Decoder Stacks,0.25892857142857145,0.1559633027522936,0.7857142857142857
machine-translation,2,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .",model,Encoder and Decoder Stacks,1,59,18,12,0,model : Encoder and Decoder Stacks,0.26339285714285715,0.1651376146788991,0.8571428571428571
machine-translation,2,We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,model,Encoder and Decoder Stacks,1,60,19,13,0,model : Encoder and Decoder Stacks,0.26785714285714285,0.1743119266055046,0.9285714285714286
machine-translation,2,"This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i.",model,Encoder and Decoder Stacks,0,61,20,14,0,model : Encoder and Decoder Stacks,0.27232142857142855,0.1834862385321101,1.0
machine-translation,2,Attention,model,Attention,1,62,21,1,0,model : Attention,0.2767857142857143,0.1926605504587156,0.3333333333333333
machine-translation,2,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",model,Attention,1,63,22,2,0,model : Attention,0.28125,0.2018348623853211,0.6666666666666666
machine-translation,2,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .",model,Attention,1,64,23,3,0,model : Attention,0.2857142857142857,0.21100917431192662,1.0
machine-translation,2,Scaled Dot - Product Attention,model,Scaled Dot-Product Attention,1,65,24,1,0,model : Scaled Dot-Product Attention,0.29017857142857145,0.22018348623853212,0.06666666666666667
machine-translation,2,"We call our particular attention "" Scaled Dot -Product Attention "" ) .",model,Scaled Dot-Product Attention,0,66,25,2,0,model : Scaled Dot-Product Attention,0.29464285714285715,0.22935779816513763,0.13333333333333333
machine-translation,2,"The input consists of queries and keys of dimension d k , and values of dimension d v .",model,Scaled Dot-Product Attention,0,67,26,3,0,model : Scaled Dot-Product Attention,0.29910714285714285,0.23853211009174313,0.2
machine-translation,2,"We compute the dot products of the query with all keys , divide each by ? d k , and apply a softmax function to obtain the weights on the values .",model,Scaled Dot-Product Attention,0,68,27,4,0,model : Scaled Dot-Product Attention,0.30357142857142855,0.24770642201834864,0.26666666666666666
machine-translation,2,"In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .",model,Scaled Dot-Product Attention,0,69,28,5,0,model : Scaled Dot-Product Attention,0.3080357142857143,0.25688073394495414,0.3333333333333333
machine-translation,2,The keys and values are also packed together into matrices K and V .,model,Scaled Dot-Product Attention,0,70,29,6,0,model : Scaled Dot-Product Attention,0.3125,0.26605504587155965,0.4
machine-translation,2,We compute the matrix of outputs as :,model,Scaled Dot-Product Attention,0,71,30,7,0,model : Scaled Dot-Product Attention,0.3169642857142857,0.27522935779816515,0.4666666666666667
machine-translation,2,"The two most commonly used attention functions are additive attention , and dot-product ( multiplicative ) attention .",model,Scaled Dot-Product Attention,0,72,31,8,0,model : Scaled Dot-Product Attention,0.32142857142857145,0.28440366972477066,0.5333333333333333
machine-translation,2,"Dot-product attention is identical to our algorithm , except for the scaling factor of 1",model,Scaled Dot-Product Attention,0,73,32,9,0,model : Scaled Dot-Product Attention,0.32589285714285715,0.29357798165137616,0.6
machine-translation,2,Additive attention computes the compatibility function using a feed - forward network with a single hidden layer .,model,Scaled Dot-Product Attention,0,74,33,10,0,model : Scaled Dot-Product Attention,0.33035714285714285,0.30275229357798167,0.6666666666666666
machine-translation,2,"While the two are similar in theoretical complexity , dot-product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code .",model,Scaled Dot-Product Attention,0,75,34,11,0,model : Scaled Dot-Product Attention,0.33482142857142855,0.3119266055045872,0.7333333333333333
machine-translation,2,"While for small values of d k the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of d k.",model,Scaled Dot-Product Attention,0,76,35,12,0,model : Scaled Dot-Product Attention,0.3392857142857143,0.3211009174311927,0.8
machine-translation,2,"We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients",model,Scaled Dot-Product Attention,0,77,36,13,0,model : Scaled Dot-Product Attention,0.34375,0.3302752293577982,0.8666666666666667
machine-translation,2,4 .,model,Scaled Dot-Product Attention,0,78,37,14,0,model : Scaled Dot-Product Attention,0.3482142857142857,0.3394495412844037,0.9333333333333333
machine-translation,2,"To counteract this effect , we scale the dot products by 1",model,Scaled Dot-Product Attention,0,79,38,15,0,model : Scaled Dot-Product Attention,0.35267857142857145,0.3486238532110092,1.0
machine-translation,2,Multi - Head Attention,model,Multi-Head Attention,1,80,39,1,0,model : Multi-Head Attention,0.35714285714285715,0.3577981651376147,0.1
machine-translation,2,"Instead of performing a single attention function with d model - dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to d k , d k and d v dimensions , respectively .",model,Multi-Head Attention,0,81,40,2,0,model : Multi-Head Attention,0.36160714285714285,0.3669724770642202,0.2
machine-translation,2,"On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding d v - dimensional output values .",model,Multi-Head Attention,0,82,41,3,0,model : Multi-Head Attention,0.36607142857142855,0.3761467889908257,0.3
machine-translation,2,"These are concatenated and once again projected , resulting in the final values , as depicted in .",model,Multi-Head Attention,0,83,42,4,0,model : Multi-Head Attention,0.3705357142857143,0.3853211009174312,0.4
machine-translation,2,Multi - head attention allows the model to jointly attend to information from different representation subspaces at different positions .,model,Multi-Head Attention,0,84,43,5,0,model : Multi-Head Attention,0.375,0.3944954128440367,0.5
machine-translation,2,"With a single attention head , averaging inhibits this .",model,Multi-Head Attention,0,85,44,6,0,model : Multi-Head Attention,0.3794642857142857,0.4036697247706422,0.6
machine-translation,2,Where the projections are parameter matrices,model,Multi-Head Attention,0,86,45,7,0,model : Multi-Head Attention,0.38392857142857145,0.41284403669724773,0.7
machine-translation,2,"In this work we employ h = 8 parallel attention layers , or heads .",model,Multi-Head Attention,0,87,46,8,0,model : Multi-Head Attention,0.38839285714285715,0.42201834862385323,0.8
machine-translation,2,For each of these we use,model,Multi-Head Attention,0,88,47,9,0,model : Multi-Head Attention,0.39285714285714285,0.43119266055045874,0.9
machine-translation,2,"Due to the reduced dimension of each head , the total computational cost is similar to that of single - head attention with full dimensionality .",model,Multi-Head Attention,0,89,48,10,0,model : Multi-Head Attention,0.39732142857142855,0.44036697247706424,1.0
machine-translation,2,Applications of Attention in our Model,model,Applications of Attention in our Model,0,90,49,1,0,model : Applications of Attention in our Model,0.4017857142857143,0.44954128440366975,0.09090909090909091
machine-translation,2,The Transformer uses multi-head attention in three different ways :,model,Applications of Attention in our Model,0,91,50,2,0,model : Applications of Attention in our Model,0.40625,0.45871559633027525,0.18181818181818182
machine-translation,2,"In "" encoder - decoder attention "" layers , the queries come from the previous decoder layer , and the memory keys and values come from the output of the encoder .",model,Applications of Attention in our Model,0,92,51,3,0,model : Applications of Attention in our Model,0.4107142857142857,0.46788990825688076,0.2727272727272727
machine-translation,2,This allows every position in the decoder to attend over all positions in the input sequence .,model,Applications of Attention in our Model,0,93,52,4,0,model : Applications of Attention in our Model,0.41517857142857145,0.47706422018348627,0.36363636363636365
machine-translation,2,This mimics the typical encoder - decoder attention mechanisms in sequence - to - sequence models such as . The encoder contains self - attention layers .,model,Applications of Attention in our Model,0,94,53,5,0,model : Applications of Attention in our Model,0.41964285714285715,0.48623853211009177,0.45454545454545453
machine-translation,2,"In a self - attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .",model,Applications of Attention in our Model,0,95,54,6,0,model : Applications of Attention in our Model,0.42410714285714285,0.4954128440366973,0.5454545454545454
machine-translation,2,Each position in the encoder can attend to all positions in the previous layer of the encoder .,model,Applications of Attention in our Model,0,96,55,7,0,model : Applications of Attention in our Model,0.42857142857142855,0.5045871559633027,0.6363636363636364
machine-translation,2,"Similarly , self - attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .",model,Applications of Attention in our Model,0,97,56,8,0,model : Applications of Attention in our Model,0.4330357142857143,0.5137614678899083,0.7272727272727273
machine-translation,2,We need to prevent leftward information flow in the decoder to preserve the auto - regressive property .,model,Applications of Attention in our Model,0,98,57,9,0,model : Applications of Attention in our Model,0.4375,0.5229357798165137,0.8181818181818182
machine-translation,2,We implement this inside of scaled dot-product attention by masking out ( setting to ?? ) all values in the input of the softmax which correspond to illegal connections .,model,Applications of Attention in our Model,0,99,58,10,0,model : Applications of Attention in our Model,0.4419642857142857,0.5321100917431193,0.9090909090909091
machine-translation,2,See.,model,Applications of Attention in our Model,0,100,59,11,0,model : Applications of Attention in our Model,0.44642857142857145,0.5412844036697247,1.0
machine-translation,2,Position - wise Feed - Forward Networks,model,Position-wise Feed-Forward Networks,1,101,60,1,0,model : Position-wise Feed-Forward Networks,0.45089285714285715,0.5504587155963303,0.14285714285714285
machine-translation,2,"In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .",model,Position-wise Feed-Forward Networks,1,102,61,2,0,model : Position-wise Feed-Forward Networks,0.45535714285714285,0.5596330275229358,0.2857142857142857
machine-translation,2,This consists of two linear transformations with a ReLU activation in between .,model,Position-wise Feed-Forward Networks,1,103,62,3,0,model : Position-wise Feed-Forward Networks,0.45982142857142855,0.5688073394495413,0.42857142857142855
machine-translation,2,"While the linear transformations are the same across different positions , they use different parameters from layer to layer .",model,Position-wise Feed-Forward Networks,0,104,63,4,0,model : Position-wise Feed-Forward Networks,0.4642857142857143,0.5779816513761468,0.5714285714285714
machine-translation,2,Another way of describing this is as two convolutions with kernel size,model,Position-wise Feed-Forward Networks,0,105,64,5,0,model : Position-wise Feed-Forward Networks,0.46875,0.5871559633027523,0.7142857142857143
machine-translation,2,1 .,model,Position-wise Feed-Forward Networks,0,106,65,6,0,model : Position-wise Feed-Forward Networks,0.4732142857142857,0.5963302752293578,0.8571428571428571
machine-translation,2,"The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .",model,Position-wise Feed-Forward Networks,0,107,66,7,0,model : Position-wise Feed-Forward Networks,0.47767857142857145,0.6055045871559633,1.0
machine-translation,2,Embeddings and Softmax,model,Embeddings and Softmax,1,108,67,1,0,model : Embeddings and Softmax,0.48214285714285715,0.6146788990825688,0.2
machine-translation,2,"Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .",model,Embeddings and Softmax,1,109,68,2,0,model : Embeddings and Softmax,0.48660714285714285,0.6238532110091743,0.4
machine-translation,2,We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .,model,Embeddings and Softmax,1,110,69,3,0,model : Embeddings and Softmax,0.49107142857142855,0.6330275229357798,0.6
machine-translation,2,"In our model , we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation , similar to .",model,Embeddings and Softmax,0,111,70,4,0,model : Embeddings and Softmax,0.4955357142857143,0.6422018348623854,0.8
machine-translation,2,"In the embedding layers , we multiply those weights by ? d model .",model,Embeddings and Softmax,0,112,71,5,0,model : Embeddings and Softmax,0.5,0.6513761467889908,1.0
machine-translation,2,Positional Encoding,model,Positional Encoding,1,113,72,1,0,model : Positional Encoding,0.5044642857142857,0.6605504587155964,0.3333333333333333
machine-translation,2,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .",model,Positional Encoding,1,114,73,2,0,model : Positional Encoding,0.5089285714285714,0.6697247706422018,0.6666666666666666
machine-translation,2,"is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self - attention .",model,Positional Encoding,0,115,74,3,0,model : Positional Encoding,0.5133928571428571,0.6788990825688074,1.0
machine-translation,2,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,116,75,1,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5178571428571429,0.6880733944954128,0.09090909090909091
machine-translation,2,tokens in the sequence .,model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,1,117,76,2,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5223214285714286,0.6972477064220184,0.18181818181818182
machine-translation,2,"To this end , we add "" positional encodings "" to the input embeddings at the bottoms of the encoder and decoder stacks .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,1,118,77,3,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5267857142857143,0.7064220183486238,0.2727272727272727
machine-translation,2,"The positional encodings have the same dimension d model as the embeddings , so that the two can be summed .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,119,78,4,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.53125,0.7155963302752294,0.36363636363636365
machine-translation,2,"There are many choices of positional encodings , learned and fixed .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,120,79,5,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5357142857142857,0.7247706422018348,0.45454545454545453
machine-translation,2,"In this work , we use sine and cosine functions of different frequencies : where pos is the position and i is the dimension .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,121,80,6,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5401785714285714,0.7339449541284404,0.5454545454545454
machine-translation,2,"That is , each dimension of the positional encoding corresponds to a sinusoid .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,122,81,7,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5446428571428571,0.7431192660550459,0.6363636363636364
machine-translation,2,The wavelengths form a geometric progression from 2 ? to 10000 2 ?.,model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,123,82,8,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5491071428571429,0.7522935779816514,0.7272727272727273
machine-translation,2,"We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PE pos+k can be represented as a linear function of PE pos .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,124,83,9,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5535714285714286,0.7614678899082569,0.8181818181818182
machine-translation,2,"We also experimented with using learned positional embeddings instead , and found that the two versions produced nearly identical results ( see row ( E ) ) .",model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,125,84,10,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5580357142857143,0.7706422018348624,0.9090909090909091
machine-translation,2,We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .,model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,126,85,11,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5625,0.7798165137614679,1.0
machine-translation,2,Why Self - Attention,model,Why Self-Attention,0,127,86,1,0,model : Why Self-Attention,0.5669642857142857,0.7889908256880734,0.041666666666666664
machine-translation,2,"In this section we compare various aspects of self - attention layers to the recurrent and convolutional layers commonly used for mapping one variable - length sequence of symbol representations ( x 1 , ... , x n ) to another sequence of equal length ( z 1 , ... , z n ) , with x i , z i ? Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .",model,Why Self-Attention,0,128,87,2,0,model : Why Self-Attention,0.5714285714285714,0.7981651376146789,0.08333333333333333
machine-translation,2,"In this section we compare various aspects of self - attention layers to the recurrent and convolutional layers commonly used for mapping one variable - length sequence of symbol representations ( x 1 , ... , x n ) to another sequence of equal length ( z 1 , ... , z n ) , with x i , z i ? Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .",model,Why Self-Attention,0,129,88,3,0,model : Why Self-Attention,0.5758928571428571,0.8073394495412844,0.125
machine-translation,2,Motivating our use of self - attention we consider three desiderata .,model,Why Self-Attention,0,130,89,4,0,model : Why Self-Attention,0.5803571428571429,0.8165137614678899,0.16666666666666666
machine-translation,2,One is the total computational complexity per layer .,model,Why Self-Attention,0,131,90,5,0,model : Why Self-Attention,0.5848214285714286,0.8256880733944955,0.20833333333333334
machine-translation,2,"Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .",model,Why Self-Attention,0,132,91,6,0,model : Why Self-Attention,0.5892857142857143,0.8348623853211009,0.25
machine-translation,2,The third is the path length between long - range dependencies in the network .,model,Why Self-Attention,0,133,92,7,0,model : Why Self-Attention,0.59375,0.8440366972477065,0.2916666666666667
machine-translation,2,Learning long - range dependencies is a key challenge in many sequence transduction tasks .,model,Why Self-Attention,0,134,93,8,0,model : Why Self-Attention,0.5982142857142857,0.8532110091743119,0.3333333333333333
machine-translation,2,One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .,model,Why Self-Attention,0,135,94,9,0,model : Why Self-Attention,0.6026785714285714,0.8623853211009175,0.375
machine-translation,2,"The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long - range dependencies .",model,Why Self-Attention,0,136,95,10,0,model : Why Self-Attention,0.6071428571428571,0.8715596330275229,0.4166666666666667
machine-translation,2,Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types .,model,Why Self-Attention,0,137,96,11,0,model : Why Self-Attention,0.6116071428571429,0.8807339449541285,0.4583333333333333
machine-translation,2,"As noted in , a self - attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O ( n ) sequential operations .",model,Why Self-Attention,0,138,97,12,0,model : Why Self-Attention,0.6160714285714286,0.8899082568807339,0.5
machine-translation,2,"In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece and byte - pair representations .",model,Why Self-Attention,0,139,98,13,0,model : Why Self-Attention,0.6205357142857143,0.8990825688073395,0.5416666666666666
machine-translation,2,"To improve computational performance for tasks involving very long sequences , self - attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position .",model,Why Self-Attention,0,140,99,14,0,model : Why Self-Attention,0.625,0.908256880733945,0.5833333333333334
machine-translation,2,This would increase the maximum path length to O ( n / r ) .,model,Why Self-Attention,0,141,100,15,0,model : Why Self-Attention,0.6294642857142857,0.9174311926605505,0.625
machine-translation,2,We plan to investigate this approach further in future work .,model,Why Self-Attention,0,142,101,16,0,model : Why Self-Attention,0.6339285714285714,0.926605504587156,0.6666666666666666
machine-translation,2,single convolutional layer with kernel width k < n does not connect all pairs of input and output positions .,model,Why Self-Attention,0,143,102,17,0,model : Why Self-Attention,0.6383928571428571,0.9357798165137615,0.7083333333333334
machine-translation,2,"Doing so requires a stack of O ( n / k ) convolutional layers in the case of contiguous kernels , or O ( log k ( n ) ) in the case of dilated convolutions , increasing the length of the longest paths between any two positions in the network .",model,Why Self-Attention,0,144,103,18,0,model : Why Self-Attention,0.6428571428571429,0.944954128440367,0.75
machine-translation,2,"Convolutional layers are generally more expensive than recurrent layers , by a factor of k.",model,Why Self-Attention,0,145,104,19,0,model : Why Self-Attention,0.6473214285714286,0.9541284403669725,0.7916666666666666
machine-translation,2,"Separable convolutions , however , decrease the complexity considerably , to O ( k n d + n d 2 ) .",model,Why Self-Attention,0,146,105,20,0,model : Why Self-Attention,0.6517857142857143,0.963302752293578,0.8333333333333334
machine-translation,2,"Even with k = n , however , the complexity of a separable convolution is equal to the combination of a self - attention layer and a point - wise feed - forward layer , the approach we take in our model .",model,Why Self-Attention,0,147,106,21,0,model : Why Self-Attention,0.65625,0.9724770642201835,0.875
machine-translation,2,"As side benefit , self - attention could yield more interpretable models .",model,Why Self-Attention,0,148,107,22,0,model : Why Self-Attention,0.6607142857142857,0.981651376146789,0.9166666666666666
machine-translation,2,We inspect attention distributions from our models and present and discuss examples in the appendix .,model,Why Self-Attention,0,149,108,23,0,model : Why Self-Attention,0.6651785714285714,0.9908256880733946,0.9583333333333334
machine-translation,2,"Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .",model,Why Self-Attention,0,150,109,24,0,model : Why Self-Attention,0.6696428571428571,1.0,1.0
machine-translation,2,Training,training,Training,0,151,1,1,0,training : Training,0.6741071428571429,0.03571428571428571,0.5
machine-translation,2,This section describes the training regime for our models .,training,Training,0,152,2,2,0,training : Training,0.6785714285714286,0.07142857142857142,1.0
machine-translation,2,Training Data and Batching,training,Training Data and Batching,0,153,3,1,0,training : Training Data and Batching,0.6830357142857143,0.10714285714285714,0.16666666666666666
machine-translation,2,We trained on the standard WMT 2014 English - German dataset consisting of about 4.5 million sentence pairs .,training,Training Data and Batching,0,154,4,2,0,training : Training Data and Batching,0.6875,0.14285714285714285,0.3333333333333333
machine-translation,2,"Sentences were encoded using byte - pair encoding , which has a shared sourcetarget vocabulary of about 37000 tokens .",training,Training Data and Batching,0,155,5,3,0,training : Training Data and Batching,0.6919642857142857,0.17857142857142858,0.5
machine-translation,2,"For English - French , we used the significantly larger WMT 2014 English - French dataset consisting of 36M sentences and split tokens into a 32000 word - piece vocabulary .",training,Training Data and Batching,0,156,6,4,0,training : Training Data and Batching,0.6964285714285714,0.21428571428571427,0.6666666666666666
machine-translation,2,Sentence pairs were batched together by approximate sequence length .,training,Training Data and Batching,0,157,7,5,0,training : Training Data and Batching,0.7008928571428571,0.25,0.8333333333333334
machine-translation,2,Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .,training,Training Data and Batching,0,158,8,6,0,training : Training Data and Batching,0.7053571428571429,0.2857142857142857,1.0
machine-translation,2,Hardware and Schedule,training,Hardware and Schedule,1,159,9,1,0,training : Hardware and Schedule,0.7098214285714286,0.32142857142857145,0.16666666666666666
machine-translation,2,We trained our models on one machine with 8 NVIDIA P100 GPUs .,training,Hardware and Schedule,1,160,10,2,0,training : Hardware and Schedule,0.7142857142857143,0.35714285714285715,0.3333333333333333
machine-translation,2,"For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .",training,Hardware and Schedule,0,161,11,3,0,training : Hardware and Schedule,0.71875,0.39285714285714285,0.5
machine-translation,2,"We trained the base models for a total of 100,000 steps or 12 hours .",training,Hardware and Schedule,1,162,12,4,0,training : Hardware and Schedule,0.7232142857142857,0.42857142857142855,0.6666666666666666
machine-translation,2,"For our big models , ( described on the bottom line of table 3 ) , step time was 1.0 seconds .",training,Hardware and Schedule,0,163,13,5,0,training : Hardware and Schedule,0.7276785714285714,0.4642857142857143,0.8333333333333334
machine-translation,2,"The big models were trained for 300,000 steps ( 3.5 days ) .",training,Hardware and Schedule,1,164,14,6,0,training : Hardware and Schedule,0.7321428571428571,0.5,1.0
machine-translation,2,Optimizer,training,Optimizer,1,165,15,1,0,training : Optimizer,0.7366071428571429,0.5357142857142857,0.2
machine-translation,2,"We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .",training,Optimizer,1,166,16,2,0,training : Optimizer,0.7410714285714286,0.5714285714285714,0.4
machine-translation,2,"We varied the learning rate over the course of training , according to the formula :",training,Optimizer,0,167,17,3,0,training : Optimizer,0.7455357142857143,0.6071428571428571,0.6
machine-translation,2,"This corresponds to increasing the learning rate linearly for the first warmup_steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .",training,Optimizer,0,168,18,4,0,training : Optimizer,0.75,0.6428571428571429,0.8
machine-translation,2,We used warmup_steps = 4000 .,training,Optimizer,1,169,19,5,0,training : Optimizer,0.7544642857142857,0.6785714285714286,1.0
machine-translation,2,Regularization,training,Regularization,1,170,20,1,0,training : Regularization,0.7589285714285714,0.7142857142857143,0.1111111111111111
machine-translation,2,We employ three types of regularization during training :,training,Regularization,0,171,21,2,0,training : Regularization,0.7633928571428571,0.75,0.2222222222222222
machine-translation,2,Residual Dropout,training,Regularization,1,172,22,3,0,training : Regularization,0.7678571428571429,0.7857142857142857,0.3333333333333333
machine-translation,2,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .",training,Regularization,1,173,23,4,0,training : Regularization,0.7723214285714286,0.8214285714285714,0.4444444444444444
machine-translation,2,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .",training,Regularization,1,174,24,5,0,training : Regularization,0.7767857142857143,0.8571428571428571,0.5555555555555556
machine-translation,2,"For the base model , we use a rate of P drop = 0.1 .",training,Regularization,0,175,25,6,0,training : Regularization,0.78125,0.8928571428571429,0.6666666666666666
machine-translation,2,Label Smoothing,training,Regularization,1,176,26,7,0,training : Regularization,0.7857142857142857,0.9285714285714286,0.7777777777777778
machine-translation,2,"During training , we employed label smoothing of value ls = 0.1 .",training,Regularization,1,177,27,8,0,training : Regularization,0.7901785714285714,0.9642857142857143,0.8888888888888888
machine-translation,2,"This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .",training,Regularization,0,178,28,9,0,training : Regularization,0.7946428571428571,1.0,1.0
machine-translation,2,Results,result,Results,0,179,1,1,0,result : Results,0.7991071428571429,0.06666666666666667,1.0
machine-translation,2,Machine Translation,result,Machine Translation,1,180,2,1,0,result : Machine Translation,0.8035714285714286,0.13333333333333333,0.07142857142857142
machine-translation,2,"On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing a new state - of - the - art BLEU score of 28.4 .",result,Machine Translation,1,181,3,2,0,result : Machine Translation,0.8080357142857143,0.2,0.14285714285714285
machine-translation,2,The configuration of this model is listed in the bottom line of .,result,Machine Translation,0,182,4,3,0,result : Machine Translation,0.8125,0.26666666666666666,0.21428571428571427
machine-translation,2,Training took 3.5 days on 8 P100 GPUs .,result,Machine Translation,0,183,5,4,0,result : Machine Translation,0.8169642857142857,0.3333333333333333,0.2857142857142857
machine-translation,2,"Even our base model surpasses all previously published models and ensembles , at a fraction of the training cost of any of the competitive models .",result,Machine Translation,0,184,6,5,0,result : Machine Translation,0.8214285714285714,0.4,0.35714285714285715
machine-translation,2,"On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .",result,Machine Translation,1,185,7,6,0,result : Machine Translation,0.8258928571428571,0.4666666666666667,0.42857142857142855
machine-translation,2,"The Transformer ( big ) model trained for English - to - French used dropout rate P drop = 0.1 , instead of 0.3 .",result,Machine Translation,0,186,8,7,0,result : Machine Translation,0.8303571428571429,0.5333333333333333,0.5
machine-translation,2,"For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 - minute intervals .",result,Machine Translation,0,187,9,8,0,result : Machine Translation,0.8348214285714286,0.6,0.5714285714285714
machine-translation,2,"For the big models , we averaged the last 20 checkpoints .",result,Machine Translation,0,188,10,9,0,result : Machine Translation,0.8392857142857143,0.6666666666666666,0.6428571428571429
machine-translation,2,We used beam search with a beam size of 4 and length penalty ? = 0.6 .,result,Machine Translation,0,189,11,10,0,result : Machine Translation,0.84375,0.7333333333333333,0.7142857142857143
machine-translation,2,These hyperparameters were chosen after experimentation on the development set .,result,Machine Translation,0,190,12,11,0,result : Machine Translation,0.8482142857142857,0.8,0.7857142857142857
machine-translation,2,"We set the maximum output length during inference to input length + 50 , but terminate early when possible .",result,Machine Translation,0,191,13,12,0,result : Machine Translation,0.8526785714285714,0.8666666666666667,0.8571428571428571
machine-translation,2,summarizes our results and compares our translation quality and training costs to other model architectures from the literature .,result,Machine Translation,0,192,14,13,0,result : Machine Translation,0.8571428571428571,0.9333333333333333,0.9285714285714286
machine-translation,2,"We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single - precision floating - point capacity of each GPU 5 .",result,Machine Translation,0,193,15,14,0,result : Machine Translation,0.8616071428571429,1.0,1.0
machine-translation,2,Model Variations,model,Model Variations,0,194,1,1,0,model : Model Variations,0.8660714285714286,0.045454545454545456,0.1
machine-translation,2,"To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English - to - German translation on the development set , newstest2013 .",model,Model Variations,0,195,2,2,0,model : Model Variations,0.8705357142857143,0.09090909090909091,0.2
machine-translation,2,"We used beam search as described in the previous section , but no checkpoint averaging .",model,Model Variations,0,196,3,3,0,model : Model Variations,0.875,0.13636363636363635,0.3
machine-translation,2,We present these results in .,model,Model Variations,0,197,4,4,0,model : Model Variations,0.8794642857142857,0.18181818181818182,0.4
machine-translation,2,"In rows ( A ) , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .",model,Model Variations,0,198,5,5,0,model : Model Variations,0.8839285714285714,0.22727272727272727,0.5
machine-translation,2,"While single - head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .",model,Model Variations,0,199,6,6,0,model : Model Variations,0.8883928571428571,0.2727272727272727,0.6
machine-translation,2,"In rows ( B ) , we observe that reducing the attention key size d k hurts model quality .",model,Model Variations,0,200,7,7,0,model : Model Variations,0.8928571428571429,0.3181818181818182,0.7
machine-translation,2,This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product maybe beneficial .,model,Model Variations,0,201,8,8,0,model : Model Variations,0.8973214285714286,0.36363636363636365,0.8
machine-translation,2,"We further observe in rows ( C ) and ( D ) that , as expected , bigger models are better , and dropout is very helpful in avoiding over-fitting .",model,Model Variations,0,202,9,9,0,model : Model Variations,0.9017857142857143,0.4090909090909091,0.9
machine-translation,2,"In row ( E ) we replace our sinusoidal positional encoding with learned positional embeddings , and observe nearly identical results to the base model .",model,Model Variations,0,203,10,10,0,model : Model Variations,0.90625,0.45454545454545453,1.0
machine-translation,2,English Constituency Parsing,model,English Constituency Parsing,1,204,11,1,0,model : English Constituency Parsing,0.9107142857142857,0.5,0.08333333333333333
machine-translation,2,To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing .,model,English Constituency Parsing,0,205,12,2,0,model : English Constituency Parsing,0.9151785714285714,0.5454545454545454,0.16666666666666666
machine-translation,2,This task presents specific challenges : the output is subject to strong structural constraints and is significantly longer than the input .,model,English Constituency Parsing,0,206,13,3,0,model : English Constituency Parsing,0.9196428571428571,0.5909090909090909,0.25
machine-translation,2,"Furthermore , RNN sequence - to - sequence models have not been able to attain state - of - the - art results in small - data regimes .",model,English Constituency Parsing,0,207,14,4,0,model : English Constituency Parsing,0.9241071428571429,0.6363636363636364,0.3333333333333333
machine-translation,2,"We trained a 4 - layer transformer with d model = 1024 on the Wall Street Journal ( WSJ ) portion of the Penn Treebank , about 40 K training sentences .",model,English Constituency Parsing,0,208,15,5,0,model : English Constituency Parsing,0.9285714285714286,0.6818181818181818,0.4166666666666667
machine-translation,2,"We also trained it in a semi-supervised setting , using the larger high - confidence and BerkleyParser corpora from with approximately 17M sentences .",model,English Constituency Parsing,0,209,16,6,0,model : English Constituency Parsing,0.9330357142857143,0.7272727272727273,0.5
machine-translation,2,We used a vocabulary of 16 K tokens for the WSJ only setting and a vocabulary of 32 K tokens for the semi-supervised setting .,model,English Constituency Parsing,0,210,17,7,0,model : English Constituency Parsing,0.9375,0.7727272727272727,0.5833333333333334
machine-translation,2,"We performed only a small number of experiments to select the dropout , both attention and residual ( section 5.4 ) , learning rates and beam size on the Section 22 development set , all other parameters remained unchanged from the English - to - German base translation model .",model,English Constituency Parsing,0,211,18,8,0,model : English Constituency Parsing,0.9419642857142857,0.8181818181818182,0.6666666666666666
machine-translation,2,"During inference , we increased the maximum output length to input length + 300 .",model,English Constituency Parsing,0,212,19,9,0,model : English Constituency Parsing,0.9464285714285714,0.8636363636363636,0.75
machine-translation,2,We used a beam size of 21 and ? = 0.3 for both WSJ only and the semi-supervised setting .,model,English Constituency Parsing,0,213,20,10,0,model : English Constituency Parsing,0.9508928571428571,0.9090909090909091,0.8333333333333334
machine-translation,2,"Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .",model,English Constituency Parsing,1,214,21,11,0,model : English Constituency Parsing,0.9553571428571429,0.9545454545454546,0.9166666666666666
machine-translation,2,"In contrast to RNN sequence - to - sequence models , the Transformer outperforms the Berkeley - Parser even when training only on the WSJ training set of 40K sentences .",model,English Constituency Parsing,0,215,22,12,0,model : English Constituency Parsing,0.9598214285714286,1.0,1.0
machine-translation,2,Conclusion,conclusion,Conclusion,0,216,1,1,0,conclusion : Conclusion,0.9642857142857143,0.1111111111111111,0.1111111111111111
machine-translation,2,"In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder - decoder architectures with multi-headed self - attention .",conclusion,Conclusion,0,217,2,2,0,conclusion : Conclusion,0.96875,0.2222222222222222,0.2222222222222222
machine-translation,2,"For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .",conclusion,Conclusion,0,218,3,3,0,conclusion : Conclusion,0.9732142857142857,0.3333333333333333,0.3333333333333333
machine-translation,2,"On both WMT 2014 English - to - German and WMT 2014 English - to - French translation tasks , we achieve a new state of the art .",conclusion,Conclusion,0,219,4,4,0,conclusion : Conclusion,0.9776785714285714,0.4444444444444444,0.4444444444444444
machine-translation,2,In the former task our best model outperforms even all previously reported ensembles .,conclusion,Conclusion,0,220,5,5,0,conclusion : Conclusion,0.9821428571428571,0.5555555555555556,0.5555555555555556
machine-translation,2,We are excited about the future of attention - based models and plan to apply them to other tasks .,conclusion,Conclusion,0,221,6,6,0,conclusion : Conclusion,0.9866071428571429,0.6666666666666666,0.6666666666666666
machine-translation,2,"We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .",conclusion,Conclusion,0,222,7,7,0,conclusion : Conclusion,0.9910714285714286,0.7777777777777778,0.7777777777777778
machine-translation,2,Making generation less sequential is another research goals of ours .,conclusion,Conclusion,0,223,8,8,0,conclusion : Conclusion,0.9955357142857143,0.8888888888888888,0.8888888888888888
machine-translation,2,The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.,conclusion,Conclusion,0,224,9,9,0,conclusion : Conclusion,1.0,1.0,1.0
machine-translation,3,Deep Recurrent Models with Fast - Forward Connections for Neural Machine Translation,title,title,0,2,1,1,0,title : title,0.006389776357827476,1.0,1.0
machine-translation,3,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.009584664536741214,0.1,0.1
machine-translation,3,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.012779552715654952,0.2,0.2
machine-translation,3,"However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01597444089456869,0.3,0.3
machine-translation,3,"In this work , we introduce a new type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.019169329073482427,0.4,0.4
machine-translation,3,Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.022364217252396165,0.5,0.5
machine-translation,3,"On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.025559105431309903,0.6,0.6
machine-translation,3,This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.02875399361022364,0.7,0.7
machine-translation,3,We can still achieve BLEU = 36.3 even without using an attention mechanism .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.03194888178913738,0.8,0.8
machine-translation,3,"After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.03514376996805112,0.9,0.9
machine-translation,3,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.038338658146964855,1.0,1.0
machine-translation,3,Introduction,introduction,introduction,0,13,1,1,0,introduction : introduction,0.04153354632587859,0.03333333333333333,0.03333333333333333
machine-translation,3,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,introduction,introduction,0,14,2,2,0,introduction : introduction,0.04472843450479233,0.06666666666666667,0.06666666666666667
machine-translation,3,"Unlike conventional statistical machine translation ( SMT ) systems which consist of multiple separately tuned components , NMT models encode the source sequence into continuous representation space and generate the target sequence in an end - to - end fashon .",introduction,introduction,0,15,3,3,0,introduction : introduction,0.04792332268370607,0.1,0.1
machine-translation,3,"Moreover , NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation .",introduction,introduction,0,16,4,4,0,introduction : introduction,0.051118210862619806,0.13333333333333333,0.13333333333333333
machine-translation,3,"In general , there are two types of NMT topologies : the encoder - decoder network and the attention network .",introduction,introduction,0,17,5,5,0,introduction : introduction,0.054313099041533544,0.16666666666666666,0.16666666666666666
machine-translation,3,The encoder - decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword .,introduction,introduction,0,18,6,6,0,introduction : introduction,0.05750798722044728,0.2,0.2
machine-translation,3,The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words .,introduction,introduction,0,19,7,7,0,introduction : introduction,0.06070287539936102,0.23333333333333334,0.23333333333333334
machine-translation,3,Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems .,introduction,introduction,0,20,8,8,0,introduction : introduction,0.06389776357827476,0.26666666666666666,0.26666666666666666
machine-translation,3,"However , a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT ' 14 English - to - French task .",introduction,introduction,0,21,9,9,0,introduction : introduction,0.0670926517571885,0.3,0.3
machine-translation,3,The best BLEU score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0 .,introduction,introduction,0,22,10,10,0,introduction : introduction,0.07028753993610223,0.3333333333333333,0.3333333333333333
machine-translation,3,We focus on improving the single model perfor - mance by increasing the model depth .,introduction,introduction,0,23,11,11,0,introduction : introduction,0.07348242811501597,0.36666666666666664,0.36666666666666664
machine-translation,3,Deep topology has been proven to outperform the shallow architecture in computer vision .,introduction,introduction,0,24,12,12,0,introduction : introduction,0.07667731629392971,0.4,0.4
machine-translation,3,In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers .,introduction,introduction,0,25,13,13,0,introduction : introduction,0.07987220447284345,0.43333333333333335,0.43333333333333335
machine-translation,3,"But in NMT , the biggest depth used successfully is only six .",introduction,introduction,0,26,14,14,0,introduction : introduction,0.08306709265175719,0.4666666666666667,0.4666666666666667
machine-translation,3,We attribute this problem to the properties of the Long Short - Term Memory ( LSTM ) which is widely used in NMT .,introduction,introduction,0,27,15,15,0,introduction : introduction,0.08626198083067092,0.5,0.5
machine-translation,3,"In the LSTM , there are more non-linear activations than in convolution layers .",introduction,introduction,0,28,16,16,0,introduction : introduction,0.08945686900958466,0.5333333333333333,0.5333333333333333
machine-translation,3,"These activations significantly decrease the magnitude of the gradient in the deep topology , especially when the gradient propagates in recurrent form .",introduction,introduction,0,29,17,17,0,introduction : introduction,0.0926517571884984,0.5666666666666667,0.5666666666666667
machine-translation,3,"There are also many efforts to increase the depth of the LSTM such as the work by , where the shortcuts do not avoid the nonlinear and recurrent computation .",introduction,introduction,0,30,18,18,0,introduction : introduction,0.09584664536741214,0.6,0.6
machine-translation,3,"In this work , we introduce a new type of linear connections for multi - layer recurrent networks .",introduction,introduction,1,31,19,19,0,introduction : introduction,0.09904153354632587,0.6333333333333333,0.6333333333333333
machine-translation,3,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .",introduction,introduction,1,32,20,20,0,introduction : introduction,0.10223642172523961,0.6666666666666666,0.6666666666666666
machine-translation,3,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .",introduction,introduction,1,33,21,21,0,introduction : introduction,0.10543130990415335,0.7,0.7
machine-translation,3,This topology can be used for both the encoder - decoder network and the attention network .,introduction,introduction,0,34,22,22,0,introduction : introduction,0.10862619808306709,0.7333333333333333,0.7333333333333333
machine-translation,3,"On the WMT ' 14 Englishto - French task , this is the deepest NMT topology that has ever been investigated .",introduction,introduction,0,35,23,23,0,introduction : introduction,0.11182108626198083,0.7666666666666667,0.7666666666666667
machine-translation,3,"With our deep attention model , the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 BLEU points .",introduction,introduction,0,36,24,24,0,introduction : introduction,0.11501597444089456,0.8,0.8
machine-translation,3,This is also the first time on this task that a single NMT model achieves state - of - the - art performance and outperforms the best conventional SMT system with an improvement of 0.7 .,introduction,introduction,0,37,25,25,0,introduction : introduction,0.1182108626198083,0.8333333333333334,0.8333333333333334
machine-translation,3,"Even without using the attention mechanism , we can still achieve 36.3 with a single model .",introduction,introduction,0,38,26,26,0,introduction : introduction,0.12140575079872204,0.8666666666666667,0.8666666666666667
machine-translation,3,"After model ensembling and unknown word processing , the BLEU score can be further improved to 40.4 .",introduction,introduction,0,39,27,27,0,introduction : introduction,0.12460063897763578,0.9,0.9
machine-translation,3,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",introduction,introduction,0,40,28,28,0,introduction : introduction,0.12779552715654952,0.9333333333333333,0.9333333333333333
machine-translation,3,"As a reference , previous work showed that oracle rescoring of the 1000 - best sequences generated by the SMT model can achieve the BLEU score of about 45 .",introduction,introduction,0,41,29,29,0,introduction : introduction,0.13099041533546327,0.9666666666666667,0.9666666666666667
machine-translation,3,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,introduction,introduction,0,42,30,30,0,introduction : introduction,0.134185303514377,1.0,1.0
machine-translation,3,Neural Machine Translation,system description,Neural Machine Translation,0,43,1,1,0,system description : Neural Machine Translation,0.13738019169329074,0.008333333333333333,0.03125
machine-translation,3,"Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models .",system description,Neural Machine Translation,0,44,2,2,0,system description : Neural Machine Translation,0.14057507987220447,0.016666666666666666,0.0625
machine-translation,3,"In this task , the likelihood p ( y | x , ? ) of the target sequence will be maximized with parameter ? to learn :",system description,Neural Machine Translation,0,45,3,3,0,system description : Neural Machine Translation,0.14376996805111822,0.025,0.09375
machine-translation,3,"In this task , the likelihood p ( y | x , ? ) of the target sequence will be maximized with parameter ? to learn :",system description,Neural Machine Translation,0,46,4,4,0,system description : Neural Machine Translation,0.14696485623003194,0.03333333333333333,0.125
machine-translation,3,where y 0:j?1 is the sub sequence from y 0 toy j?1 . y 0 and y m + 1 denote the start mark and end mark of target sequence respectively .,system description,Neural Machine Translation,0,47,5,5,0,system description : Neural Machine Translation,0.1501597444089457,0.041666666666666664,0.15625
machine-translation,3,"The process can be explicitly split into an encoding part , a decoding part and the interface between these two parts .",system description,Neural Machine Translation,0,48,6,6,0,system description : Neural Machine Translation,0.15335463258785942,0.05,0.1875
machine-translation,3,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .",system description,Neural Machine Translation,0,49,7,7,0,system description : Neural Machine Translation,0.15654952076677317,0.058333333333333334,0.21875
machine-translation,3,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,system description,Neural Machine Translation,0,50,8,8,0,system description : Neural Machine Translation,0.1597444089456869,0.06666666666666667,0.25
machine-translation,3,"At the decoding step , the target sequence is generated from the representation c.",system description,Neural Machine Translation,0,51,9,9,0,system description : Neural Machine Translation,0.16293929712460065,0.075,0.28125
machine-translation,3,"Recently , there have been two types of NMT models which are different in the interface part .",system description,Neural Machine Translation,0,52,10,10,0,system description : Neural Machine Translation,0.16613418530351437,0.08333333333333333,0.3125
machine-translation,3,"In the encoder - decoder model , a single vector extracted from e is used as the representation .",system description,Neural Machine Translation,0,53,11,11,0,system description : Neural Machine Translation,0.16932907348242812,0.09166666666666666,0.34375
machine-translation,3,"In the attention model , c is dynamically obtained according to the relationship between the target sequence and the source sequence .",system description,Neural Machine Translation,0,54,12,12,0,system description : Neural Machine Translation,0.17252396166134185,0.1,0.375
machine-translation,3,"The recurrent neural network ( RNN ) , or its specific form the LSTM , is generally used as the basic unit of the encoding and decoding part .",system description,Neural Machine Translation,0,55,13,13,0,system description : Neural Machine Translation,0.1757188498402556,0.10833333333333334,0.40625
machine-translation,3,"However , the topology of most of the existing models is shallow .",system description,Neural Machine Translation,0,56,14,14,0,system description : Neural Machine Translation,0.17891373801916932,0.11666666666666667,0.4375
machine-translation,3,"In the attention network , the encoding part and the decoding part have only one LSTM layer respectively .",system description,Neural Machine Translation,0,57,15,15,0,system description : Neural Machine Translation,0.18210862619808307,0.125,0.46875
machine-translation,3,"In the encoder - decoder network , researchers have used at most six LSTM layers .",system description,Neural Machine Translation,0,58,16,16,0,system description : Neural Machine Translation,0.1853035143769968,0.13333333333333333,0.5
machine-translation,3,"Because machine translation is a difficult problem , we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence .",system description,Neural Machine Translation,0,59,17,17,0,system description : Neural Machine Translation,0.18849840255591055,0.14166666666666666,0.53125
machine-translation,3,"In this work , we focus on enhancing the complexity of the encoding / decoding architecture by increasing the model depth .",system description,Neural Machine Translation,0,60,18,18,0,system description : Neural Machine Translation,0.19169329073482427,0.15,0.5625
machine-translation,3,Deep neural models have been studied in a wide range of problems .,system description,Neural Machine Translation,0,61,19,19,0,system description : Neural Machine Translation,0.19488817891373802,0.15833333333333333,0.59375
machine-translation,3,"In computer vision , models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years .",system description,Neural Machine Translation,0,62,20,20,0,system description : Neural Machine Translation,0.19808306709265175,0.16666666666666666,0.625
machine-translation,3,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path .,system description,Neural Machine Translation,0,63,21,21,0,system description : Neural Machine Translation,0.2012779552715655,0.175,0.65625
machine-translation,3,"Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",system description,Neural Machine Translation,0,64,22,22,0,system description : Neural Machine Translation,0.20447284345047922,0.18333333333333332,0.6875
machine-translation,3,"Because of the existence of many more nonlinear activations and the recurrent computation , gradient values are not stable and are generally smaller .",system description,Neural Machine Translation,0,65,23,23,0,system description : Neural Machine Translation,0.20766773162939298,0.19166666666666668,0.71875
machine-translation,3,"Following the same spirit for convolutional networks , a lot of effort has also been spent on training deep LSTM networks .",system description,Neural Machine Translation,0,66,24,24,0,system description : Neural Machine Translation,0.2108626198083067,0.2,0.75
machine-translation,3,"introduced depth - gated shortcuts , connecting LSTM cells at adjacent layers , to provide a fast way to propagate the gradients .",system description,Neural Machine Translation,0,67,25,25,0,system description : Neural Machine Translation,0.21405750798722045,0.20833333333333334,0.78125
machine-translation,3,They validated the modification of these shortcuts on an MT task and a language modeling task .,system description,Neural Machine Translation,0,68,26,26,0,system description : Neural Machine Translation,0.21725239616613418,0.21666666666666667,0.8125
machine-translation,3,"However , the best score was obtained using models with three layers .",system description,Neural Machine Translation,0,69,27,27,0,system description : Neural Machine Translation,0.22044728434504793,0.225,0.84375
machine-translation,3,"Similarly , proposed a two dimensional structure for the LSTM .",system description,Neural Machine Translation,0,70,28,28,0,system description : Neural Machine Translation,0.22364217252396165,0.23333333333333334,0.875
machine-translation,3,Their structure decreases the number of nonlinear activations and path length .,system description,Neural Machine Translation,0,71,29,29,0,system description : Neural Machine Translation,0.2268370607028754,0.24166666666666667,0.90625
machine-translation,3,"However , the gradient propagation still relies on the recurrent computation .",system description,Neural Machine Translation,0,72,30,30,0,system description : Neural Machine Translation,0.23003194888178913,0.25,0.9375
machine-translation,3,"The investigations were also made on question - answering to encode the questions , whereat most two LSTM layers were stacked .",system description,Neural Machine Translation,0,73,31,31,0,system description : Neural Machine Translation,0.23322683706070288,0.25833333333333336,0.96875
machine-translation,3,"Based on the above considerations , we propose new connections to facilitate gradient propagation in the following section .",system description,Neural Machine Translation,0,74,32,32,0,system description : Neural Machine Translation,0.2364217252396166,0.26666666666666666,1.0
machine-translation,3,Deep Topology,system description,Deep Topology,0,75,33,1,0,system description : Deep Topology,0.23961661341853036,0.275,0.2
machine-translation,3,We build the deep LSTM network with the new proposed linear connections .,system description,Deep Topology,0,76,34,2,0,system description : Deep Topology,0.24281150159744408,0.2833333333333333,0.4
machine-translation,3,The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation .,system description,Deep Topology,0,77,35,3,0,system description : Deep Topology,0.24600638977635783,0.2916666666666667,0.6
machine-translation,3,We call these connections fastforward connections .,system description,Deep Topology,0,78,36,4,0,system description : Deep Topology,0.24920127795527156,0.3,0.8
machine-translation,3,"Within the deep topology , we also introduce an interleaved bi-directional architecture to stack the LSTM layers .",system description,Deep Topology,0,79,37,5,0,system description : Deep Topology,0.2523961661341853,0.30833333333333335,1.0
machine-translation,3,Network,system description,Network,0,80,38,1,0,system description : Network,0.25559105431309903,0.31666666666666665,0.012048192771084338
machine-translation,3,Our entire deep neural network is shown in .,system description,Network,0,81,39,2,0,system description : Network,0.25878594249201275,0.325,0.024096385542168676
machine-translation,3,"This topology can be divided into three parts : the encoder part ( P -E ) on the left , the decoder part ( P - D ) on the right and the interface between these two parts ( P - I ) which extracts the representation of the source sequence .",system description,Network,0,82,40,3,0,system description : Network,0.26198083067092653,0.3333333333333333,0.03614457831325301
machine-translation,3,"We have two instantiations of this topology : Deep - ED and Deep - Att , which correspond to the extension of the encoder - decoder network and the attention network respectively .",system description,Network,0,83,41,4,0,system description : Network,0.26517571884984026,0.3416666666666667,0.04819277108433735
machine-translation,3,Our main innovation is the novel scheme for connecting adjacent recurrent layers .,system description,Network,0,84,42,5,0,system description : Network,0.268370607028754,0.35,0.060240963855421686
machine-translation,3,We will start with the basic RNN model for the sake of clarity .,system description,Network,0,85,43,6,0,system description : Network,0.2715654952076677,0.35833333333333334,0.07228915662650602
machine-translation,3,Recurrent layer :,system description,Network,0,86,44,7,0,system description : Network,0.2747603833865815,0.36666666666666664,0.08433734939759036
machine-translation,3,"When an input sequence {x 1 , . . . , x m } is given to a recurrent layer , the output ht at each time step t can be computed as ( see )",system description,Network,0,87,45,8,0,system description : Network,0.2779552715654952,0.375,0.0963855421686747
machine-translation,3,where the bias parameter is not included for simplicity .,system description,Network,0,88,46,9,0,system description : Network,0.28115015974440893,0.38333333333333336,0.10843373493975904
machine-translation,3,We use a red circle and a blue empty square to denote an input and a hidden state .,system description,Network,0,89,47,10,0,system description : Network,0.28434504792332266,0.39166666666666666,0.12048192771084337
machine-translation,3,"blue square with a "" - "" denotes the previous hidden state .",system description,Network,0,90,48,11,0,system description : Network,0.28753993610223644,0.4,0.13253012048192772
machine-translation,3,dotted line means that the hidden state is used recurrently .,system description,Network,0,91,49,12,0,system description : Network,0.29073482428115016,0.4083333333333333,0.14457831325301204
machine-translation,3,This computation can be equivalently split into two consecutive steps :,system description,Network,0,92,50,13,0,system description : Network,0.2939297124600639,0.4166666666666667,0.1566265060240964
machine-translation,3,"Feed-Forward computation : ft = W f x t . Left part in ) . "" f "" block .",system description,Network,0,93,51,14,0,system description : Network,0.2971246006389776,0.425,0.1686746987951807
machine-translation,3,"Recurrent computation : RNN ( f t , h t?1 ) .",system description,Network,0,94,52,15,0,system description : Network,0.3003194888178914,0.43333333333333335,0.18072289156626506
machine-translation,3,Right part and the sum operation ( + ) followed by activation in .,system description,Network,0,95,53,16,0,system description : Network,0.3035143769968051,0.44166666666666665,0.1927710843373494
machine-translation,3,"r "" block .",system description,Network,0,96,54,17,0,system description : Network,0.30670926517571884,0.45,0.20481927710843373
machine-translation,3,"For a deep topology with stacked recurrent layers , the input of each block "" f "" at recurrent layer k ( denoted by f k ) is usually the output of block "" r "" at its previous recurrent layer k ? 1 ( denoted by h k?1 ) .",system description,Network,0,97,55,18,0,system description : Network,0.30990415335463256,0.4583333333333333,0.21686746987951808
machine-translation,3,"For a deep topology with stacked recurrent layers , the input of each block "" f "" at recurrent layer k ( denoted by f k ) is usually the output of block "" r "" at its previous recurrent layer k ? 1 ( denoted by h k?1 ) .",system description,Network,0,98,56,19,0,system description : Network,0.31309904153354634,0.4666666666666667,0.2289156626506024
machine-translation,3,"In our work , we add fast - forward connections ( F - F connections ) which connect two feed - forward computation blocks "" f "" of adjacent recurrent layers .",system description,Network,0,99,57,20,0,system description : Network,0.31629392971246006,0.475,0.24096385542168675
machine-translation,3,"It means that each block "" f "" at recurrent layer k takes both the outputs of block "" f "" and block "" r "" at its previous layer as input ( ) .",system description,Network,0,100,58,21,0,system description : Network,0.3194888178913738,0.48333333333333334,0.25301204819277107
machine-translation,3,- F connections are denoted by dashed red lines in and .,system description,Network,0,101,59,22,0,system description : Network,0.3226837060702875,0.49166666666666664,0.26506024096385544
machine-translation,3,The path of F - F connections contains neither nonlinear activations nor recurrent computation .,system description,Network,0,102,60,23,0,system description : Network,0.3258785942492013,0.5,0.27710843373493976
machine-translation,3,"It provides a fast path for information to propagate , so we call this path fast - forward connections .",system description,Network,0,103,61,24,0,system description : Network,0.329073482428115,0.5083333333333333,0.2891566265060241
machine-translation,3,"Additionally , in order to learn more temporal dependencies , the sequences can be processed in different directions at each pair of adjacent recurrent layers .",system description,Network,0,104,62,25,0,system description : Network,0.33226837060702874,0.5166666666666667,0.30120481927710846
machine-translation,3,This is quantitatively expressed in Eq. 3 :,system description,Network,0,105,63,26,0,system description : Network,0.3354632587859425,0.525,0.3132530120481928
machine-translation,3,The opposite directions are marked by the direction term ( ? 1 ) k .,system description,Network,0,106,64,27,0,system description : Network,0.33865814696485624,0.5333333333333333,0.3253012048192771
machine-translation,3,"At the first recurrent layer , the block "" f "" takes x t as the input .",system description,Network,0,107,65,28,0,system description : Network,0.34185303514376997,0.5416666666666666,0.3373493975903614
machine-translation,3,", ] denotes the concatenation of vectors .",system description,Network,0,108,66,29,0,system description : Network,0.3450479233226837,0.55,0.3493975903614458
machine-translation,3,This is shown in .,system description,Network,0,109,67,30,0,system description : Network,0.34824281150159747,0.5583333333333333,0.3614457831325301
machine-translation,3,The two changes are summarized here :,system description,Network,0,110,68,31,0,system description : Network,0.3514376996805112,0.5666666666666667,0.37349397590361444
machine-translation,3,We add a connection between f kt and f k ?1 t .,system description,Network,0,111,69,32,0,system description : Network,0.3546325878594249,0.575,0.3855421686746988
machine-translation,3,"Without f k ?1 t , our model will be reduced to the traditional stacked model .",system description,Network,0,112,70,33,0,system description : Network,0.35782747603833864,0.5833333333333334,0.39759036144578314
machine-translation,3,We alternate the RNN direction at different layers k with the direction term ( ? 1 ) k .,system description,Network,0,113,71,34,0,system description : Network,0.3610223642172524,0.5916666666666667,0.40963855421686746
machine-translation,3,"If we fix the direction term to ? 1 , all layers work in the forward direction .",system description,Network,0,114,72,35,0,system description : Network,0.36421725239616615,0.6,0.42168674698795183
machine-translation,3,LSTM layer :,system description,Network,0,115,73,36,0,system description : Network,0.36741214057507987,0.6083333333333333,0.43373493975903615
machine-translation,3,"In our experiments , instead of an RNN , a specific type of recurrent layer called LSTM ) is used .",system description,Network,0,116,74,37,0,system description : Network,0.3706070287539936,0.6166666666666667,0.4457831325301205
machine-translation,3,The LSTM is structurally more complex than the basic RNN in Eq .,system description,Network,0,117,75,38,0,system description : Network,0.3738019169329074,0.625,0.4578313253012048
machine-translation,3,". We define the computation of the LSTM as a function which maps the input f and its state - output pair ( h , s ) at the previous time step to the current stateoutput pair .",system description,Network,0,118,76,39,0,system description : Network,0.3769968051118211,0.6333333333333333,0.46987951807228917
machine-translation,3,"The exact computations for ( h t , st ) = LSTM ( f t , h t?1 , s t?1 ) are the following :",system description,Network,0,119,77,40,0,system description : Network,0.3801916932907348,0.6416666666666667,0.4819277108433735
machine-translation,3,"is the concatenation of four vectors of equal size , means element - wise multiplication , ? i is the input activation function , ? o is the output activation function , ? g is the activation function for gates , and W r , ? ? , ? ? , and ? ? are the parameters of the LSTM .",system description,Network,0,120,78,41,0,system description : Network,0.38338658146964855,0.65,0.4939759036144578
machine-translation,3,"is the concatenation of four vectors of equal size , means element - wise multiplication , ? i is the input activation function , ? o is the output activation function , ? g is the activation function for gates , and W r , ? ? , ? ? , and ? ? are the parameters of the LSTM .",system description,Network,0,121,79,42,0,system description : Network,0.3865814696485623,0.6583333333333333,0.5060240963855421
machine-translation,3,"is the concatenation of four vectors of equal size , means element - wise multiplication , ? i is the input activation function , ? o is the output activation function , ? g is the activation function for gates , and W r , ? ? , ? ? , and ? ? are the parameters of the LSTM .",system description,Network,0,122,80,43,0,system description : Network,0.38977635782747605,0.6666666666666666,0.5180722891566265
machine-translation,3,"is the concatenation of four vectors of equal size , means element - wise multiplication , ? i is the input activation function , ? o is the output activation function , ? g is the activation function for gates , and W r , ? ? , ? ? , and ? ? are the parameters of the LSTM .",system description,Network,0,123,81,44,0,system description : Network,0.3929712460063898,0.675,0.5301204819277109
machine-translation,3,"is the concatenation of four vectors of equal size , means element - wise multiplication , ? i is the input activation function , ? o is the output activation function , ? g is the activation function for gates , and W r , ? ? , ? ? , and ? ? are the parameters of the LSTM .",system description,Network,0,124,82,45,0,system description : Network,0.3961661341853035,0.6833333333333333,0.5421686746987951
machine-translation,3,It is slightly different from the standard notation in that we do not have a matrix to multiply with the input fin our notation .,system description,Network,0,125,83,46,0,system description : Network,0.3993610223642173,0.6916666666666667,0.5542168674698795
machine-translation,3,"With this notation , we can write down the computations for our deep bi-directional LSTM model with F - F connections :",system description,Network,0,126,84,47,0,system description : Network,0.402555910543131,0.7,0.5662650602409639
machine-translation,3,where x t is the input to the deep bi-directional LSTM model .,system description,Network,0,127,85,48,0,system description : Network,0.4057507987220447,0.7083333333333334,0.5783132530120482
machine-translation,3,"For the encoder , x t is the embedding of the t th word in the source sentence .",system description,Network,0,128,86,49,0,system description : Network,0.40894568690095845,0.7166666666666667,0.5903614457831325
machine-translation,3,For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t.,system description,Network,0,129,87,50,0,system description : Network,0.41214057507987223,0.725,0.6024096385542169
machine-translation,3,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",system description,Network,0,130,88,51,0,system description : Network,0.41533546325878595,0.7333333333333333,0.6144578313253012
machine-translation,3,". Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",system description,Network,0,131,89,52,0,system description : Network,0.4185303514376997,0.7416666666666667,0.6265060240963856
machine-translation,3,The use of Half ( ) is to reduce the parameter size and does not affect the performance .,system description,Network,0,132,90,53,0,system description : Network,0.4217252396166134,0.75,0.6385542168674698
machine-translation,3,"We observed noticeable performance degradation when using only the first third of the elements of "" f "" .",system description,Network,0,133,91,54,0,system description : Network,0.4249201277955272,0.7583333333333333,0.6506024096385542
machine-translation,3,"With the F - F connections , we build a fast channel to propagate the gradients in the deep topology .",system description,Network,0,134,92,55,0,system description : Network,0.4281150159744409,0.7666666666666667,0.6626506024096386
machine-translation,3,- F connections can accelerate the model convergence and while improving the performance .,system description,Network,0,135,93,56,0,system description : Network,0.43130990415335463,0.775,0.6746987951807228
machine-translation,3,similar idea was also used in .,system description,Network,0,136,94,57,0,system description : Network,0.43450479233226835,0.7833333333333333,0.6867469879518072
machine-translation,3,Encoder : The LSTM layers are stacked following Eq. 5 .,system description,Network,0,137,95,58,0,system description : Network,0.43769968051118213,0.7916666666666666,0.6987951807228916
machine-translation,3,We call this type of encoder interleaved bidirectional encoder .,system description,Network,0,138,96,59,0,system description : Network,0.44089456869009586,0.8,0.7108433734939759
machine-translation,3,"In addition , there are two similar columns ( a 1 and a 2 ) in the encoder part .",system description,Network,0,139,97,60,0,system description : Network,0.4440894568690096,0.8083333333333333,0.7228915662650602
machine-translation,3,Each column consists of n e stacked LSTM layers .,system description,Network,0,140,98,61,0,system description : Network,0.4472843450479233,0.8166666666666667,0.7349397590361446
machine-translation,3,There is no connection between the two columns .,system description,Network,0,141,99,62,0,system description : Network,0.4504792332268371,0.825,0.7469879518072289
machine-translation,3,The first layers of the two columns process the word representations of the source sequence in different directions .,system description,Network,0,142,100,63,0,system description : Network,0.4536741214057508,0.8333333333333334,0.7590361445783133
machine-translation,3,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",system description,Network,0,143,101,64,0,system description : Network,0.45686900958466453,0.8416666666666667,0.7710843373493976
machine-translation,3,The group size is the same as the length of the input sequence .,system description,Network,0,144,102,65,0,system description : Network,0.46006389776357826,0.85,0.7831325301204819
machine-translation,3,Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,system description,Network,0,145,103,66,0,system description : Network,0.46325878594249204,0.8583333333333333,0.7951807228915663
machine-translation,3,"In our work , as a consequence of the introduced F - F connections , we have 4 output vectors ( h For Deep - Att , we do not need the above two operations .",system description,Network,0,146,104,67,0,system description : Network,0.46645367412140576,0.8666666666666667,0.8072289156626506
machine-translation,3,"We only concatenate the 4 output vectors at each time step to obtain e t , and a soft attention mechanism is used to calculate the final representation ct from e t .",system description,Network,0,147,105,68,0,system description : Network,0.4696485623003195,0.875,0.8192771084337349
machine-translation,3,t is summarized as :,system description,Network,0,148,106,69,0,system description : Network,0.4728434504792332,0.8833333333333333,0.8313253012048193
machine-translation,3,Note that the vector dimensionality off is four times larger than that of h ( see Eq. 4 ) .,system description,Network,0,149,107,70,0,system description : Network,0.476038338658147,0.8916666666666667,0.8433734939759037
machine-translation,3,ct is summarized as :,system description,Network,0,150,108,71,0,system description : Network,0.4792332268370607,0.9,0.8554216867469879
machine-translation,3,"t,t is the normalized attention weight computed by :",system description,Network,0,151,109,72,0,system description : Network,0.48242811501597443,0.9083333333333333,0.8674698795180723
machine-translation,3,"t,t is the normalized attention weight computed by :",system description,Network,0,152,110,73,0,system description : Network,0.48562300319488816,0.9166666666666666,0.8795180722891566
machine-translation,3,"the concatenated vector e t to a vector with 1 / 4 dimension size , denoted by the ( fully connected ) block "" fc "" in .",system description,Network,0,153,111,74,0,system description : Network,0.48881789137380194,0.925,0.891566265060241
machine-translation,3,Decoder :,system description,Network,0,154,112,75,0,system description : Network,0.49201277955271566,0.9333333333333333,0.9036144578313253
machine-translation,3,The decoder follows Eq. 5 and Eq. 6 with fixed direction term ? 1 .,system description,Network,0,155,113,76,0,system description : Network,0.4952076677316294,0.9416666666666667,0.9156626506024096
machine-translation,3,"At the first layer , we use the following x t :",system description,Network,0,156,114,77,0,system description : Network,0.4984025559105431,0.95,0.927710843373494
machine-translation,3,t?1 is the target word embedding at the previous time step and y 0 is zero .,system description,Network,0,157,115,78,0,system description : Network,0.5015974440894568,0.9583333333333334,0.9397590361445783
machine-translation,3,There is a single column of n d stacked LSTM layers .,system description,Network,0,158,116,79,0,system description : Network,0.5047923322683706,0.9666666666666667,0.9518072289156626
machine-translation,3,We also use the F - F connections like those in the encoder and all layers are in the forward direction .,system description,Network,0,159,117,80,0,system description : Network,0.5079872204472844,0.975,0.963855421686747
machine-translation,3,"Note that at the last LSTM layer , we only use ht to make the prediction with a softmax layer .",system description,Network,0,160,118,81,0,system description : Network,0.5111821086261981,0.9833333333333333,0.9759036144578314
machine-translation,3,"Although the network is deep , the training technique is straightforward .",system description,Network,0,161,119,82,0,system description : Network,0.5143769968051118,0.9916666666666667,0.9879518072289156
machine-translation,3,We will describe this in the next part .,system description,Network,0,162,120,83,0,system description : Network,0.5175718849840255,1.0,1.0
machine-translation,3,Training technique,training,Training technique,0,163,1,1,0,training : Training technique,0.5207667731629393,0.047619047619047616,0.07692307692307693
machine-translation,3,We take the parallel data as the only input without using any monolingual data for either word representation pre-training or language modeling .,training,Training technique,0,164,2,2,0,training : Training technique,0.5239616613418531,0.09523809523809523,0.15384615384615385
machine-translation,3,"Because of the deep bi-directional structure , we do not need to reverse the sequence order as .",training,Training technique,0,165,3,3,0,training : Training technique,0.5271565495207667,0.14285714285714285,0.23076923076923078
machine-translation,3,"The deep topology brings difficulties for the model training , especially when first order methods such as stochastic gradient descent ( SGD ) are used .",training,Training technique,0,166,4,4,0,training : Training technique,0.5303514376996805,0.19047619047619047,0.3076923076923077
machine-translation,3,The parameters should be properly initialized and the converging process can be slow .,training,Training technique,0,167,5,5,0,training : Training technique,0.5335463258785943,0.23809523809523808,0.38461538461538464
machine-translation,3,"We tried several optimization techniques such as AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman and and .",training,Training technique,0,168,6,6,0,training : Training technique,0.536741214057508,0.2857142857142857,0.46153846153846156
machine-translation,3,We found that all of them were able to speedup the process a lot compared to simple SGD while no significant performance difference was observed among them .,training,Training technique,0,169,7,7,0,training : Training technique,0.5399361022364217,0.3333333333333333,0.5384615384615384
machine-translation,3,"In this work , we chose Adam for model training and do not present a detailed comparison with other optimization methods .",training,Training technique,0,170,8,8,0,training : Training technique,0.5431309904153354,0.38095238095238093,0.6153846153846154
machine-translation,3,Dropout is also used to avoid over-fitting .,training,Training technique,0,171,9,9,0,training : Training technique,0.5463258785942492,0.42857142857142855,0.6923076923076923
machine-translation,3,It is utilized on the LSTM nodes h kt ( See Eq. 5 ) with a ratio of pd for both the encoder and decoder .,training,Training technique,0,172,10,10,0,training : Training technique,0.549520766773163,0.47619047619047616,0.7692307692307693
machine-translation,3,"During the whole model training process , we keep all hyper parameters fixed without any intermediate interruption .",training,Training technique,0,173,11,11,0,training : Training technique,0.5527156549520766,0.5238095238095238,0.8461538461538461
machine-translation,3,The hyper parameters are selected according to the performance on the development set .,training,Training technique,0,174,12,12,0,training : Training technique,0.5559105431309904,0.5714285714285714,0.9230769230769231
machine-translation,3,"For such a deep and large network , it is not easy to determine the tuning strategy and this will be considered in future work .",training,Training technique,0,175,13,13,0,training : Training technique,0.5591054313099042,0.6190476190476191,1.0
machine-translation,3,Generation,training,Generation,0,176,14,1,0,training : Generation,0.5623003194888179,0.6666666666666666,0.125
machine-translation,3,We use the common left - to - right beam - search method for sequence generation .,training,Generation,0,177,15,2,0,training : Generation,0.5654952076677316,0.7142857142857143,0.25
machine-translation,3,"At each time step t , the wordy t can be predicted by :",training,Generation,0,178,16,3,0,training : Generation,0.5686900958466453,0.7619047619047619,0.375
machine-translation,3,"where ? t is the predicted target word .? 0:t ? 1 is the generated sequence from time step 0 tot ? 1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",training,Generation,0,179,17,4,0,training : Generation,0.5718849840255591,0.8095238095238095,0.5
machine-translation,3,"where ? t is the predicted target word .? 0:t ? 1 is the generated sequence from time step 0 tot ? 1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",training,Generation,0,180,18,5,0,training : Generation,0.5750798722044729,0.8571428571428571,0.625
machine-translation,3,"where ? t is the predicted target word .? 0:t ? 1 is the generated sequence from time step 0 tot ? 1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",training,Generation,0,181,19,6,0,training : Generation,0.5782747603833865,0.9047619047619048,0.75
machine-translation,3,"where ? t is the predicted target word .? 0:t ? 1 is the generated sequence from time step 0 tot ? 1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",training,Generation,0,182,20,7,0,training : Generation,0.5814696485623003,0.9523809523809523,0.875
machine-translation,3,"The hypotheses are ranked by the total likelihood of the generated sequence , although normalized likelihood is used in some works .",training,Generation,0,183,21,8,0,training : Generation,0.5846645367412141,1.0,1.0
machine-translation,3,Experiments,experiment,Experiments,0,184,1,1,0,experiment : Experiments,0.5878594249201278,0.25,0.25
machine-translation,3,We evaluate our method mainly on the widely used WMT ' 14 English - to - French translation task .,experiment,Experiments,0,185,2,2,0,experiment : Experiments,0.5910543130990416,0.5,0.5
machine-translation,3,"In order to validate our model on more difficult language pairs , we also provide results on the WMT ' 14 English - to - German translation task .",experiment,Experiments,0,186,3,3,0,experiment : Experiments,0.5942492012779552,0.75,0.75
machine-translation,3,Our models are implemented in the PADDLE ( PArallel Distributed Deep LEarning ) platform .,experiment,Experiments,0,187,4,4,0,experiment : Experiments,0.597444089456869,1.0,1.0
machine-translation,3,Data sets,data set,Data sets,0,188,1,1,0,data set : Data sets,0.6006389776357828,0.07692307692307693,0.07692307692307693
machine-translation,3,"For both tasks , we use the full WMT ' 14 parallel corpus as our training data .",data set,Data sets,0,189,2,2,0,data set : Data sets,0.6038338658146964,0.15384615384615385,0.15384615384615385
machine-translation,3,The detailed data sets are listed below :,data set,Data sets,0,190,3,3,0,data set : Data sets,0.6070287539936102,0.23076923076923078,0.23076923076923078
machine-translation,3,"English - to - French : Europarl v7 , Common Crawl , UN , News Commentary , Gigaword",data set,Data sets,0,191,4,4,0,data set : Data sets,0.610223642172524,0.3076923076923077,0.3076923076923077
machine-translation,3,"English - to - German : Europarl v7 , Common Crawl , News Commentary",data set,Data sets,0,192,5,5,0,data set : Data sets,0.6134185303514377,0.38461538461538464,0.38461538461538464
machine-translation,3,"In total , the English - to - French corpus includes 36 million sentence pairs , and the English - to - German corpus includes 4.5 million sentence pairs .",data set,Data sets,0,193,6,6,0,data set : Data sets,0.6166134185303515,0.46153846153846156,0.46153846153846156
machine-translation,3,"The news - test - 2012 and news - test - 2013 are concatenated as our development set , and the news - test - 2014 is the test set .",data set,Data sets,0,194,7,7,0,data set : Data sets,0.6198083067092651,0.5384615384615384,0.5384615384615384
machine-translation,3,Our data partition is consistent with previous works on NMT to ensure fair comparison .,data set,Data sets,0,195,8,8,0,data set : Data sets,0.6230031948881789,0.6153846153846154,0.6153846153846154
machine-translation,3,"For the source language , we select the most frequent 200K words as the input vocabulary .",data set,Data sets,0,196,9,9,0,data set : Data sets,0.6261980830670927,0.6923076923076923,0.6923076923076923
machine-translation,3,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,data set,Data sets,0,197,10,10,0,data set : Data sets,0.6293929712460063,0.7692307692307693,0.7692307692307693
machine-translation,3,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",data set,Data sets,0,198,11,11,0,data set : Data sets,0.6325878594249201,0.8461538461538461,0.8461538461538461
machine-translation,3,Out - of - vocabulary words are replaced with the unknown symbol unk .,data set,Data sets,0,199,12,12,0,data set : Data sets,0.6357827476038339,0.9230769230769231,0.9230769230769231
machine-translation,3,"For complete comparison to previous work on the Englishto - French task , we also show the results with a smaller vocabulary of 30K input words and 30 K output words on the sub train set with selected 12M parallel sequences .",data set,Data sets,0,200,13,13,0,data set : Data sets,0.6389776357827476,1.0,1.0
machine-translation,3,Model settings,model,Model settings,0,201,1,1,0,model : Model settings,0.6421725239616614,0.03125,0.09090909090909091
machine-translation,3,"We have two models as described above , named Deep - ED and Deep - Att.",model,Model settings,0,202,2,2,0,model : Model settings,0.645367412140575,0.0625,0.18181818181818182
machine-translation,3,Both models have exactly the same configuration and layer size except the interface part P - I.,model,Model settings,0,203,3,3,0,model : Model settings,0.6485623003194888,0.09375,0.2727272727272727
machine-translation,3,We use 256 dimensional word embeddings for both the source and target languages .,model,Model settings,1,204,4,4,0,model : Model settings,0.6517571884984026,0.125,0.36363636363636365
machine-translation,3,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .",model,Model settings,1,205,5,5,0,model : Model settings,0.6549520766773163,0.15625,0.45454545454545453
machine-translation,3,The output layer size is the same as the size of the target vocabulary .,model,Model settings,0,206,6,6,0,model : Model settings,0.65814696485623,0.1875,0.5454545454545454
machine-translation,3,The dimension of ct is 5120 and 1280 for Deep - ED and Deep - Att respectively .,model,Model settings,0,207,7,7,0,model : Model settings,0.6613418530351438,0.21875,0.6363636363636364
machine-translation,3,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",model,Model settings,1,208,8,8,0,model : Model settings,0.6645367412140575,0.25,0.7272727272727273
machine-translation,3,Our network is narrow on word embeddings and LSTM layers .,model,Model settings,0,209,9,9,0,model : Model settings,0.6677316293929713,0.28125,0.8181818181818182
machine-translation,3,"Note that in previous work , 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used .",model,Model settings,0,210,10,10,0,model : Model settings,0.670926517571885,0.3125,0.9090909090909091
machine-translation,3,We also tried larger scale models but did not obtain further improvements .,model,Model settings,0,211,11,11,0,model : Model settings,0.6741214057507987,0.34375,1.0
machine-translation,3,Optimization,model,Optimization,0,212,12,1,0,model : Optimization,0.6773162939297125,0.375,0.047619047619047616
machine-translation,3,"Note that each LSTM layer includes two parts as described in Eq. 3 , feed - forward computation and recurrent computation .",model,Optimization,0,213,13,2,0,model : Optimization,0.6805111821086262,0.40625,0.09523809523809523
machine-translation,3,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",model,Optimization,1,214,14,3,0,model : Optimization,0.6837060702875399,0.4375,0.14285714285714285
machine-translation,3,Word embeddings and the softmax layer also use this learning rate l f .,model,Optimization,0,215,15,4,0,model : Optimization,0.6869009584664537,0.46875,0.19047619047619047
machine-translation,3,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,model,Optimization,0,216,16,5,0,model : Optimization,0.6900958466453674,0.5,0.23809523809523808
machine-translation,3,"Because of the large model size , we use strong L 2 regularization to constrain the parameter matrix v in the following way :",model,Optimization,0,217,17,6,0,model : Optimization,0.6932907348242812,0.53125,0.2857142857142857
machine-translation,3,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",model,Optimization,0,218,18,7,0,model : Optimization,0.6964856230031949,0.5625,0.3333333333333333
machine-translation,3,The two embedding layers are not regularized .,model,Optimization,0,219,19,8,0,model : Optimization,0.6996805111821086,0.59375,0.38095238095238093
machine-translation,3,All the other layers have the same r = 2 .,model,Optimization,0,220,20,9,0,model : Optimization,0.7028753993610224,0.625,0.42857142857142855
machine-translation,3,The parameters of the recurrent computation part are initialized to zero .,model,Optimization,0,221,21,10,0,model : Optimization,0.7060702875399361,0.65625,0.47619047619047616
machine-translation,3,All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07 .,model,Optimization,0,222,22,11,0,model : Optimization,0.7092651757188498,0.6875,0.5238095238095238
machine-translation,3,detailed guide for setting hyperparameters can be found in .,model,Optimization,0,223,23,12,0,model : Optimization,0.7124600638977636,0.71875,0.5714285714285714
machine-translation,3,The dropout ratio pd is 0.1 .,model,Optimization,1,224,24,13,0,model : Optimization,0.7156549520766773,0.75,0.6190476190476191
machine-translation,3,"In each batch , there are 500 ? 800 sequences in our work .",model,Optimization,0,225,25,14,0,model : Optimization,0.7188498402555911,0.78125,0.6666666666666666
machine-translation,3,The exact number depends on the sequence lengths and model size .,model,Optimization,0,226,26,15,0,model : Optimization,0.7220447284345048,0.8125,0.7142857142857143
machine-translation,3,We also find that larger batch size results in better convergence although the improvement is not large .,model,Optimization,0,227,27,16,0,model : Optimization,0.7252396166134185,0.84375,0.7619047619047619
machine-translation,3,"However , the largest batch size is constrained by the GPU memory .",model,Optimization,0,228,28,17,0,model : Optimization,0.7284345047923323,0.875,0.8095238095238095
machine-translation,3,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,model,Optimization,1,229,29,18,0,model : Optimization,0.731629392971246,0.90625,0.8571428571428571
machine-translation,3,It takes nearly 1.5 days for each pass .,model,Optimization,0,230,30,19,0,model : Optimization,0.7348242811501597,0.9375,0.9047619047619048
machine-translation,3,One thing we want to emphasize here is that our deep model is not sensitive to these settings .,model,Optimization,0,231,31,20,0,model : Optimization,0.7380191693290735,0.96875,0.9523809523809523
machine-translation,3,Small variation does not affect the final performance .,model,Optimization,0,232,32,21,0,model : Optimization,0.7412140575079872,1.0,1.0
machine-translation,3,Results,result,Results,0,233,1,1,0,result : Results,0.744408945686901,0.25,0.25
machine-translation,3,We evaluate the same way as previous NMT works .,result,Results,0,234,2,2,0,result : Results,0.7476038338658147,0.5,0.5
machine-translation,3,All reported BLEU scores are computed with the multi-bleu. perl 1 script which is also used in the above works .,result,Results,0,235,3,3,0,result : Results,0.7507987220447284,0.75,0.75
machine-translation,3,The results are for tokenized and case sensitive evaluation .,result,Results,0,236,4,4,0,result : Results,0.7539936102236422,1.0,1.0
machine-translation,3,Single models,model,Single models,1,237,1,1,0,model : Single models,0.7571884984025559,0.06666666666666667,0.06666666666666667
machine-translation,3,English - to - French : First we list our single model results on the English - to - French task in Tab .,model,Single models,1,238,2,2,0,model : Single models,0.7603833865814696,0.13333333333333333,0.13333333333333333
machine-translation,3,. In the first block we show the results with the full corpus .,model,Single models,0,239,3,3,0,model : Single models,0.7635782747603834,0.2,0.2
machine-translation,3,The previous best single NMT encoderdecoder model ( Enc - Dec ) with six layers achieves BLEU = 31.5 .,model,Single models,0,240,4,4,0,model : Single models,0.7667731629392971,0.26666666666666666,0.26666666666666666
machine-translation,3,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .",model,Single models,1,241,5,5,0,model : Single models,0.7699680511182109,0.3333333333333333,0.3333333333333333
machine-translation,3,"This result is even better than the ensemble result of eight Enc - Dec models , which is 35.6 .",model,Single models,0,242,6,6,0,model : Single models,0.7731629392971247,0.4,0.4
machine-translation,3,"This shows that , in addition to the convolutional layers for computer vision , deep topologies can also work for LSTM layers .",model,Single models,0,243,7,7,0,model : Single models,0.7763578274760383,0.4666666666666667,0.4666666666666667
machine-translation,3,"For Deep - Att , the performance is further improved to 37.7 .",model,Single models,1,244,8,8,0,model : Single models,0.7795527156549521,0.5333333333333333,0.5333333333333333
machine-translation,3,We also list the previous state - of - the - art performance from a conventional SMT system with the BLEU of 37.0 .,model,Single models,0,245,9,9,0,model : Single models,0.7827476038338658,0.6,0.6
machine-translation,3,This is the first time that a single NMT model trained in an end - to - end form beats the best conventional system on this task .,model,Single models,0,246,10,10,0,model : Single models,0.7859424920127795,0.6666666666666666,0.6666666666666666
machine-translation,3,We also show the results on the smaller data set with 12M sentence pairs and 30 K vocabulary in the second block .,model,Single models,0,247,11,11,0,model : Single models,0.7891373801916933,0.7333333333333333,0.7333333333333333
machine-translation,3,"The two attention models , RNNsearch and RNNsearch - LV , achieve BLEU scores of 28.5 and 32.7 respectively .",model,Single models,0,248,12,12,0,model : Single models,0.792332268370607,0.8,0.8
machine-translation,3,Note that RNNsearch - LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch .,model,Single models,0,249,13,13,0,model : Single models,0.7955271565495208,0.8666666666666667,0.8666666666666667
machine-translation,3,We obtain BLEU = 35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points .,model,Single models,0,250,14,14,0,model : Single models,0.7987220447284346,0.9333333333333333,0.9333333333333333
machine-translation,3,The SMT result from is also listed and falls behind our model by 2.6 BLEU points .,model,Single models,0,251,15,15,0,model : Single models,0.8019169329073482,1.0,1.0
machine-translation,3,Methods,method,Methods,0,252,1,1,0,method : Methods,0.805111821086262,0.05,0.3333333333333333
machine-translation,3,Data Voc BLEU RNNsearch 4.5M 50K 16.5 RNNsearch-LV 4.5M 500K 16.9 SMT 4.5 M Full 20.7 Deep - Att ( Ours ) 4.5M 160K 20.6 : English - to - German task : BLEU scores of single neural models .,method,Methods,0,253,2,2,0,method : Methods,0.8083067092651757,0.1,0.6666666666666666
machine-translation,3,We also list the conventional SMT system for comparison .,method,Methods,0,254,3,3,0,method : Methods,0.8115015974440895,0.15,1.0
machine-translation,3,Post processing,method,Post processing,0,255,4,1,0,method : Post processing,0.8146964856230032,0.2,0.058823529411764705
machine-translation,3,Two post processing techniques are used to improve the performance further on the English - to - French task .,method,Post processing,0,256,5,2,0,method : Post processing,0.8178913738019169,0.25,0.11764705882352941
machine-translation,3,"First , three Deep - Att models are built for ensemble results .",method,Post processing,0,257,6,3,0,method : Post processing,0.8210862619808307,0.3,0.17647058823529413
machine-translation,3,"They are initialized with different random parameters ; in addition , the training corpus for these models is shuffled with different random seeds .",method,Post processing,0,258,7,4,0,method : Post processing,0.8242811501597445,0.35,0.23529411764705882
machine-translation,3,We sum over the predicted probabilities of the target words and normalize the final distribution to generate the next word .,method,Post processing,0,259,8,5,0,method : Post processing,0.8274760383386581,0.4,0.29411764705882354
machine-translation,3,It is shown in Tab .,method,Post processing,0,260,9,6,0,method : Post processing,0.8306709265175719,0.45,0.35294117647058826
machine-translation,3,that the model ensemble can improve the performance further to 38.9 .,method,Post processing,0,261,10,7,0,method : Post processing,0.8338658146964856,0.5,0.4117647058823529
machine-translation,3,"In and there are eight models for the best scores , but we only use three models and we do not obtain further gain from more models . : BLEU scores of different models .",method,Post processing,0,262,11,8,0,method : Post processing,0.8370607028753994,0.55,0.47058823529411764
machine-translation,3,The first two blocks are our results of two single models and models with post processing .,method,Post processing,0,263,12,9,0,method : Post processing,0.8402555910543131,0.6,0.5294117647058824
machine-translation,3,In the last block we list two baselines of the best conventional SMT system and NMT system .,method,Post processing,0,264,13,10,0,method : Post processing,0.8434504792332268,0.65,0.5882352941176471
machine-translation,3,"Second , we recover the unknown words in the generated sequences with the Positional Unknown ( Pos Unk ) model introduced in .",method,Post processing,0,265,14,11,0,method : Post processing,0.8466453674121406,0.7,0.6470588235294118
machine-translation,3,The full parallel corpus is used to obtain the word mappings .,method,Post processing,0,266,15,12,0,method : Post processing,0.8498402555910544,0.75,0.7058823529411765
machine-translation,3,"We find this method provides an additional 1.5 BLEU points , which is consistent with the conclusion in .",method,Post processing,0,267,16,13,0,method : Post processing,0.853035143769968,0.8,0.7647058823529411
machine-translation,3,We obtain the new BLEU score of 39.2 with a single Deep - Att model .,method,Post processing,0,268,17,14,0,method : Post processing,0.8562300319488818,0.85,0.8235294117647058
machine-translation,3,"For the ensemble models of Deep - Att , the BLEU score rises to 40.4 .",method,Post processing,0,269,18,15,0,method : Post processing,0.8594249201277955,0.9,0.8823529411764706
machine-translation,3,"In the last two lines , we list the conventional SMT model and the previous best neural models based system Enc - Dec for comparison .",method,Post processing,0,270,19,16,0,method : Post processing,0.8626198083067093,0.95,0.9411764705882353
machine-translation,3,We find our best score outperforms the previous best score by nearly 3 points .,method,Post processing,0,271,20,17,0,method : Post processing,0.865814696485623,1.0,1.0
machine-translation,3,Analysis,analysis,Analysis,0,272,1,1,0,analysis : Analysis,0.8690095846645367,0.034482758620689655,1.0
machine-translation,3,Length,analysis,Length,0,273,2,1,0,analysis : Length,0.8722044728434505,0.06896551724137931,0.16666666666666666
machine-translation,3,"On the English - to - French task , we analyze the effect of the source sentence length on our models as shown in .",analysis,Length,0,274,3,2,0,analysis : Length,0.8753993610223643,0.10344827586206896,0.3333333333333333
machine-translation,3,"Here we show five curves : our Deep - Att single model , our Deep - Att ensemble model , our Deep - ED model , a previously proposed Enc - Dec model with four layers and an SMT model .",analysis,Length,0,275,4,3,0,analysis : Length,0.8785942492012779,0.13793103448275862,0.5
machine-translation,3,We find our Deep - Att model works better than the previous two models ( Enc - Dec and SMT ) on nearly all sentence lengths .,analysis,Length,0,276,5,4,0,analysis : Length,0.8817891373801917,0.1724137931034483,0.6666666666666666
machine-translation,3,"It is also shown that for very long sequences with length over 70 words , the performance of our Deep - Att does not degrade , when compared to another NMT model Enc - Dec.",analysis,Length,0,277,6,5,0,analysis : Length,0.8849840255591054,0.20689655172413793,0.8333333333333334
machine-translation,3,"Our Deep - ED also has much better performance than the shallow Enc - Dec model on nearly all lengths , although for long sequences it degrades and starts to fall behind Deep - Att .",analysis,Length,0,278,7,6,0,analysis : Length,0.8881789137380192,0.2413793103448276,1.0
machine-translation,3,Unknown words,analysis,Unknown words,0,279,8,1,0,analysis : Unknown words,0.8913738019169329,0.27586206896551724,0.09090909090909091
machine-translation,3,Next we look into the detail of the effect of unknown words on the English - to - French task .,analysis,Unknown words,0,280,9,2,0,analysis : Unknown words,0.8945686900958466,0.3103448275862069,0.18181818181818182
machine-translation,3,We select the subset without unknown words on target sentences from the original test set .,analysis,Unknown words,0,281,10,3,0,analysis : Unknown words,0.8977635782747604,0.3448275862068966,0.2727272727272727
machine-translation,3,There are 1705 such sentences ( 56.8 % ) .,analysis,Unknown words,0,282,11,4,0,analysis : Unknown words,0.9009584664536742,0.3793103448275862,0.36363636363636365
machine-translation,3,We compute the BLEU scores on this subset and the results are shown in Tab .,analysis,Unknown words,0,283,12,5,0,analysis : Unknown words,0.9041533546325878,0.41379310344827586,0.45454545454545453
machine-translation,3,. We also list the results from SMT model the score 37.7 on the full test set .,analysis,Unknown words,0,284,13,6,0,analysis : Unknown words,0.9073482428115016,0.4482758620689655,0.5454545454545454
machine-translation,3,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",analysis,Unknown words,0,285,14,7,0,analysis : Unknown words,0.9105431309904153,0.4827586206896552,0.6363636363636364
machine-translation,3,This suggests that the difficulty on this subset is not much different from that on the full set .,analysis,Unknown words,0,286,15,8,0,analysis : Unknown words,0.9137380191693291,0.5172413793103449,0.7272727272727273
machine-translation,3,We therefore attribute the larger gap for Deep - att to the existence of unknown words .,analysis,Unknown words,0,287,16,9,0,analysis : Unknown words,0.9169329073482428,0.5517241379310345,0.8181818181818182
machine-translation,3,We also compute the BLEU score on the subset of the ensemble model and obtain 41.4 .,analysis,Unknown words,0,288,17,10,0,analysis : Unknown words,0.9201277955271565,0.5862068965517241,0.9090909090909091
machine-translation,3,"As a reference related to human performance , in , it has been tested that the BLEU score of oracle re-scoring the LIUM 1000 - best results is 45 .",analysis,Unknown words,0,289,18,11,0,analysis : Unknown words,0.9233226837060703,0.6206896551724138,1.0
machine-translation,3,Over-fitting,analysis,Over-fitting,0,290,19,1,0,analysis : Over-fitting,0.9265175718849841,0.6551724137931034,0.09090909090909091
machine-translation,3,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .",analysis,Over-fitting,0,291,20,2,0,analysis : Over-fitting,0.9297124600638977,0.6896551724137931,0.18181818181818182
machine-translation,3,"However , our experimental results suggest that deep models are less prone to the problem of over-fitting .",analysis,Over-fitting,0,292,21,3,0,analysis : Over-fitting,0.9329073482428115,0.7241379310344828,0.2727272727272727
machine-translation,3,"In , we show three results from models with a different depth on the English - to - French task .",analysis,Over-fitting,0,293,22,4,0,analysis : Over-fitting,0.9361022364217252,0.7586206896551724,0.36363636363636365
machine-translation,3,"These three models are evaluated by token error rate , which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input .",analysis,Over-fitting,0,294,23,5,0,analysis : Over-fitting,0.939297124600639,0.7931034482758621,0.45454545454545453
machine-translation,3,The curve with square marks corresponds to Deep - Att with n e = 9 and n d = 7 .,analysis,Over-fitting,0,295,24,6,0,analysis : Over-fitting,0.9424920127795527,0.8275862068965517,0.5454545454545454
machine-translation,3,The curve with circle marks corresponds ton e = 5 and n d = 3 .,analysis,Over-fitting,0,296,25,7,0,analysis : Over-fitting,0.9456869009584664,0.8620689655172413,0.6363636363636364
machine-translation,3,The curve with triangle marks corresponds ton e = 1 and n d = 1 .,analysis,Over-fitting,0,297,26,8,0,analysis : Over-fitting,0.9488817891373802,0.896551724137931,0.7272727272727273
machine-translation,3,We find that the deep model has better performance on the test set when the token error rate is the same as that of the shallow models on the training set .,analysis,Over-fitting,0,298,27,9,0,analysis : Over-fitting,0.952076677316294,0.9310344827586207,0.8181818181818182
machine-translation,3,"This shows that , with decreased token error rate , the deep model is more advantageous in avoiding the over - fitting phenomenon .",analysis,Over-fitting,0,299,28,10,0,analysis : Over-fitting,0.9552715654952076,0.9655172413793104,0.9090909090909091
machine-translation,3,"We only plot the early training stage curves because , during the late training stage , the curves are not smooth .",analysis,Over-fitting,0,300,29,11,0,analysis : Over-fitting,0.9584664536741214,1.0,1.0
machine-translation,3,Conclusion,conclusion,Conclusion,0,301,1,1,0,conclusion : Conclusion,0.9616613418530351,0.07692307692307693,0.07692307692307693
machine-translation,3,"With the introduction of fast - forward connections to the deep LSTM network , we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom .",conclusion,Conclusion,0,302,2,2,0,conclusion : Conclusion,0.9648562300319489,0.15384615384615385,0.15384615384615385
machine-translation,3,"On this path , gradients decay much slower compared to the standard deep network .",conclusion,Conclusion,0,303,3,3,0,conclusion : Conclusion,0.9680511182108626,0.23076923076923078,0.23076923076923078
machine-translation,3,This enables us to build the deep topology of NMT models .,conclusion,Conclusion,0,304,4,4,0,conclusion : Conclusion,0.9712460063897763,0.3076923076923077,0.3076923076923077
machine-translation,3,We trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on the WMT ' 14 English - to - French translation task .,conclusion,Conclusion,0,305,5,5,0,conclusion : Conclusion,0.9744408945686901,0.38461538461538464,0.38461538461538464
machine-translation,3,This is the deepest topology that has been investigated in the NMT are a on this task .,conclusion,Conclusion,0,306,6,6,0,conclusion : Conclusion,0.9776357827476039,0.46153846153846156,0.46153846153846156
machine-translation,3,"We showed that our Deep - Att exhibits 6.2 BLEU points improvement over the previous best single model , achieving a 37.7 BLEU score .",conclusion,Conclusion,0,307,7,7,0,conclusion : Conclusion,0.9808306709265175,0.5384615384615384,0.5384615384615384
machine-translation,3,This single end - toend NMT model outperforms the best conventional SMT system and achieves a state - of - the - art performance .,conclusion,Conclusion,0,308,8,8,0,conclusion : Conclusion,0.9840255591054313,0.6153846153846154,0.6153846153846154
machine-translation,3,"After utilizing unknown word processing and model ensemble of three models , we obtained a BLEU score of 40.4 , an improvement of 2.9 BLEU points over the previous best result .",conclusion,Conclusion,0,309,9,9,0,conclusion : Conclusion,0.987220447284345,0.6923076923076923,0.6923076923076923
machine-translation,3,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",conclusion,Conclusion,0,310,10,10,0,conclusion : Conclusion,0.9904153354632588,0.7692307692307693,0.7692307692307693
machine-translation,3,Our model is also validated on the more difficult English - to - German task .,conclusion,Conclusion,0,311,11,11,0,conclusion : Conclusion,0.9936102236421726,0.8461538461538461,0.8461538461538461
machine-translation,3,Our model is also efficient in sequence generation .,conclusion,Conclusion,0,312,12,12,0,conclusion : Conclusion,0.9968051118210862,0.9230769230769231,0.9230769230769231
machine-translation,3,"The best results from both a single model and model ensemble are obtained with a beam size of 3 , much smaller than previous NMT systems where beam size is about 12",conclusion,Conclusion,0,313,13,13,0,conclusion : Conclusion,1.0,1.0,1.0
machine-translation,4,Unsupervised Neural Machine Translation with Weight Sharing,title,title,0,2,1,1,0,title : title,0.008368200836820083,1.0,1.0
machine-translation,4,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.012552301255230125,0.16666666666666666,0.16666666666666666
machine-translation,4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.016736401673640166,0.3333333333333333,0.3333333333333333
machine-translation,4,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",abstract,abstract,1,5,3,3,0,abstract : abstract,0.02092050209205021,0.5,0.5
machine-translation,4,"To address this issue , we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high - level representations of the input sentences .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02510460251046025,0.6666666666666666,0.6666666666666666
machine-translation,4,"Besides , two different generative adversarial networks ( GANs ) , namely the local GAN and global GAN , are proposed to enhance the cross - language translation .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.029288702928870293,0.8333333333333334,0.8333333333333334
machine-translation,4,"With this new approach , we achieve significant improvements on English - German , English - French and Chinese - to - English translation tasks .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03347280334728033,1.0,1.0
machine-translation,4,Introduction,introduction,introduction,0,9,1,1,0,introduction : introduction,0.03765690376569038,0.027777777777777776,0.027777777777777776
machine-translation,4,"Neural machine translation , directly applying a single neural network to transform the source sentence into the target sentence , has now reached impressive performance .",introduction,introduction,0,10,2,2,0,introduction : introduction,0.04184100418410042,0.05555555555555555,0.05555555555555555
machine-translation,4,The NMT typically consists of two sub neural networks .,introduction,introduction,0,11,3,3,0,introduction : introduction,0.04602510460251046,0.08333333333333333,0.08333333333333333
machine-translation,4,"The encoder network reads and encodes the source sentence into a 1 Feng Wang is the corresponding author of this paper context vector , and the decoder network generates the target sentence iteratively based on the context vector .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.0502092050209205,0.1111111111111111,0.1111111111111111
machine-translation,4,NMT can be studied in supervised and unsupervised learning settings .,introduction,introduction,0,13,5,5,0,introduction : introduction,0.05439330543933055,0.1388888888888889,0.1388888888888889
machine-translation,4,"In the supervised setting , bilingual corpora is available for training the NMT model .",introduction,introduction,0,14,6,6,0,introduction : introduction,0.058577405857740586,0.16666666666666666,0.16666666666666666
machine-translation,4,"In the unsupervised setting , we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages .",introduction,introduction,0,15,7,7,0,introduction : introduction,0.06276150627615062,0.19444444444444445,0.19444444444444445
machine-translation,4,"Due to lack of alignment information , the unsupervised NMT is considered more challenging .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.06694560669456066,0.2222222222222222,0.2222222222222222
machine-translation,4,"However , this task is very promising , since the monolingual corpora is usually easy to be collected .",introduction,introduction,0,17,9,9,0,introduction : introduction,0.07112970711297072,0.25,0.25
machine-translation,4,"Motivated by recent success in unsupervised cross - lingual embeddings , the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared - latent space .",introduction,introduction,0,18,10,10,0,introduction : introduction,0.07531380753138076,0.2777777777777778,0.2777777777777778
machine-translation,4,"Following this assumption , use a single encoder and a single decoder for both the source and target languages .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.0794979079497908,0.3055555555555556,0.3055555555555556
machine-translation,4,"The encoder and decoder , acting as a standard auto - encoder ( AE ) , are trained to reconstruct the inputs .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.08368200836820083,0.3333333333333333,0.3333333333333333
machine-translation,4,And utilize a shared encoder but two independent decoders .,introduction,introduction,0,21,13,13,0,introduction : introduction,0.08786610878661087,0.3611111111111111,0.3611111111111111
machine-translation,4,"With some good performance , they share a glaring defect , i.e. , only one encoder is shared by the source and target languages .",introduction,introduction,0,22,14,14,0,introduction : introduction,0.09205020920502092,0.3888888888888889,0.3888888888888889
machine-translation,4,"Although the shared encoder is vital for mapping sentences from different languages into the shared - latent space , it is weak in keeping the uniqueness and internal characteristics of each language , such as the style , terminology and sentence structure .",introduction,introduction,0,23,15,15,0,introduction : introduction,0.09623430962343096,0.4166666666666667,0.4166666666666667
machine-translation,4,"Since each language has its own characteristics , the source and target languages should be encoded and learned independently .",introduction,introduction,0,24,16,16,0,introduction : introduction,0.100418410041841,0.4444444444444444,0.4444444444444444
machine-translation,4,"Therefore , we conjecture that the shared encoder maybe a factor limit - ing the potential translation performance .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.10460251046025104,0.4722222222222222,0.4722222222222222
machine-translation,4,"In order to address this issue , we extend the encoder - shared model , i.e. , the model with one shared encoder , by leveraging two independent encoders with each for one language .",introduction,introduction,1,26,18,18,0,introduction : introduction,0.1087866108786611,0.5,0.5
machine-translation,4,"Similarly , two independent decoders are utilized .",introduction,introduction,1,27,19,19,0,introduction : introduction,0.11297071129707113,0.5277777777777778,0.5277777777777778
machine-translation,4,"For each language , the encoder and its corresponding decoder perform an AE , where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations .",introduction,introduction,1,28,20,20,0,introduction : introduction,0.11715481171548117,0.5555555555555556,0.5555555555555556
machine-translation,4,"To map the latent representations from different languages to a shared - latent space , we propose the weightsharing constraint to the two AEs .",introduction,introduction,1,29,21,21,0,introduction : introduction,0.12133891213389121,0.5833333333333334,0.5833333333333334
machine-translation,4,"Specifically , we share the weights of the last few layers of two encoders thatare responsible for extracting highlevel representations of input sentences .",introduction,introduction,0,30,22,22,0,introduction : introduction,0.12552301255230125,0.6111111111111112,0.6111111111111112
machine-translation,4,"Similarly , we share the weights of the first few layers of two decoders .",introduction,introduction,0,31,23,23,0,introduction : introduction,0.1297071129707113,0.6388888888888888,0.6388888888888888
machine-translation,4,"To enforce the shared - latent space , the word embeddings are used as a reinforced encoding component in our encoders .",introduction,introduction,0,32,24,24,0,introduction : introduction,0.13389121338912133,0.6666666666666666,0.6666666666666666
machine-translation,4,"For cross - language translation , we utilize the backtranslation following .",introduction,introduction,1,33,25,25,0,introduction : introduction,0.13807531380753138,0.6944444444444444,0.6944444444444444
machine-translation,4,"Additionally , two different generative adversarial networks ( GAN ) , namely the local and global GAN , are proposed to further improve the cross - language translation .",introduction,introduction,1,34,26,26,0,introduction : introduction,0.14225941422594143,0.7222222222222222,0.7222222222222222
machine-translation,4,"We utilize the local GAN to constrain the source and target latent representations to have the same distribution , whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation .",introduction,introduction,1,35,27,27,0,introduction : introduction,0.14644351464435146,0.75,0.75
machine-translation,4,"We apply the global GAN to finetune the corresponding generator , i.e. , the composition of the encoder and decoder of the other language , where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 .",introduction,introduction,1,36,28,28,0,introduction : introduction,0.1506276150627615,0.7777777777777778,0.7777777777777778
machine-translation,4,"In summary , we mainly make the following contributions :",introduction,introduction,0,37,29,29,0,introduction : introduction,0.15481171548117154,0.8055555555555556,0.8055555555555556
machine-translation,4,"We propose the weight - sharing constraint to unsupervised NMT , enabling the model to utilize an independent encoder for each language .",introduction,introduction,0,38,30,30,0,introduction : introduction,0.1589958158995816,0.8333333333333334,0.8333333333333334
machine-translation,4,"To enforce the shared - latent space , we also propose the embedding - reinforced encoders and two different GANs for our model .",introduction,introduction,0,39,31,31,0,introduction : introduction,0.16317991631799164,0.8611111111111112,0.8611111111111112
machine-translation,4,We conduct extensive experiments on The code that we utilized to train and evaluate our models can be found at https://github.com/ZhenYangIACAS/unsupervised-NMT,introduction,introduction,0,40,32,32,0,introduction : introduction,0.16736401673640167,0.8888888888888888,0.8888888888888888
machine-translation,4,"English - German , English - French and Chinese - to - English translation tasks .",introduction,introduction,0,41,33,33,0,introduction : introduction,0.17154811715481172,0.9166666666666666,0.9166666666666666
machine-translation,4,Experimental results show that the proposed approach consistently achieves great success .,introduction,introduction,0,42,34,34,0,introduction : introduction,0.17573221757322174,0.9444444444444444,0.9444444444444444
machine-translation,4,"Last but not least , we introduce the directional self - attention to model temporal order information for the proposed model .",introduction,introduction,0,43,35,35,0,introduction : introduction,0.1799163179916318,0.9722222222222222,0.9722222222222222
machine-translation,4,Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self - attention layers of NMT .,introduction,introduction,0,44,36,36,0,introduction : introduction,0.18410041841004185,1.0,1.0
machine-translation,4,Related Work,related work,Related Work,0,45,1,1,0,related work : Related Work,0.18828451882845187,0.08333333333333333,0.08333333333333333
machine-translation,4,Several approaches have been proposed to train NMT models without direct parallel corpora .,related work,Related Work,0,46,2,2,0,related work : Related Work,0.19246861924686193,0.16666666666666666,0.16666666666666666
machine-translation,4,The scenario that has been widely investigated is one where two languages have little parallel data between them but are well connected by one pivot language .,related work,Related Work,0,47,3,3,0,related work : Related Work,0.19665271966527198,0.25,0.25
machine-translation,4,The most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language ( Saha et al . The two works mentioned above both use a single shared encoder to guarantee the shared latent space .,related work,Related Work,0,48,4,4,0,related work : Related Work,0.200836820083682,0.3333333333333333,0.3333333333333333
machine-translation,4,The most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language ( Saha et al . The two works mentioned above both use a single shared encoder to guarantee the shared latent space .,related work,Related Work,0,49,5,5,0,related work : Related Work,0.20502092050209206,0.4166666666666667,0.4166666666666667
machine-translation,4,"However , a concomitant defect is that the shared encoder is weak in keeping the uniqueness of each language .",related work,Related Work,0,50,6,6,0,related work : Related Work,0.20920502092050208,0.5,0.5
machine-translation,4,"Our work also belongs to this more ambitious scenario , and to the best of our knowledge , we are one among the first endeavors to investigate how to train an NMT model with monolingual corpora only .",related work,Related Work,0,51,7,7,0,related work : Related Work,0.21338912133891214,0.5833333333333334,0.5833333333333334
machine-translation,4,is the translation in reversed direction .,related work,Related Work,0,52,8,8,0,related work : Related Work,0.2175732217573222,0.6666666666666666,0.6666666666666666
machine-translation,4,l is utilized to assess whether the hidden representation of the encoder is from the source or target language .,related work,Related Work,0,53,9,9,0,related work : Related Work,0.2217573221757322,0.75,0.75
machine-translation,4,g 1 and D g 2 are used to evaluate whether the translated sentences are realistic for each language respectively .,related work,Related Work,0,54,10,10,0,related work : Related Work,0.22594142259414227,0.8333333333333334,0.8333333333333334
machine-translation,4,represents the shared - latent space .,related work,Related Work,0,55,11,11,0,related work : Related Work,0.2301255230125523,0.9166666666666666,0.9166666666666666
machine-translation,4,3,related work,Related Work,0,56,12,12,0,related work : Related Work,0.23430962343096234,1.0,1.0
machine-translation,4,The Approach,approach,approach,0,57,1,1,0,approach : approach,0.2384937238493724,1.0,1.0
machine-translation,4,Model Architecture,model,Model Architecture,0,58,1,1,0,model : Model Architecture,0.24267782426778242,0.025,0.07142857142857142
machine-translation,4,"The model architecture , as illustrated in figure 1 , is based on the AE and GAN .",model,Model Architecture,0,59,2,2,0,model : Model Architecture,0.24686192468619247,0.05,0.14285714285714285
machine-translation,4,"It consists of seven sub networks : including two encoders Enc sand Enc t , two decoders Dec sand Dec t , the local discriminator D l , and the global discriminators D g 1 and D g 2 .",model,Model Architecture,0,60,3,3,0,model : Model Architecture,0.2510460251046025,0.075,0.21428571428571427
machine-translation,4,"For the encoder and decoder , we follow the newly emerged Transformer .",model,Model Architecture,0,61,4,4,0,model : Model Architecture,0.25523012552301255,0.1,0.2857142857142857
machine-translation,4,"Specifically , the encoder is composed of a stack of four identical layers",model,Model Architecture,0,62,5,5,0,model : Model Architecture,0.2594142259414226,0.125,0.35714285714285715
machine-translation,4,2 .,model,Model Architecture,0,63,6,6,0,model : Model Architecture,0.26359832635983266,0.15,0.42857142857142855
machine-translation,4,Each layer consists of a multi-head self - attention and a simple position - wise fully connected feed - forward network .,model,Model Architecture,0,64,7,7,0,model : Model Architecture,0.26778242677824265,0.175,0.5
machine-translation,4,The decoder is also composed of four identical layers .,model,Model Architecture,0,65,8,8,0,model : Model Architecture,0.2719665271966527,0.2,0.5714285714285714
machine-translation,4,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sublayer , which performs multi-head attention over the output of the encoder stack .",model,Model Architecture,0,66,9,9,0,model : Model Architecture,0.27615062761506276,0.225,0.6428571428571429
machine-translation,4,"For more details about the multi-head self - attention layer , we refer the reader to .",model,Model Architecture,0,67,10,10,0,model : Model Architecture,0.2803347280334728,0.25,0.7142857142857143
machine-translation,4,We implement the local discriminator as a multi -layer perceptron and implement the global discriminator based on the convolutional neural network ( CNN ) .,model,Model Architecture,0,68,11,11,0,model : Model Architecture,0.28451882845188287,0.275,0.7857142857142857
machine-translation,4,Several ways exist to interpret the roles of the sub networks are summarised in table,model,Model Architecture,0,69,12,12,0,model : Model Architecture,0.28870292887029286,0.3,0.8571428571428571
machine-translation,4,1 .,model,Model Architecture,0,70,13,13,0,model : Model Architecture,0.2928870292887029,0.325,0.9285714285714286
machine-translation,4,"The proposed system has several striking components , which are critical either for the system to be trained in an 2 The layer number is selected according to our preliminary experiment , which is presented in appendix A. unsupervised manner or for improving the translation performance .",model,Model Architecture,0,71,14,14,0,model : Model Architecture,0.29707112970711297,0.35,1.0
machine-translation,4,Networks,model,Networks,0,72,15,1,0,model : Networks,0.301255230125523,0.375,0.038461538461538464
machine-translation,4,Roles : Interpretation of the roles for the subnetworks in the proposed system .,model,Networks,0,73,16,2,0,model : Networks,0.3054393305439331,0.4,0.07692307692307693
machine-translation,4,Directional self - attention,model,Networks,0,74,17,3,0,model : Networks,0.30962343096234307,0.425,0.11538461538461539
machine-translation,4,"Compared to recurrent neural network , a dis advantage of the simple self - attention mechanism is that the temporal order information is lost .",model,Networks,0,75,18,4,0,model : Networks,0.3138075313807531,0.45,0.15384615384615385
machine-translation,4,"Although the Transformer applies the positional encoding to the sequence before processed by the self - attention , how to model temporal order information within an attention is still an open question .",model,Networks,0,76,19,5,0,model : Networks,0.3179916317991632,0.475,0.19230769230769232
machine-translation,4,"Following , we build the encoders in our model on the directional self - attention which utilizes the positional masks to encode temporal order information into attention output .",model,Networks,0,77,20,6,0,model : Networks,0.32217573221757323,0.5,0.23076923076923078
machine-translation,4,"More concretely , two positional masks , namely the forward mask M f and backward mask Mb , are calculated as :",model,Networks,0,78,21,7,0,model : Networks,0.3263598326359833,0.525,0.2692307692307692
machine-translation,4,"With the forward mask M f , the later token only makes attention connections to the early tokens in the sequence , and vice versa with the backward mask .",model,Networks,0,79,22,8,0,model : Networks,0.3305439330543933,0.55,0.3076923076923077
machine-translation,4,"Similar to , we utilize a self - attention network to process the input sequence in forward direction .",model,Networks,0,80,23,9,0,model : Networks,0.33472803347280333,0.575,0.34615384615384615
machine-translation,4,"The output of this layer is taken by an upper self - attention network as input , processed in the reverse direction .",model,Networks,0,81,24,10,0,model : Networks,0.3389121338912134,0.6,0.38461538461538464
machine-translation,4,Weight sharing,model,Networks,0,82,25,11,0,model : Networks,0.34309623430962344,0.625,0.4230769230769231
machine-translation,4,"Based on the shared - latent space assumption , we apply the weight sharing constraint to relate the two AEs .",model,Networks,0,83,26,12,0,model : Networks,0.3472803347280335,0.65,0.46153846153846156
machine-translation,4,"Specifically , we share the weights of the last few layers of the Enc sand Enc t , which are responsible for extracting high - level representations of the input sentences .",model,Networks,0,84,27,13,0,model : Networks,0.3514644351464435,0.675,0.5
machine-translation,4,"Similarly , we also share the first few layers of the Dec sand Dec t , which are expected to decode high - level representations thatare vital for reconstructing the input sentences .",model,Networks,0,85,28,14,0,model : Networks,0.35564853556485354,0.7,0.5384615384615384
machine-translation,4,"Compared to which use the fully shared encoder , we only share partial weights for the encoders and decoders .",model,Networks,0,86,29,15,0,model : Networks,0.3598326359832636,0.725,0.5769230769230769
machine-translation,4,"In the proposed model , the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language , such as the terminology , style , and sentence structure .",model,Networks,0,87,30,16,0,model : Networks,0.36401673640167365,0.75,0.6153846153846154
machine-translation,4,The shared weights are utilized to map the hidden features extracted by the independent weights to the shared - latent space .,model,Networks,0,88,31,17,0,model : Networks,0.3682008368200837,0.775,0.6538461538461539
machine-translation,4,Embedding reinforced encoder,model,Networks,0,89,32,18,0,model : Networks,0.3723849372384937,0.8,0.6923076923076923
machine-translation,4,We use pretrained cross - lingual embeddings in the encoders thatare kept fixed during training .,model,Networks,0,90,33,19,0,model : Networks,0.37656903765690375,0.825,0.7307692307692307
machine-translation,4,And the fixed embeddings are used as a reinforced encoding component in our encoder .,model,Networks,0,91,34,20,0,model : Networks,0.3807531380753138,0.85,0.7692307692307693
machine-translation,4,"Formally , given the input sequence embedding vectors E = {e 1 , . . . , e t } and the initial output sequence of the encoder stack H = {h 1 , . . . , ht } , we compute H r as :",model,Networks,0,92,35,21,0,model : Networks,0.38493723849372385,0.875,0.8076923076923077
machine-translation,4,"where H r is the final output sequence of the encoder which will be attended by the decoder ( In Transformer , H is the final output of the encoder ) , g is agate unit and computed as :",model,Networks,0,93,36,22,0,model : Networks,0.3891213389121339,0.9,0.8461538461538461
machine-translation,4,"where W 1 , W 2 and bare trainable parameters and they are shared by the two encoders .",model,Networks,0,94,37,23,0,model : Networks,0.39330543933054396,0.925,0.8846153846153846
machine-translation,4,The motivation behind is twofold .,model,Networks,0,95,38,24,0,model : Networks,0.39748953974895396,0.95,0.9230769230769231
machine-translation,4,"Firstly , taking the fixed cross - lingual embedding as the other encoding component is helpful to reinforce the sharedlatent space .",model,Networks,0,96,39,25,0,model : Networks,0.401673640167364,0.975,0.9615384615384616
machine-translation,4,"Additionally , from the point of multichannel encoders , providing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suiting its own linguistic structure .",model,Networks,0,97,40,26,0,model : Networks,0.40585774058577406,1.0,1.0
machine-translation,4,Unsupervised Training,training,Unsupervised Training,0,98,1,1,0,training : Unsupervised Training,0.4100418410041841,0.022727272727272728,0.022727272727272728
machine-translation,4,"Based on the architecture proposed above , we train the NMT model with the monolingual corpora only using the following four strategies :",training,Unsupervised Training,0,99,2,2,0,training : Unsupervised Training,0.41422594142259417,0.045454545454545456,0.045454545454545456
machine-translation,4,Denoising auto - encoding,training,Unsupervised Training,0,100,3,3,0,training : Unsupervised Training,0.41841004184100417,0.06818181818181818,0.06818181818181818
machine-translation,4,"Firstly , we train the two AEs to reconstruct their inputs respectively .",training,Unsupervised Training,0,101,4,4,0,training : Unsupervised Training,0.4225941422594142,0.09090909090909091,0.09090909090909091
machine-translation,4,"In this form , each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language .",training,Unsupervised Training,0,102,5,5,0,training : Unsupervised Training,0.42677824267782427,0.11363636363636363,0.11363636363636363
machine-translation,4,"Nevertheless , without any constraint , the AE quickly learns to merely copy every word one by one , without capturing any internal structure of the language involved .",training,Unsupervised Training,0,103,6,6,0,training : Unsupervised Training,0.4309623430962343,0.13636363636363635,0.13636363636363635
machine-translation,4,"To address this problem , we utilize the same strategy of denoising AE and add some noise to the input sentences .",training,Unsupervised Training,0,104,7,7,0,training : Unsupervised Training,0.4351464435146444,0.1590909090909091,0.1590909090909091
machine-translation,4,"To this end , we shuffle the input sentences randomly .",training,Unsupervised Training,0,105,8,8,0,training : Unsupervised Training,0.4393305439330544,0.18181818181818182,0.18181818181818182
machine-translation,4,"Specifically , we apply a random permutation ? to the input sentence , verifying the condition :",training,Unsupervised Training,0,106,9,9,0,training : Unsupervised Training,0.4435146443514644,0.20454545454545456,0.20454545454545456
machine-translation,4,"Specifically , we apply a random permutation ? to the input sentence , verifying the condition :",training,Unsupervised Training,0,107,10,10,0,training : Unsupervised Training,0.4476987447698745,0.22727272727272727,0.22727272727272727
machine-translation,4,"where n is the length of the input sentence , steps is the global steps the model has been updated , k and s are the tunable parameters which can beset by users beforehand .",training,Unsupervised Training,0,108,11,11,0,training : Unsupervised Training,0.45188284518828453,0.25,0.25
machine-translation,4,"This way , the system needs to learn some useful structure of the involved languages to be able to recover the correct word order .",training,Unsupervised Training,0,109,12,12,0,training : Unsupervised Training,0.4560669456066946,0.2727272727272727,0.2727272727272727
machine-translation,4,"In practice , we set k = 2 and s = 100000 .",training,Unsupervised Training,0,110,13,13,0,training : Unsupervised Training,0.4602510460251046,0.29545454545454547,0.29545454545454547
machine-translation,4,Back - translation,training,Unsupervised Training,0,111,14,14,0,training : Unsupervised Training,0.46443514644351463,0.3181818181818182,0.3181818181818182
machine-translation,4,"In spite of denoising autoencoding , the training procedure still involves a single language at each time , without considering our final goal of mapping an input sentence from the source / target language to the target / source language .",training,Unsupervised Training,0,112,15,15,0,training : Unsupervised Training,0.4686192468619247,0.3409090909090909,0.3409090909090909
machine-translation,4,"For the cross language training , we utilize the back - translation approach for our unsupervised training procedure .",training,Unsupervised Training,0,113,16,16,0,training : Unsupervised Training,0.47280334728033474,0.36363636363636365,0.36363636363636365
machine-translation,4,Back - translation has shown its great effectiveness on improving NMT model with monolingual data and has been widely investigated by .,training,Unsupervised Training,0,114,17,17,0,training : Unsupervised Training,0.4769874476987448,0.38636363636363635,0.38636363636363635
machine-translation,4,"In our approach , given an input sentence in a given language , we apply the corresponding encoder and the decoder of the other language to translate it to the other language 3 .",training,Unsupervised Training,0,115,18,18,0,training : Unsupervised Training,0.4811715481171548,0.4090909090909091,0.4090909090909091
machine-translation,4,"By combining the translation with its original sentence , we get a pseudo - parallel corpus which is utilized to train the model to reconstruct the original sentence from its translation .",training,Unsupervised Training,0,116,19,19,0,training : Unsupervised Training,0.48535564853556484,0.4318181818181818,0.4318181818181818
machine-translation,4,Local GAN,training,Unsupervised Training,0,117,20,20,0,training : Unsupervised Training,0.4895397489539749,0.45454545454545453,0.45454545454545453
machine-translation,4,"Although the weight sharing constraint is vital for the shared - latent space assumption , it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code .",training,Unsupervised Training,0,118,21,21,0,training : Unsupervised Training,0.49372384937238495,0.4772727272727273,0.4772727272727273
machine-translation,4,"To further enforce the shared - latent space , we train a discriminative neural network , referred to as the local discriminator , to classify between the encoding of source sentences and the encoding of target sentences .",training,Unsupervised Training,0,119,22,22,0,training : Unsupervised Training,0.497907949790795,0.5,0.5
machine-translation,4,"The local discriminator , implemented as a multilayer perceptron with two hidden layers of size 256 , takes the output of the encoder , i.e. , H r calculated as equation 3 , as input , and produces a binary prediction about the language of the input sentence .",training,Unsupervised Training,0,120,23,23,0,training : Unsupervised Training,0.502092050209205,0.5227272727272727,0.5227272727272727
machine-translation,4,The local discriminator is trained to predict the language by minimizing the following crossentropy loss :,training,Unsupervised Training,0,121,24,24,0,training : Unsupervised Training,0.5062761506276151,0.5454545454545454,0.5454545454545454
machine-translation,4,"where ? D l represents the parameters of the local discriminator and f ? {s , t}.",training,Unsupervised Training,0,122,25,25,0,training : Unsupervised Training,0.5104602510460251,0.5681818181818182,0.5681818181818182
machine-translation,4,"where ? D l represents the parameters of the local discriminator and f ? {s , t}.",training,Unsupervised Training,0,123,26,26,0,training : Unsupervised Training,0.5146443514644351,0.5909090909090909,0.5909090909090909
machine-translation,4,The encoders are trained to fool the local discriminator :,training,Unsupervised Training,0,124,27,27,0,training : Unsupervised Training,0.5188284518828452,0.6136363636363636,0.6136363636363636
machine-translation,4,where ? Encs and ? Enct are the parameters of the two encoders .,training,Unsupervised Training,0,125,28,28,0,training : Unsupervised Training,0.5230125523012552,0.6363636363636364,0.6363636363636364
machine-translation,4,where ? Encs and ? Enct are the parameters of the two encoders .,training,Unsupervised Training,0,126,29,29,0,training : Unsupervised Training,0.5271966527196653,0.6590909090909091,0.6590909090909091
machine-translation,4,where ? Encs and ? Enct are the parameters of the two encoders .,training,Unsupervised Training,0,127,30,30,0,training : Unsupervised Training,0.5313807531380753,0.6818181818181818,0.6818181818181818
machine-translation,4,Global GAN,training,Unsupervised Training,0,128,31,31,0,training : Unsupervised Training,0.5355648535564853,0.7045454545454546,0.7045454545454546
machine-translation,4,"We apply the global GANs to fine tune the whole model so that the model is able to generate sentences undistinguishable from the true data , i.e. , sentences in the training corpus .",training,Unsupervised Training,0,129,32,32,0,training : Unsupervised Training,0.5397489539748954,0.7272727272727273,0.7272727272727273
machine-translation,4,"Different from the local GANs which updates the parameters of the encoders locally , the global GANs are utilized to update the whole parameters of the proposed model , including the parameters of encoders and decoders .",training,Unsupervised Training,0,130,33,33,0,training : Unsupervised Training,0.5439330543933054,0.75,0.75
machine-translation,4,The proposed model has two global GANs : GAN g 1 and GAN g 2 .,training,Unsupervised Training,0,131,34,34,0,training : Unsupervised Training,0.5481171548117155,0.7727272727272727,0.7727272727272727
machine-translation,4,"In GAN g 1 , the Enc t and Dec s act as the generator , which generates the sentencex t 4 from x t .",training,Unsupervised Training,0,132,35,35,0,training : Unsupervised Training,0.5523012552301255,0.7954545454545454,0.7954545454545454
machine-translation,4,"The D g 1 , implemented based on CNN , assesses whether the generated sentencex t is the true target - language sentence or the generated sentence .",training,Unsupervised Training,0,133,36,36,0,training : Unsupervised Training,0.5564853556485355,0.8181818181818182,0.8181818181818182
machine-translation,4,"The global discriminator aims to distinguish among the true sentences and generated sentences , and it is trained to minimize its classification error rate .",training,Unsupervised Training,0,134,37,37,0,training : Unsupervised Training,0.5606694560669456,0.8409090909090909,0.8409090909090909
machine-translation,4,"During training , the D g 1 feeds back its assessment to finetune the encoder Enc t and decoder Dec s .",training,Unsupervised Training,0,135,38,38,0,training : Unsupervised Training,0.5648535564853556,0.8636363636363636,0.8636363636363636
machine-translation,4,"Since the machine translation is a sequence generation problem , following , we leverage policy gradient reinforcement training to back - propagate the assessment .",training,Unsupervised Training,0,136,39,39,0,training : Unsupervised Training,0.5690376569037657,0.8863636363636364,0.8863636363636364
machine-translation,4,We apply a similar processing to GAN g2 ( The details about the architecture of the global discriminator and the training procedure of the global GANs can be seen in appendix B and C ) .,training,Unsupervised Training,0,137,40,40,0,training : Unsupervised Training,0.5732217573221757,0.9090909090909091,0.9090909090909091
machine-translation,4,There are two stages in the proposed unsupervised training .,training,Unsupervised Training,0,138,41,41,0,training : Unsupervised Training,0.5774058577405857,0.9318181818181818,0.9318181818181818
machine-translation,4,"In the first stage , we train the proposed model with denoising auto - encoding , backtranslation and the local GANs , until no improvement is achieved on the development set .",training,Unsupervised Training,0,139,42,42,0,training : Unsupervised Training,0.5815899581589958,0.9545454545454546,0.9545454545454546
machine-translation,4,"Specifically , we perform one batch of denoising autoencoding for the source and target languages , one batch of back - translation for the two languages , and another batch of local GAN for the two languages .",training,Unsupervised Training,0,140,43,43,0,training : Unsupervised Training,0.5857740585774058,0.9772727272727273,0.9772727272727273
machine-translation,4,"In the second stage , we fine tune the proposed model with the global GANs .",training,Unsupervised Training,0,141,44,44,0,training : Unsupervised Training,0.5899581589958159,1.0,1.0
machine-translation,4,Experiments and Results,experiment,Experiments and Results,0,142,1,1,0,experiment : Experiments and Results,0.5941422594142259,0.25,0.25
machine-translation,4,"We evaluate the proposed approach on English - German , English - French and Chinese - to - English translation tasks",experiment,Experiments and Results,0,143,2,2,0,experiment : Experiments and Results,0.5983263598326359,0.5,0.5
machine-translation,4,5 .,experiment,Experiments and Results,0,144,3,3,0,experiment : Experiments and Results,0.602510460251046,0.75,0.75
machine-translation,4,"We firstly describe the datasets , pre-processing and model hyper - parameters we used , then we introduce the baseline systems , and finally we present our experimental results .",experiment,Experiments and Results,0,145,4,4,0,experiment : Experiments and Results,0.606694560669456,1.0,1.0
machine-translation,4,Data Sets and Preprocessing,data set,Data Sets and Preprocessing,0,146,1,1,0,data set : Data Sets and Preprocessing,0.6108786610878661,0.047619047619047616,0.047619047619047616
machine-translation,4,"In English - German and English - French translation , we make our experiments comparable with previous work by using the datasets from the WMT 2014 and WMT 2016 shared tasks respectively .",data set,Data Sets and Preprocessing,0,147,2,2,0,data set : Data Sets and Preprocessing,0.6150627615062761,0.09523809523809523,0.09523809523809523
machine-translation,4,"For Chinese - to - English translation , we use the datasets from LDC , which has been widely utilized by previous works .",data set,Data Sets and Preprocessing,0,148,3,3,0,data set : Data Sets and Preprocessing,0.6192468619246861,0.14285714285714285,0.14285714285714285
machine-translation,4,"WMT14 English - French Similar to , we use the full training set of 36M sentence pairs and we lower - case them and remove sentences longer than 50 words , resulting in a parallel corpus of about 30M pairs of sentences .",data set,Data Sets and Preprocessing,0,149,4,4,0,data set : Data Sets and Preprocessing,0.6234309623430963,0.19047619047619047,0.19047619047619047
machine-translation,4,"To guarantee no exact correspondence between the source and target monolingual sets , we build monolingual corpora by selecting English sentences from 15M random pairs , and selecting the French sentences from the complementary set .",data set,Data Sets and Preprocessing,0,150,5,5,0,data set : Data Sets and Preprocessing,0.6276150627615062,0.23809523809523808,0.23809523809523808
machine-translation,4,"Sentences are encoded with byte - pair encoding , which has an English vocabulary of about 32000 tokens , and French vocabulary of about 33000 tokens .",data set,Data Sets and Preprocessing,0,151,6,6,0,data set : Data Sets and Preprocessing,0.6317991631799164,0.2857142857142857,0.2857142857142857
machine-translation,4,We report results on newstest2014 .,data set,Data Sets and Preprocessing,0,152,7,7,0,data set : Data Sets and Preprocessing,0.6359832635983264,0.3333333333333333,0.3333333333333333
machine-translation,4,WMT16 English - German,data set,Data Sets and Preprocessing,0,153,8,8,0,data set : Data Sets and Preprocessing,0.6401673640167364,0.38095238095238093,0.38095238095238093
machine-translation,4,"We follow the same procedure mentioned above to create monolingual training corpora for English - German translation , and we get two monolingual training data of 1.8 M sentences each .",data set,Data Sets and Preprocessing,0,154,9,9,0,data set : Data Sets and Preprocessing,0.6443514644351465,0.42857142857142855,0.42857142857142855
machine-translation,4,The two languages share a vocabulary of about 32000 tokens .,data set,Data Sets and Preprocessing,0,155,10,10,0,data set : Data Sets and Preprocessing,0.6485355648535565,0.47619047619047616,0.47619047619047616
machine-translation,4,We report results on newstest2016 .,data set,Data Sets and Preprocessing,0,156,11,11,0,data set : Data Sets and Preprocessing,0.6527196652719666,0.5238095238095238,0.5238095238095238
machine-translation,4,"LDC Chinese - English For Chinese - to - English translation , our training data consists of 1.6 M sentence pairs randomly extracted from LDC corpora",data set,Data Sets and Preprocessing,0,157,12,12,0,data set : Data Sets and Preprocessing,0.6569037656903766,0.5714285714285714,0.5714285714285714
machine-translation,4,6 .,data set,Data Sets and Preprocessing,0,158,13,13,0,data set : Data Sets and Preprocessing,0.6610878661087866,0.6190476190476191,0.6190476190476191
machine-translation,4,"Since the data set is not big enough , we just build the monolingual data set by randomly shuffling the Chinese and English sentences respectively .",data set,Data Sets and Preprocessing,0,159,14,14,0,data set : Data Sets and Preprocessing,0.6652719665271967,0.6666666666666666,0.6666666666666666
machine-translation,4,"In spite of the fact that some correspondence between examples in these two monolingual sets may exist , we never utilize this alignment information in our training procedure ( see Section 3.2 ) .",data set,Data Sets and Preprocessing,0,160,15,15,0,data set : Data Sets and Preprocessing,0.6694560669456067,0.7142857142857143,0.7142857142857143
machine-translation,4,Both the Chinese and English sentences are encoded with byte - pair encoding .,data set,Data Sets and Preprocessing,0,161,16,16,0,data set : Data Sets and Preprocessing,0.6736401673640168,0.7619047619047619,0.7619047619047619
machine-translation,4,"We get an English vocabulary of about 34000 tokens , and Chinese vocabulary of about 38000 tokens .",data set,Data Sets and Preprocessing,0,162,17,17,0,data set : Data Sets and Preprocessing,0.6778242677824268,0.8095238095238095,0.8095238095238095
machine-translation,4,The results are reported on N IST 02 .,data set,Data Sets and Preprocessing,0,163,18,18,0,data set : Data Sets and Preprocessing,0.6820083682008368,0.8571428571428571,0.8571428571428571
machine-translation,4,"Since the proposed system relies on the pretrained cross - lingual embeddings , we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2 vec .",data set,Data Sets and Preprocessing,0,164,19,19,0,data set : Data Sets and Preprocessing,0.6861924686192469,0.9047619047619048,0.9047619047619048
machine-translation,4,"We then apply the public implementation 7 of the method proposed by to map these 6 LDC2002L27 , LDC2002T01 , LDC2002E18 , LDC2003E07 , LDC2004T08 , LDC2004E12 , LDC2005T10 7 https://github.com/artetxem/vecmap",data set,Data Sets and Preprocessing,0,165,20,20,0,data set : Data Sets and Preprocessing,0.6903765690376569,0.9523809523809523,0.9523809523809523
machine-translation,4,embeddings to a shared - latent space 8 .,data set,Data Sets and Preprocessing,0,166,21,21,0,data set : Data Sets and Preprocessing,0.694560669456067,1.0,1.0
machine-translation,4,Model Hyper - parameters and Evaluation,system description,Model Hyper-parameters and Evaluation,0,167,1,1,0,system description : Model Hyper-parameters and Evaluation,0.698744769874477,0.09090909090909091,0.09090909090909091
machine-translation,4,"Following the base model in , we set the dimension of word embedding as 512 , dropout rate as 0.1 and the head number as 8 .",system description,Model Hyper-parameters and Evaluation,1,168,2,2,0,system description : Model Hyper-parameters and Evaluation,0.702928870292887,0.18181818181818182,0.18181818181818182
machine-translation,4,We use beam search with a beam size of 4 and length penalty ? = 0.6 .,system description,Model Hyper-parameters and Evaluation,1,169,3,3,0,system description : Model Hyper-parameters and Evaluation,0.7071129707112971,0.2727272727272727,0.2727272727272727
machine-translation,4,The model is implemented in TensorFlow and trained on up to four K80 GPUs synchronously in a multi - GPU setup on a single machine .,system description,Model Hyper-parameters and Evaluation,1,170,4,4,0,system description : Model Hyper-parameters and Evaluation,0.7112970711297071,0.36363636363636365,0.36363636363636365
machine-translation,4,"For model selection , we stop training when the model achieves no improvement for the tenth evaluation on the development set , which is comprised of 3000 source and target sentences extracted randomly from the monolingual training corpora .",system description,Model Hyper-parameters and Evaluation,0,171,5,5,0,system description : Model Hyper-parameters and Evaluation,0.7154811715481172,0.45454545454545453,0.45454545454545453
machine-translation,4,"Following ( Lample et al. , 2017 ) , we translate the source sentences to the target language , and then translate the resulting sentences back to the source language .",system description,Model Hyper-parameters and Evaluation,0,172,6,6,0,system description : Model Hyper-parameters and Evaluation,0.7196652719665272,0.5454545454545454,0.5454545454545454
machine-translation,4,The quality of the model is then evaluated by computing the BLEU score over the original inputs and their reconstructions via this two - step translation process .,system description,Model Hyper-parameters and Evaluation,0,173,7,7,0,system description : Model Hyper-parameters and Evaluation,0.7238493723849372,0.6363636363636364,0.6363636363636364
machine-translation,4,"The performance is finally averaged over two directions , i.e. , from source to target and from target to source .",system description,Model Hyper-parameters and Evaluation,0,174,8,8,0,system description : Model Hyper-parameters and Evaluation,0.7280334728033473,0.7272727272727273,0.7272727272727273
machine-translation,4,BLEU is utilized as the evaluation metric .,system description,Model Hyper-parameters and Evaluation,0,175,9,9,0,system description : Model Hyper-parameters and Evaluation,0.7322175732217573,0.8181818181818182,0.8181818181818182
machine-translation,4,"For Chinese - to - English , we apply the script mteval - v11 b. pl to evaluate the translation performance .",system description,Model Hyper-parameters and Evaluation,0,176,10,10,0,system description : Model Hyper-parameters and Evaluation,0.7364016736401674,0.9090909090909091,0.9090909090909091
machine-translation,4,"For English - German and English - French , we evaluate the translation performance with the script multi-belu.pl 9 .",system description,Model Hyper-parameters and Evaluation,0,177,11,11,0,system description : Model Hyper-parameters and Evaluation,0.7405857740585774,1.0,1.0
machine-translation,4,Baseline Systems,baseline,Baseline Systems,0,178,1,1,0,baseline : Baseline Systems,0.7447698744769874,0.1,0.1
machine-translation,4,Word - by - word translation ( WBW ) The first baseline we consider is a system that performs word - by - word translations using the inferred bilingual dictionary .,baseline,Baseline Systems,1,179,2,2,0,baseline : Baseline Systems,0.7489539748953975,0.2,0.2
machine-translation,4,"Specifically , it translates a sentence word - by - word , replacing each word with its nearest neighbor in the other language .",baseline,Baseline Systems,0,180,3,3,0,baseline : Baseline Systems,0.7531380753138075,0.3,0.3
machine-translation,4,Lample et al .,baseline,Baseline Systems,1,181,4,4,0,baseline : Baseline Systems,0.7573221757322176,0.4,0.4
machine-translation,4,The second baseline is a previous work that uses the same training and testing sets with this paper .,baseline,Baseline Systems,0,182,5,5,0,baseline : Baseline Systems,0.7615062761506276,0.5,0.5
machine-translation,4,"Their model belongs to the standard attention - based encoder - decoder framework , which implements the encoder using a bidirectional long short term memory network ( LSTM ) and implements the decoder using a sim - 8 The configuration we used to run these open - source toolkits can be found in appendix D 9 https://github.com/mosessmt/mosesdecoder/blob/617e8c8/scripts/generic/multibleu.perl;mteval-v11b.pl",baseline,Baseline Systems,0,183,6,6,0,baseline : Baseline Systems,0.7656903765690377,0.6,0.6
machine-translation,4,en - de de - en en - fr fr- en zh - en are copied directly from their paper .,baseline,Baseline Systems,0,184,7,7,0,baseline : Baseline Systems,0.7698744769874477,0.7,0.7
machine-translation,4,"We do not present the results of ( Artetxe et al. , 2017 b ) since we use different training sets .",baseline,Baseline Systems,0,185,8,8,0,baseline : Baseline Systems,0.7740585774058577,0.8,0.8
machine-translation,4,ple forward LSTM .,baseline,Baseline Systems,0,186,9,9,0,baseline : Baseline Systems,0.7782426778242678,0.9,0.9
machine-translation,4,They apply one single encoder and decoder for the source and target languages .,baseline,Baseline Systems,0,187,10,10,0,baseline : Baseline Systems,0.7824267782426778,1.0,1.0
machine-translation,4,Supervised training,training,training,1,188,1,1,0,training : training,0.7866108786610879,0.3333333333333333,0.3333333333333333
machine-translation,4,"We finally consider exactly the same model as ours , but trained using the standard cross - entropy loss on the original parallel sentences .",training,training,0,189,2,2,0,training : training,0.7907949790794979,0.6666666666666666,0.6666666666666666
machine-translation,4,This model can be viewed as an upper bound for the proposed unsupervised model .,training,training,0,190,3,3,0,training : training,0.7949790794979079,1.0,1.0
machine-translation,4,Results and Analysis,analysis,Results and Analysis,0,191,1,1,0,analysis : Results and Analysis,0.799163179916318,0.043478260869565216,1.0
machine-translation,4,Number of weight - sharing layers,analysis,Number of weight-sharing layers,1,192,2,1,0,analysis : Number of weight-sharing layers,0.803347280334728,0.08695652173913043,0.045454545454545456
machine-translation,4,We firstly investigate how the number of weightsharing layers affects the translation performance .,analysis,Number of weight-sharing layers,0,193,3,2,0,analysis : Number of weight-sharing layers,0.8075313807531381,0.13043478260869565,0.09090909090909091
machine-translation,4,"In this experiment , we vary the number of weightsharing layers in the AEs from 0 to 4 .",analysis,Number of weight-sharing layers,0,194,4,3,0,analysis : Number of weight-sharing layers,0.8117154811715481,0.17391304347826086,0.13636363636363635
machine-translation,4,"Sharing one layer in AEs means sharing one layer for the encoders and in the meanwhile , sharing one layer for the decoders .",analysis,Number of weight-sharing layers,0,195,5,4,0,analysis : Number of weight-sharing layers,0.8158995815899581,0.21739130434782608,0.18181818181818182
machine-translation,4,"The BLEU scores of English - to - German , English - to - French and Chinese - to - English translation tasks are reported in figure",analysis,Number of weight-sharing layers,0,196,6,5,0,analysis : Number of weight-sharing layers,0.8200836820083682,0.2608695652173913,0.22727272727272727
machine-translation,4,. Each curve corresponds to a different translation task and the x - axis denotes the number of weight - sharing layers for the AEs .,analysis,Number of weight-sharing layers,0,197,7,6,0,analysis : Number of weight-sharing layers,0.8242677824267782,0.30434782608695654,0.2727272727272727
machine-translation,4,We find that the number of weight - sharing layers shows much effect on the translation performance .,analysis,Number of weight-sharing layers,0,198,8,7,0,analysis : Number of weight-sharing layers,0.8284518828451883,0.34782608695652173,0.3181818181818182
machine-translation,4,And the best translation performance is achieved when only one layer is shared in our system .,analysis,Number of weight-sharing layers,1,199,9,8,0,analysis : Number of weight-sharing layers,0.8326359832635983,0.391304347826087,0.36363636363636365
machine-translation,4,"When all of the four layers are shared , i.e. , only one shared encoder is utilized , we get poor translation performance in all of the three translation tasks .",analysis,Number of weight-sharing layers,1,200,10,9,0,analysis : Number of weight-sharing layers,0.8368200836820083,0.43478260869565216,0.4090909090909091
machine-translation,4,This verifies our conjecture that the shared encoder is detrimental to the performance of unsupervised NMT especially for the translation tasks on distant language pairs .,analysis,Number of weight-sharing layers,0,201,11,10,0,analysis : Number of weight-sharing layers,0.8410041841004184,0.4782608695652174,0.45454545454545453
machine-translation,4,"More concretely , for the related language pair translation , i.e. , English - to - French , the encoder - shared model achieves - 0.53 BLEU points decline than the best model where only one layer is shared .",analysis,Number of weight-sharing layers,0,202,12,11,0,analysis : Number of weight-sharing layers,0.8451882845188284,0.5217391304347826,0.5
machine-translation,4,"For the more distant language pair English - to - German , the encoder - shared model achieves more significant decline , i.e. , - 0.85 BLEU points decline .",analysis,Number of weight-sharing layers,0,203,13,12,0,analysis : Number of weight-sharing layers,0.8493723849372385,0.5652173913043478,0.5454545454545454
machine-translation,4,"And for the most distant language pair Chinese - to - English , the decline is as large as - 1.66 BLEU points .",analysis,Number of weight-sharing layers,0,204,14,13,0,analysis : Number of weight-sharing layers,0.8535564853556485,0.6086956521739131,0.5909090909090909
machine-translation,4,"We explain this as that the more distant the language pair is , the more different characteristics they have .",analysis,Number of weight-sharing layers,0,205,15,14,0,analysis : Number of weight-sharing layers,0.8577405857740585,0.6521739130434783,0.6363636363636364
machine-translation,4,And the shared encoder is weak in keeping the unique characteristic of each language .,analysis,Number of weight-sharing layers,0,206,16,15,0,analysis : Number of weight-sharing layers,0.8619246861924686,0.6956521739130435,0.6818181818181818
machine-translation,4,"Additionally , we also notice that using two completely independent encoders , i.e. , setting the number of weight - sharing layers as 0 , results in poor translation performance too .",analysis,Number of weight-sharing layers,0,207,17,16,0,analysis : Number of weight-sharing layers,0.8661087866108786,0.7391304347826086,0.7272727272727273
machine-translation,4,This confirms our intuition that the shared layers are vital to map the source and target latent representations to a shared - latent space .,analysis,Number of weight-sharing layers,0,208,18,17,0,analysis : Number of weight-sharing layers,0.8702928870292888,0.782608695652174,0.7727272727272727
machine-translation,4,"In the rest of our experiments , we set the number of weightsharing layer as 1 . model only trained with monolingual data effectively learns to use the context information and the internal structure of each language .",analysis,Number of weight-sharing layers,0,209,19,18,0,analysis : Number of weight-sharing layers,0.8744769874476988,0.8260869565217391,0.8181818181818182
machine-translation,4,"Compared to the work of ( Lample et al. , 2017 ) , our model also achieves up to + 1.92 BLEU points improvement on English - to - French translation task .",analysis,Number of weight-sharing layers,0,210,20,19,0,analysis : Number of weight-sharing layers,0.8786610878661087,0.8695652173913043,0.8636363636363636
machine-translation,4,We believe that the unsupervised NMT is very promising .,analysis,Number of weight-sharing layers,0,211,21,20,0,analysis : Number of weight-sharing layers,0.8828451882845189,0.9130434782608695,0.9090909090909091
machine-translation,4,"However , there is still a large room for improvement compared to the supervised upper bound .",analysis,Number of weight-sharing layers,0,212,22,21,0,analysis : Number of weight-sharing layers,0.8870292887029289,0.9565217391304348,0.9545454545454546
machine-translation,4,The gap between the supervised and unsupervised model is as large as 12.3 - 25.5 BLEU points depending on the language pair and translation direction .,analysis,Number of weight-sharing layers,0,213,23,22,0,analysis : Number of weight-sharing layers,0.891213389121339,1.0,1.0
machine-translation,4,Translation results,result,Translation results,0,214,1,1,0,result : Translation results,0.895397489539749,1.0,1.0
machine-translation,4,Ablation study,ablation,Ablation study,0,215,1,1,0,ablation : Ablation study,0.899581589958159,0.08333333333333333,0.08333333333333333
machine-translation,4,"To understand the importance of different components of the proposed system , we perform an ablation study by training multiple versions of our model with some missing components : the local GANs , the global GANs , the directional self - attention , the weight - sharing , the embeddingreinforced encoders , etc .",ablation,Ablation study,0,216,2,2,0,ablation : Ablation study,0.9037656903765691,0.16666666666666666,0.16666666666666666
machine-translation,4,Results are reported in table 3 .,ablation,Ablation study,0,217,3,3,0,ablation : Ablation study,0.9079497907949791,0.25,0.25
machine-translation,4,"We do not test the the importance of the auto - encoding , back - translation and the pre-trained embeddings because they have been widely tested in .",ablation,Ablation study,0,218,4,4,0,ablation : Ablation study,0.9121338912133892,0.3333333333333333,0.3333333333333333
machine-translation,4,shows that the best performance is obtained with the simultaneous use of all the tested elements .,ablation,Ablation study,0,219,5,5,0,ablation : Ablation study,0.9163179916317992,0.4166666666666667,0.4166666666666667
machine-translation,4,"The most critical component is the weight - sharing constraint , which is vital to map sentences of different languages to the shared - latent space .",ablation,Ablation study,1,220,6,6,0,ablation : Ablation study,0.9205020920502092,0.5,0.5
machine-translation,4,The embedding - reinforced encoder also brings some improvement on all of the translation tasks .,ablation,Ablation study,1,221,7,7,0,ablation : Ablation study,0.9246861924686193,0.5833333333333334,0.5833333333333334
machine-translation,4,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .",ablation,Ablation study,1,222,8,8,0,ablation : Ablation study,0.9288702928870293,0.6666666666666666,0.6666666666666666
machine-translation,4,This indicates that it deserves more efforts to investigate the temporal order information in self - attention mechanism .,ablation,Ablation study,0,223,9,9,0,ablation : Ablation study,0.9330543933054394,0.75,0.75
machine-translation,4,The GANs also significantly improve the translation performance of our system .,ablation,Ablation study,1,224,10,10,0,ablation : Ablation study,0.9372384937238494,0.8333333333333334,0.8333333333333334
machine-translation,4,"Specifically , the global GANs achieve improvement up to + 0.78 BLEU points on English - to - French translation and the local GANs also obtain improvement up to + 0.57 BLEU points on English - to - French translation .",ablation,Ablation study,0,225,11,11,0,ablation : Ablation study,0.9414225941422594,0.9166666666666666,0.9166666666666666
machine-translation,4,This reveals that the proposed model benefits a lot from the crossdomain loss defined by GANs .,ablation,Ablation study,0,226,12,12,0,ablation : Ablation study,0.9456066945606695,1.0,1.0
machine-translation,4,Conclusion and Future work,conclusion,Conclusion and Future work,0,227,1,1,0,conclusion : Conclusion and Future work,0.9497907949790795,0.07692307692307693,0.07692307692307693
machine-translation,4,The models proposed recently for unsupervised NMT use a single encoder to map sentences from different languages to a shared - latent space .,conclusion,Conclusion and Future work,0,228,2,2,0,conclusion : Conclusion and Future work,0.9539748953974896,0.15384615384615385,0.15384615384615385
machine-translation,4,We conjecture that the shared encoder is problematic for keeping the unique and inherent characteristic of each language .,conclusion,Conclusion and Future work,0,229,3,3,0,conclusion : Conclusion and Future work,0.9581589958158996,0.23076923076923078,0.23076923076923078
machine-translation,4,"In this paper , we propose the weight - sharing constraint in unsupervised NMT to address this issue .",conclusion,Conclusion and Future work,0,230,4,4,0,conclusion : Conclusion and Future work,0.9623430962343096,0.3076923076923077,0.3076923076923077
machine-translation,4,"To enhance the cross - language translation performance , we also propose the embedding - reinforced encoders , local GAN and global GAN into the proposed system .",conclusion,Conclusion and Future work,0,231,5,5,0,conclusion : Conclusion and Future work,0.9665271966527197,0.38461538461538464,0.38461538461538464
machine-translation,4,"Additionally , the directional self - attention is introduced to model the temporal order information for our system .",conclusion,Conclusion and Future work,0,232,6,6,0,conclusion : Conclusion and Future work,0.9707112970711297,0.46153846153846156,0.46153846153846156
machine-translation,4,"We test the proposed model on English - German , English - French and Chinese - to - English translation tasks .",conclusion,Conclusion and Future work,0,233,7,7,0,conclusion : Conclusion and Future work,0.9748953974895398,0.5384615384615384,0.5384615384615384
machine-translation,4,The experimental results reveal that our approach achieves significant improvement and verify our conjecture that the shared encoder is really a bottleneck for improving the unsupervised NMT .,conclusion,Conclusion and Future work,0,234,8,8,0,conclusion : Conclusion and Future work,0.9790794979079498,0.6153846153846154,0.6153846153846154
machine-translation,4,The ablation study shows that each component of our system achieves some improvement for the final translation performance .,conclusion,Conclusion and Future work,0,235,9,9,0,conclusion : Conclusion and Future work,0.9832635983263598,0.6923076923076923,0.6923076923076923
machine-translation,4,Unsupervised NMT opens exciting opportunities for the future research .,conclusion,Conclusion and Future work,0,236,10,10,0,conclusion : Conclusion and Future work,0.9874476987447699,0.7692307692307693,0.7692307692307693
machine-translation,4,"However , there is still a large room for improvement compared to the supervised NMT .",conclusion,Conclusion and Future work,0,237,11,11,0,conclusion : Conclusion and Future work,0.9916317991631799,0.8461538461538461,0.8461538461538461
machine-translation,4,"In the future , we would like to investigate how to utilize the monolingual data more effectively , such as incorporating the language model and syntactic information into unsupervised NMT .",conclusion,Conclusion and Future work,0,238,12,12,0,conclusion : Conclusion and Future work,0.99581589958159,0.9230769230769231,0.9230769230769231
machine-translation,4,"Besides , we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model .",conclusion,Conclusion and Future work,0,239,13,13,0,conclusion : Conclusion and Future work,1.0,1.0,1.0
machine-translation,5,Tilde 's Machine Translation Systems for WMT 2018,title,title,1,2,1,1,0,title : title,0.013986013986013986,1.0,1.0
machine-translation,5,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.02097902097902098,0.2,0.2
machine-translation,5,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.027972027972027972,0.4,0.4
machine-translation,5,"We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.03496503496503497,0.6,0.6
machine-translation,5,"For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.04195804195804196,0.8,0.8
machine-translation,5,The submitted systems were trained using Transformer models .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.04895104895104895,1.0,1.0
machine-translation,5,Introduction,introduction,introduction,0,8,1,1,0,introduction : introduction,0.055944055944055944,0.0625,0.0625
machine-translation,5,Neural machine translation ( NMT ) is a rapidly changing research area .,introduction,introduction,1,9,2,2,0,introduction : introduction,0.06293706293706294,0.125,0.125
machine-translation,5,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.06993006993006994,0.1875,0.1875
machine-translation,5,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,introduction,introduction,0,11,4,4,0,introduction : introduction,0.07692307692307693,0.25,0.25
machine-translation,5,"In 2017 , multiplicative long short - term memory ( MLSTM ) units and deep GRU models were introduced in NMT .",introduction,introduction,0,12,5,5,0,introduction : introduction,0.08391608391608392,0.3125,0.3125
machine-translation,5,"The same year , selfattentional ( Transformer ) models were introduced .",introduction,introduction,0,13,6,6,0,introduction : introduction,0.09090909090909091,0.375,0.375
machine-translation,5,"Consequently , in 2018 , most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation ( WMT ) were trained using Transformer models",introduction,introduction,0,14,7,7,0,introduction : introduction,0.0979020979020979,0.4375,0.4375
machine-translation,5,1 .,introduction,introduction,0,15,8,8,0,introduction : introduction,0.1048951048951049,0.5,0.5
machine-translation,5,"However , it is already evident that the state - of - the - art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models .",introduction,introduction,0,16,9,9,0,introduction : introduction,0.11188811188811189,0.5625,0.5625
machine-translation,5,be pushed even further in 2018 .,introduction,introduction,0,17,10,10,0,introduction : introduction,0.11888111888111888,0.625,0.625
machine-translation,5,"For instance , have recently proposed RNMT + models that combine deep LSTM - based models with multi-head attention and showed that the models outperform Transformer models .",introduction,introduction,0,18,11,11,0,introduction : introduction,0.1258741258741259,0.6875,0.6875
machine-translation,5,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .",introduction,introduction,0,19,12,12,0,introduction : introduction,0.13286713286713286,0.75,0.75
machine-translation,5,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.13986013986013987,0.8125,0.8125
machine-translation,5,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",introduction,introduction,0,21,14,14,0,introduction : introduction,0.14685314685314685,0.875,0.875
machine-translation,5,The paper is further structured as follows :,introduction,introduction,0,22,15,15,0,introduction : introduction,0.15384615384615385,0.9375,0.9375
machine-translation,5,"Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation , Section 3 describes the data used to train the NMT systems and the data pre-processing workflows , Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems , Section 5 provides automatic evaluation results , and Section 6 concludes the paper .",introduction,introduction,0,23,16,16,0,introduction : introduction,0.16083916083916083,1.0,1.0
machine-translation,5,System Overview,system,System Overview,0,24,1,1,0,system : System Overview,0.16783216783216784,0.013513513513513514,0.1
machine-translation,5,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .",system,System Overview,1,25,2,2,0,system : System Overview,0.17482517482517482,0.02702702702702703,0.2
machine-translation,5,The following is a list of the five MT systems submitted :,system,System Overview,1,26,3,3,0,system : System Overview,0.18181818181818182,0.04054054054054054,0.3
machine-translation,5,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,system,System Overview,1,27,4,4,0,system : System Overview,0.1888111888111888,0.05405405405405406,0.4
machine-translation,5,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,system,System Overview,1,28,5,5,0,system : System Overview,0.1958041958041958,0.06756756756756757,0.5
machine-translation,5,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,system,System Overview,1,29,6,6,0,system : System Overview,0.20279720279720279,0.08108108108108109,0.6
machine-translation,5,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",system,System Overview,1,30,7,7,0,system : System Overview,0.2097902097902098,0.0945945945945946,0.7
machine-translation,5,constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,system,System Overview,1,31,8,8,0,system : System Overview,0.21678321678321677,0.10810810810810811,0.8
machine-translation,5,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,system,System Overview,1,32,9,9,0,system : System Overview,0.22377622377622378,0.12162162162162163,0.9
machine-translation,5,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,system,System Overview,1,33,10,10,0,system : System Overview,0.23076923076923078,0.13513513513513514,1.0
machine-translation,5,Data,system,Data,0,34,11,1,0,system : Data,0.23776223776223776,0.14864864864864866,0.3333333333333333
machine-translation,5,"Data preparation was done using one of two distinct workflows - we used the full workflow for tilde - c - nmt , tilde - nc - nmt and tilde - c - nmt - comb submissions .",system,Data,0,35,12,2,0,system : Data,0.24475524475524477,0.16216216216216217,0.6666666666666666
machine-translation,5,For the tilde - c - nmt - 2 bt submission we used the light data preparation workflow .,system,Data,0,36,13,3,0,system : Data,0.2517482517482518,0.17567567567567569,1.0
machine-translation,5,Full Workflow,system,Full Workflow,0,37,14,1,0,system : Full Workflow,0.25874125874125875,0.1891891891891892,0.08333333333333333
machine-translation,5,"First , we trained constrained system baseline models using the filtered datasets .",system,Full Workflow,0,38,15,2,0,system : Full Workflow,0.26573426573426573,0.20270270270270271,0.16666666666666666
machine-translation,5,"For baseline models , we used the MLSTM and transf configurations ( see ) .",system,Full Workflow,0,39,16,3,0,system : Full Workflow,0.2727272727272727,0.21621621621621623,0.25
machine-translation,5,"Then , we used the best - performing models ( based on translation quality on the vali -dation set ) , which were the Transformer models ( see , and back - translated monolingual data .",system,Full Workflow,0,40,17,4,0,system : Full Workflow,0.27972027972027974,0.22972972972972974,0.3333333333333333
machine-translation,5,"As mentioned before , for the unconstrained systems , we back - translated the monolingual data using pre-existing MLSTM - based NMT systems .",system,Full Workflow,0,41,18,5,0,system : Full Workflow,0.2867132867132867,0.24324324324324326,0.4166666666666667
machine-translation,5,"Then , using the final training data ( parallel and the two synthetic corpora ) , we trained final Transformer models .",system,Full Workflow,0,42,19,6,0,system : Full Workflow,0.2937062937062937,0.25675675675675674,0.5
machine-translation,5,"For the constrained scenario , we trained multiple models ( three for each translation direction ) by experimenting with multiple model configurations .",system,Full Workflow,0,43,20,7,0,system : Full Workflow,0.3006993006993007,0.2702702702702703,0.5833333333333334
machine-translation,5,"For the unconstrained scenario , we trained one model in each of the directions .",system,Full Workflow,0,44,21,8,0,system : Full Workflow,0.3076923076923077,0.28378378378378377,0.6666666666666666
machine-translation,5,"In order to acquire the translations for the submissions , we performed model averaging and ensembling as follows :",system,Full Workflow,0,45,22,9,0,system : Full Workflow,0.3146853146853147,0.2972972972972973,0.75
machine-translation,5,"For the tilde - c - nmt ( constrained NMT ) systems , we performed model averaging of the best four models ( according to perplexity ) of the three different run NMT systems and deployed the averaged models in an ensemble .",system,Full Workflow,0,46,23,10,0,system : Full Workflow,0.32167832167832167,0.3108108108108108,0.8333333333333334
machine-translation,5,"For the tilde - nc - nmt ( unconstrained NMT ) systems , we performed model averaging of the best four models .",system,Full Workflow,0,47,24,11,0,system : Full Workflow,0.32867132867132864,0.32432432432432434,0.9166666666666666
machine-translation,5,"For the tilde - c - nmt - comb Estonian - English system , we performed majority voting ( see Section 4.3 ) of translations produced by six different runs of different constrained systems ( using best BLEU models , averaged models , ensembled averaged models , ensembled models , and larger beam search ( 10 instead of 5 ) ) .",system,Full Workflow,0,48,25,12,0,system : Full Workflow,0.3356643356643357,0.33783783783783783,1.0
machine-translation,5,Data Filtering,system,Data Filtering,0,49,26,1,0,system : Data Filtering,0.34265734265734266,0.35135135135135137,0.06666666666666667
machine-translation,5,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",system,Data Filtering,0,50,27,2,0,system : Data Filtering,0.34965034965034963,0.36486486486486486,0.13333333333333333
machine-translation,5,"The parallel corpora filtering methods remove sentence pairs that have indications of data corruption or low parallelity ( e.g. , source - target length ratio , content overlap , digit mismatch , language adherence , etc. ) issues .",system,Data Filtering,0,51,28,3,0,system : Data Filtering,0.35664335664335667,0.3783783783783784,0.2
machine-translation,5,"Contrary to Tilde 's submissions for WMT 2017 , isolated sentence pair filtering for the WMT 2018 submissions was supplemented with a maximum content overlap filter ( i.e. only one target sentence for each source sentence was preserved and vice versa based on the content overlap filter 's score for each sentence pair ) .",system,Data Filtering,0,52,29,4,0,system : Data Filtering,0.36363636363636365,0.3918918918918919,0.26666666666666666
machine-translation,5,"For filtering , we required probabilistic dictionaries , which were obtained from the parallel corpora ( different dictionaries for the constrained and unconstrained scenarios ) using fast align .",system,Data Filtering,0,53,30,5,0,system : Data Filtering,0.3706293706293706,0.40540540540540543,0.3333333333333333
machine-translation,5,The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,system,Data Filtering,0,54,31,6,0,system : Data Filtering,0.3776223776223776,0.4189189189189189,0.4
machine-translation,5,"During filtering , we identified that one of the corpora that were provided by the organisers contained a significant amount of data corruption .",system,Data Filtering,0,55,32,7,0,system : Data Filtering,0.38461538461538464,0.43243243243243246,0.4666666666666667
machine-translation,5,It was the Estonian ? English ParaCrawl corpus,system,Data Filtering,0,56,33,8,0,system : Data Filtering,0.3916083916083916,0.44594594594594594,0.5333333333333333
machine-translation,5,It was the Estonian ? English ParaCrawl corpus,system,Data Filtering,0,57,34,9,0,system : Data Filtering,0.3986013986013986,0.4594594594594595,0.6
machine-translation,5,3 .,system,Data Filtering,0,58,35,10,0,system : Data Filtering,0.40559440559440557,0.47297297297297297,0.6666666666666666
machine-translation,5,The corpus consisted of 1.30 million sentence pairs out of which 0.77 million were identified as being corrupt .,system,Data Filtering,0,59,36,11,0,system : Data Filtering,0.4125874125874126,0.4864864864864865,0.7333333333333333
machine-translation,5,"To reduce the high level of noise , this corpus was filtered using stricter content overlap ( a threshold of 0.3 instead of 0.1 ) and language adherence filters ( both the language detection and the valid alphabet filters had to validate a sentence pair instead of just one of the filters ) than all other corpora .",system,Data Filtering,0,60,37,12,0,system : Data Filtering,0.4195804195804196,0.5,0.8
machine-translation,5,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",system,Data Filtering,0,61,38,13,0,system : Data Filtering,0.42657342657342656,0.5135135135135135,0.8666666666666667
machine-translation,5,"Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",system,Data Filtering,0,62,39,14,0,system : Data Filtering,0.43356643356643354,0.527027027027027,0.9333333333333333
machine-translation,5,The corpora statistics before and after filtering are provided in .,system,Data Filtering,0,63,40,15,0,system : Data Filtering,0.4405594405594406,0.5405405405405406,1.0
machine-translation,5,Data Pre-processing,system,Data Pre-processing,0,64,41,1,0,system : Data Pre-processing,0.44755244755244755,0.5540540540540541,0.07142857142857142
machine-translation,5,All corpora were pre-processed using the parallel data pre-processing workflow from the Tilde MT platform ) that performs the following pre-processing steps :,system,Data Pre-processing,0,65,42,2,0,system : Data Pre-processing,0.45454545454545453,0.5675675675675675,0.14285714285714285
machine-translation,5,"First , parallel corpora are cleaned by removing HTML and XML tags , decoding escaped symbols , normalising whitespaces and punctuation marks , replacing control characters with spaces , etc .",system,Data Pre-processing,0,66,43,3,0,system : Data Pre-processing,0.46153846153846156,0.581081081081081,0.21428571428571427
machine-translation,5,This step is performed only on the training data .,system,Data Pre-processing,0,67,44,4,0,system : Data Pre-processing,0.46853146853146854,0.5945945945945946,0.2857142857142857
machine-translation,5,"Then , non-translatable entities , such as email addresses , URLs , file paths , etc. are identified and replaced with place - holders .",system,Data Pre-processing,0,68,45,5,0,system : Data Pre-processing,0.4755244755244755,0.6081081081081081,0.35714285714285715
machine-translation,5,This allows reducing data sparsity where it is not needed .,system,Data Pre-processing,0,69,46,6,0,system : Data Pre-processing,0.4825174825174825,0.6216216216216216,0.42857142857142855
machine-translation,5,"Then , the data are tokenised using the Tilde MT regular expression - based tokeniser .",system,Data Pre-processing,0,70,47,7,0,system : Data Pre-processing,0.48951048951048953,0.6351351351351351,0.5
machine-translation,5,The Moses truecasing script truecase .,system,Data Pre-processing,0,71,48,8,0,system : Data Pre-processing,0.4965034965034965,0.6486486486486487,0.5714285714285714
machine-translation,5,perl is used to truecase the first word of every sentence .,system,Data Pre-processing,0,72,49,9,0,system : Data Pre-processing,0.5034965034965035,0.6621621621621622,0.6428571428571429
machine-translation,5,"Then , tokens are split into sub - word units using byte - pair encoding ( BPE ) .",system,Data Pre-processing,0,73,50,10,0,system : Data Pre-processing,0.5104895104895105,0.6756756756756757,0.7142857142857143
machine-translation,5,"For the constrained and unconstrained systems , we use BPE models consisting of 24,500 and 49,500 merging operations respectively .",system,Data Pre-processing,0,74,51,11,0,system : Data Pre-processing,0.5174825174825175,0.6891891891891891,0.7857142857142857
machine-translation,5,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .",system,Data Pre-processing,0,75,52,12,0,system : Data Pre-processing,0.5244755244755245,0.7027027027027027,0.8571428571428571
machine-translation,5,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",system,Data Pre-processing,0,76,53,13,0,system : Data Pre-processing,0.5314685314685315,0.7162162162162162,0.9285714285714286
machine-translation,5,"The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",system,Data Pre-processing,0,77,54,14,0,system : Data Pre-processing,0.5384615384615384,0.7297297297297297,1.0
machine-translation,5,Synthetic Data,system,Synthetic Data,0,78,55,1,0,system : Synthetic Data,0.5454545454545454,0.7432432432432432,0.09090909090909091
machine-translation,5,"Similarly to Tilde 's 2017 systems , we submitted systems that were trained using synthetic data :",system,Synthetic Data,0,79,56,2,0,system : Synthetic Data,0.5524475524475524,0.7567567567567568,0.18181818181818182
machine-translation,5,") back - translated data , and 2 ) data infused with unknown token identifiers .",system,Synthetic Data,0,80,57,3,0,system : Synthetic Data,0.5594405594405595,0.7702702702702703,0.2727272727272727
machine-translation,5,"The back - translated data allow performing domain adaptation and the second type of synthetic data allow training NMT models thatare robust to unknown phenomena ( e.g. , code - mixed content , target language words in the source text , rare or unseen words , etc . ) .",system,Synthetic Data,0,81,58,4,0,system : Synthetic Data,0.5664335664335665,0.7837837837837838,0.36363636363636365
machine-translation,5,"To create the synthetic corpora with unknown phenomena , we extracted fast align the parallel corpora and randomly replaced one to three unambiguously ( one - to - one ) aligned content words with unknown word identifiers .",system,Synthetic Data,0,82,59,5,0,system : Synthetic Data,0.5734265734265734,0.7972972972972973,0.45454545454545453
machine-translation,5,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",system,Synthetic Data,0,83,60,6,0,system : Synthetic Data,0.5804195804195804,0.8108108108108109,0.5454545454545454
machine-translation,5,The back - translated data were acquired from two sources :,system,Synthetic Data,0,84,61,7,0,system : Synthetic Data,0.5874125874125874,0.8243243243243243,0.6363636363636364
machine-translation,5,") the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",system,Synthetic Data,0,85,62,8,0,system : Synthetic Data,0.5944055944055944,0.8378378378378378,0.7272727272727273
machine-translation,5,"In order to limit noise , the back - translated data were filtered using the same parallel data filtering methods that were described in Section 3.1.1 ( although with a higher threshold for the content overlap filter ) .",system,Synthetic Data,0,86,63,9,0,system : Synthetic Data,0.6013986013986014,0.8513513513513513,0.8181818181818182
machine-translation,5,"Furthermore , in order to train the final systems , we also generated unknown phenomena infused data for the back - translated filtered data , thereby also almost doubling the sizes of the back - translated data .",system,Synthetic Data,0,87,64,10,0,system : Synthetic Data,0.6083916083916084,0.8648648648648649,0.9090909090909091
machine-translation,5,The synthetic corpora statistics and the sizes of the total training data are given in .,system,Synthetic Data,0,88,65,11,0,system : Synthetic Data,0.6153846153846154,0.8783783783783784,1.0
machine-translation,5,Light Workflow,system,Light Workflow,0,89,66,1,0,system : Light Workflow,0.6223776223776224,0.8918918918918919,0.125
machine-translation,5,The light workflow was used to produce the tilde - c - nmt - 2 bt ( constrained NMT with two sets of back - translated data ) systems .,system,Light Workflow,0,90,67,2,0,system : Light Workflow,0.6293706293706294,0.9054054054054054,0.25
machine-translation,5,"First , we trained baseline models using only filtered parallel datasets ( Parallel - only in ) .",system,Light Workflow,0,91,68,3,0,system : Light Workflow,0.6363636363636364,0.918918918918919,0.375
machine-translation,5,"Then , we back - translated the first batches of monolingual news data and trained intermediate NMT systems ( Parallel + First Back - translated ) .",system,Light Workflow,0,92,69,4,0,system : Light Workflow,0.6433566433566433,0.9324324324324325,0.5
machine-translation,5,"Finally , we used the intermediate NMT systems to backtranslate the second batches of monolingual news data and trained final NMT systems ( Parallel + Second Back - translated ) .",system,Light Workflow,0,93,70,5,0,system : Light Workflow,0.6503496503496503,0.9459459459459459,0.625
machine-translation,5,"The training progress in shows that the English - Estonian system benefits from the additional data , but the system in the other direction - not so much .",system,Light Workflow,0,94,71,6,0,system : Light Workflow,0.6573426573426573,0.9594594594594594,0.75
machine-translation,5,"For the final translations , we used a postprocessing script to replace consecutive repeating n-grams and repeating ngrams that have a preposition between them ( i.e. , victim of the victim ) with a single n-gram .",system,Light Workflow,0,95,72,7,0,system : Light Workflow,0.6643356643356644,0.972972972972973,0.875
machine-translation,5,"This problem was more apparent in RNN - based NMT systems , but it was also noticable in our Transformer model outputs .",system,Light Workflow,0,96,73,8,0,system : Light Workflow,0.6713286713286714,0.9864864864864865,1.0
machine-translation,5,NMT,system,NMT Systems,0,97,74,1,0,system : NMT Systems,0.6783216783216783,1.0,1.0
machine-translation,5,Systems,system,system,0,98,1,1,0,system : system,0.6853146853146853,0.034482758620689655,0.16666666666666666
machine-translation,5,"In order to train the NMT systems , we used the Nematus ( Sennrich et al. , 2017 b ) ( for MLSTM models ) and Sockeye ) ( for Transformer models ) toolkits .",system,system,0,99,2,2,0,system : system,0.6923076923076923,0.06896551724137931,0.3333333333333333
machine-translation,5,"All models were trained until convergence ( i.e. , until an early stopping criterion was met ) .",system,system,0,100,3,3,0,system : system,0.6993006993006993,0.10344827586206896,0.5
machine-translation,5,Figure 1 : NMT system training progress ( BLEU scores on the validation set ) for English - Estonian ( left ) and,system,system,0,101,4,4,0,system : system,0.7062937062937062,0.13793103448275862,0.6666666666666666
machine-translation,5,Estonian - English ( right ) .,system,system,0,102,5,5,0,system : system,0.7132867132867133,0.1724137931034483,0.8333333333333334
machine-translation,5,"Note that batch size may differ between different architectures and BLEU scores are calculated on raw ( token level ) pre-processed validation sets , therefore , the scores are slightly higher than evaluation results for the final translations !",system,system,0,103,6,6,0,system : system,0.7202797202797203,0.20689655172413793,1.0
machine-translation,5,Automatic Post - editing of Named Entities,system,Automatic Post-editing of Named Entities,0,104,7,1,0,system : Automatic Post-editing of Named Entities,0.7272727272727273,0.2413793103448276,0.09090909090909091
machine-translation,5,"NMT models so far have struggled with translating rare or unseen words ( not different surface forms , but rather different words ) correctly .",system,Automatic Post-editing of Named Entities,0,105,8,2,0,system : Automatic Post-editing of Named Entities,0.7342657342657343,0.27586206896551724,0.18181818181818182
machine-translation,5,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .",system,Automatic Post-editing of Named Entities,0,106,9,3,0,system : Automatic Post-editing of Named Entities,0.7412587412587412,0.3103448275862069,0.2727272727272727
machine-translation,5,"In order to aid the NMT model in translating such tokens better , we extracted named entity and non-translatable token dictionaries from the parallel corpora .",system,Automatic Post-editing of Named Entities,0,107,10,4,0,system : Automatic Post-editing of Named Entities,0.7482517482517482,0.3448275862068966,0.36363636363636365
machine-translation,5,"This was done by performing word alignment of the parallel corpora using fast align and searching ( in a language - agnostic manner ) for transliterated source - target word pairs using a similarity metric based on Levenshtein distance , which start with upper-case letters .",system,Automatic Post-editing of Named Entities,0,108,11,5,0,system : Automatic Post-editing of Named Entities,0.7552447552447552,0.3793103448275862,0.45454545454545453
machine-translation,5,The dictionaries consist of 15.6 ( 94.7 ) thousand and 6.2 ( 149.8 ) thousand entries for the constrained ( unconstrained ) English - Estonian and Estonian - English NMT systems respectively .,system,Automatic Post-editing of Named Entities,0,109,12,6,0,system : Automatic Post-editing of Named Entities,0.7622377622377622,0.41379310344827586,0.5454545454545454
machine-translation,5,"When the NMT systems had translated a sentence , source - to - target word alignment was extracted from the source sentence and the translation .",system,Automatic Post-editing of Named Entities,0,110,13,7,0,system : Automatic Post-editing of Named Entities,0.7692307692307693,0.4482758620689655,0.6363636363636364
machine-translation,5,"Then named entity recognition ( based on dictionary look - up ) was performed on the source text and , if a named entity was found , the target translation was validated against the entries in the dic-tionary .",system,Automatic Post-editing of Named Entities,0,111,14,8,0,system : Automatic Post-editing of Named Entities,0.7762237762237763,0.4827586206896552,0.7272727272727273
machine-translation,5,"In order to capture different surface forms , a stemming tool was used .",system,Automatic Post-editing of Named Entities,0,112,15,9,0,system : Automatic Post-editing of Named Entities,0.7832167832167832,0.5172413793103449,0.8181818181818182
machine-translation,5,"If a translation was contradicting the entries in the dictionary , it was replaced with the closest matching ( by looking for the longest matching suffix ) translation from the dictionary .",system,Automatic Post-editing of Named Entities,0,113,16,10,0,system : Automatic Post-editing of Named Entities,0.7902097902097902,0.5517241379310345,0.9090909090909091
machine-translation,5,"The automatic post-editing method for named entities has a marginal impact on translation quality , however , manual analysis showed that more named entities were corrected than ruined .",system,Automatic Post-editing of Named Entities,0,114,17,11,0,system : Automatic Post-editing of Named Entities,0.7972027972027972,0.5862068965517241,1.0
machine-translation,5,System Combination,system,System Combination,0,115,18,1,0,system : System Combination,0.8041958041958042,0.6206896551724138,0.08333333333333333
machine-translation,5,We attempted to increase the quality of existing translations by employing a voting scheme in which multiple machine translation outputs are combined to produce a single translation .,system,System Combination,0,116,19,2,0,system : System Combination,0.8111888111888111,0.6551724137931034,0.16666666666666666
machine-translation,5,We used a custom implementation of the majority voting algorithm to combine six of our best - scoring outputs in the Estonian - English translation direction in the constrained scenario .,system,System Combination,0,117,20,3,0,system : System Combination,0.8181818181818182,0.6896551724137931,0.25
machine-translation,5,We did not perform the combination for English - Estonian due to lack of support for alignment extraction for Estonian in Meteor .,system,System Combination,0,118,21,4,0,system : System Combination,0.8251748251748252,0.7241379310344828,0.3333333333333333
machine-translation,5,MT system translation combination happens on the sentence level .,system,System Combination,0,119,22,5,0,system : System Combination,0.8321678321678322,0.7586206896551724,0.4166666666666667
machine-translation,5,The majority voting scheme assumes a single base translation hypothesis ( primary hypothesis ) which is aligned at the word level to each of the other hypotheses ( secondary hypotheses ) .,system,System Combination,0,120,23,6,0,system : System Combination,0.8391608391608392,0.7931034482758621,0.5
machine-translation,5,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,system,System Combination,0,121,24,7,0,system : System Combination,0.8461538461538461,0.8275862068965517,0.5833333333333334
machine-translation,5,The table is then used to count the number of occurrences of different translations .,system,System Combination,0,122,25,8,0,system : System Combination,0.8531468531468531,0.8620689655172413,0.6666666666666666
machine-translation,5,The word translations with the highest count at each position constitute the resulting combined hypothesis .,system,System Combination,0,123,26,9,0,system : System Combination,0.8601398601398601,0.896551724137931,0.75
machine-translation,5,To acquire the necessary word alignments we used Meteor .,system,System Combination,0,124,27,10,0,system : System Combination,0.8671328671328671,0.9310344827586207,0.8333333333333334
machine-translation,5,Meteor outputs were then converted to a more easily manageable form using the Jane toolkit ) ( we used an awk script distributed with Jane ) .,system,System Combination,0,125,28,11,0,system : System Combination,0.8741258741258742,0.9655172413793104,0.9166666666666666
machine-translation,5,The majority voting algorithm was implemented in Python .,system,System Combination,0,126,29,12,0,system : System Combination,0.8811188811188811,1.0,1.0
machine-translation,5,Results,result,Results,0,127,1,1,0,result : Results,0.8881118881118881,0.1111111111111111,0.1111111111111111
machine-translation,5,We performed automatic evaluation of the NMT systems using the SacreBLEU evaluation tool .,result,Results,0,128,2,2,0,result : Results,0.8951048951048951,0.2222222222222222,0.2222222222222222
machine-translation,5,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,result,Results,1,129,3,3,0,result : Results,0.9020979020979021,0.3333333333333333,0.3333333333333333
machine-translation,5,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .",result,Results,1,130,4,4,0,result : Results,0.9090909090909091,0.4444444444444444,0.4444444444444444
machine-translation,5,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,result,Results,1,131,5,5,0,result : Results,0.916083916083916,0.5555555555555556,0.5555555555555556
machine-translation,5,"Although the unconstrained models were not trained on factored data , the datasets were 17 times larger than the constrained datasets .",result,Results,0,132,6,6,0,result : Results,0.9230769230769231,0.6666666666666666,0.6666666666666666
machine-translation,5,"However , the difference is rather minimal and shows that the current NMT architectures may notable to learn effectively from large datasets .",result,Results,0,133,7,7,0,result : Results,0.9300699300699301,0.7777777777777778,0.7777777777777778
machine-translation,5,The official human evaluation results ( see Table 5 ) from the WMT 2018 shared task on news translation our unconstrained scenario systems ( tilde - nc - nmt ) ranked significantly higher than any other submission for both translation directions .,result,Results,0,134,8,8,0,result : Results,0.9370629370629371,0.8888888888888888,0.8888888888888888
machine-translation,5,"Our best constrained systems were the second highest ranked systems among all constrained scenario systems , at the same time sharing the same cluster with the highest ranked systems .",result,Results,0,135,9,9,0,result : Results,0.9440559440559441,1.0,1.0
machine-translation,5,Conclusion,conclusion,Conclusion,0,136,1,1,0,conclusion : Conclusion,0.951048951048951,0.125,0.125
machine-translation,5,The paper described the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,conclusion,Conclusion,0,137,2,2,0,conclusion : Conclusion,0.958041958041958,0.25,0.25
machine-translation,5,We compared Transformer models to MLSTMbased models and showed that the Transformer models outperform the older NMT architecture .,conclusion,Conclusion,0,138,3,3,0,conclusion : Conclusion,0.965034965034965,0.375,0.375
machine-translation,5,We also showed that double back - translation may improve translation quality further than single back - translation .,conclusion,Conclusion,0,139,4,4,0,conclusion : Conclusion,0.972027972027972,0.5,0.5
machine-translation,5,"In terms of model ensembling and averaging , we showed that the best results in the constrained scenario were achieved by en - :",conclusion,Conclusion,0,140,5,5,0,conclusion : Conclusion,0.9790209790209791,0.625,0.625
machine-translation,5,Top three systems for the constrained ( C ) and unconstrained ( U ) scenarios according to the official results of the WMT 2018 shared task on news translation ; ordered by the direct assessment ( DA ) standardized mean score sembling different run averaged models .,conclusion,Conclusion,0,141,6,6,0,conclusion : Conclusion,0.986013986013986,0.75,0.75
machine-translation,5,"In total , seven systems were submitted by Tilde for the English ? Estonian language pair .",conclusion,Conclusion,0,142,7,7,0,conclusion : Conclusion,0.993006993006993,0.875,0.875
machine-translation,5,"In total , seven systems were submitted by Tilde for the English ? Estonian language pair .",conclusion,Conclusion,0,143,8,8,0,conclusion : Conclusion,1.0,1.0,1.0
machine-translation,6,FRAGE : Frequency - Agnostic Word Representation,title,title,1,2,1,1,0,title : title,0.006872852233676976,1.0,1.0
machine-translation,6,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.010309278350515464,0.14285714285714285,0.14285714285714285
machine-translation,6,Continuous word representation ( aka word embedding ) is a basic building block in many neural network - based models used in natural language processing tasks .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.013745704467353952,0.2857142857142857,0.2857142857142857
machine-translation,6,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .",abstract,abstract,1,5,3,3,0,abstract : abstract,0.01718213058419244,0.42857142857142855,0.42857142857142855
machine-translation,6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.020618556701030927,0.5714285714285714,0.5714285714285714
machine-translation,6,"In this paper , we develop a neat , simple yet effective way to learn FRequency - AGnostic word Embedding ( FRAGE ) using adversarial training .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.024054982817869417,0.7142857142857143,0.7142857142857143
machine-translation,6,"We conducted comprehensive studies on ten datasets across four natural language processing tasks , including word similarity , language modeling , machine translation and text classification .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.027491408934707903,0.8571428571428571,0.8571428571428571
machine-translation,6,"Results show that with FRAGE , we achieve higher performance than the baselines in all tasks .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.030927835051546393,1.0,1.0
machine-translation,6,Introduction,introduction,introduction,0,10,1,1,0,introduction : introduction,0.03436426116838488,0.02857142857142857,0.02857142857142857
machine-translation,6,"Word embeddings , which are distributed and continuous vector representations for word tokens , have been one of the basic building blocks for many neural network - based models used in natural language processing ( NLP ) tasks , such as language modeling , text classification and machine translation .",introduction,introduction,0,11,2,2,0,introduction : introduction,0.037800687285223365,0.05714285714285714,0.05714285714285714
machine-translation,6,"Different from classic one - hot representation , the learned word embeddings contain semantic information which can measure the semantic similarity between words , and can also be transferred into other learning tasks .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.041237113402061855,0.08571428571428572,0.08571428571428572
machine-translation,6,"In deep learning approaches for NLP tasks , word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.044673539518900345,0.11428571428571428,0.11428571428571428
machine-translation,6,"As the inputs of the neural network , word embeddings carryall the information of words that will be further processed by the network , and the quality of embeddings is critical and highly impacts the final performance of the learning task .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.048109965635738834,0.14285714285714285,0.14285714285714285
machine-translation,6,"Unfortunately , we find the word embeddings learned by many deep learning approaches are far from perfect .",introduction,introduction,0,15,6,6,0,introduction : introduction,0.05154639175257732,0.17142857142857143,0.17142857142857143
machine-translation,6,"As shown in ( a ) and 1 ( b ) , in the embedding space learned by word2 vec model , the nearest neighbors of word "" Peking "" includes "" quickest "" , "" multicellular "" , and "" epigenetic "" , which are not semantically similar , while semantically related words such as "" Beijing "" and "" China "" are far from it .",introduction,introduction,0,16,7,7,0,introduction : introduction,0.054982817869415807,0.2,0.2
machine-translation,6,Similar phenomena are observed from the word embeddings learned from translation tasks .,introduction,introduction,0,17,8,8,0,introduction : introduction,0.058419243986254296,0.22857142857142856,0.22857142857142856
machine-translation,6,"With a careful study , we find a more general problem which is rooted in low - frequency words in the text corpus .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.061855670103092786,0.2571428571428571,0.2571428571428571
machine-translation,6,"Without any confusion , we also call high - frequency words as popular words and call low - frequency words as rare words .",introduction,introduction,0,19,10,10,0,introduction : introduction,0.06529209621993128,0.2857142857142857,0.2857142857142857
machine-translation,6,"As is well known , the frequency distribution of words roughly follows a simple mathematical form known as Zipf 's law .",introduction,introduction,0,20,11,11,0,introduction : introduction,0.06872852233676977,0.3142857142857143,0.3142857142857143
machine-translation,6,"When the size of a text corpus grows , the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words .",introduction,introduction,0,21,12,12,0,introduction : introduction,0.07216494845360824,0.34285714285714286,0.34285714285714286
machine-translation,6,"Interestingly , the learned embeddings of rare words and popular words behave differently .",introduction,introduction,1,22,13,13,0,introduction : introduction,0.07560137457044673,0.37142857142857144,0.37142857142857144
machine-translation,6,"In the embedding space , a popular word usually has semantically related neighbors , while a rare word usually does not .",introduction,introduction,0,23,14,14,0,introduction : introduction,0.07903780068728522,0.4,0.4
machine-translation,6,"Moreover , the nearest neighbors of more than 85 % rare words are rare words .",introduction,introduction,0,24,15,15,0,introduction : introduction,0.08247422680412371,0.42857142857142855,0.42857142857142855
machine-translation,6,Word embeddings encode frequency information .,introduction,introduction,0,25,16,16,0,introduction : introduction,0.0859106529209622,0.45714285714285713,0.45714285714285713
machine-translation,6,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .",introduction,introduction,1,26,17,17,0,introduction : introduction,0.08934707903780069,0.4857142857142857,0.4857142857142857
machine-translation,6,Such a phenomenon is also observed in .,introduction,introduction,0,27,18,18,0,introduction : introduction,0.09278350515463918,0.5142857142857142,0.5142857142857142
machine-translation,6,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,introduction,introduction,1,28,19,19,0,introduction : introduction,0.09621993127147767,0.5428571428571428,0.5428571428571428
machine-translation,6,"First , such embeddings will affect the semantic understanding of words .",introduction,introduction,0,29,20,20,0,introduction : introduction,0.09965635738831616,0.5714285714285714,0.5714285714285714
machine-translation,6,We observe more than half of the rare words are nouns or variants of popular words .,introduction,introduction,0,30,21,21,0,introduction : introduction,0.10309278350515463,0.6,0.6
machine-translation,6,Those rare words should have similar meanings or share the same topics with popular words .,introduction,introduction,0,31,22,22,0,introduction : introduction,0.10652920962199312,0.6285714285714286,0.6285714285714286
machine-translation,6,"Second , the neighbors of a large number of rare words are semantically unrelated rare words .",introduction,introduction,0,32,23,23,0,introduction : introduction,0.10996563573883161,0.6571428571428571,0.6571428571428571
machine-translation,6,"To some extent , those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding .",introduction,introduction,0,33,24,24,0,introduction : introduction,0.1134020618556701,0.6857142857142857,0.6857142857142857
machine-translation,6,It will consequently limit the performance of down - stream tasks using the embeddings .,introduction,introduction,0,34,25,25,0,introduction : introduction,0.11683848797250859,0.7142857142857143,0.7142857142857143
machine-translation,6,"For example , in text classification , it can not be well guaranteed that the label of a sentence does not change when you replace one popular / rare word in the sentence by its rare / popular alternatives .",introduction,introduction,0,35,26,26,0,introduction : introduction,0.12027491408934708,0.7428571428571429,0.7428571428571429
machine-translation,6,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .",introduction,introduction,1,36,27,27,0,introduction : introduction,0.12371134020618557,0.7714285714285715,0.7714285714285715
machine-translation,6,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",introduction,introduction,1,37,28,28,0,introduction : introduction,0.12714776632302405,0.8,0.8
machine-translation,6,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .",introduction,introduction,1,38,29,29,0,introduction : introduction,0.13058419243986255,0.8285714285714286,0.8285714285714286
machine-translation,6,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .",introduction,introduction,1,39,30,30,0,introduction : introduction,0.13402061855670103,0.8571428571428571,0.8571428571428571
machine-translation,6,"Consequently , rare words lie in the same region as and are mixed with popular words in the embedding space .",introduction,introduction,0,40,31,31,0,introduction : introduction,0.13745704467353953,0.8857142857142857,0.8857142857142857
machine-translation,6,Then FRAGE will catch better semantic information and help the task - specific model to perform better .,introduction,introduction,0,41,32,32,0,introduction : introduction,0.140893470790378,0.9142857142857143,0.9142857142857143
machine-translation,6,"We conduct experiments on four types of NLP tasks , including three word similarity tasks , two language modeling tasks , three sentiment classification tasks and two machine translation tasks to test our method .",introduction,introduction,0,42,33,33,0,introduction : introduction,0.14432989690721648,0.9428571428571428,0.9428571428571428
machine-translation,6,"In all tasks , FRAGE outperforms the baselines .",introduction,introduction,0,43,34,34,0,introduction : introduction,0.14776632302405499,0.9714285714285714,0.9714285714285714
machine-translation,6,"Specifically , in language modeling and machine translation , we achieve better performance than the state - of - the - art results on PTB , WT2 and WMT14 English - German datasets .",introduction,introduction,0,44,35,35,0,introduction : introduction,0.15120274914089346,1.0,1.0
machine-translation,6,Background,background,Background,0,45,1,1,0,background : Background,0.15463917525773196,1.0,1.0
machine-translation,6,Word Representation,system description,Word Representation,0,46,1,1,0,system description : Word Representation,0.15807560137457044,0.16666666666666666,0.16666666666666666
machine-translation,6,"Words are the basic units of natural languages , and distributed word representations ( i.e. , word embeddings ) are the basic units of many models in NLP tasks including language modeling and machine translation .",system description,Word Representation,0,47,2,2,0,system description : Word Representation,0.16151202749140894,0.3333333333333333,0.3333333333333333
machine-translation,6,It has been demonstrated that word representations learned from one task can be transferred to other tasks and achieve competitive performance .,system description,Word Representation,0,48,3,3,0,system description : Word Representation,0.16494845360824742,0.5,0.5
machine-translation,6,"While word embeddings play an important role in neural network - based models in NLP and achieve great success , one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences .",system description,Word Representation,0,49,4,4,0,system description : Word Representation,0.16838487972508592,0.6666666666666666,0.6666666666666666
machine-translation,6,develops a novel way to split word into sub- word units which is widely used in neural machine translation .,system description,Word Representation,0,50,5,5,0,system description : Word Representation,0.1718213058419244,0.8333333333333334,0.8333333333333334
machine-translation,6,"However , the low - frequency sub- word units are still difficult to train : provides a comprehensive study which shows that the rare ( sub ) words are usually under-estimated in neural machine translation : during inference step , the model tends to choose popular words over their rare alternatives .",system description,Word Representation,0,51,6,6,0,system description : Word Representation,0.17525773195876287,1.0,1.0
machine-translation,6,Adversarial Training,training,Adversarial Training,0,52,1,1,0,training : Adversarial Training,0.17869415807560138,0.07692307692307693,0.1
machine-translation,6,"The basic idea of our work to address the above problem is adversarial training , in which two or more models learn together by pursuing competing goals .",training,Adversarial Training,0,53,2,2,0,training : Adversarial Training,0.18213058419243985,0.15384615384615385,0.2
machine-translation,6,"representative example of adversarial training is Generative Adversarial Networks ( GANs ) for image generation , in which a discriminator and a generator compete with each other : the generator aims to generate images similar to the natural ones , and the discriminator aims to detect the generated ones from the natural ones .",training,Adversarial Training,0,54,3,3,0,training : Adversarial Training,0.18556701030927836,0.23076923076923078,0.3
machine-translation,6,"Recently , adversarial training has been successfully applied to NLP tasks .",training,Adversarial Training,0,55,4,4,0,training : Adversarial Training,0.18900343642611683,0.3076923076923077,0.4
machine-translation,6,introduce an additional discriminator to differentiate the semantics learned from different languages in non-parallel bilingual data .,training,Adversarial Training,0,56,5,5,0,training : Adversarial Training,0.19243986254295534,0.38461538461538464,0.5
machine-translation,6,develops a discriminator to classify whether a sentence is created by human or generated by a model .,training,Adversarial Training,0,57,6,6,0,training : Adversarial Training,0.1958762886597938,0.46153846153846156,0.6
machine-translation,6,Our proposed method is under the adversarial training framework but not exactly the conventional generator - discriminator approach since there is no generator in our scenario .,training,Adversarial Training,0,58,7,7,0,training : Adversarial Training,0.19931271477663232,0.5384615384615384,0.7
machine-translation,6,"For an NLP task and its neural network model ( including word embeddings ) , we introduce a discriminator to differentiate embeddings of popular words and rare words ; while the NN model aims to fool the discriminator and minimize the task - specific loss simultaneously .",training,Adversarial Training,0,59,8,8,0,training : Adversarial Training,0.2027491408934708,0.6153846153846154,0.8
machine-translation,6,Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the negative effects of domain shift between training and testing .,training,Adversarial Training,0,60,9,9,0,training : Adversarial Training,0.20618556701030927,0.6923076923076923,0.9
machine-translation,6,"The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing ; instead , we aim to improve the effectiveness of word embeddings and consequently improve the performance of end - to - end NLP tasks .",training,Adversarial Training,0,61,10,10,0,training : Adversarial Training,0.20962199312714777,0.7692307692307693,1.0
machine-translation,6,Empirical Study,training,Empirical Study,0,62,11,1,0,training : Empirical Study,0.21305841924398625,0.8461538461538461,0.3333333333333333
machine-translation,6,"In this section , we study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English - German translation task using Transformer .",training,Empirical Study,0,63,12,2,0,training : Empirical Study,0.21649484536082475,0.9230769230769231,0.6666666666666666
machine-translation,6,The implementation details can be found in the supplementary material ( part A ) .,training,Empirical Study,0,64,13,3,0,training : Empirical Study,0.21993127147766323,1.0,1.0
machine-translation,6,Experimental Design,experiment,Experimental Design,0,65,1,1,0,experiment : Experimental Design,0.22336769759450173,0.05,0.058823529411764705
machine-translation,6,"In both tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words ( roughly speaking , we set a word as a rare word if it s relative frequency is lower than 10 ? 6 in WMT14 dataset and 10 ? 7 in Google News dataset ) .",experiment,Experimental Design,0,66,2,2,0,experiment : Experimental Design,0.2268041237113402,0.1,0.11764705882352941
machine-translation,6,We have tried other thresholds such as 10 % or 25 % and found the observations are similar .,experiment,Experimental Design,0,67,3,3,0,experiment : Experimental Design,0.23024054982817868,0.15,0.17647058823529413
machine-translation,6,We study whether the semantic relationship between two words is reasonable .,experiment,Experimental Design,0,68,4,4,0,experiment : Experimental Design,0.23367697594501718,0.2,0.23529411764705882
machine-translation,6,"To achieve this , we randomly sampled some rare / popular words and checked the embeddings trained from different tasks .",experiment,Experimental Design,0,69,5,5,0,experiment : Experimental Design,0.23711340206185566,0.25,0.29411764705882354
machine-translation,6,"For each sampled word , we determined its nearest neighbors based on the cosine similarity between its embeddings and others '.",experiment,Experimental Design,0,70,6,6,0,experiment : Experimental Design,0.24054982817869416,0.3,0.35294117647058826
machine-translation,6,We also manually chose words which are semantically similar to it .,experiment,Experimental Design,0,71,7,7,0,experiment : Experimental Design,0.24398625429553264,0.35,0.4117647058823529
machine-translation,6,"For simplicity , for each word , we call the nearest words predicted from the embeddings as model - predicted neighbors , and call our chosen words as semantic neighbors .",experiment,Experimental Design,0,72,8,8,0,experiment : Experimental Design,0.24742268041237114,0.4,0.47058823529411764
machine-translation,6,Observation,experiment,Experimental Design,0,73,9,9,0,experiment : Experimental Design,0.2508591065292096,0.45,0.5294117647058824
machine-translation,6,"To visualize word embeddings , we reduce their dimensionalities by SVD and plot two cases in .",experiment,Experimental Design,0,74,10,10,0,experiment : Experimental Design,0.2542955326460481,0.5,0.5882352941176471
machine-translation,6,More cases and other studies without dimensionality reduction can be found in the supplementary material ( part C ) .,experiment,Experimental Design,0,75,11,11,0,experiment : Experimental Design,0.25773195876288657,0.55,0.6470588235294118
machine-translation,6,We find that the embeddings trained from different tasks share some common patterns .,experiment,Experimental Design,0,76,12,12,0,experiment : Experimental Design,0.2611683848797251,0.6,0.7058823529411765
machine-translation,6,"For both tasks , more than 90 % of model - predicted neighbors of rare words are rare words .",experiment,Experimental Design,0,77,13,13,0,experiment : Experimental Design,0.2646048109965636,0.65,0.7647058823529411
machine-translation,6,"For each rare word , the model - predicted neighbor is usually not semantically related to this word , and semantic neighbors we chose are faraway from it in the embedding space .",experiment,Experimental Design,0,78,14,14,0,experiment : Experimental Design,0.26804123711340205,0.7,0.8235294117647058
machine-translation,6,"In contrast , the model - predicted neighbors of popular words are very reasonable .",experiment,Experimental Design,0,79,15,15,0,experiment : Experimental Design,0.27147766323024053,0.75,0.8823529411764706
machine-translation,6,"As the patterns in rare words are different from that of popular words , we further check the whole embedding matrix to make a general understanding .",experiment,Experimental Design,0,80,16,16,0,experiment : Experimental Design,0.27491408934707906,0.8,0.9411764705882353
machine-translation,6,We also visualize the word embeddings using SVD by keeping the two directions with top - 2 largest eigenvalues as in and plot them in,experiment,Experimental Design,0,81,17,17,0,experiment : Experimental Design,0.27835051546391754,0.85,1.0
machine-translation,6,Input Tokens Word Embeddings,experiment,Input Tokens Word Embeddings,0,82,18,1,0,experiment : Input Tokens Word Embeddings,0.281786941580756,0.9,0.5
machine-translation,6,Task - specific Outputs,experiment,Input Tokens Word Embeddings,0,83,19,2,0,experiment : Input Tokens Word Embeddings,0.2852233676975945,0.95,1.0
machine-translation,6,Task - specific,experiment,Task-specific Model,0,84,20,1,0,experiment : Task-specific Model,0.28865979381443296,1.0,1.0
machine-translation,6,Model,model,model,0,85,1,1,0,model : model,0.2920962199312715,0.06666666666666667,1.0
machine-translation,6,Loss,model,Loss,0,86,2,1,0,model : Loss,0.29553264604810997,0.13333333333333333,0.07142857142857142
machine-translation,6,Rare / Popular Labels Discriminator,model,Loss,0,87,3,2,0,model : Loss,0.29896907216494845,0.2,0.14285714285714285
machine-translation,6,Loss predict predict :,model,Loss,0,88,4,3,0,model : Loss,0.3024054982817869,0.26666666666666666,0.21428571428571427
machine-translation,6,"The proposed learning framework includes a task - specific predictor and a discriminator , whose function is to classify rare and popular words .",model,Loss,0,89,5,4,0,model : Loss,0.30584192439862545,0.3333333333333333,0.2857142857142857
machine-translation,6,Both modules use word embeddings as the input .,model,Loss,0,90,6,5,0,model : Loss,0.30927835051546393,0.4,0.35714285714285715
machine-translation,6,"certain degree : the rare words and popular words lie in different regions after this linear projection , and thus they occupy different regions in the original embedding space .",model,Loss,0,91,7,6,0,model : Loss,0.3127147766323024,0.4666666666666667,0.42857142857142855
machine-translation,6,This strange phenomenon is also observed in other learned embeddings ( e.g. CBOW and GLOVE ) and mentioned in .,model,Loss,0,92,8,7,0,model : Loss,0.3161512027491409,0.5333333333333333,0.5
machine-translation,6,Explanation,model,Loss,0,93,9,8,0,model : Loss,0.31958762886597936,0.6,0.5714285714285714
machine-translation,6,"From the empirical study above , we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason .",model,Loss,0,94,10,9,0,model : Loss,0.3230240549828179,0.6666666666666666,0.6428571428571429
machine-translation,6,We simply take word2vec as an example which is trained by stochastic gradient descent .,model,Loss,0,95,11,10,0,model : Loss,0.32646048109965636,0.7333333333333333,0.7142857142857143
machine-translation,6,"During training , the sample rate of a popular word is high and the embedding of a popular word updates frequently .",model,Loss,0,96,12,11,0,model : Loss,0.32989690721649484,0.8,0.7857142857142857
machine-translation,6,"For a rare word , the sample rate is low and its embedding rarely updates .",model,Loss,0,97,13,12,0,model : Loss,0.3333333333333333,0.8666666666666667,0.8571428571428571
machine-translation,6,"According to our study , on average , the moving distance of the embedding for a popular word is twice longer than that of a rare word during training .",model,Loss,0,98,14,13,0,model : Loss,0.33676975945017185,0.9333333333333333,0.9285714285714286
machine-translation,6,"As all word embeddings are usually initialized around the origin with a small variance , we observe in the final model , the embeddings of rare words are still around the origin and the popular words have moved faraway .",model,Loss,0,99,15,14,0,model : Loss,0.3402061855670103,1.0,1.0
machine-translation,6,Discussion,discussion,Discussion,0,100,1,1,0,discussion : Discussion,0.3436426116838488,0.1111111111111111,0.1111111111111111
machine-translation,6,We have strong evidence that the current phenomena are problematic .,discussion,Discussion,0,101,2,2,0,discussion : Discussion,0.3470790378006873,0.2222222222222222,0.2222222222222222
machine-translation,6,"First , according to our study , in both tasks , more than half of the rare words are nouns , e.g. , company names , city names .",discussion,Discussion,0,102,3,3,0,discussion : Discussion,0.35051546391752575,0.3333333333333333,0.3333333333333333
machine-translation,6,"They may share some similar topics to popular entities , e.g. , big companies and cities ; around 10 % percent of rare words include a hyphen ( which is usually used to join popular words ) , and over 30 % rare words are different PoSs of popular words .",discussion,Discussion,0,103,4,4,0,discussion : Discussion,0.3539518900343643,0.4444444444444444,0.4444444444444444
machine-translation,6,These words should have mixed or similar semantics to some popular words .,discussion,Discussion,0,104,5,5,0,discussion : Discussion,0.35738831615120276,0.5555555555555556,0.5555555555555556
machine-translation,6,"These facts show that rare words and popular words should lie in the same region of the embedding space , which is different from what we observed .",discussion,Discussion,0,105,6,6,0,discussion : Discussion,0.36082474226804123,0.6666666666666666,0.6666666666666666
machine-translation,6,"Second , as we can see from the cases , for rare words , model - predicted neighbors are usually not semantically related words but frequency - related words ( rare words ) .",discussion,Discussion,0,106,7,7,0,discussion : Discussion,0.3642611683848797,0.7777777777777778,0.7777777777777778
machine-translation,6,"This shows , for rare words , the embeddings encode more frequency information than semantic information .",discussion,Discussion,0,107,8,8,0,discussion : Discussion,0.36769759450171824,0.8888888888888888,0.8888888888888888
machine-translation,6,"It is not good to use such word embeddings into semantic understanding tasks , e.g. , text classification , language modeling , language understanding and translation .",discussion,Discussion,0,108,9,9,0,discussion : Discussion,0.3711340206185567,1.0,1.0
machine-translation,6,Our Method,method,Our Method,0,109,1,1,0,method : Our Method,0.3745704467353952,0.020833333333333332,0.020833333333333332
machine-translation,6,"In this section , we present our method to improve word representations .",method,Our Method,0,110,2,2,0,method : Our Method,0.37800687285223367,0.041666666666666664,0.041666666666666664
machine-translation,6,"As we have a strong prior that many rare words should share the same region in the embedding space as popular words , the basic idea of our algorithm is to train the word embeddings in an adversarial framework :",method,Our Method,0,111,3,3,0,method : Our Method,0.38144329896907214,0.0625,0.0625
machine-translation,6,We introduce a discriminator to categorize word embeddings into two classes : popular ones or rare ones .,method,Our Method,0,112,4,4,0,method : Our Method,0.3848797250859107,0.08333333333333333,0.08333333333333333
machine-translation,6,We hope the learned word embeddings not only minimize the task - specific training loss but also fool the discriminator .,method,Our Method,0,113,5,5,0,method : Our Method,0.38831615120274915,0.10416666666666667,0.10416666666666667
machine-translation,6,"By doing so , the frequency information is removed from the embedding and we call our method frequency - agnostic word embedding ( FRAGE ) .",method,Our Method,0,114,6,6,0,method : Our Method,0.3917525773195876,0.125,0.125
machine-translation,6,We first define some notations and then introduce our algorithm .,method,Our Method,0,115,7,7,0,method : Our Method,0.3951890034364261,0.14583333333333334,0.14583333333333334
machine-translation,6,"We develop three types of notations : embeddings , task - specific parameters / loss , and discriminator parameters / loss .",method,Our Method,0,116,8,8,0,method : Our Method,0.39862542955326463,0.16666666666666666,0.16666666666666666
machine-translation,6,"Denote ? emb ? R d|V | as the word embedding matrix to be learned , where d is the dimension of the embedding vectors and | V | is the vocabulary size .",method,Our Method,0,117,9,9,0,method : Our Method,0.4020618556701031,0.1875,0.1875
machine-translation,6,"Denote ? emb ? R d|V | as the word embedding matrix to be learned , where d is the dimension of the embedding vectors and | V | is the vocabulary size .",method,Our Method,0,118,10,10,0,method : Our Method,0.4054982817869416,0.20833333333333334,0.20833333333333334
machine-translation,6,Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words .,method,Our Method,0,119,11,11,0,method : Our Method,0.40893470790378006,0.22916666666666666,0.22916666666666666
machine-translation,6,Then the embedding matrix ? emb can be divided into two parts : ? emb pop for popular words and ? emb rare for rare words .,method,Our Method,0,120,12,12,0,method : Our Method,0.41237113402061853,0.25,0.25
machine-translation,6,Then the embedding matrix ? emb can be divided into two parts : ? emb pop for popular words and ? emb rare for rare words .,method,Our Method,0,121,13,13,0,method : Our Method,0.41580756013745707,0.2708333333333333,0.2708333333333333
machine-translation,6,Then the embedding matrix ? emb can be divided into two parts : ? emb pop for popular words and ? emb rare for rare words .,method,Our Method,0,122,14,14,0,method : Our Method,0.41924398625429554,0.2916666666666667,0.2916666666666667
machine-translation,6,Then the embedding matrix ? emb can be divided into two parts : ? emb pop for popular words and ? emb rare for rare words .,method,Our Method,0,123,15,15,0,method : Our Method,0.422680412371134,0.3125,0.3125
machine-translation,6,Let ? emb w denote the embedding of word w .,method,Our Method,0,124,16,16,0,method : Our Method,0.4261168384879725,0.3333333333333333,0.3333333333333333
machine-translation,6,Let ? emb w denote the embedding of word w .,method,Our Method,0,125,17,17,0,method : Our Method,0.42955326460481097,0.3541666666666667,0.3541666666666667
machine-translation,6,Let ? model denote all the other task - specific parameters except word embeddings .,method,Our Method,0,126,18,18,0,method : Our Method,0.4329896907216495,0.375,0.375
machine-translation,6,Let ? model denote all the other task - specific parameters except word embeddings .,method,Our Method,0,127,19,19,0,method : Our Method,0.436426116838488,0.3958333333333333,0.3958333333333333
machine-translation,6,"For instance , for language modeling , ? model is the parameters of the RNN or LSTM ; for neural machine translation , ? model is the parameters of the encoder , attention module and decoder .",method,Our Method,0,128,20,20,0,method : Our Method,0.43986254295532645,0.4166666666666667,0.4166666666666667
machine-translation,6,"For instance , for language modeling , ? model is the parameters of the RNN or LSTM ; for neural machine translation , ? model is the parameters of the encoder , attention module and decoder .",method,Our Method,0,129,21,21,0,method : Our Method,0.44329896907216493,0.4375,0.4375
machine-translation,6,"For instance , for language modeling , ? model is the parameters of the RNN or LSTM ; for neural machine translation , ? model is the parameters of the encoder , attention module and decoder .",method,Our Method,0,130,22,22,0,method : Our Method,0.44673539518900346,0.4583333333333333,0.4583333333333333
machine-translation,6,"Let L T ( S ; ? model , ? emb ) denote the task - specific loss over a dataset S. Taking language modeling as an example , the loss L T ( S ; ? model , ? emb ) is defined as the negative log likelihood of the data :",method,Our Method,0,131,23,23,0,method : Our Method,0.45017182130584193,0.4791666666666667,0.4791666666666667
machine-translation,6,where y is a sentence .,method,Our Method,0,132,24,24,0,method : Our Method,0.4536082474226804,0.5,0.5
machine-translation,6,"Let f ? D denote a discriminator with parameters ? D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",method,Our Method,0,133,25,25,0,method : Our Method,0.4570446735395189,0.5208333333333334,0.5208333333333334
machine-translation,6,"Let f ? D denote a discriminator with parameters ? D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",method,Our Method,0,134,26,26,0,method : Our Method,0.46048109965635736,0.5416666666666666,0.5416666666666666
machine-translation,6,"Let f ? D denote a discriminator with parameters ? D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",method,Our Method,0,135,27,27,0,method : Our Method,0.4639175257731959,0.5625,0.5625
machine-translation,6,"Let L D ( V ; ? D , ? emb ) denote the loss of the discriminator :",method,Our Method,0,136,28,28,0,method : Our Method,0.46735395189003437,0.5833333333333334,0.5833333333333334
machine-translation,6,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ? model and ? emb ) and the discriminator ( ? D ) as below :",method,Our Method,0,137,29,29,0,method : Our Method,0.47079037800687284,0.6041666666666666,0.6041666666666666
machine-translation,6,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ? model and ? emb ) and the discriminator ( ? D ) as below :",method,Our Method,0,138,30,30,0,method : Our Method,0.4742268041237113,0.625,0.625
machine-translation,6,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ? model and ? emb ) and the discriminator ( ? D ) as below :",method,Our Method,0,139,31,31,0,method : Our Method,0.47766323024054985,0.6458333333333334,0.6458333333333334
machine-translation,6,where ? is a coefficient to trade off the two loss terms .,method,Our Method,0,140,32,32,0,method : Our Method,0.48109965635738833,0.6666666666666666,0.6666666666666666
machine-translation,6,where ? is a coefficient to trade off the two loss terms .,method,Our Method,0,141,33,33,0,method : Our Method,0.4845360824742268,0.6875,0.6875
machine-translation,6,"We can see that when the model parameter ? model and the embedding ? emb are fixed , the optimization of the discriminator ? D becomes",method,Our Method,0,142,34,34,0,method : Our Method,0.4879725085910653,0.7083333333333334,0.7083333333333334
machine-translation,6,"We can see that when the model parameter ? model and the embedding ? emb are fixed , the optimization of the discriminator ? D becomes",method,Our Method,0,143,35,35,0,method : Our Method,0.49140893470790376,0.7291666666666666,0.7291666666666666
machine-translation,6,"We can see that when the model parameter ? model and the embedding ? emb are fixed , the optimization of the discriminator ? D becomes",method,Our Method,0,144,36,36,0,method : Our Method,0.4948453608247423,0.75,0.75
machine-translation,6,"We can see that when the model parameter ? model and the embedding ? emb are fixed , the optimization of the discriminator ? D becomes",method,Our Method,0,145,37,37,0,method : Our Method,0.49828178694158076,0.7708333333333334,0.7708333333333334
machine-translation,6,which is to minimize the classification error of popular and rare words .,method,Our Method,0,146,38,38,0,method : Our Method,0.5017182130584192,0.7916666666666666,0.7916666666666666
machine-translation,6,"When the discriminator ? Dis fixed , the optimization of ? model and ? emb becomes",method,Our Method,0,147,39,39,0,method : Our Method,0.5051546391752577,0.8125,0.8125
machine-translation,6,"When the discriminator ? Dis fixed , the optimization of ? model and ? emb becomes",method,Our Method,0,148,40,40,0,method : Our Method,0.5085910652920962,0.8333333333333334,0.8333333333333334
machine-translation,6,"When the discriminator ? Dis fixed , the optimization of ? model and ? emb becomes",method,Our Method,0,149,41,41,0,method : Our Method,0.5120274914089347,0.8541666666666666,0.8541666666666666
machine-translation,6,"When the discriminator ? Dis fixed , the optimization of ? model and ? emb becomes",method,Our Method,0,150,42,42,0,method : Our Method,0.5154639175257731,0.875,0.875
machine-translation,6,", to optimize the task performance as well as fooling the discriminator .",method,Our Method,0,151,43,43,0,method : Our Method,0.5189003436426117,0.8958333333333334,0.8958333333333334
machine-translation,6,"We train ? model , ? emb and ? D iteratively by stochastic gradient descent or its variants .",method,Our Method,0,152,44,44,0,method : Our Method,0.5223367697594502,0.9166666666666666,0.9166666666666666
machine-translation,6,"We train ? model , ? emb and ? D iteratively by stochastic gradient descent or its variants .",method,Our Method,0,153,45,45,0,method : Our Method,0.5257731958762887,0.9375,0.9375
machine-translation,6,"We train ? model , ? emb and ? D iteratively by stochastic gradient descent or its variants .",method,Our Method,0,154,46,46,0,method : Our Method,0.5292096219931272,0.9583333333333334,0.9583333333333334
machine-translation,6,"We train ? model , ? emb and ? D iteratively by stochastic gradient descent or its variants .",method,Our Method,0,155,47,47,0,method : Our Method,0.5326460481099656,0.9791666666666666,0.9791666666666666
machine-translation,6,The general training process is shown in Algorithm 1 .,method,Our Method,0,156,48,48,0,method : Our Method,0.5360824742268041,1.0,1.0
machine-translation,6,Experiment,experiment,Experiment,0,157,1,1,0,experiment : Experiment,0.5395189003436426,0.019230769230769232,0.2
machine-translation,6,"We test our method on a wide range of tasks , including word similarity , language modeling , machine translation and text classification .",experiment,Experiment,0,158,2,2,0,experiment : Experiment,0.5429553264604811,0.038461538461538464,0.4
machine-translation,6,"For each task , we choose the state - of - the - art architecture together with the state - of - the - art training method as our baseline .",experiment,Experiment,0,159,3,3,0,experiment : Experiment,0.5463917525773195,0.057692307692307696,0.6
machine-translation,6,Sample a minibatch ? from S.,experiment,Experiment,0,160,4,4,0,experiment : Experiment,0.5498281786941581,0.07692307692307693,0.8
machine-translation,6,Sample a minibatch ? from S.,experiment,Experiment,0,161,5,5,0,experiment : Experiment,0.5532646048109966,0.09615384615384616,1.0
machine-translation,6,4 :,experiment,4:,0,162,6,1,0,experiment : 4:,0.5567010309278351,0.11538461538461539,0.3333333333333333
machine-translation,6,Sample a minibatchV = V pop ? V rare from V .,experiment,4:,0,163,7,2,0,experiment : 4:,0.5601374570446735,0.1346153846153846,0.6666666666666666
machine-translation,6,Sample a minibatchV = V pop ? V rare from V .,experiment,4:,0,164,8,3,0,experiment : 4:,0.563573883161512,0.15384615384615385,1.0
machine-translation,6,5 :,experiment,5:,0,165,9,1,0,experiment : 5:,0.5670103092783505,0.17307692307692307,0.1
machine-translation,6,"Update ? model , ? emb by gradient descent according to Eqn. ( 5 ) with data ?.",experiment,5:,0,166,10,2,0,experiment : 5:,0.570446735395189,0.19230769230769232,0.2
machine-translation,6,"Update ? model , ? emb by gradient descent according to Eqn. ( 5 ) with data ?.",experiment,5:,0,167,11,3,0,experiment : 5:,0.5738831615120275,0.21153846153846154,0.3
machine-translation,6,Update ? D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,experiment,5:,0,168,12,4,0,experiment : 5:,0.5773195876288659,0.23076923076923078,0.4
machine-translation,6,Update ? D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,experiment,5:,0,169,13,5,0,experiment : 5:,0.5807560137457045,0.25,0.5
machine-translation,6,": until Converge 8 : Output : ? model , ? emb , ? D .",experiment,5:,0,170,14,6,0,experiment : 5:,0.584192439862543,0.2692307692307692,0.6
machine-translation,6,": until Converge 8 : Output : ? model , ? emb , ? D .",experiment,5:,0,171,15,7,0,experiment : 5:,0.5876288659793815,0.28846153846153844,0.7
machine-translation,6,"For fair comparisons , for each task , our method shares the same model architecture as the baseline .",experiment,5:,0,172,16,8,0,experiment : 5:,0.5910652920962199,0.3076923076923077,0.8
machine-translation,6,The only difference is that we use the original task - specific loss function with an additional adversarial loss as in Eqn..,experiment,5:,0,173,17,9,0,experiment : 5:,0.5945017182130584,0.3269230769230769,0.9
machine-translation,6,"Due to space limitations , we put dataset description , model description , hyperparameter configuration into supplementary material ( part A ) .",experiment,5:,0,174,18,10,0,experiment : 5:,0.5979381443298969,0.34615384615384615,1.0
machine-translation,6,Settings,experiment,Settings,0,175,19,1,0,experiment : Settings,0.6013745704467354,0.36538461538461536,0.029411764705882353
machine-translation,6,We conduct experiments on the following tasks .,experiment,Settings,0,176,20,2,0,experiment : Settings,0.6048109965635738,0.38461538461538464,0.058823529411764705
machine-translation,6,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .",experiment,Settings,1,177,21,3,0,experiment : Settings,0.6082474226804123,0.40384615384615385,0.08823529411764706
machine-translation,6,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .",experiment,Settings,1,178,22,4,0,experiment : Settings,0.6116838487972509,0.4230769230769231,0.11764705882352941
machine-translation,6,"We test the baseline and our method on three datasets : RG65 , WS and RW .",experiment,Settings,1,179,23,5,0,experiment : Settings,0.6151202749140894,0.4423076923076923,0.14705882352941177
machine-translation,6,The RW dataset is a dataset for the evaluation of rare words .,experiment,Settings,0,180,24,6,0,experiment : Settings,0.6185567010309279,0.46153846153846156,0.17647058823529413
machine-translation,6,"Following common practice , we use cosine distance while computing the similarity between two word embeddings .",experiment,Settings,0,181,25,7,0,experiment : Settings,0.6219931271477663,0.4807692307692308,0.20588235294117646
machine-translation,6,Language Modeling is a basic task in natural language processing .,experiment,Settings,1,182,26,8,0,experiment : Settings,0.6254295532646048,0.5,0.23529411764705882
machine-translation,6,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,experiment,Settings,1,183,27,9,0,experiment : Settings,0.6288659793814433,0.5192307692307693,0.2647058823529412
machine-translation,6,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",experiment,Settings,1,184,28,10,0,experiment : Settings,0.6323024054982818,0.5384615384615384,0.29411764705882354
machine-translation,6,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .",experiment,Settings,1,185,29,11,0,experiment : Settings,0.6357388316151202,0.5576923076923077,0.3235294117647059
machine-translation,6,Machine Translation is a popular task in both deep learning and natural language processing .,experiment,Settings,1,186,30,12,0,experiment : Settings,0.6391752577319587,0.5769230769230769,0.35294117647058826
machine-translation,6,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .",experiment,Settings,1,187,31,13,0,experiment : Settings,0.6426116838487973,0.5961538461538461,0.38235294117647056
machine-translation,6,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .",experiment,Settings,1,188,32,14,0,experiment : Settings,0.6460481099656358,0.6153846153846154,0.4117647058823529
machine-translation,6,We use transformer_base and transformer_big configurations following tensor2 tensor .,experiment,Settings,0,189,33,15,0,experiment : Settings,0.6494845360824743,0.6346153846153846,0.4411764705882353
machine-translation,6,Text Classification is a conventional machine learning task and is evaluated by accuracy .,experiment,Settings,1,190,34,16,0,experiment : Settings,0.6529209621993127,0.6538461538461539,0.47058823529411764
machine-translation,6,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .",experiment,Settings,1,191,35,17,0,experiment : Settings,0.6563573883161512,0.6730769230769231,0.5
machine-translation,6,"In all tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words , which is the same as our empirical study .",experiment,Settings,0,192,36,18,0,experiment : Settings,0.6597938144329897,0.6923076923076923,0.5294117647058824
machine-translation,6,"For all the tasks except word embedding , we use full - batch gradient descent to update the discriminator .",experiment,Settings,0,193,37,19,0,experiment : Settings,0.6632302405498282,0.7115384615384616,0.5588235294117647
machine-translation,6,"For word embedding , mini- batch stochastic gradient descent is used to update the discriminator with a batch size 3000 , since the vocabulary size is large .",experiment,Settings,0,194,38,20,0,experiment : Settings,0.6666666666666666,0.7307692307692307,0.5882352941176471
machine-translation,6,"For language modeling and machine translation tasks , we use logistic regression as the discriminator .",experiment,Settings,0,195,39,21,0,experiment : Settings,0.6701030927835051,0.75,0.6176470588235294
machine-translation,6,"For other tasks , we find using a shallow neural network with 5 https://github.com/tensorflow/models/blob/master/tutorials/embedding",experiment,Settings,0,196,40,22,0,experiment : Settings,0.6735395189003437,0.7692307692307693,0.6470588235294118
machine-translation,6,http://mattmahoney.net/dc/textdata.html,experiment,Settings,0,197,41,23,0,experiment : Settings,0.6769759450171822,0.7884615384615384,0.6764705882352942
machine-translation,6,https://github.com/salesforce/awd-lstm-lm,experiment,Settings,0,198,42,24,0,experiment : Settings,0.6804123711340206,0.8076923076923077,0.7058823529411765
machine-translation,6,https://github.com/zihangdai/mos,experiment,Settings,0,199,43,25,0,experiment : Settings,0.6838487972508591,0.8269230769230769,0.7352941176470589
machine-translation,6,https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,experiment,Settings,0,200,44,26,0,experiment : Settings,0.6872852233676976,0.8461538461538461,0.7647058823529411
machine-translation,6,"To improve the training for imbalanced labeled data , a common method is to adjust loss function by reweighting the training samples ; To regularize the parameter space , a common method is to use l 2 regularization .",experiment,Settings,0,201,45,27,0,experiment : Settings,0.6907216494845361,0.8653846153846154,0.7941176470588235
machine-translation,6,We tested these methods in machine translation and found the performance is not good .,experiment,Settings,0,202,46,28,0,experiment : Settings,0.6941580756013745,0.8846153846153846,0.8235294117647058
machine-translation,6,Detailed analysis is provided in the supplementary material ( part B ) .,experiment,Settings,0,203,47,29,0,experiment : Settings,0.697594501718213,0.9038461538461539,0.8529411764705882
machine-translation,6,https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size .,experiment,Settings,0,204,48,30,0,experiment : Settings,0.7010309278350515,0.9230769230769231,0.8823529411764706
machine-translation,6,"In all tasks , we set the hyper - parameter ? to 0.1 .",experiment,Settings,0,205,49,31,0,experiment : Settings,0.7044673539518901,0.9423076923076923,0.9117647058823529
machine-translation,6,We list other hyper - parameters related to different task - specific models in the supplementary material ( part A ) .,experiment,Settings,0,206,50,32,0,experiment : Settings,0.7079037800687286,0.9615384615384616,0.9411764705882353
machine-translation,6,"In this subsection , we provide the experimental results of all tasks .",experiment,Settings,0,207,51,33,0,experiment : Settings,0.711340206185567,0.9807692307692307,0.9705882352941176
machine-translation,6,"For simplicity , we use "" with FRAGE "" as our proposed method in the tables .",experiment,Settings,0,208,52,34,0,experiment : Settings,0.7147766323024055,1.0,1.0
machine-translation,6,Results,result,Results,0,209,1,1,0,result : Results,0.718213058419244,0.2,1.0
machine-translation,6,RG65,result,RG65,0,210,2,1,0,result : RG65,0.7216494845360825,0.4,1.0
machine-translation,6,Word Similarity,result,Word Similarity,0,211,3,1,0,result : Word Similarity,0.7250859106529209,0.6,0.3333333333333333
machine-translation,6,The results on three word similarity tasks are listed in .,result,Word Similarity,0,212,4,2,0,result : Word Similarity,0.7285223367697594,0.8,0.6666666666666666
machine-translation,6,"Paras "" denotes the number of model parameters .",result,Word Similarity,0,213,5,3,0,result : Word Similarity,0.7319587628865979,1.0,1.0
machine-translation,6,Language Modeling,model,Language Modeling,1,214,1,1,0,model : Language Modeling,0.7353951890034365,0.05555555555555555,0.14285714285714285
machine-translation,6,The results of language modeling on PTB and WT2 datasets are presented in .,model,Language Modeling,0,215,2,2,0,model : Language Modeling,0.738831615120275,0.1111111111111111,0.2857142857142857
machine-translation,6,"We test our model and the baselines at several checkpoints used in the baseline papers : without finetune , with finetune , with post -process ( continuous cache pointer or dynamic evaluation ) .",model,Language Modeling,0,216,3,3,0,model : Language Modeling,0.7422680412371134,0.16666666666666666,0.42857142857142855
machine-translation,6,"In all these settings , our method outperforms the two baselines .",model,Language Modeling,1,217,4,4,0,model : Language Modeling,0.7457044673539519,0.2222222222222222,0.5714285714285714
machine-translation,6,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .",model,Language Modeling,1,218,5,5,0,model : Language Modeling,0.7491408934707904,0.2777777777777778,0.7142857142857143
machine-translation,6,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .",model,Language Modeling,1,219,6,6,0,model : Language Modeling,0.7525773195876289,0.3333333333333333,0.8571428571428571
machine-translation,6,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .",model,Language Modeling,1,220,7,7,0,model : Language Modeling,0.7560137457044673,0.3888888888888889,1.0
machine-translation,6,Machine Translation,model,Machine Translation,1,221,8,1,0,model : Machine Translation,0.7594501718213058,0.4444444444444444,0.16666666666666666
machine-translation,6,The results of neural machine translation on WMT14 English - German and IWSLT14 German - English tasks are shown in .,model,Machine Translation,0,222,9,2,0,model : Machine Translation,0.7628865979381443,0.5,0.3333333333333333
machine-translation,6,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,model,Machine Translation,1,223,10,3,0,model : Machine Translation,0.7663230240549829,0.5555555555555556,0.5
machine-translation,6,"task , respectively .",model,Machine Translation,0,224,11,4,0,model : Machine Translation,0.7697594501718213,0.6111111111111112,0.6666666666666666
machine-translation,6,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,model,Machine Translation,1,225,12,5,0,model : Machine Translation,0.7731958762886598,0.6666666666666666,0.8333333333333334
machine-translation,6,These results show improving word embeddings can achieve better results in more complicated tasks and larger datasets .,model,Machine Translation,0,226,13,6,0,model : Machine Translation,0.7766323024054983,0.7222222222222222,1.0
machine-translation,6,Text Classification,model,Text Classification,1,227,14,1,0,model : Text Classification,0.7800687285223368,0.7777777777777778,0.2
machine-translation,6,The results are listed in .,model,Text Classification,0,228,15,2,0,model : Text Classification,0.7835051546391752,0.8333333333333334,0.4
machine-translation,6,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,model,Text Classification,1,229,16,3,0,model : Text Classification,0.7869415807560137,0.8888888888888888,0.6
machine-translation,6,"As a summary , our experiments on four different tasks with 10 datasets verify the effectiveness of our method .",model,Text Classification,0,230,17,4,0,model : Text Classification,0.7903780068728522,0.9444444444444444,0.8
machine-translation,6,"We provide some case studies and visualizations of our method in the supplementary material ( part C ) , which show that the semantic similarities are reasonable and the popular / rare words are better mixed together in the embedding space .",model,Text Classification,0,231,18,5,0,model : Text Classification,0.7938144329896907,1.0,1.0
machine-translation,6,Conclusion,conclusion,Conclusion,0,232,1,1,0,conclusion : Conclusion,0.7972508591065293,0.027777777777777776,0.03571428571428571
machine-translation,6,"In this paper , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of high - frequency and low - frequency words lie in different subregions of the embedding space .",conclusion,Conclusion,0,233,2,2,0,conclusion : Conclusion,0.8006872852233677,0.05555555555555555,0.07142857142857142
machine-translation,6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",conclusion,Conclusion,0,234,3,3,0,conclusion : Conclusion,0.8041237113402062,0.08333333333333333,0.10714285714285714
machine-translation,6,"We propose a neat , simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks .",conclusion,Conclusion,0,235,4,4,0,conclusion : Conclusion,0.8075601374570447,0.1111111111111111,0.14285714285714285
machine-translation,6,We will explore several directions in the future .,conclusion,Conclusion,0,236,5,5,0,conclusion : Conclusion,0.8109965635738832,0.1388888888888889,0.17857142857142858
machine-translation,6,"First , we will investigate the theoretical aspects of word embedding learning and our adversarial training method .",conclusion,Conclusion,0,237,6,6,0,conclusion : Conclusion,0.8144329896907216,0.16666666666666666,0.21428571428571427
machine-translation,6,"Second , we will study more applications which have the similar problem even beyond NLP .",conclusion,Conclusion,0,238,7,7,0,conclusion : Conclusion,0.8178694158075601,0.19444444444444445,0.25
machine-translation,6,Experimental settings A.1 Dataset Description,conclusion,Conclusion,0,239,8,8,0,conclusion : Conclusion,0.8213058419243986,0.2222222222222222,0.2857142857142857
machine-translation,6,"For word similarity , we use three test datasets .",conclusion,Conclusion,0,240,9,9,0,conclusion : Conclusion,0.8247422680412371,0.25,0.32142857142857145
machine-translation,6,"WordSim-353 ( WS ) dataset consists of 353 pairs of commonly used verbs and nouns ; The rare - words ( RW ) dataset contains rarely used words ; The RG65 dataset contains 65 word pairs , and the similarity values in the dataset are the means of judgments made by 51 subjects .",conclusion,Conclusion,0,241,10,10,0,conclusion : Conclusion,0.8281786941580757,0.2777777777777778,0.35714285714285715
machine-translation,6,"For language modeling tasks , we use Penn Treebank dataset and WikiText - 2 dataset .",conclusion,Conclusion,0,242,11,11,0,conclusion : Conclusion,0.8316151202749141,0.3055555555555556,0.39285714285714285
machine-translation,6,The details of the datasets are provided in . :,conclusion,Conclusion,0,243,12,12,0,conclusion : Conclusion,0.8350515463917526,0.3333333333333333,0.42857142857142855
machine-translation,6,"Statistics of the Penn Treebank , and WikiText - 2 dataset used in language modeling .",conclusion,Conclusion,0,244,13,13,0,conclusion : Conclusion,0.8384879725085911,0.3611111111111111,0.4642857142857143
machine-translation,6,The out of vocabulary ( OOV ) words will be replaced by < unk > during training and testing .,conclusion,Conclusion,0,245,14,14,0,conclusion : Conclusion,0.8419243986254296,0.3888888888888889,0.5
machine-translation,6,"For machine translation , we use WMT14 English - German and IWSLT14 German - English datasets .",conclusion,Conclusion,0,246,15,15,0,conclusion : Conclusion,0.845360824742268,0.4166666666666667,0.5357142857142857
machine-translation,6,The training set of WMT14 English - German task consists of 4.5 M sentence pairs .,conclusion,Conclusion,0,247,16,16,0,conclusion : Conclusion,0.8487972508591065,0.4444444444444444,0.5714285714285714
machine-translation,6,Source and target tokens are processed into 37 K shared sub - word units based on byte - pair encoding ( BPE ) .,conclusion,Conclusion,0,248,17,17,0,conclusion : Conclusion,0.852233676975945,0.4722222222222222,0.6071428571428571
machine-translation,6,We use the concatenation of newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set following all previous works .,conclusion,Conclusion,0,249,18,18,0,conclusion : Conclusion,0.8556701030927835,0.5,0.6428571428571429
machine-translation,6,IWSLT14 German - English dataset contains 160K training sentence pairs and 7K validation sentence pairs .,conclusion,Conclusion,0,250,19,19,0,conclusion : Conclusion,0.8591065292096219,0.5277777777777778,0.6785714285714286
machine-translation,6,Tokens are processed using BPE and eventually we obtain a shared vocabulary of about 32 K tokens .,conclusion,Conclusion,0,251,20,20,0,conclusion : Conclusion,0.8625429553264605,0.5555555555555556,0.7142857142857143
machine-translation,6,"We use the concatenation of dev2010 , tst2010 , tst2011 and tst 2011 as the test set , which is widely adopted in .",conclusion,Conclusion,0,252,21,21,0,conclusion : Conclusion,0.865979381443299,0.5833333333333334,0.75
machine-translation,6,"For text classification tasks , we use three datasets : AG 's News , IMDB and 20 NG .",conclusion,Conclusion,0,253,22,22,0,conclusion : Conclusion,0.8694158075601375,0.6111111111111112,0.7857142857142857
machine-translation,6,"AG 's news corpus is a news article corpus with categorized articles from more than 2,000 news .",conclusion,Conclusion,0,254,23,23,0,conclusion : Conclusion,0.872852233676976,0.6388888888888888,0.8214285714285714
machine-translation,6,IMDB movie review dataset is a sentiment classification dataset .,conclusion,Conclusion,0,255,24,24,0,conclusion : Conclusion,0.8762886597938144,0.6666666666666666,0.8571428571428571
machine-translation,6,It consists of movie review comments with binary sentiment labels .,conclusion,Conclusion,0,256,25,25,0,conclusion : Conclusion,0.8797250859106529,0.6944444444444444,0.8928571428571429
machine-translation,6,"20 Newsgroups is an email collection dataset , in which the emails are categorized into 20 different groups .",conclusion,Conclusion,0,257,26,26,0,conclusion : Conclusion,0.8831615120274914,0.7222222222222222,0.9285714285714286
machine-translation,6,"We use the bydate version and select 4 major categories ( comp , politics , rec , and religion ) following . :",conclusion,Conclusion,0,258,27,27,0,conclusion : Conclusion,0.8865979381443299,0.75,0.9642857142857143
machine-translation,6,Detailed statistics about text classification datasets .,conclusion,Conclusion,0,259,28,28,0,conclusion : Conclusion,0.8900343642611683,0.7777777777777778,1.0
machine-translation,6,Hyper - parameter configurations,conclusion,Hyper-parameter configurations,0,260,29,1,0,conclusion : Hyper-parameter configurations,0.8934707903780069,0.8055555555555556,0.125
machine-translation,6,The hyper -parameters used for AWD - LSTM with / without MoS in language modeling experiment is shown in .,conclusion,Hyper-parameter configurations,0,261,30,2,0,conclusion : Hyper-parameter configurations,0.8969072164948454,0.8333333333333334,0.25
machine-translation,6,"For machine translation tasks , we choose Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 , and follow the learning rate schedule in .",conclusion,Hyper-parameter configurations,0,262,31,3,0,conclusion : Hyper-parameter configurations,0.9003436426116839,0.8611111111111112,0.375
machine-translation,6,"For evaluation , we use the case - sensitive tokenized BLEU score for WMT14 English - German and case - insensitive tokenized BLEU score for IWSLT14 German - English .",conclusion,Hyper-parameter configurations,0,263,32,4,0,conclusion : Hyper-parameter configurations,0.9037800687285223,0.8888888888888888,0.5
machine-translation,6,The hyper - parameters used in machine translation task are summarized in .,conclusion,Hyper-parameter configurations,0,264,33,5,0,conclusion : Hyper-parameter configurations,0.9072164948453608,0.9166666666666666,0.625
machine-translation,6,The hyper - parameters used in word embedding task are summarized in .,conclusion,Hyper-parameter configurations,0,265,34,6,0,conclusion : Hyper-parameter configurations,0.9106529209621993,0.9444444444444444,0.75
machine-translation,6,"For all text classification tasks , we use convolutional kernel with size 2 , 3 , 5 .",conclusion,Hyper-parameter configurations,0,266,35,7,0,conclusion : Hyper-parameter configurations,0.9140893470790378,0.9722222222222222,0.875
machine-translation,6,"We implement batch normalization and shortcut connection , and use Adam optimizer with ? 1 = 0.9 , ? 2 = 0.99 , ? = 10 ?8 .: Hyper- parameter used for word embedding training .",conclusion,Hyper-parameter configurations,0,267,36,8,0,conclusion : Hyper-parameter configurations,0.9175257731958762,1.0,1.0
machine-translation,6,AWD - LSTM + MoS,AWD-LSTM + MoS,AWD-LSTM + MoS,0,268,1,1,0,AWD-LSTM + MoS : AWD-LSTM + MoS,0.9209621993127147,0.041666666666666664,1.0
machine-translation,6,Models Description,AWD-LSTM + MoS,Models Description,0,269,2,1,0,AWD-LSTM + MoS : Models Description,0.9243986254295533,0.08333333333333333,0.08333333333333333
machine-translation,6,We use task - specific baseline models .,AWD-LSTM + MoS,Models Description,0,270,3,2,0,AWD-LSTM + MoS : Models Description,0.9278350515463918,0.125,0.16666666666666666
machine-translation,6,"In language modeling , AWD - LSTM is a weight - dropped LSTM which uses Drop Connect on hidden - to - hidden weights as a means of recurrent regularization .",AWD-LSTM + MoS,Models Description,0,271,4,3,0,AWD-LSTM + MoS : Models Description,0.9312714776632303,0.16666666666666666,0.25
machine-translation,6,"The model is trained by NT - ASGD , which is a variant of the averaged stochastic gradient method .",AWD-LSTM + MoS,Models Description,0,272,5,4,0,AWD-LSTM + MoS : Models Description,0.9347079037800687,0.20833333333333334,0.3333333333333333
machine-translation,6,"The training process has two steps , in the second step , the model is finetuned using another configuration of NT - ASGD .",AWD-LSTM + MoS,Models Description,0,273,6,5,0,AWD-LSTM + MoS : Models Description,0.9381443298969072,0.25,0.4166666666666667
machine-translation,6,AWD - LSTM - MoS uses the Mixture of Softmaxes structure to the vanilla AWD - LSTM and achieves the state - of - the - art result on PTB and WT2 .,AWD-LSTM + MoS,Models Description,0,274,7,6,0,AWD-LSTM + MoS : Models Description,0.9415807560137457,0.2916666666666667,0.5
machine-translation,6,"For machine translation , Transformer is a recently developed architecture in which the selfattention network is used during encoding and decoding step .",AWD-LSTM + MoS,Models Description,0,275,8,7,0,AWD-LSTM + MoS : Models Description,0.9450171821305842,0.3333333333333333,0.5833333333333334
machine-translation,6,"It achieves the best performances on several machine translation tasks , e.g.",AWD-LSTM + MoS,Models Description,0,276,9,8,0,AWD-LSTM + MoS : Models Description,0.9484536082474226,0.375,0.6666666666666666
machine-translation,6,"WMT14 English - German , WMT14 English - French datasets .",AWD-LSTM + MoS,Models Description,0,277,10,9,0,AWD-LSTM + MoS : Models Description,0.9518900343642611,0.4166666666666667,0.75
machine-translation,6,Word2vec is one of the pioneer works on using deep learning to NLP tasks .,AWD-LSTM + MoS,Models Description,0,278,11,10,0,AWD-LSTM + MoS : Models Description,0.9553264604810997,0.4583333333333333,0.8333333333333334
machine-translation,6,"Based on the co-occurrence of words , it produces distributed representations of words ( word embeddings ) .",AWD-LSTM + MoS,Models Description,0,279,12,11,0,AWD-LSTM + MoS : Models Description,0.9587628865979382,0.5,0.9166666666666666
machine-translation,6,"RCNN contains both recurrent and convolutional layers to catch the key components in texts , and is widely used in text classification tasks .",AWD-LSTM + MoS,Models Description,0,280,13,12,0,AWD-LSTM + MoS : Models Description,0.9621993127147767,0.5416666666666666,1.0
machine-translation,6,Additional Comparisons,AWD-LSTM + MoS,Additional Comparisons,0,281,14,1,0,AWD-LSTM + MoS : Additional Comparisons,0.9656357388316151,0.5833333333333334,0.25
machine-translation,6,"We compare some other simple methods with ours on machine translation tasks , which include reweighting method and l 2 regularization ( weight decay ) .",AWD-LSTM + MoS,Additional Comparisons,0,282,15,2,0,AWD-LSTM + MoS : Additional Comparisons,0.9690721649484536,0.625,0.5
machine-translation,6,Results are listed in .,AWD-LSTM + MoS,Additional Comparisons,0,283,16,3,0,AWD-LSTM + MoS : Additional Comparisons,0.9725085910652921,0.6666666666666666,0.75
machine-translation,6,"We notice that those simple methods do notwork for the tasks , even have negative effects . : BLEU scores on test set of the WMT14 English - German task and IWSLT14 German - English task .",AWD-LSTM + MoS,Additional Comparisons,0,284,17,4,0,AWD-LSTM + MoS : Additional Comparisons,0.9759450171821306,0.7083333333333334,1.0
machine-translation,6,WMT,AWD-LSTM + MoS,WMT,0,285,18,1,0,AWD-LSTM + MoS : WMT,0.979381443298969,0.75,0.5
machine-translation,6,"Our method is denoted as "" FRAGE "" , "" Reweighting "" denotes reweighting the loss of each word by reciprocal of its frequency , and "" Weight Decay "" denotes putting weight decay rate ( 0.2 ) on embeddings .",AWD-LSTM + MoS,WMT,0,286,19,2,0,AWD-LSTM + MoS : WMT,0.9828178694158075,0.7916666666666666,1.0
machine-translation,6,Case Study on Original Models and Qualitative Analysis of Our Method,AWD-LSTM + MoS,Case Study on Original Models and Qualitative Analysis of Our Method,0,287,20,1,0,AWD-LSTM + MoS : Case Study on Original Models and Qualitative Analysis of Our Method,0.9862542955326461,0.8333333333333334,0.2
machine-translation,6,We provide more word similarity cases in to justify our statement in Section 3 .,AWD-LSTM + MoS,Case Study on Original Models and Qualitative Analysis of Our Method,0,288,21,2,0,AWD-LSTM + MoS : Case Study on Original Models and Qualitative Analysis of Our Method,0.9896907216494846,0.875,0.4
machine-translation,6,We also present the effectiveness of our method by showcase and embedding visualizations .,AWD-LSTM + MoS,Case Study on Original Models and Qualitative Analysis of Our Method,0,289,22,3,0,AWD-LSTM + MoS : Case Study on Original Models and Qualitative Analysis of Our Method,0.993127147766323,0.9166666666666666,0.6
machine-translation,6,"From the cases and visualizations in and , we find the word similarities are improved and popular / rare words are better mixed together .",AWD-LSTM + MoS,Case Study on Original Models and Qualitative Analysis of Our Method,0,290,23,4,0,AWD-LSTM + MoS : Case Study on Original Models and Qualitative Analysis of Our Method,0.9965635738831615,0.9583333333333334,0.8
machine-translation,6,"a ) ( b ) : These figures show that , in different tasks , the embeddings of rare and popular words are better mixed together after applying our method .",AWD-LSTM + MoS,Case Study on Original Models and Qualitative Analysis of Our Method,0,291,24,5,0,AWD-LSTM + MoS : Case Study on Original Models and Qualitative Analysis of Our Method,1.0,1.0,1.0
machine-translation,7,OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,title,title,0,2,1,1,0,title : title,0.005361930294906166,1.0,1.0
machine-translation,7,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.00804289544235925,0.024390243902439025,0.024390243902439025
machine-translation,7,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.010723860589812333,0.04878048780487805,0.04878048780487805
machine-translation,7,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract,abstract,1,5,3,3,0,abstract : abstract,0.013404825737265416,0.07317073170731707,0.07317073170731707
machine-translation,7,"In practice , however , there are significant algorithmic and performance challenges .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.0160857908847185,0.0975609756097561,0.0975609756097561
machine-translation,7,"In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.01876675603217158,0.12195121951219512,0.12195121951219512
machine-translation,7,"We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.021447721179624665,0.14634146341463414,0.14634146341463414
machine-translation,7,trainable gating network determines a sparse combination of these experts to use for each example .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.024128686327077747,0.17073170731707318,0.17073170731707318
machine-translation,7,"We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.02680965147453083,0.1951219512195122,0.1951219512195122
machine-translation,7,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.029490616621983913,0.21951219512195122,0.21951219512195122
machine-translation,7,"On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.032171581769437,0.24390243902439024,0.24390243902439024
machine-translation,7,Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency ),abstract,abstract,0,13,11,11,0,abstract : abstract,0.03485254691689008,0.2682926829268293,0.2682926829268293
machine-translation,7,INTRODUCTION AND RELATED WORK 1 .,abstract,abstract,0,14,12,12,0,abstract : abstract,0.03753351206434316,0.2926829268292683,0.2926829268292683
machine-translation,7,CONDITIONAL COMPUTATION,abstract,abstract,0,15,13,13,0,abstract : abstract,0.040214477211796246,0.3170731707317073,0.3170731707317073
machine-translation,7,Exploiting scale in both training data and model size has been central to the success of deep learning .,abstract,abstract,1,16,14,14,0,abstract : abstract,0.04289544235924933,0.34146341463414637,0.34146341463414637
machine-translation,7,"When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy .",abstract,abstract,0,17,15,15,0,abstract : abstract,0.045576407506702415,0.36585365853658536,0.36585365853658536
machine-translation,7,"This has been shown in domains such as text , images , and audio .",abstract,abstract,0,18,16,16,0,abstract : abstract,0.04825737265415549,0.3902439024390244,0.3902439024390244
machine-translation,7,"For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .",abstract,abstract,0,19,17,17,0,abstract : abstract,0.05093833780160858,0.4146341463414634,0.4146341463414634
machine-translation,7,"Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand .",abstract,abstract,0,20,18,18,0,abstract : abstract,0.05361930294906166,0.43902439024390244,0.43902439024390244
machine-translation,7,Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .,abstract,abstract,0,21,19,19,0,abstract : abstract,0.05630026809651475,0.4634146341463415,0.4634146341463415
machine-translation,7,"In these schemes , large parts of a network are active or inactive on a per-example basis .",abstract,abstract,0,22,20,20,0,abstract : abstract,0.058981233243967826,0.4878048780487805,0.4878048780487805
machine-translation,7,"The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .",abstract,abstract,0,23,21,21,0,abstract : abstract,0.06166219839142091,0.5121951219512195,0.5121951219512195
machine-translation,7,Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions .,abstract,abstract,0,24,22,22,0,abstract : abstract,0.064343163538874,0.5365853658536586,0.5365853658536586
machine-translation,7,"While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .",abstract,abstract,0,25,23,23,0,abstract : abstract,0.06702412868632708,0.5609756097560976,0.5609756097560976
machine-translation,7,We blame this on a combination of the following challenges :,abstract,abstract,0,26,24,24,0,abstract : abstract,0.06970509383378017,0.5853658536585366,0.5853658536585366
machine-translation,7,"Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .",abstract,abstract,0,27,25,25,0,abstract : abstract,0.07238605898123325,0.6097560975609756,0.6097560975609756
machine-translation,7,Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .,abstract,abstract,0,28,26,26,0,abstract : abstract,0.07506702412868632,0.6341463414634146,0.6341463414634146
machine-translation,7,"Large batch sizes are critical for performance , as they amortize the costs of parameter transfers and updates .",abstract,abstract,0,29,27,27,0,abstract : abstract,0.0777479892761394,0.6585365853658537,0.6585365853658537
machine-translation,7,Conditional computation reduces the batch sizes for the conditionally active chunks of the network .,abstract,abstract,0,30,28,28,0,abstract : abstract,0.08042895442359249,0.6829268292682927,0.6829268292682927
machine-translation,7,Network bandwidth can be a bottleneck .,abstract,abstract,0,31,29,29,0,abstract : abstract,0.08310991957104558,0.7073170731707317,0.7073170731707317
machine-translation,7,cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .,abstract,abstract,0,32,30,30,0,abstract : abstract,0.08579088471849866,0.7317073170731707,0.7317073170731707
machine-translation,7,"To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .",abstract,abstract,0,33,31,31,0,abstract : abstract,0.08847184986595175,0.7560975609756098,0.7560975609756098
machine-translation,7,"Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .",abstract,abstract,0,34,32,32,0,abstract : abstract,0.09115281501340483,0.7804878048780488,0.7804878048780488
machine-translation,7,"Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .",abstract,abstract,0,35,33,33,0,abstract : abstract,0.0938337801608579,0.8048780487804879,0.8048780487804879
machine-translation,7,"Depending on the scheme , loss terms maybe necessary to achieve the desired level of sparsity per-chunk and / or per example .",abstract,abstract,0,36,34,34,0,abstract : abstract,0.09651474530831099,0.8292682926829268,0.8292682926829268
machine-translation,7,use three such terms .,abstract,abstract,0,37,35,35,0,abstract : abstract,0.09919571045576407,0.8536585365853658,0.8536585365853658
machine-translation,7,These issues can affect both model quality and load - balancing .,abstract,abstract,0,38,36,36,0,abstract : abstract,0.10187667560321716,0.8780487804878049,0.8780487804878049
machine-translation,7,Model capacity is most critical for very large data sets .,abstract,abstract,0,39,37,37,0,abstract : abstract,0.10455764075067024,0.9024390243902439,0.9024390243902439
machine-translation,7,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",abstract,abstract,0,40,38,38,0,abstract : abstract,0.10723860589812333,0.926829268292683,0.926829268292683
machine-translation,7,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",abstract,abstract,0,41,39,39,0,abstract : abstract,0.10991957104557641,0.9512195121951219,0.9512195121951219
machine-translation,7,"In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation .",abstract,abstract,0,42,40,40,0,abstract : abstract,0.1126005361930295,0.975609756097561,0.975609756097561
machine-translation,7,We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets .,abstract,abstract,0,43,41,41,0,abstract : abstract,0.11528150134048257,1.0,1.0
machine-translation,7,OUR APPROACH : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0,44,1,1,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.11796246648793565,0.019230769230769232,0.1111111111111111
machine-translation,7,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,1,45,2,2,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.12064343163538874,0.038461538461538464,0.2222222222222222
machine-translation,7,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,1,46,3,3,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.12332439678284182,0.057692307692307696,0.3333333333333333
machine-translation,7,All parts of the network are trained jointly by back - propagation .,system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,1,47,4,4,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.1260053619302949,0.07692307692307693,0.4444444444444444
machine-translation,7,"While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models .",system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0,48,5,5,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.128686327077748,0.09615384615384616,0.5555555555555556
machine-translation,7,"In particular , we apply a MoE convolutionally between stacked LSTM layers , as in .",system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0,49,6,6,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.13136729222520108,0.11538461538461539,0.6666666666666666
machine-translation,7,"The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position .",system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0,50,7,7,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.13404825737265416,0.1346153846153846,0.7777777777777778
machine-translation,7,The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .,system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0,51,8,8,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.13672922252010725,0.15384615384615385,0.8888888888888888
machine-translation,7,"On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost .",system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0,52,9,9,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.13941018766756033,0.17307692307692307,1.0
machine-translation,7,RELATED WORK ON MIXTURES OF EXPERTS,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,53,10,1,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.14209115281501342,0.19230769230769232,0.06666666666666667
machine-translation,7,"Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,54,11,2,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.1447721179624665,0.21153846153846154,0.13333333333333333
machine-translation,7,"Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,55,12,3,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.14745308310991956,0.23076923076923078,0.2
machine-translation,7,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,56,13,4,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.15013404825737264,0.25,0.26666666666666666
machine-translation,7,suggest an ensemble model in the format of mixture of experts for machine translation .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,57,14,5,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.15281501340482573,0.2692307692307692,0.3333333333333333
machine-translation,7,The gating network is trained on a pre-trained ensemble NMT model .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,58,15,6,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.1554959785522788,0.28846153846153844,0.4
machine-translation,7,The works above concern top - level mixtures of experts .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,59,16,7,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.1581769436997319,0.3076923076923077,0.4666666666666667
machine-translation,7,The mixture of experts is the whole model .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,60,17,8,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.16085790884718498,0.3269230769230769,0.5333333333333333
machine-translation,7,introduce the idea of using multiple,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,61,18,9,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.16353887399463807,0.34615384615384615,0.6
machine-translation,7,MoEs with their own gating networks as parts of a deep model .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,62,19,10,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.16621983914209115,0.36538461538461536,0.6666666666666666
machine-translation,7,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,63,20,11,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.16890080428954424,0.38461538461538464,0.7333333333333333
machine-translation,7,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,64,21,12,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.17158176943699732,0.40384615384615385,0.8
machine-translation,7,Our work builds on this use of MoEs as a general purpose neural network component .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,65,22,13,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.1742627345844504,0.4230769230769231,0.8666666666666667
machine-translation,7,"While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,66,23,14,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.1769436997319035,0.4423076923076923,0.9333333333333333
machine-translation,7,We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,67,24,15,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.17962466487935658,0.46153846153846156,1.0
machine-translation,7,THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,68,25,1,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.18230563002680966,0.4807692307692308,0.0625
machine-translation,7,"The Mixture - of - Experts ( MoE ) layer consists of a set of n "" expert networks "" E 1 , , E n , and a "" gating network "" G whose output is a sparse n-dimensional vector .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,69,26,2,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.18498659517426275,0.5,0.125
machine-translation,7,shows an overview of the MoE module .,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,70,27,3,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.1876675603217158,0.5192307692307693,0.1875
machine-translation,7,"The experts are themselves neural networks , each with their own parameters .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,71,28,4,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.1903485254691689,0.5384615384615384,0.25
machine-translation,7,"Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,72,29,5,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.19302949061662197,0.5576923076923077,0.3125
machine-translation,7,Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network for a given input x .,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,73,30,6,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.19571045576407506,0.5769230769230769,0.375
machine-translation,7,The output y of the MoE module can be written as follows :,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,74,31,7,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.19839142091152814,0.5961538461538461,0.4375
machine-translation,7,We save computation based on the sparsity of the output of G ( x ) .,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,75,32,8,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.20107238605898123,0.6153846153846154,0.5
machine-translation,7,"Wherever G (x ) i = 0 , we need not compute E i ( x ) .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,76,33,9,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.2037533512064343,0.6346153846153846,0.5625
machine-translation,7,"In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,77,34,10,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.2064343163538874,0.6538461538461539,0.625
machine-translation,7,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,78,35,11,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.20911528150134048,0.6730769230769231,0.6875
machine-translation,7,In the following we focus on ordinary MoEs .,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,79,36,12,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.21179624664879357,0.6923076923076923,0.75
machine-translation,7,We provide more details on hierarchical MoEs in Appendix B.,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,80,37,13,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.21447721179624665,0.7115384615384616,0.8125
machine-translation,7,Our implementation is related to other models of conditional computation .,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,81,38,14,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.21715817694369974,0.7307692307692307,0.875
machine-translation,7,MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .,system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,82,39,15,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.21983914209115282,0.75,0.9375
machine-translation,7,"MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .",system description,THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0,83,40,16,0,system description : THE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER,0.2225201072386059,0.7692307692307693,1.0
machine-translation,7,GATING NETWORK,system description,GATING NETWORK,0,84,41,1,0,system description : GATING NETWORK,0.225201072386059,0.7884615384615384,0.08333333333333333
machine-translation,7,Softmax Gating :,system description,GATING NETWORK,0,85,42,2,0,system description : GATING NETWORK,0.22788203753351208,0.8076923076923077,0.16666666666666666
machine-translation,7,simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .,system description,GATING NETWORK,0,86,43,3,0,system description : GATING NETWORK,0.23056300268096513,0.8269230769230769,0.25
machine-translation,7,Noisy Top - K,system description,GATING NETWORK,0,87,44,4,0,system description : GATING NETWORK,0.23324396782841822,0.8461538461538461,0.3333333333333333
machine-translation,7,Gating :,system description,GATING NETWORK,0,88,45,5,0,system description : GATING NETWORK,0.2359249329758713,0.8653846153846154,0.4166666666666667
machine-translation,7,We add two components to the Softmax gating network : sparsity and noise .,system description,GATING NETWORK,0,89,46,6,0,system description : GATING NETWORK,0.2386058981233244,0.8846153846153846,0.5
machine-translation,7,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ?? ( which causes the corresponding gate values to equal 0 ) .",system description,GATING NETWORK,0,90,47,7,0,system description : GATING NETWORK,0.24128686327077747,0.9038461538461539,0.5833333333333334
machine-translation,7,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ?? ( which causes the corresponding gate values to equal 0 ) .",system description,GATING NETWORK,0,91,48,8,0,system description : GATING NETWORK,0.24396782841823056,0.9230769230769231,0.6666666666666666
machine-translation,7,"The sparsity serves to save computation , as described above .",system description,GATING NETWORK,0,92,49,9,0,system description : GATING NETWORK,0.24664879356568364,0.9423076923076923,0.75
machine-translation,7,"While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .",system description,GATING NETWORK,0,93,50,10,0,system description : GATING NETWORK,0.24932975871313673,0.9615384615384616,0.8333333333333334
machine-translation,7,"The noise term helps with load balancing , as will be discussed in Appendix A .",system description,GATING NETWORK,0,94,51,11,0,system description : GATING NETWORK,0.2520107238605898,0.9807692307692307,0.9166666666666666
machine-translation,7,The amount of noise per component is controlled by a second trainable weight matrix W noise .,system description,GATING NETWORK,0,95,52,12,0,system description : GATING NETWORK,0.2546916890080429,1.0,1.0
machine-translation,7,Training the Gating Network,training,training,0,96,1,1,0,training : training,0.257372654155496,0.16666666666666666,0.16666666666666666
machine-translation,7,"We train the gating network by simple back - propagation , along with the rest of the model .",training,training,0,97,2,2,0,training : training,0.26005361930294907,0.3333333333333333,0.3333333333333333
machine-translation,7,"If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .",training,training,0,98,3,3,0,training : training,0.26273458445040215,0.5,0.5
machine-translation,7,This type of occasionally - sensitive behavior is described in with respect to noisy rectifiers .,training,training,0,99,4,4,0,training : training,0.26541554959785524,0.6666666666666666,0.6666666666666666
machine-translation,7,Gradients also backpropagate through the gating network to its inputs .,training,training,0,100,5,5,0,training : training,0.2680965147453083,0.8333333333333334,0.8333333333333334
machine-translation,7,Our method differs here from who use boolean gates and a REINFORCE - style approach to train the gating network .,training,training,0,101,6,6,0,training : training,0.2707774798927614,1.0,1.0
machine-translation,7,ADDRESSING PERFORMANCE,performance,ADDRESSING PERFORMANCE CHALLENGES,0,102,1,1,0,performance : ADDRESSING PERFORMANCE CHALLENGES,0.2734584450402145,0.008620689655172414,0.5
machine-translation,7,CHALLENGES,performance,ADDRESSING PERFORMANCE CHALLENGES,0,103,2,2,0,performance : ADDRESSING PERFORMANCE CHALLENGES,0.2761394101876676,0.017241379310344827,1.0
machine-translation,7,THE SHRINKING BATCH PROBLEM,performance,THE SHRINKING BATCH PROBLEM,0,104,3,1,0,performance : THE SHRINKING BATCH PROBLEM,0.27882037533512066,0.02586206896551724,0.03125
machine-translation,7,"On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .",performance,THE SHRINKING BATCH PROBLEM,0,105,4,2,0,performance : THE SHRINKING BATCH PROBLEM,0.28150134048257375,0.034482758620689655,0.0625
machine-translation,7,"If the gating network chooses k out of n experts for each example , then for a batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .",performance,THE SHRINKING BATCH PROBLEM,0,106,5,3,0,performance : THE SHRINKING BATCH PROBLEM,0.28418230563002683,0.04310344827586207,0.09375
machine-translation,7,This causes a naive MoE implementation to become very inefficient as the number of experts increases .,performance,THE SHRINKING BATCH PROBLEM,0,107,6,4,0,performance : THE SHRINKING BATCH PROBLEM,0.2868632707774799,0.05172413793103448,0.125
machine-translation,7,The solution to this shrinking batch problem is to make the original batch size as large as possible .,performance,THE SHRINKING BATCH PROBLEM,0,108,7,5,0,performance : THE SHRINKING BATCH PROBLEM,0.289544235924933,0.0603448275862069,0.15625
machine-translation,7,"However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .",performance,THE SHRINKING BATCH PROBLEM,0,109,8,6,0,performance : THE SHRINKING BATCH PROBLEM,0.29222520107238603,0.06896551724137931,0.1875
machine-translation,7,We propose the following techniques for increasing the batch size :,performance,THE SHRINKING BATCH PROBLEM,0,110,9,7,0,performance : THE SHRINKING BATCH PROBLEM,0.2949061662198391,0.07758620689655173,0.21875
machine-translation,7,Mixing Data Parallelism and Model Parallelism :,performance,THE SHRINKING BATCH PROBLEM,0,111,10,8,0,performance : THE SHRINKING BATCH PROBLEM,0.2975871313672922,0.08620689655172414,0.25
machine-translation,7,"In a conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers .",performance,THE SHRINKING BATCH PROBLEM,0,112,11,9,0,performance : THE SHRINKING BATCH PROBLEM,0.3002680965147453,0.09482758620689655,0.28125
machine-translation,7,"In our technique , these different batches run synchronously so that they can be combined for the MoE layer .",performance,THE SHRINKING BATCH PROBLEM,0,113,12,10,0,performance : THE SHRINKING BATCH PROBLEM,0.30294906166219837,0.10344827586206896,0.3125
machine-translation,7,"We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert .",performance,THE SHRINKING BATCH PROBLEM,0,114,13,11,0,performance : THE SHRINKING BATCH PROBLEM,0.30563002680965146,0.11206896551724138,0.34375
machine-translation,7,Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches .,performance,THE SHRINKING BATCH PROBLEM,0,115,14,12,0,performance : THE SHRINKING BATCH PROBLEM,0.30831099195710454,0.1206896551724138,0.375
machine-translation,7,The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) .,performance,THE SHRINKING BATCH PROBLEM,0,116,15,13,0,performance : THE SHRINKING BATCH PROBLEM,0.3109919571045576,0.12931034482758622,0.40625
machine-translation,7,"If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .",performance,THE SHRINKING BATCH PROBLEM,0,117,16,14,0,performance : THE SHRINKING BATCH PROBLEM,0.3136729222520107,0.13793103448275862,0.4375
machine-translation,7,"Thus , we achieve a factor of d improvement inexpert batch size .",performance,THE SHRINKING BATCH PROBLEM,0,118,17,15,0,performance : THE SHRINKING BATCH PROBLEM,0.3163538873994638,0.14655172413793102,0.46875
machine-translation,7,"In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .",performance,THE SHRINKING BATCH PROBLEM,0,119,18,16,0,performance : THE SHRINKING BATCH PROBLEM,0.3190348525469169,0.15517241379310345,0.5
machine-translation,7,Each secondary MoE resides on one device .,performance,THE SHRINKING BATCH PROBLEM,0,120,19,17,0,performance : THE SHRINKING BATCH PROBLEM,0.32171581769436997,0.16379310344827586,0.53125
machine-translation,7,This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .,performance,THE SHRINKING BATCH PROBLEM,0,121,20,18,0,performance : THE SHRINKING BATCH PROBLEM,0.32439678284182305,0.1724137931034483,0.5625
machine-translation,7,"The total batch size increases , keeping the batch size per expert constant .",performance,THE SHRINKING BATCH PROBLEM,0,122,21,19,0,performance : THE SHRINKING BATCH PROBLEM,0.32707774798927614,0.1810344827586207,0.59375
machine-translation,7,"The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .",performance,THE SHRINKING BATCH PROBLEM,0,123,22,20,0,performance : THE SHRINKING BATCH PROBLEM,0.3297587131367292,0.1896551724137931,0.625
machine-translation,7,It is our goal to train a trillionparameter model on a trillion - word corpus .,performance,THE SHRINKING BATCH PROBLEM,0,124,23,21,0,performance : THE SHRINKING BATCH PROBLEM,0.3324396782841823,0.19827586206896552,0.65625
machine-translation,7,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .",performance,THE SHRINKING BATCH PROBLEM,0,125,24,22,0,performance : THE SHRINKING BATCH PROBLEM,0.3351206434316354,0.20689655172413793,0.6875
machine-translation,7,Taking Advantage of Convolutionality :,performance,THE SHRINKING BATCH PROBLEM,0,126,25,23,0,performance : THE SHRINKING BATCH PROBLEM,0.3378016085790885,0.21551724137931033,0.71875
machine-translation,7,"In our language models , we apply the same MoE to each time step of the previous layer .",performance,THE SHRINKING BATCH PROBLEM,0,127,26,24,0,performance : THE SHRINKING BATCH PROBLEM,0.34048257372654156,0.22413793103448276,0.75
machine-translation,7,"If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch .",performance,THE SHRINKING BATCH PROBLEM,0,128,27,25,0,performance : THE SHRINKING BATCH PROBLEM,0.34316353887399464,0.23275862068965517,0.78125
machine-translation,7,Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps .,performance,THE SHRINKING BATCH PROBLEM,0,129,28,26,0,performance : THE SHRINKING BATCH PROBLEM,0.34584450402144773,0.2413793103448276,0.8125
machine-translation,7,Increasing Batch Size for a,performance,THE SHRINKING BATCH PROBLEM,0,130,29,27,0,performance : THE SHRINKING BATCH PROBLEM,0.3485254691689008,0.25,0.84375
machine-translation,7,Recurrent MoE :,performance,THE SHRINKING BATCH PROBLEM,0,131,30,28,0,performance : THE SHRINKING BATCH PROBLEM,0.3512064343163539,0.25862068965517243,0.875
machine-translation,7,We suspect that even more powerful models may involve applying a MoE recurrently .,performance,THE SHRINKING BATCH PROBLEM,0,132,31,29,0,performance : THE SHRINKING BATCH PROBLEM,0.353887399463807,0.2672413793103448,0.90625
machine-translation,7,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",performance,THE SHRINKING BATCH PROBLEM,0,133,32,30,0,performance : THE SHRINKING BATCH PROBLEM,0.35656836461126007,0.27586206896551724,0.9375
machine-translation,7,"Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .",performance,THE SHRINKING BATCH PROBLEM,0,134,33,31,0,performance : THE SHRINKING BATCH PROBLEM,0.35924932975871315,0.28448275862068967,0.96875
machine-translation,7,This would allow for a large increase in batch size .,performance,THE SHRINKING BATCH PROBLEM,0,135,34,32,0,performance : THE SHRINKING BATCH PROBLEM,0.36193029490616624,0.29310344827586204,1.0
machine-translation,7,NETWORK BANDWIDTH,performance,NETWORK BANDWIDTH,0,136,35,1,0,performance : NETWORK BANDWIDTH,0.3646112600536193,0.3017241379310345,0.125
machine-translation,7,Another major performance concern in distributed computing is network bandwidth .,performance,NETWORK BANDWIDTH,0,137,36,2,0,performance : NETWORK BANDWIDTH,0.3672922252010724,0.3103448275862069,0.25
machine-translation,7,"Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .",performance,NETWORK BANDWIDTH,0,138,37,3,0,performance : NETWORK BANDWIDTH,0.3699731903485255,0.31896551724137934,0.375
machine-translation,7,"To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .",performance,NETWORK BANDWIDTH,0,139,38,4,0,performance : NETWORK BANDWIDTH,0.3726541554959786,0.3275862068965517,0.5
machine-translation,7,"For GPUs , this maybe thousands to one .",performance,NETWORK BANDWIDTH,0,140,39,5,0,performance : NETWORK BANDWIDTH,0.3753351206434316,0.33620689655172414,0.625
machine-translation,7,"In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units .",performance,NETWORK BANDWIDTH,0,141,40,6,0,performance : NETWORK BANDWIDTH,0.3780160857908847,0.3448275862068966,0.75
machine-translation,7,"Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .",performance,NETWORK BANDWIDTH,0,142,41,7,0,performance : NETWORK BANDWIDTH,0.3806970509383378,0.35344827586206895,0.875
machine-translation,7,"Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .",performance,NETWORK BANDWIDTH,0,143,42,8,0,performance : NETWORK BANDWIDTH,0.38337801608579086,0.3620689655172414,1.0
machine-translation,7,BALANCING EXPERT UTILIZATION,performance,BALANCING EXPERT UTILIZATION,0,144,43,1,0,performance : BALANCING EXPERT UTILIZATION,0.38605898123324395,0.3706896551724138,0.02564102564102564
machine-translation,7,We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts .,performance,BALANCING EXPERT UTILIZATION,0,145,44,2,0,performance : BALANCING EXPERT UTILIZATION,0.38873994638069703,0.3793103448275862,0.05128205128205128
machine-translation,7,"This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network .",performance,BALANCING EXPERT UTILIZATION,0,146,45,3,0,performance : BALANCING EXPERT UTILIZATION,0.3914209115281501,0.3879310344827586,0.07692307692307693
machine-translation,7,"describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .",performance,BALANCING EXPERT UTILIZATION,0,147,46,4,0,performance : BALANCING EXPERT UTILIZATION,0.3941018766756032,0.39655172413793105,0.10256410256410256
machine-translation,7,include a soft constraint on the batch - wise average of each gate .,performance,BALANCING EXPERT UTILIZATION,0,148,47,5,0,performance : BALANCING EXPERT UTILIZATION,0.3967828418230563,0.4051724137931034,0.1282051282051282
machine-translation,7,We take a soft constraint approach .,performance,BALANCING EXPERT UTILIZATION,0,149,48,6,0,performance : BALANCING EXPERT UTILIZATION,0.39946380697050937,0.41379310344827586,0.15384615384615385
machine-translation,7,We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .,performance,BALANCING EXPERT UTILIZATION,0,150,49,7,0,performance : BALANCING EXPERT UTILIZATION,0.40214477211796246,0.4224137931034483,0.1794871794871795
machine-translation,7,"We define an additional loss L importance , which is added to the over all loss function for the model .",performance,BALANCING EXPERT UTILIZATION,0,151,50,8,0,performance : BALANCING EXPERT UTILIZATION,0.40482573726541554,0.43103448275862066,0.20512820512820512
machine-translation,7,"This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .",performance,BALANCING EXPERT UTILIZATION,0,152,51,9,0,performance : BALANCING EXPERT UTILIZATION,0.4075067024128686,0.4396551724137931,0.23076923076923078
machine-translation,7,This additional loss encourages all experts to have equal importance .,performance,BALANCING EXPERT UTILIZATION,0,153,52,10,0,performance : BALANCING EXPERT UTILIZATION,0.4101876675603217,0.4482758620689655,0.2564102564102564
machine-translation,7,importance ( X ) = w importance CV ( Importance ( X ) ),performance,BALANCING EXPERT UTILIZATION,0,154,53,11,0,performance : BALANCING EXPERT UTILIZATION,0.4128686327077748,0.45689655172413796,0.28205128205128205
machine-translation,7,. The number of parameters in the LSTM layers of these models vary from 2 million to 151 million .,performance,BALANCING EXPERT UTILIZATION,0,155,54,12,0,performance : BALANCING EXPERT UTILIZATION,0.4155495978552279,0.46551724137931033,0.3076923076923077
machine-translation,7,"Quality increases greatly with parameter count , as do computational costs .",performance,BALANCING EXPERT UTILIZATION,0,156,55,13,0,performance : BALANCING EXPERT UTILIZATION,0.41823056300268097,0.47413793103448276,0.3333333333333333
machine-translation,7,Results for these models form the top line of - right .,performance,BALANCING EXPERT UTILIZATION,0,157,56,14,0,performance : BALANCING EXPERT UTILIZATION,0.42091152815013405,0.4827586206896552,0.358974358974359
machine-translation,7,MoE Models :,performance,BALANCING EXPERT UTILIZATION,0,158,57,15,0,performance : BALANCING EXPERT UTILIZATION,0.42359249329758714,0.49137931034482757,0.38461538461538464
machine-translation,7,Our models consist of two stacked LSTM layers with a MoE layer between them ( see ) .,performance,BALANCING EXPERT UTILIZATION,0,159,58,16,0,performance : BALANCING EXPERT UTILIZATION,0.4262734584450402,0.5,0.41025641025641024
machine-translation,7,We vary the sizes of the layers and the number of experts .,performance,BALANCING EXPERT UTILIZATION,0,160,59,17,0,performance : BALANCING EXPERT UTILIZATION,0.4289544235924933,0.5086206896551724,0.4358974358974359
machine-translation,7,"For full details on model architecture , training regimen , additional baselines and results , see Appendix C .",performance,BALANCING EXPERT UTILIZATION,0,161,60,18,0,performance : BALANCING EXPERT UTILIZATION,0.4316353887399464,0.5172413793103449,0.46153846153846156
machine-translation,7,The results of these models are shown in - left .,performance,BALANCING EXPERT UTILIZATION,0,162,61,19,0,performance : BALANCING EXPERT UTILIZATION,0.4343163538873995,0.5258620689655172,0.48717948717948717
machine-translation,7,"The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .",performance,BALANCING EXPERT UTILIZATION,0,163,62,20,0,performance : BALANCING EXPERT UTILIZATION,0.43699731903485256,0.5344827586206896,0.5128205128205128
machine-translation,7,"Varied Computation , High Capacity :",performance,BALANCING EXPERT UTILIZATION,0,164,63,21,0,performance : BALANCING EXPERT UTILIZATION,0.43967828418230565,0.5431034482758621,0.5384615384615384
machine-translation,7,"In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets .",performance,BALANCING EXPERT UTILIZATION,0,165,64,22,0,performance : BALANCING EXPERT UTILIZATION,0.44235924932975873,0.5517241379310345,0.5641025641025641
machine-translation,7,"These models had larger LSTMs , and fewer but larger and experts .",performance,BALANCING EXPERT UTILIZATION,0,166,65,23,0,performance : BALANCING EXPERT UTILIZATION,0.4450402144772118,0.5603448275862069,0.5897435897435898
machine-translation,7,Details can be found in Appendix C.2 .,performance,BALANCING EXPERT UTILIZATION,0,167,66,24,0,performance : BALANCING EXPERT UTILIZATION,0.4477211796246649,0.5689655172413793,0.6153846153846154
machine-translation,7,Results of these three models form the bottom line of - right .,performance,BALANCING EXPERT UTILIZATION,0,168,67,25,0,performance : BALANCING EXPERT UTILIZATION,0.450402144772118,0.5775862068965517,0.6410256410256411
machine-translation,7,compares the results of these models to the best previously - published result on this dataset .,performance,BALANCING EXPERT UTILIZATION,0,169,68,26,0,performance : BALANCING EXPERT UTILIZATION,0.45308310991957107,0.5862068965517241,0.6666666666666666
machine-translation,7,"Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation .",performance,BALANCING EXPERT UTILIZATION,0,170,69,27,0,performance : BALANCING EXPERT UTILIZATION,0.45576407506702415,0.5948275862068966,0.6923076923076923
machine-translation,7,Computational,performance,BALANCING EXPERT UTILIZATION,0,171,70,28,0,performance : BALANCING EXPERT UTILIZATION,0.4584450402144772,0.603448275862069,0.717948717948718
machine-translation,7,Efficiency : We trained our models using TensorFlow on clusters containing 16 - 32 Tesla K40 GPUs .,performance,BALANCING EXPERT UTILIZATION,0,172,71,29,0,performance : BALANCING EXPERT UTILIZATION,0.46112600536193027,0.6120689655172413,0.7435897435897436
machine-translation,7,"For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster .",performance,BALANCING EXPERT UTILIZATION,0,173,72,30,0,performance : BALANCING EXPERT UTILIZATION,0.46380697050938335,0.6206896551724138,0.7692307692307693
machine-translation,7,"The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .",performance,BALANCING EXPERT UTILIZATION,0,174,73,31,0,performance : BALANCING EXPERT UTILIZATION,0.46648793565683644,0.6293103448275862,0.7948717948717948
machine-translation,7,"For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total .",performance,BALANCING EXPERT UTILIZATION,0,175,74,32,0,performance : BALANCING EXPERT UTILIZATION,0.4691689008042895,0.6379310344827587,0.8205128205128205
machine-translation,7,"For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU .",performance,BALANCING EXPERT UTILIZATION,0,176,75,33,0,performance : BALANCING EXPERT UTILIZATION,0.4718498659517426,0.646551724137931,0.8461538461538461
machine-translation,7,"For our low-computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism .",performance,BALANCING EXPERT UTILIZATION,0,177,76,34,0,performance : BALANCING EXPERT UTILIZATION,0.4745308310991957,0.6551724137931034,0.8717948717948718
machine-translation,7,"Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices .",performance,BALANCING EXPERT UTILIZATION,0,178,77,35,0,performance : BALANCING EXPERT UTILIZATION,0.4772117962466488,0.6637931034482759,0.8974358974358975
machine-translation,7,These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .,performance,BALANCING EXPERT UTILIZATION,0,179,78,36,0,performance : BALANCING EXPERT UTILIZATION,0.47989276139410186,0.6724137931034483,0.9230769230769231
machine-translation,7,"Detailed results are in Appendix C , .",performance,BALANCING EXPERT UTILIZATION,0,180,79,37,0,performance : BALANCING EXPERT UTILIZATION,0.48257372654155495,0.6810344827586207,0.9487179487179487
machine-translation,7,"On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in - left .",performance,BALANCING EXPERT UTILIZATION,0,181,80,38,0,performance : BALANCING EXPERT UTILIZATION,0.48525469168900803,0.6896551724137931,0.9743589743589743
machine-translation,7,"We hypothesized that for a larger training set , even higher capacities would produce significant quality improvements .",performance,BALANCING EXPERT UTILIZATION,0,182,81,39,0,performance : BALANCING EXPERT UTILIZATION,0.4879356568364611,0.6982758620689655,1.0
machine-translation,7,100 BILLION WORD GOOGLE NEWS CORPUS,performance,100 BILLION WORD GOOGLE NEWS CORPUS,1,183,82,1,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.4906166219839142,0.7068965517241379,0.1
machine-translation,7,"We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .",performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,184,83,2,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.4932975871313673,0.7155172413793104,0.2
machine-translation,7,"Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .",performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,185,84,3,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.4959785522788204,0.7241379310344828,0.3
machine-translation,7,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .",performance,100 BILLION WORD GOOGLE NEWS CORPUS,1,186,85,4,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.49865951742627346,0.7327586206896551,0.4
machine-translation,7,This corresponds to up to 137 billion parameters in the MoE layer .,performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,187,86,5,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.5013404825737265,0.7413793103448276,0.5
machine-translation,7,"Details on architecture , training , and results are given in Appendix D.",performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,188,87,6,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.5040214477211796,0.75,0.6
machine-translation,7,Results : shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) .,performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,189,88,7,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.5067024128686327,0.7586206896551724,0.7
machine-translation,7,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .",performance,100 BILLION WORD GOOGLE NEWS CORPUS,1,190,89,8,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.5093833780160858,0.7672413793103449,0.8
machine-translation,7,The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets .,performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,191,90,9,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.5120643431635389,0.7758620689655172,0.9
machine-translation,7,"Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU .",performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,192,91,10,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.514745308310992,0.7844827586206896,1.0
machine-translation,7,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),1,193,92,1,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.517426273458445,0.7931034482758621,0.14285714285714285
machine-translation,7,Model Architecture :,performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0,194,93,2,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.5201072386058981,0.8017241379310345,0.2857142857142857
machine-translation,7,Our model was a modified version of the GNMT model described in .,performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),1,195,94,3,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.5227882037533512,0.8103448275862069,0.42857142857142855
machine-translation,7,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),1,196,95,4,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.5254691689008043,0.8189655172413793,0.5714285714285714
machine-translation,7,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),1,197,96,5,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.5281501340482574,0.8275862068965517,0.7142857142857143
machine-translation,7,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .",performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),1,198,97,6,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.5308310991957105,0.8362068965517241,0.8571428571428571
machine-translation,7,"Further details on model architecture , testing procedure and results can be found in Appendix E.",performance,MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0,199,98,7,0,performance : MACHINE TRANSLATION (SINGLE LANGUAGE PAIR),0.5335120643431636,0.8448275862068966,1.0
machine-translation,7,Datasets :,performance,Datasets:,0,200,99,1,0,performance : Datasets:,0.5361930294906166,0.853448275862069,0.09090909090909091
machine-translation,7,"We benchmarked our method on the WMT ' 14 En? Fr and En ? De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",performance,Datasets:,0,201,100,2,0,performance : Datasets:,0.5388739946380697,0.8620689655172413,0.18181818181818182
machine-translation,7,"We benchmarked our method on the WMT ' 14 En? Fr and En ? De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",performance,Datasets:,0,202,101,3,0,performance : Datasets:,0.5415549597855228,0.8706896551724138,0.2727272727272727
machine-translation,7,"The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .",performance,Datasets:,0,203,102,4,0,performance : Datasets:,0.5442359249329759,0.8793103448275862,0.36363636363636365
machine-translation,7,We also tested the same model on a Google 's Production English to French data . 2.79 39.22 214M 278M 6 days/96 k 80s GNMT+RL 2.96 39.92 214M 278M 6 days/96 k80s PBMT 37.0 LSTM ( 6-layer ) 31.5 LSTM ( 6-layer + PosUnk ) 33.1 DeepAtt 37.7 DeepAtt+PosUnk 39.2 5.25 24.91 214M 278M 1 day/96 k80s GNMT + RL 8.08 24.66 214M 278M 1 day/96 k80s PBMT 20.7 DeepAtt 20.6,performance,Datasets:,0,204,103,5,0,performance : Datasets:,0.546916890080429,0.8879310344827587,0.45454545454545453
machine-translation,7,"Results : show the results of our largest models , compared with published results .",performance,Datasets:,0,205,104,6,0,performance : Datasets:,0.5495978552278821,0.896551724137931,0.5454545454545454
machine-translation,7,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,performance,Datasets:,1,206,105,7,0,performance : Datasets:,0.5522788203753352,0.9051724137931034,0.6363636363636364
machine-translation,7,"As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in .",performance,Datasets:,0,207,106,8,0,performance : Datasets:,0.5549597855227882,0.9137931034482759,0.7272727272727273
machine-translation,7,The perplexity scores are also better .,performance,Datasets:,0,208,107,9,0,performance : Datasets:,0.5576407506702413,0.9224137931034483,0.8181818181818182
machine-translation,7,2,performance,Datasets:,0,209,108,10,0,performance : Datasets:,0.5603217158176944,0.9310344827586207,0.9090909090909091
machine-translation,7,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .",performance,Datasets:,1,210,109,11,0,performance : Datasets:,0.5630026809651475,0.9396551724137931,1.0
machine-translation,7,MULTILINGUAL MACHINE TRANSLATION,performance,MULTILINGUAL MACHINE TRANSLATION,1,211,110,1,0,performance : MULTILINGUAL MACHINE TRANSLATION,0.5656836461126006,0.9482758620689655,1.0
machine-translation,7,Results :,performance,Results:,0,212,111,1,0,performance : Results:,0.5683646112600537,0.9568965517241379,0.16666666666666666
machine-translation,7,"Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in .",performance,Results:,0,213,112,2,0,performance : Results:,0.5710455764075067,0.9655172413793104,0.3333333333333333
machine-translation,7,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,performance,Results:,1,214,113,3,0,performance : Results:,0.5737265415549598,0.9741379310344828,0.5
machine-translation,7,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",performance,Results:,1,215,114,4,0,performance : Results:,0.5764075067024129,0.9827586206896551,0.6666666666666666
machine-translation,7,"The poor performance on English ? Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",performance,Results:,0,216,115,5,0,performance : Results:,0.579088471849866,0.9913793103448276,0.8333333333333334
machine-translation,7,"The poor performance on English ? Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",performance,Results:,0,217,116,6,0,performance : Results:,0.5817694369973191,1.0,1.0
machine-translation,7,CONCLUSION,conclusion,CONCLUSION,0,218,1,1,0,conclusion : CONCLUSION,0.5844504021447721,0.2,0.2
machine-translation,7,This work is the first to demonstrate major wins from conditional computation in deep networks .,conclusion,CONCLUSION,0,219,2,2,0,conclusion : CONCLUSION,0.5871313672922251,0.4,0.4
machine-translation,7,We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions .,conclusion,CONCLUSION,0,220,3,3,0,conclusion : CONCLUSION,0.5898123324396782,0.6,0.6
machine-translation,7,"While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets .",conclusion,CONCLUSION,0,221,4,4,0,conclusion : CONCLUSION,0.5924932975871313,0.8,0.8
machine-translation,7,We look forward to seeing many novel implementations and applications of conditional computation in the years to come .,conclusion,CONCLUSION,0,222,5,5,0,conclusion : CONCLUSION,0.5951742627345844,1.0,1.0
machine-translation,7,APPENDICES A LOAD - BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,223,1,1,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.5978552278820375,0.006622516556291391,0.05555555555555555
machine-translation,7,"As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,224,2,2,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6005361930294906,0.013245033112582781,0.1111111111111111
machine-translation,7,"Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,225,3,3,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6032171581769437,0.019867549668874173,0.16666666666666666
machine-translation,7,"Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert for a batch X of inputs .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,226,4,4,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6058981233243967,0.026490066225165563,0.2222222222222222
machine-translation,7,The smoothness allows us to back - propagate gradients through the estimator .,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,227,5,5,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6085790884718498,0.033112582781456956,0.2777777777777778
machine-translation,7,This is the purpose of the noise term in the gating function .,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,228,6,6,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6112600536193029,0.039735099337748346,0.3333333333333333
machine-translation,7,"We define P ( x , i ) as the probability that G (x ) i is nonzero , given a new random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,229,7,7,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.613941018766756,0.046357615894039736,0.3888888888888889
machine-translation,7,"To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,230,8,8,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6166219839142091,0.052980132450331126,0.4444444444444444
machine-translation,7,The probability works out to be :,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,231,9,9,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6193029490616622,0.059602649006622516,0.5
machine-translation,7,"Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,232,10,10,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6219839142091153,0.06622516556291391,0.5555555555555556
machine-translation,7,"Simplifying , we get :",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,233,11,11,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6246648793565683,0.0728476821192053,0.6111111111111112
machine-translation,7,Where ? is the CDF of the standard normal distribution .,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,234,12,12,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6273458445040214,0.07947019867549669,0.6666666666666666
machine-translation,7,Where ? is the CDF of the standard normal distribution .,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,235,13,13,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6300268096514745,0.08609271523178808,0.7222222222222222
machine-translation,7,"We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,236,14,14,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6327077747989276,0.09271523178807947,0.7777777777777778
machine-translation,7,load ( X ) = w load CV ( Load ( X ) ),APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,237,15,15,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6353887399463807,0.09933774834437085,0.8333333333333334
machine-translation,7,( 11 ) Initial Load Imbalance :,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,238,16,16,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6380697050938338,0.10596026490066225,0.8888888888888888
machine-translation,7,"To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need sometime to work ) .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,239,17,17,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6407506702412868,0.11258278145695365,0.9444444444444444
machine-translation,7,"To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise .",APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,240,18,18,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6434316353887399,0.11920529801324503,1.0
machine-translation,7,Experiments :,APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,241,19,1,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.646112600536193,0.12582781456953643,0.1
machine-translation,7,"We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load .",APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,242,20,2,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6487935656836461,0.13245033112582782,0.2
machine-translation,7,"We trained each model for 10 epochs , then measured perplexity on the test set .",APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,243,21,3,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6514745308310992,0.1390728476821192,0.3
machine-translation,7,"We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load .",APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,244,22,4,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6541554959785523,0.1456953642384106,0.4
machine-translation,7,This last value is significant for load balancing purposes on distributed hardware .,APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,245,23,5,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6568364611260054,0.152317880794702,0.5
machine-translation,7,All of these metrics were averaged over several training batches .,APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,246,24,6,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6595174262734584,0.15894039735099338,0.6
machine-translation,7,Results :,APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,247,25,7,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6621983914209115,0.16556291390728478,0.7
machine-translation,7,Results are reported in .,APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,248,26,8,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6648793565683646,0.17218543046357615,0.8
machine-translation,7,"All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse .",APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,249,27,9,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6675603217158177,0.17880794701986755,0.9
machine-translation,7,Models with higher values of w load had lower loads on the most overloaded expert .,APPENDICES A LOAD-BALANCING LOSS,Experiments:,0,250,28,10,0,APPENDICES A LOAD-BALANCING LOSS : Experiments:,0.6702412868632708,0.18543046357615894,1.0
machine-translation,7,HIERACHICAL MIXTURE OF EXPERTS,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,251,29,1,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6729222520107239,0.19205298013245034,0.1
machine-translation,7,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,252,30,2,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.675603217158177,0.1986754966887417,0.2
machine-translation,7,3,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,253,31,3,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.67828418230563,0.2052980132450331,0.3
machine-translation,7,"If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by , and the expert networks by ( E 0 , 0 , E 0 , 1 ..E a , b ) .",APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,254,32,4,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6809651474530831,0.2119205298013245,0.4
machine-translation,7,The output of the MoE is given by :,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,255,33,5,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6836461126005362,0.2185430463576159,0.5
machine-translation,7,Our metrics of expert utilization change to the following :,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,256,34,6,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6863270777479893,0.2251655629139073,0.6
machine-translation,7,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,257,35,7,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6890080428954424,0.23178807947019867,0.7
machine-translation,7,( i ) denotes the subset of X for which G primary ( x ) i >,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,258,36,8,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6916890080428955,0.23841059602649006,0.8
machine-translation,7,0 .,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,259,37,9,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6943699731903485,0.24503311258278146,0.9
machine-translation,7,"It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above .",APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,260,38,10,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6970509383378016,0.25165562913907286,1.0
machine-translation,7,1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS,APPENDICES A LOAD-BALANCING LOSS,1 BILLION WORD LANGUAGE MODELING BENCHMARK -EXPERIMENTAL DETAILS,0,261,39,1,0,APPENDICES A LOAD-BALANCING LOSS : 1 BILLION WORD LANGUAGE MODELING BENCHMARK -EXPERIMENTAL DETAILS,0.6997319034852547,0.2582781456953642,1.0
machine-translation,7,8- MILLION - OPERATIONS - PER - TIMESTEP MODELS,APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,262,40,1,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7024128686327078,0.26490066225165565,0.09090909090909091
machine-translation,7,Model Architecture :,APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,263,41,2,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7050938337801609,0.271523178807947,0.18181818181818182
machine-translation,7,"Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer , a MoE layer , a second LSTM layer , and a softmax layer .",APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,264,42,3,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.707774798927614,0.2781456953642384,0.2727272727272727
machine-translation,7,"The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 .",APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,265,43,4,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.710455764075067,0.2847682119205298,0.36363636363636365
machine-translation,7,"For every layer other than the softmax , we apply drouput to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 ? DropP rob ) .",APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,266,44,5,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7131367292225201,0.2913907284768212,0.45454545454545453
machine-translation,7,"After dropout , the output of the previous layer is added to the layer output .",APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,267,45,6,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7158176943699732,0.2980132450331126,0.5454545454545454
machine-translation,7,This residual connection encourages gradient flow .,APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,268,46,7,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7184986595174263,0.304635761589404,0.6363636363636364
machine-translation,7,"For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .",APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,269,47,8,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7211796246648794,0.31125827814569534,0.7272727272727273
machine-translation,7,We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers .,APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,270,48,9,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7238605898123325,0.31788079470198677,0.8181818181818182
machine-translation,7,"Thus , each example is processed by exactly 4 experts for a total of 4M ops / timestep .",APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,271,49,10,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7265415549597856,0.32450331125827814,0.9090909090909091
machine-translation,7,The two LSTM layers contribute 2M ops / timestep each for the desired total of 8 M .,APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,272,50,11,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7292225201072386,0.33112582781456956,1.0
machine-translation,7,Computationally - Matched Baselines :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,273,51,1,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7319034852546917,0.33774834437086093,0.02127659574468085
machine-translation,7,"The MoE - 4 model does not employ sparsity , since all 4 experts are always used .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,274,52,2,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7345844504021448,0.3443708609271523,0.0425531914893617
machine-translation,7,"In addition , we trained four more computationally - matched baseline models with no sparsity :",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,275,53,3,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7372654155495979,0.3509933774834437,0.06382978723404255
machine-translation,7,MoE - 1 - Wide :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,276,54,4,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.739946380697051,0.3576158940397351,0.0851063829787234
machine-translation,7,"The MoE layer consists of a single "" expert "" containing one ReLU - activated hidden layer of size 4096 .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,277,55,5,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7426273458445041,0.36423841059602646,0.10638297872340426
machine-translation,7,MoE - 1 - Deep :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,278,56,6,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7453083109919572,0.3708609271523179,0.1276595744680851
machine-translation,7,"The MoE layer consists of a single "" expert "" containing four ReLU - activated hidden layers , each with size 1024 .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,279,57,7,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7479892761394102,0.37748344370860926,0.14893617021276595
machine-translation,7,4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,280,58,8,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7506702412868632,0.3841059602649007,0.1702127659574468
machine-translation,7,LSTM - 2048-512 :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,281,59,9,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7533512064343163,0.39072847682119205,0.19148936170212766
machine-translation,7,The model contains one 2048 - unit LSTM layer ( and no MoE ) .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,282,60,10,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7560321715817694,0.3973509933774834,0.2127659574468085
machine-translation,7,The output of the LSTM is projected down to 512 dimensions .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,283,61,11,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7587131367292225,0.40397350993377484,0.23404255319148937
machine-translation,7,The next timestep of the LSTM receives the projected output .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,284,62,12,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7613941018766756,0.4105960264900662,0.2553191489361702
machine-translation,7,This is identical to one of the models published in .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,285,63,13,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7640750670241286,0.41721854304635764,0.2765957446808511
machine-translation,7,"We re-ran it to account for differences in training regimen , and obtained results very similar to the published ones .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,286,64,14,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7667560321715817,0.423841059602649,0.2978723404255319
machine-translation,7,Training :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,287,65,15,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7694369973190348,0.4304635761589404,0.3191489361702128
machine-translation,7,The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,288,66,16,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7721179624664879,0.4370860927152318,0.3404255319148936
machine-translation,7,"Each batch consisted of a set of sentences totaling roughly 300,000 words .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,289,67,17,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.774798927613941,0.44370860927152317,0.3617021276595745
machine-translation,7,"In the interest of time , we limited training to 10 epochs , ( 27,000 steps ) .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,290,68,18,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7774798927613941,0.4503311258278146,0.3829787234042553
machine-translation,7,"Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,291,69,19,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7801608579088471,0.45695364238410596,0.40425531914893614
machine-translation,7,We used the Adam optimizer .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,292,70,20,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7828418230563002,0.46357615894039733,0.425531914893617
machine-translation,7,"The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,293,71,21,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7855227882037533,0.47019867549668876,0.44680851063829785
machine-translation,7,The Softmax output layer was trained efficiently using importance sampling similarly to the models in .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,294,72,22,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7882037533512064,0.4768211920529801,0.46808510638297873
machine-translation,7,"For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,295,73,23,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7908847184986595,0.48344370860927155,0.48936170212765956
machine-translation,7,"To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A.",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,296,74,24,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7935656836461126,0.4900662251655629,0.5106382978723404
machine-translation,7,Results :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,297,75,25,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7962466487935657,0.4966887417218543,0.5319148936170213
machine-translation,7,"We evaluate our model using perplexity on the holdout dataset , used by .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,298,76,26,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.7989276139410187,0.5033112582781457,0.5531914893617021
machine-translation,7,We follow the standard procedure and sum over all the words including the end of sentence symbol .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,299,77,27,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8016085790884718,0.5099337748344371,0.574468085106383
machine-translation,7,Results are reported in .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,300,78,28,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8042895442359249,0.5165562913907285,0.5957446808510638
machine-translation,7,"For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,301,79,29,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.806970509383378,0.5231788079470199,0.6170212765957447
machine-translation,7,We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,302,80,30,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8096514745308311,0.5298013245033113,0.6382978723404256
machine-translation,7,"First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,303,81,31,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8123324396782842,0.5364238410596026,0.6595744680851063
machine-translation,7,"Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage :",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,304,82,32,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8150134048257373,0.543046357615894,0.6808510638297872
machine-translation,7,The Adam optimizer keeps first and second moment estimates of the perparameter gradients .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,305,83,33,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8176943699731903,0.5496688741721855,0.7021276595744681
machine-translation,7,This triples the required memory .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,306,84,34,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8203753351206434,0.5562913907284768,0.723404255319149
machine-translation,7,"To avoid keeping a first - moment estimator , we set ? 1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,307,85,35,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8230563002680965,0.5629139072847682,0.7446808510638298
machine-translation,7,"To avoid keeping a first - moment estimator , we set ? 1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,308,86,36,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8257372654155496,0.5695364238410596,0.7659574468085106
machine-translation,7,"For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,309,87,37,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8284182305630027,0.5761589403973509,0.7872340425531915
machine-translation,7,"At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,310,88,38,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8310991957104558,0.5827814569536424,0.8085106382978723
machine-translation,7,This technique could similarly be applied to Adagrad .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,311,89,39,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8337801608579088,0.5894039735099338,0.8297872340425532
machine-translation,7,Results :,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,312,90,40,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8364611260053619,0.5960264900662252,0.851063829787234
machine-translation,7,We evaluate our model using perplexity on a holdout dataset .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,313,91,41,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.839142091152815,0.6026490066225165,0.8723404255319149
machine-translation,7,Results are reported in .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,314,92,42,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8418230563002681,0.609271523178808,0.8936170212765957
machine-translation,7,Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,315,93,43,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8445040214477212,0.6158940397350994,0.9148936170212766
machine-translation,7,It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,316,94,44,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8471849865951743,0.6225165562913907,0.9361702127659575
machine-translation,7,"This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,317,95,45,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8498659517426274,0.6291390728476821,0.9574468085106383
machine-translation,7,"For comparison , we include results for a computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,318,96,46,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8525469168900804,0.6357615894039735,0.9787234042553191
machine-translation,7,4,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,319,97,47,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8552278820375335,0.6423841059602649,1.0
machine-translation,7,MACHINE TRANSLATION - EXPERIMENTAL DETAILS,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,320,98,1,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8579088471849866,0.6490066225165563,0.03125
machine-translation,7,Model Architecture for Single Language,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,321,99,2,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8605898123324397,0.6556291390728477,0.0625
machine-translation,7,Pair MoE Models :,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,322,100,3,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8632707774798928,0.6622516556291391,0.09375
machine-translation,7,Our model is a modified version of the GNMT model described in .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,323,101,4,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8659517426273459,0.6688741721854304,0.125
machine-translation,7,"To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,324,102,5,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.868632707774799,0.6754966887417219,0.15625
machine-translation,7,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,325,103,6,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.871313672922252,0.6821192052980133,0.1875
machine-translation,7,"We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,326,104,7,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8739946380697051,0.6887417218543046,0.21875
machine-translation,7,All of the layers in our model have input and output dimensionality of 512 .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,327,105,8,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8766756032171582,0.695364238410596,0.25
machine-translation,7,"Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,328,106,9,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8793565683646113,0.7019867549668874,0.28125
machine-translation,7,We add residual connections around all LSTM and MoE layers to encourage gradient flow .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,329,107,10,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8820375335120644,0.7086092715231788,0.3125
machine-translation,7,"Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as "" wordpieces "" )",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,330,108,11,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8847184986595175,0.7152317880794702,0.34375
machine-translation,7,"Schuster & Nakajima , 2012 ) for inputs and outputs in our system .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,331,109,12,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8873994638069705,0.7218543046357616,0.375
machine-translation,7,We use a shared source and target vocabulary of 32 K wordpieces .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,332,110,13,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8900804289544236,0.7284768211920529,0.40625
machine-translation,7,We also used the same beam search technique as proposed in Model Architecture for Multilingual MoE Model :,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,333,111,14,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8927613941018767,0.7350993377483444,0.4375
machine-translation,7,"We used the same model architecture as for the single - language - pair models , with the following exceptions :",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,334,112,15,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8954423592493298,0.7417218543046358,0.46875
machine-translation,7,"We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,335,113,16,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8981233243967829,0.7483443708609272,0.5
machine-translation,7,Each expert has a larger hidden layer of size 8192 .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,336,114,17,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.900804289544236,0.7549668874172185,0.53125
machine-translation,7,"This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,337,115,18,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.903485254691689,0.7615894039735099,0.5625
machine-translation,7,Training :,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,338,116,19,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9061662198391421,0.7682119205298014,0.59375
machine-translation,7,We trained our networks using the Adam optimizer .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,339,117,20,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9088471849865952,0.7748344370860927,0.625
machine-translation,7,"The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,340,118,21,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9115281501340483,0.7814569536423841,0.65625
machine-translation,7,"For the single - language - pair models , similarly to , we applied dropout to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,341,119,22,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9142091152815014,0.7880794701986755,0.6875
machine-translation,7,Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,342,120,23,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9168900804289544,0.7947019867549668,0.71875
machine-translation,7,Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,343,121,24,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9195710455764075,0.8013245033112583,0.75
machine-translation,7,"To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A.",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,344,122,25,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9222520107238605,0.8079470198675497,0.78125
machine-translation,7,Metrics : We evaluated our models using the perplexity and the standard BLEU score metric .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,345,123,26,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9249329758713136,0.8145695364238411,0.8125
machine-translation,7,"We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,346,124,27,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9276139410187667,0.8211920529801324,0.84375
machine-translation,7,Results : and 4 in Section 5.3 show comparisons of our results to other published methods .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,347,125,28,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9302949061662198,0.8278145695364238,0.875
machine-translation,7,shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,348,126,29,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9329758713136729,0.8344370860927153,0.90625
machine-translation,7,"As can be seen from the as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,349,127,30,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.935656836461126,0.8410596026490066,0.9375
machine-translation,7,"We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,350,128,31,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.938337801608579,0.847682119205298,0.96875
machine-translation,7,"For example , one expert is used when the indefinite article "" a "" introduces the direct object in a verb phrase indicating importance or leadership .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,351,129,32,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9410187667560321,0.8543046357615894,1.0
machine-translation,7,STRICTLY BALANCED GATING,APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,352,130,1,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9436997319034852,0.8609271523178808,0.07142857142857142
machine-translation,7,"Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size .",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,353,131,2,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9463806970509383,0.8675496688741722,0.14285714285714285
machine-translation,7,"To accommodate this , we used a different gating function which we describe below .",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,354,132,3,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9490616621983914,0.8741721854304636,0.21428571428571427
machine-translation,7,Recall that we define the softmax gating function to be :,APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,355,133,4,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9517426273458445,0.8807947019867549,0.2857142857142857
machine-translation,7,Sparse Gating ( alternate formulation ) :,APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,356,134,5,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9544235924932976,0.8874172185430463,0.35714285714285715
machine-translation,7,"To obtain a sparse gating vector , we multiply G ? ( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,357,135,6,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9571045576407506,0.8940397350993378,0.42857142857142855
machine-translation,7,"To obtain a sparse gating vector , we multiply G ? ( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,358,136,7,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9597855227882037,0.9006622516556292,0.5
machine-translation,7,"The mask itself is a function of G ? ( x ) and specifies which experts are assigned to each input example : M batchwise ( X , m ) j , i = 1 if X j, i is in the top m values for to expert i 0 otherwise",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,359,137,8,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9624664879356568,0.9072847682119205,0.5714285714285714
machine-translation,7,"As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,360,138,9,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9651474530831099,0.9139072847682119,0.6428571428571429
machine-translation,7,Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask .,APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,361,139,10,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.967828418230563,0.9205298013245033,0.7142857142857143
machine-translation,7,We use the following mask at inference time :,APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,362,140,11,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9705093833780161,0.9271523178807947,0.7857142857142857
machine-translation,7,"To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,363,141,12,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9731903485254692,0.9337748344370861,0.8571428571428571
machine-translation,7,"batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ? M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,364,142,13,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9758713136729222,0.9403973509933775,0.9285714285714286
machine-translation,7,"batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ? M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,365,143,14,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9785522788203753,0.9470198675496688,1.0
machine-translation,7,ATTENTION FUNCTION,APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,366,144,1,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9812332439678284,0.9536423841059603,0.125
machine-translation,7,"The attention mechanism described in GNMT involves a learned "" Attention Function "" A ( x i , y j ) which takes a "" source vector "" x i and a "" target vector "" y j , and must be computed for every source time step i and target time step j .",APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,367,145,2,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9839142091152815,0.9602649006622517,0.25
machine-translation,7,"In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n.",APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,368,146,3,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9865951742627346,0.9668874172185431,0.375
machine-translation,7,It can be expressed as :,APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,369,147,4,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9892761394101877,0.9735099337748344,0.5
machine-translation,7,Where U and Ware trainable weight matrices and V is a trainable weight vector .,APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,370,148,5,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9919571045576407,0.9801324503311258,0.625
machine-translation,7,"For performance reasons , in our models , we used a slightly different attention function :",APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,371,149,6,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9946380697050938,0.9867549668874173,0.75
machine-translation,7,"With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications .",APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,372,150,7,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,0.9973190348525469,0.9933774834437086,0.875
machine-translation,7,We found little difference in quality between the two functions .,APPENDICES A LOAD-BALANCING LOSS,ATTENTION FUNCTION,0,373,151,8,0,APPENDICES A LOAD-BALANCING LOSS : ATTENTION FUNCTION,1.0,1.0,1.0
machine-translation,8,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,title,title,1,2,1,1,0,title : title,0.006042296072507553,1.0,1.0
machine-translation,8,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.00906344410876133,0.14285714285714285,0.14285714285714285
machine-translation,8,Neural machine translation is a recently proposed approach to machine translation .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.012084592145015106,0.2857142857142857,0.2857142857142857
machine-translation,8,"Unlike the traditional statistical machine translation , the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.015105740181268883,0.42857142857142855,0.42857142857142855
machine-translation,8,The models proposed recently for neural machine translation often belong to a family of encoder - decoders and encode a source sentence into a fixed - length vector from which a decoder generates a translation .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.01812688821752266,0.5714285714285714,0.5714285714285714
machine-translation,8,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.021148036253776436,0.7142857142857143,0.7142857142857143
machine-translation,8,"With this new approach , we achieve a translation performance comparable to the existing state - of - the - art phrase - based system on the task of English - to - French translation .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02416918429003021,0.8571428571428571,0.8571428571428571
machine-translation,8,"Furthermore , qualitative analysis reveals that the ( soft - ) alignments found by the model agree well with our intuition .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.027190332326283987,1.0,1.0
machine-translation,8,INTRODUCTION,introduction,introduction,0,10,1,1,0,introduction : introduction,0.030211480362537766,0.047619047619047616,0.047619047619047616
machine-translation,8,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",introduction,introduction,1,11,2,2,0,introduction : introduction,0.03323262839879154,0.09523809523809523,0.09523809523809523
machine-translation,8,"Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components thatare tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.03625377643504532,0.14285714285714285,0.14285714285714285
machine-translation,8,"Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.03927492447129909,0.19047619047619047,0.19047619047619047
machine-translation,8,An encoder neural network reads and encodes a source sentence into a fixed - length vector .,introduction,introduction,0,14,5,5,0,introduction : introduction,0.04229607250755287,0.23809523809523808,0.23809523809523808
machine-translation,8,decoder then outputs a translation from the encoded vector .,introduction,introduction,0,15,6,6,0,introduction : introduction,0.045317220543806644,0.2857142857142857,0.2857142857142857
machine-translation,8,"The whole encoder - decoder system , which consists of the encoder and the decoder for a language pair , is jointly trained to maximize the probability of a correct translation given a source sentence .",introduction,introduction,0,16,7,7,0,introduction : introduction,0.04833836858006042,0.3333333333333333,0.3333333333333333
machine-translation,8,potential issue with this encoder - decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed - length vector .,introduction,introduction,0,17,8,8,0,introduction : introduction,0.0513595166163142,0.38095238095238093,0.38095238095238093
machine-translation,8,"This may make it difficult for the neural network to cope with long sentences , especially those thatare longer than the sentences in the training corpus .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.054380664652567974,0.42857142857142855,0.42857142857142855
machine-translation,8,showed that indeed the performance of a basic encoder - decoder deteriorates rapidly as the length of an input sentence increases .,introduction,introduction,0,19,10,10,0,introduction : introduction,0.05740181268882175,0.47619047619047616,0.47619047619047616
machine-translation,8,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .",introduction,introduction,1,20,11,11,0,introduction : introduction,0.06042296072507553,0.5238095238095238,0.5238095238095238
machine-translation,8,"Each time the proposed model generates a word in a translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated .",introduction,introduction,1,21,12,12,0,introduction : introduction,0.0634441087613293,0.5714285714285714,0.5714285714285714
machine-translation,8,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,introduction,introduction,1,22,13,13,0,introduction : introduction,0.06646525679758308,0.6190476190476191,0.6190476190476191
machine-translation,8,The most important distinguishing feature of this approach from the basic encoder - decoder is that it does not attempt to encode a whole input sentence into a single fixed - length vector .,introduction,introduction,0,23,14,14,0,introduction : introduction,0.06948640483383686,0.6666666666666666,0.6666666666666666
machine-translation,8,"Instead , it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation .",introduction,introduction,0,24,15,15,0,introduction : introduction,0.07250755287009064,0.7142857142857143,0.7142857142857143
machine-translation,8,"This frees a neural translation model from having to squash all the information of a source sentence , regardless of its length , into a fixed - length vector .",introduction,introduction,0,25,16,16,0,introduction : introduction,0.0755287009063444,0.7619047619047619,0.7619047619047619
machine-translation,8,We show this allows a model to cope better with long sentences .,introduction,introduction,0,26,17,17,0,introduction : introduction,0.07854984894259819,0.8095238095238095,0.8095238095238095
machine-translation,8,"In this paper , we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder - decoder approach .",introduction,introduction,0,27,18,18,0,introduction : introduction,0.08157099697885196,0.8571428571428571,0.8571428571428571
machine-translation,8,"The improvement is more apparent with longer sentences , but can be observed with sentences of any length .",introduction,introduction,0,28,19,19,0,introduction : introduction,0.08459214501510574,0.9047619047619048,0.9047619047619048
machine-translation,8,"On the task of English - to - French translation , the proposed approach achieves , with a single model , a translation performance comparable , or close , to the conventional phrase - based system .",introduction,introduction,0,29,20,20,0,introduction : introduction,0.08761329305135952,0.9523809523809523,0.9523809523809523
machine-translation,8,"Furthermore , qualitative analysis reveals that the proposed model finds a linguistically plausible ( soft - ) alignment between a source sentence and the corresponding target sentence .",introduction,introduction,0,30,21,21,0,introduction : introduction,0.09063444108761329,1.0,1.0
machine-translation,8,BACKGROUND : NEURAL MACHINE TRANSLATION,system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,31,1,1,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.09365558912386707,0.013888888888888888,0.1
machine-translation,8,"From a probabilistic perspective , translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x , i.e. , arg max y p ( y | x ) .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,32,2,2,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.09667673716012085,0.027777777777777776,0.2
machine-translation,8,"In neural machine translation , we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,33,3,3,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.09969788519637462,0.041666666666666664,0.3
machine-translation,8,"Once the conditional distribution is learned by a translation model , given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,34,4,4,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.1027190332326284,0.05555555555555555,0.4
machine-translation,8,"Recently , a number of papers have proposed the use of neural networks to directly learn this conditional distribution ( see , e.g. , .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,35,5,5,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.10574018126888217,0.06944444444444445,0.5
machine-translation,8,"This neural machine translation approach typically consists of two components , the first of which encodes a source sentence x and the second decodes to a target sentence y .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,36,6,6,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.10876132930513595,0.08333333333333333,0.6
machine-translation,8,"For instance , two recurrent neural networks ( RNN ) were used by and to encode a variable - length source sentence into a fixed - length vector and to decode the vector into a variable - length target sentence .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,37,7,7,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.11178247734138973,0.09722222222222222,0.7
machine-translation,8,"Despite being a quite new approach , neural machine translation has already shown promising results .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,38,8,8,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.1148036253776435,0.1111111111111111,0.8
machine-translation,8,reported that the neural machine translation based on RNNs with long shortterm memory ( LSTM ) units achieves close to the state - of - the - art performance of the conventional phrase - based machine translation system on an English - to - French translation task .,system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,39,9,9,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.11782477341389729,0.125,0.9
machine-translation,8,"Adding neural components to existing translation systems , for instance , to score the phrase pairs in the phrase table or to re-rank candidate translations , has allowed to surpass the previous state - of - the - art performance level .",system description,BACKGROUND: NEURAL MACHINE TRANSLATION,0,40,10,10,0,system description : BACKGROUND: NEURAL MACHINE TRANSLATION,0.12084592145015106,0.1388888888888889,1.0
machine-translation,8,RNN ENCODER - DECODER,system description,RNN ENCODER-DECODER,0,41,11,1,0,system description : RNN ENCODER-DECODER,0.12386706948640483,0.1527777777777778,0.07692307692307693
machine-translation,8,"Here , we describe briefly the underlying framework , called RNN Encoder - Decoder , proposed by and upon which we build a novel architecture that learns to align and translate simultaneously .",system description,RNN ENCODER-DECODER,0,42,12,2,0,system description : RNN ENCODER-DECODER,0.1268882175226586,0.16666666666666666,0.15384615384615385
machine-translation,8,"In the Encoder - Decoder framework , an encoder reads the input sentence , a sequence of vectors x = ( x 1 , , x Tx ) , into a vector c. 2 The most common approach is to use an RNN such that",system description,RNN ENCODER-DECODER,0,43,13,3,0,system description : RNN ENCODER-DECODER,0.1299093655589124,0.18055555555555555,0.23076923076923078
machine-translation,8,"1 ) and c = q ( {h 1 , , h Tx } ) , where ht ? Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .",system description,RNN ENCODER-DECODER,0,44,14,4,0,system description : RNN ENCODER-DECODER,0.13293051359516617,0.19444444444444445,0.3076923076923077
machine-translation,8,"1 ) and c = q ( {h 1 , , h Tx } ) , where ht ? Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .",system description,RNN ENCODER-DECODER,0,45,15,5,0,system description : RNN ENCODER-DECODER,0.13595166163141995,0.20833333333333334,0.38461538461538464
machine-translation,8,and q are some nonlinear functions .,system description,RNN ENCODER-DECODER,0,46,16,6,0,system description : RNN ENCODER-DECODER,0.13897280966767372,0.2222222222222222,0.46153846153846156
machine-translation,8,"used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .",system description,RNN ENCODER-DECODER,0,47,17,7,0,system description : RNN ENCODER-DECODER,0.1419939577039275,0.2361111111111111,0.5384615384615384
machine-translation,8,"The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.",system description,RNN ENCODER-DECODER,0,48,18,8,0,system description : RNN ENCODER-DECODER,0.14501510574018128,0.25,0.6153846153846154
machine-translation,8,"In other words , the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals :",system description,RNN ENCODER-DECODER,0,49,19,9,0,system description : RNN ENCODER-DECODER,0.14803625377643503,0.2638888888888889,0.6923076923076923
machine-translation,8,"where y = y 1 , , y Ty .",system description,RNN ENCODER-DECODER,0,50,20,10,0,system description : RNN ENCODER-DECODER,0.1510574018126888,0.2777777777777778,0.7692307692307693
machine-translation,8,"With an RNN , each conditional probability is modeled as",system description,RNN ENCODER-DECODER,0,51,21,11,0,system description : RNN ENCODER-DECODER,0.1540785498489426,0.2916666666666667,0.8461538461538461
machine-translation,8,"where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .",system description,RNN ENCODER-DECODER,0,52,22,12,0,system description : RNN ENCODER-DECODER,0.15709969788519637,0.3055555555555556,0.9230769230769231
machine-translation,8,It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .,system description,RNN ENCODER-DECODER,0,53,23,13,0,system description : RNN ENCODER-DECODER,0.16012084592145015,0.3194444444444444,1.0
machine-translation,8,LEARNING TO ALIGN AND TRANSLATE,system description,LEARNING TO ALIGN AND TRANSLATE,0,54,24,1,0,system description : LEARNING TO ALIGN AND TRANSLATE,0.16314199395770393,0.3333333333333333,0.3333333333333333
machine-translation,8,"In this section , we propose a novel architecture for neural machine translation .",system description,LEARNING TO ALIGN AND TRANSLATE,0,55,25,2,0,system description : LEARNING TO ALIGN AND TRANSLATE,0.1661631419939577,0.3472222222222222,0.6666666666666666
machine-translation,8,The new architecture consists of a bidirectional RNN as an encoder ( Sec. 3.2 ) and a decoder that emulates searching through a source sentence during decoding a translation ( Sec. 3.1 ) .,system description,LEARNING TO ALIGN AND TRANSLATE,0,56,26,3,0,system description : LEARNING TO ALIGN AND TRANSLATE,0.1691842900302115,0.3611111111111111,1.0
machine-translation,8,DECODER : GENERAL DESCRIPTION,system description,DECODER: GENERAL DESCRIPTION,0,57,27,1,0,system description : DECODER: GENERAL DESCRIPTION,0.17220543806646527,0.375,0.03333333333333333
machine-translation,8,1 x 2 x 3 x T :,system description,DECODER: GENERAL DESCRIPTION,0,58,28,2,0,system description : DECODER: GENERAL DESCRIPTION,0.17522658610271905,0.3888888888888889,0.06666666666666667
machine-translation,8,"The graphical illustration of the proposed model trying to generate the t-th target wordy t given a source sentence ( x 1 , x 2 , . . . , x T ) .",system description,DECODER: GENERAL DESCRIPTION,0,59,29,3,0,system description : DECODER: GENERAL DESCRIPTION,0.1782477341389728,0.4027777777777778,0.1
machine-translation,8,"In a new model architecture , we define each conditional probability in Eq .",system description,DECODER: GENERAL DESCRIPTION,0,60,30,4,0,system description : DECODER: GENERAL DESCRIPTION,0.18126888217522658,0.4166666666666667,0.13333333333333333
machine-translation,8,2 ) as :,system description,DECODER: GENERAL DESCRIPTION,0,61,31,5,0,system description : DECODER: GENERAL DESCRIPTION,0.18429003021148035,0.4305555555555556,0.16666666666666666
machine-translation,8,"where s i is an RNN hidden state for time i , computed by",system description,DECODER: GENERAL DESCRIPTION,0,62,32,6,0,system description : DECODER: GENERAL DESCRIPTION,0.18731117824773413,0.4444444444444444,0.2
machine-translation,8,It should be noted that unlike the existing encoder - decoder approach ( see Eq.,system description,DECODER: GENERAL DESCRIPTION,0,63,33,7,0,system description : DECODER: GENERAL DESCRIPTION,0.1903323262839879,0.4583333333333333,0.23333333333333334
machine-translation,8,"2 ) ) , here the probability is conditioned on a distinct context vector c i for each target wordy i .",system description,DECODER: GENERAL DESCRIPTION,0,64,34,8,0,system description : DECODER: GENERAL DESCRIPTION,0.1933534743202417,0.4722222222222222,0.26666666666666666
machine-translation,8,"The context vector c i depends on a sequence of annotations ( h 1 , , h Tx ) to which an encoder maps the input sentence .",system description,DECODER: GENERAL DESCRIPTION,0,65,35,9,0,system description : DECODER: GENERAL DESCRIPTION,0.19637462235649547,0.4861111111111111,0.3
machine-translation,8,Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i - th word of the input sequence .,system description,DECODER: GENERAL DESCRIPTION,0,66,36,10,0,system description : DECODER: GENERAL DESCRIPTION,0.19939577039274925,0.5,0.3333333333333333
machine-translation,8,We explain in detail how the annotations are computed in the next section .,system description,DECODER: GENERAL DESCRIPTION,0,67,37,11,0,system description : DECODER: GENERAL DESCRIPTION,0.20241691842900303,0.5138888888888888,0.36666666666666664
machine-translation,8,"The context vector c i is , then , computed as a weighted sum of these annotations hi :",system description,DECODER: GENERAL DESCRIPTION,0,68,38,12,0,system description : DECODER: GENERAL DESCRIPTION,0.2054380664652568,0.5277777777777778,0.4
machine-translation,8,The weight ? ij of each annotation h j is computed by,system description,DECODER: GENERAL DESCRIPTION,0,69,39,13,0,system description : DECODER: GENERAL DESCRIPTION,0.2084592145015106,0.5416666666666666,0.43333333333333335
machine-translation,8,The weight ? ij of each annotation h j is computed by,system description,DECODER: GENERAL DESCRIPTION,0,70,40,14,0,system description : DECODER: GENERAL DESCRIPTION,0.21148036253776434,0.5555555555555556,0.4666666666666667
machine-translation,8,"where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .",system description,DECODER: GENERAL DESCRIPTION,0,71,41,15,0,system description : DECODER: GENERAL DESCRIPTION,0.21450151057401812,0.5694444444444444,0.5
machine-translation,8,"The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .",system description,DECODER: GENERAL DESCRIPTION,0,72,42,16,0,system description : DECODER: GENERAL DESCRIPTION,0.2175226586102719,0.5833333333333334,0.5333333333333333
machine-translation,8,We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system .,system description,DECODER: GENERAL DESCRIPTION,0,73,43,17,0,system description : DECODER: GENERAL DESCRIPTION,0.22054380664652568,0.5972222222222222,0.5666666666666667
machine-translation,8,"Note that unlike in traditional machine translation , the alignment is not considered to be a latent variable .",system description,DECODER: GENERAL DESCRIPTION,0,74,44,18,0,system description : DECODER: GENERAL DESCRIPTION,0.22356495468277945,0.6111111111111112,0.6
machine-translation,8,"Instead , the alignment model directly computes a soft alignment , which allows the gradient of the cost function to be backpropagated through .",system description,DECODER: GENERAL DESCRIPTION,0,75,45,19,0,system description : DECODER: GENERAL DESCRIPTION,0.22658610271903323,0.625,0.6333333333333333
machine-translation,8,This gradient can be used to train the alignment model as well as the whole translation model jointly .,system description,DECODER: GENERAL DESCRIPTION,0,76,46,20,0,system description : DECODER: GENERAL DESCRIPTION,0.229607250755287,0.6388888888888888,0.6666666666666666
machine-translation,8,"We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .",system description,DECODER: GENERAL DESCRIPTION,0,77,47,21,0,system description : DECODER: GENERAL DESCRIPTION,0.2326283987915408,0.6527777777777778,0.7
machine-translation,8,"Let ? ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .",system description,DECODER: GENERAL DESCRIPTION,0,78,48,22,0,system description : DECODER: GENERAL DESCRIPTION,0.23564954682779457,0.6666666666666666,0.7333333333333333
machine-translation,8,"Let ? ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .",system description,DECODER: GENERAL DESCRIPTION,0,79,49,23,0,system description : DECODER: GENERAL DESCRIPTION,0.23867069486404835,0.6805555555555556,0.7666666666666667
machine-translation,8,"Then , the i - th context vector c i is the expected annotation over all the annotations with probabilities ? ij .",system description,DECODER: GENERAL DESCRIPTION,0,80,50,24,0,system description : DECODER: GENERAL DESCRIPTION,0.24169184290030213,0.6944444444444444,0.8
machine-translation,8,"The probability ? ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .",system description,DECODER: GENERAL DESCRIPTION,0,81,51,25,0,system description : DECODER: GENERAL DESCRIPTION,0.24471299093655588,0.7083333333333334,0.8333333333333334
machine-translation,8,"The probability ? ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .",system description,DECODER: GENERAL DESCRIPTION,0,82,52,26,0,system description : DECODER: GENERAL DESCRIPTION,0.24773413897280966,0.7222222222222222,0.8666666666666667
machine-translation,8,"Intuitively , this implements a mechanism of attention in the decoder .",system description,DECODER: GENERAL DESCRIPTION,0,83,53,27,0,system description : DECODER: GENERAL DESCRIPTION,0.25075528700906347,0.7361111111111112,0.9
machine-translation,8,The decoder decides parts of the source sentence to pay attention to .,system description,DECODER: GENERAL DESCRIPTION,0,84,54,28,0,system description : DECODER: GENERAL DESCRIPTION,0.2537764350453172,0.75,0.9333333333333333
machine-translation,8,"By letting the decoder have an attention mechanism , we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector .",system description,DECODER: GENERAL DESCRIPTION,0,85,55,29,0,system description : DECODER: GENERAL DESCRIPTION,0.256797583081571,0.7638888888888888,0.9666666666666667
machine-translation,8,"With this new approach the information can be spread throughout the sequence of annotations , which can be selectively retrieved by the decoder accordingly .",system description,DECODER: GENERAL DESCRIPTION,0,86,56,30,0,system description : DECODER: GENERAL DESCRIPTION,0.2598187311178248,0.7777777777777778,1.0
machine-translation,8,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,87,57,1,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.2628398791540785,0.7916666666666666,0.14285714285714285
machine-translation,8,"The usual RNN , described in Eq. ( 1 ) , reads an input sequence x in order starting from the first symbol x 1 to the last one x Tx .",system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,88,58,2,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.26586102719033233,0.8055555555555556,0.2857142857142857
machine-translation,8,"However , in the proposed scheme , we would like the annotation of each word to summarize not only the preceding words , but also the following words .",system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,89,59,3,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.2688821752265861,0.8194444444444444,0.42857142857142855
machine-translation,8,"Hence , we propose to use a bidirectional RNN ( BiRNN , , which has been successfully used recently in speech recognition ( see , e.g. , .",system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,90,60,4,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.2719033232628399,0.8333333333333334,0.5714285714285714
machine-translation,8,BiRNN consists of forward and backward RNN 's .,system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,91,61,5,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.27492447129909364,0.8472222222222222,0.7142857142857143
machine-translation,8,The forward RNN ? ? f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (,system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,92,62,6,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.27794561933534745,0.8611111111111112,0.8571428571428571
machine-translation,8,The forward RNN ? ? f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (,system description,ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0,93,63,7,0,system description : ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,0.2809667673716012,0.875,1.0
machine-translation,8,The backward RNN,system description,The backward RNN,0,94,64,1,0,system description : The backward RNN,0.283987915407855,0.8888888888888888,0.1111111111111111
machine-translation,8,"? f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (",system description,The backward RNN,0,95,65,2,0,system description : The backward RNN,0.28700906344410876,0.9027777777777778,0.2222222222222222
machine-translation,8,"? f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (",system description,The backward RNN,0,96,66,3,0,system description : The backward RNN,0.29003021148036257,0.9166666666666666,0.3333333333333333
machine-translation,8,We obtain an annotation for each word x j by concatenating the forward hidden state ? ? h j and the backward one,system description,The backward RNN,0,97,67,4,0,system description : The backward RNN,0.2930513595166163,0.9305555555555556,0.4444444444444444
machine-translation,8,We obtain an annotation for each word x j by concatenating the forward hidden state ? ? h j and the backward one,system description,The backward RNN,0,98,68,5,0,system description : The backward RNN,0.29607250755287007,0.9444444444444444,0.5555555555555556
machine-translation,8,"In this way , the annotation h j contains the summaries of both the preceding words and the following words .",system description,The backward RNN,0,99,69,6,0,system description : The backward RNN,0.2990936555891239,0.9583333333333334,0.6666666666666666
machine-translation,8,"Due to the tendency of RNNs to better represent recent inputs , the annotation h j will be focused on the words around x j .",system description,The backward RNN,0,100,70,7,0,system description : The backward RNN,0.3021148036253776,0.9722222222222222,0.7777777777777778
machine-translation,8,This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) .,system description,The backward RNN,0,101,71,8,0,system description : The backward RNN,0.30513595166163143,0.9861111111111112,0.8888888888888888
machine-translation,8,See for the graphical illustration of the proposed model .,system description,The backward RNN,0,102,72,9,0,system description : The backward RNN,0.3081570996978852,1.0,1.0
machine-translation,8,EXPERIMENT SETTINGS,experiment,EXPERIMENT SETTINGS,0,103,1,1,0,experiment : EXPERIMENT SETTINGS,0.311178247734139,0.14285714285714285,0.14285714285714285
machine-translation,8,We evaluate the proposed approach on the task of English - to - French translation .,experiment,EXPERIMENT SETTINGS,0,104,2,2,0,experiment : EXPERIMENT SETTINGS,0.31419939577039274,0.2857142857142857,0.2857142857142857
machine-translation,8,"We use the bilingual , parallel corpora provided by ACL WMT ' 14 .",experiment,EXPERIMENT SETTINGS,0,105,3,3,0,experiment : EXPERIMENT SETTINGS,0.31722054380664655,0.42857142857142855,0.42857142857142855
machine-translation,8,3,experiment,EXPERIMENT SETTINGS,0,106,4,4,0,experiment : EXPERIMENT SETTINGS,0.3202416918429003,0.5714285714285714,0.5714285714285714
machine-translation,8,"As a comparison , we also report the performance of an RNN Encoder - Decoder which was proposed recently by .",experiment,EXPERIMENT SETTINGS,0,107,5,5,0,experiment : EXPERIMENT SETTINGS,0.32326283987915405,0.7142857142857143,0.7142857142857143
machine-translation,8,We use the same training procedures and the same dataset for both models .,experiment,EXPERIMENT SETTINGS,0,108,6,6,0,experiment : EXPERIMENT SETTINGS,0.32628398791540786,0.8571428571428571,0.8571428571428571
machine-translation,8,4,experiment,EXPERIMENT SETTINGS,0,109,7,7,0,experiment : EXPERIMENT SETTINGS,0.3293051359516616,1.0,1.0
machine-translation,8,DATASET,dataset,DATASET,0,110,1,1,0,dataset : DATASET,0.3323262839879154,0.14285714285714285,0.14285714285714285
machine-translation,8,"WMT ' 14 contains the following English - French parallel corpora : Europarl ( 61 M words ) , news commentary ( 5.5 M ) , UN ( 421M ) and two crawled corpora of 90 M and 272.5 M words respectively , totaling 850M words .",dataset,DATASET,0,111,2,2,0,dataset : DATASET,0.33534743202416917,0.2857142857142857,0.2857142857142857
machine-translation,8,"Following the procedure described in , we reduce the size of the combined corpus to have 348M words using the data selection method by .",dataset,DATASET,0,112,3,3,0,dataset : DATASET,0.338368580060423,0.42857142857142855,0.42857142857142855
machine-translation,8,"We do not use any monolingual data other than the mentioned parallel corpora , although it maybe possible to use a much larger monolingual corpus to pretrain an encoder .",dataset,DATASET,0,113,4,4,0,dataset : DATASET,0.3413897280966767,0.5714285714285714,0.5714285714285714
machine-translation,8,"We concatenate news - test - After a usual tokenization 6 , we use a shortlist of 30,000 most frequent words in each language to train our models .",dataset,DATASET,0,114,5,5,0,dataset : DATASET,0.34441087613293053,0.7142857142857143,0.7142857142857143
machine-translation,8,Any word not included in the shortlist is mapped to a special token ( [ UNK ] ) .,dataset,DATASET,0,115,6,6,0,dataset : DATASET,0.3474320241691843,0.8571428571428571,0.8571428571428571
machine-translation,8,"We do not apply any other special preprocessing , such as lowercasing or stemming , to the data .",dataset,DATASET,0,116,7,7,0,dataset : DATASET,0.3504531722054381,1.0,1.0
machine-translation,8,MODELS,model,MODELS,0,117,1,1,0,model : MODELS,0.35347432024169184,0.07142857142857142,0.07142857142857142
machine-translation,8,We train two types of models .,model,MODELS,1,118,2,2,0,model : MODELS,0.3564954682779456,0.14285714285714285,0.14285714285714285
machine-translation,8,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",model,MODELS,1,119,3,3,0,model : MODELS,0.3595166163141994,0.21428571428571427,0.21428571428571427
machine-translation,8,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .",model,MODELS,1,120,4,4,0,model : MODELS,0.36253776435045315,0.2857142857142857,0.2857142857142857
machine-translation,8,The encoder and decoder of the RNNencdec have 1000 hidden units each .,model,MODELS,1,121,5,5,0,model : MODELS,0.36555891238670696,0.35714285714285715,0.35714285714285715
machine-translation,8,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,model,MODELS,1,122,6,6,0,model : MODELS,0.3685800604229607,0.42857142857142855,0.42857142857142855
machine-translation,8,It s decoder has 1000 hidden units .,model,MODELS,1,123,7,7,0,model : MODELS,0.3716012084592145,0.5,0.5
machine-translation,8,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .",model,MODELS,1,124,8,8,0,model : MODELS,0.37462235649546827,0.5714285714285714,0.5714285714285714
machine-translation,8,We use a minibatch stochastic gradient descent ( SGD ) algorithm together with Adadelta to train each model .,model,MODELS,0,125,9,9,0,model : MODELS,0.3776435045317221,0.6428571428571429,0.6428571428571429
machine-translation,8,Each SGD update direction is computed using a minibatch of 80 sentences .,model,MODELS,0,126,10,10,0,model : MODELS,0.3806646525679758,0.7142857142857143,0.7142857142857143
machine-translation,8,We trained each model for approximately 5 days .,model,MODELS,0,127,11,11,0,model : MODELS,0.38368580060422963,0.7857142857142857,0.7857142857142857
machine-translation,8,"Once a model is trained , we use a beam search to find a translation that approximately maximizes the conditional probability ( see , e.g. , .",model,MODELS,0,128,12,12,0,model : MODELS,0.3867069486404834,0.8571428571428571,0.8571428571428571
machine-translation,8,used this approach to generate translations from their neural machine translation model .,model,MODELS,0,129,13,13,0,model : MODELS,0.38972809667673713,0.9285714285714286,0.9285714285714286
machine-translation,8,"For more details on the architectures of the models and training procedure used in the experiments , see Appendices A and B.",model,MODELS,0,130,14,14,0,model : MODELS,0.39274924471299094,1.0,1.0
machine-translation,8,RESULTS,result,RESULTS,0,131,1,1,0,result : RESULTS,0.3957703927492447,0.06666666666666667,1.0
machine-translation,8,QUANTITATIVE RESULTS,result,QUANTITATIVE RESULTS,0,132,2,1,0,result : QUANTITATIVE RESULTS,0.3987915407854985,0.13333333333333333,0.07142857142857142
machine-translation,8,In : Four sample alignments found by RNNsearch - 50 .,result,QUANTITATIVE RESULTS,0,133,3,2,0,result : QUANTITATIVE RESULTS,0.40181268882175225,0.2,0.14285714285714285
machine-translation,8,"The x - axis and y-axis of each plot correspond to the words in the source sentence ( English ) and the generated translation ( French ) , respectively .",result,QUANTITATIVE RESULTS,0,134,4,3,0,result : QUANTITATIVE RESULTS,0.40483383685800606,0.26666666666666666,0.21428571428571427
machine-translation,8,"Each pixel shows the weight ? ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .",result,QUANTITATIVE RESULTS,0,135,5,4,0,result : QUANTITATIVE RESULTS,0.4078549848942598,0.3333333333333333,0.2857142857142857
machine-translation,8,"Each pixel shows the weight ? ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .",result,QUANTITATIVE RESULTS,0,136,6,5,0,result : QUANTITATIVE RESULTS,0.4108761329305136,0.4,0.35714285714285715
machine-translation,8,a ) an arbitrary sentence .,result,QUANTITATIVE RESULTS,0,137,7,6,0,result : QUANTITATIVE RESULTS,0.41389728096676737,0.4666666666666667,0.42857142857142855
machine-translation,8,b - d ) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set .,result,QUANTITATIVE RESULTS,0,138,8,7,0,result : QUANTITATIVE RESULTS,0.4169184290030212,0.5333333333333333,0.5
machine-translation,8,One of the motivations behind the proposed approach was the use of a fixed - length context vector in the basic encoder - decoder approach .,result,QUANTITATIVE RESULTS,0,139,9,8,0,result : QUANTITATIVE RESULTS,0.4199395770392749,0.6,0.5714285714285714
machine-translation,8,We conjectured that this limitation may make the basic encoder - decoder approach to underperform with long sentences .,result,QUANTITATIVE RESULTS,0,140,10,9,0,result : QUANTITATIVE RESULTS,0.4229607250755287,0.6666666666666666,0.6428571428571429
machine-translation,8,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .",result,QUANTITATIVE RESULTS,1,141,11,10,0,result : QUANTITATIVE RESULTS,0.4259818731117825,0.7333333333333333,0.7142857142857143
machine-translation,8,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .",result,QUANTITATIVE RESULTS,1,142,12,11,0,result : QUANTITATIVE RESULTS,0.42900302114803623,0.8,0.7857142857142857
machine-translation,8,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .",result,QUANTITATIVE RESULTS,1,143,13,12,0,result : QUANTITATIVE RESULTS,0.43202416918429004,0.8666666666666667,0.8571428571428571
machine-translation,8,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,result,QUANTITATIVE RESULTS,1,144,14,13,0,result : QUANTITATIVE RESULTS,0.4350453172205438,0.9333333333333333,0.9285714285714286
machine-translation,8,tokens when only the sentences having no unknown words were evaluated ( last column ) .,result,QUANTITATIVE RESULTS,0,145,15,14,0,result : QUANTITATIVE RESULTS,0.4380664652567976,1.0,1.0
machine-translation,8,QUALITATIVE ANALYSIS,analysis,QUALITATIVE ANALYSIS,0,146,1,1,0,analysis : QUALITATIVE ANALYSIS,0.44108761329305135,0.03125,1.0
machine-translation,8,ALIGNMENT,analysis,ALIGNMENT,0,147,2,1,0,analysis : ALIGNMENT,0.44410876132930516,0.0625,0.08333333333333333
machine-translation,8,The proposed approach provides an intuitive way to inspect the ( soft - ) alignment between the words in a generated translation and those in a source sentence .,analysis,ALIGNMENT,0,148,3,2,0,analysis : ALIGNMENT,0.4471299093655589,0.09375,0.16666666666666666
machine-translation,8,"This is done by visualizing the annotation weights ? ij from Eq. , as in .",analysis,ALIGNMENT,0,149,4,3,0,analysis : ALIGNMENT,0.4501510574018127,0.125,0.25
machine-translation,8,"This is done by visualizing the annotation weights ? ij from Eq. , as in .",analysis,ALIGNMENT,0,150,5,4,0,analysis : ALIGNMENT,0.45317220543806647,0.15625,0.3333333333333333
machine-translation,8,Each row of a matrix in each plot indicates the weights associated with the annotations .,analysis,ALIGNMENT,0,151,6,5,0,analysis : ALIGNMENT,0.4561933534743202,0.1875,0.4166666666666667
machine-translation,8,From this we see which positions in the source sentence were considered more important when generating the target word .,analysis,ALIGNMENT,0,152,7,6,0,analysis : ALIGNMENT,0.459214501510574,0.21875,0.5
machine-translation,8,We can see from the alignments in that the alignment of words between English and French is largely monotonic .,analysis,ALIGNMENT,0,153,8,7,0,analysis : ALIGNMENT,0.4622356495468278,0.25,0.5833333333333334
machine-translation,8,We see strong weights along the diagonal of each matrix .,analysis,ALIGNMENT,0,154,9,8,0,analysis : ALIGNMENT,0.4652567975830816,0.28125,0.6666666666666666
machine-translation,8,"However , we also observe a number of non-trivial , non-monotonic alignments .",analysis,ALIGNMENT,0,155,10,9,0,analysis : ALIGNMENT,0.46827794561933533,0.3125,0.75
machine-translation,8,"Adjectives and nouns are typically ordered differently between French and English , and we see an example in The strength of the soft - alignment , opposed to a hard - alignment , is evident , for instance , from ].",analysis,ALIGNMENT,0,156,11,10,0,analysis : ALIGNMENT,0.47129909365558914,0.34375,0.8333333333333334
machine-translation,8,We observe similar behaviors in all the presented cases in .,analysis,ALIGNMENT,0,157,12,11,0,analysis : ALIGNMENT,0.4743202416918429,0.375,0.9166666666666666
machine-translation,8,"An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths , without requiring a counter - intuitive way of mapping some words to or from nowhere ( [ NULL ] ) ( see , e.g. , Chapters 4 and 5 of .",analysis,ALIGNMENT,0,158,13,12,0,analysis : ALIGNMENT,0.4773413897280967,0.40625,1.0
machine-translation,8,LONG SENTENCES,analysis,LONG SENTENCES,0,159,14,1,0,analysis : LONG SENTENCES,0.48036253776435045,0.4375,0.1111111111111111
machine-translation,8,As clearly visible from the proposed model ( RNNsearch ) is much better than the conventional model ( RNNencdec ) at translating long sentences .,analysis,LONG SENTENCES,0,160,15,2,0,analysis : LONG SENTENCES,0.48338368580060426,0.46875,0.2222222222222222
machine-translation,8,"This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .",analysis,LONG SENTENCES,0,161,16,3,0,analysis : LONG SENTENCES,0.486404833836858,0.5,0.3333333333333333
machine-translation,8,"As an example , consider this source sentence from the test set :",analysis,LONG SENTENCES,0,162,17,4,0,analysis : LONG SENTENCES,0.48942598187311176,0.53125,0.4444444444444444
machine-translation,8,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",analysis,LONG SENTENCES,0,163,18,5,0,analysis : LONG SENTENCES,0.49244712990936557,0.5625,0.5555555555555556
machine-translation,8,The RNNencdec - 50 translated this sentence into :,analysis,LONG SENTENCES,0,164,19,6,0,analysis : LONG SENTENCES,0.4954682779456193,0.59375,0.6666666666666666
machine-translation,8,Un privilge d'admission est le droit d'un mdecin de reconnatre un patient l'hpital ou un centre mdical d'un diagnostic ou de prendre un diagnostic en fonction de sontat de sant .,analysis,LONG SENTENCES,0,165,20,7,0,analysis : LONG SENTENCES,0.4984894259818731,0.625,0.7777777777777778
machine-translation,8,"On the other hand , the RNNsearch - 50 generated the following correct translation , preserving the whole meaning of the input sentence without omitting any details :",analysis,LONG SENTENCES,0,166,21,8,0,analysis : LONG SENTENCES,0.5015105740181269,0.65625,0.8888888888888888
machine-translation,8,"Un privilge d'admission est le droit d'un mdecin d'admettre un patient un hpital ou un centre mdical pour effectuer un diagnostic ou une procdure , selon son statut de travailleur des soins de sant l'hpital .",analysis,LONG SENTENCES,0,167,22,9,0,analysis : LONG SENTENCES,0.5045317220543807,0.6875,1.0
machine-translation,8,Let us consider another sentence from the test set :,analysis,Let us consider another sentence from the test set:,0,168,23,1,0,analysis : Let us consider another sentence from the test set:,0.5075528700906344,0.71875,0.5
machine-translation,8,"This kind of experience is part of Disney 's efforts to "" extend the lifetime of its series and build new relationships with audiences via digital platforms thatare becoming evermore important , "" he added .",analysis,Let us consider another sentence from the test set:,0,169,24,2,0,analysis : Let us consider another sentence from the test set:,0.5105740181268882,0.75,1.0
machine-translation,8,The translation by the RNNencdec - 50 is,analysis,The translation by the RNNencdec-50 is,0,170,25,1,0,analysis : The translation by the RNNencdec-50 is,0.513595166163142,0.78125,0.125
machine-translation,8,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",analysis,The translation by the RNNencdec-50 is,0,171,26,2,0,analysis : The translation by the RNNencdec-50 is,0.5166163141993958,0.8125,0.25
machine-translation,8,"As with the previous example , the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words ( see the underlined phrase ) .",analysis,The translation by the RNNencdec-50 is,0,172,27,3,0,analysis : The translation by the RNNencdec-50 is,0.5196374622356495,0.84375,0.375
machine-translation,8,"After that point , the quality of the translation deteriorates , with basic mistakes such as the lack of a closing quotation mark .",analysis,The translation by the RNNencdec-50 is,0,173,28,4,0,analysis : The translation by the RNNencdec-50 is,0.5226586102719033,0.875,0.5
machine-translation,8,"Again , the RNNsearch - 50 was able to translate this long sentence correctly :",analysis,The translation by the RNNencdec-50 is,0,174,29,5,0,analysis : The translation by the RNNencdec-50 is,0.525679758308157,0.90625,0.625
machine-translation,8,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",analysis,The translation by the RNNencdec-50 is,0,175,30,6,0,analysis : The translation by the RNNencdec-50 is,0.5287009063444109,0.9375,0.75
machine-translation,8,"In conjunction with the quantitative results presented already , these qualitative observations confirm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model .",analysis,The translation by the RNNencdec-50 is,0,176,31,7,0,analysis : The translation by the RNNencdec-50 is,0.5317220543806647,0.96875,0.875
machine-translation,8,"In Appendix C , we provide a few more sample translations of long source sentences generated by the RNNencdec - 50 , RNNsearch - 50 and Google Translate along with the reference translations .",analysis,The translation by the RNNencdec-50 is,0,177,32,8,0,analysis : The translation by the RNNencdec-50 is,0.5347432024169184,1.0,1.0
machine-translation,8,RELATED WORK,related work,related work,0,178,1,1,0,related work : related work,0.5377643504531722,1.0,1.0
machine-translation,8,LEARNING TO ALIGN,system description,LEARNING TO ALIGN,0,179,1,1,0,system description : LEARNING TO ALIGN,0.540785498489426,0.05263157894736842,0.1
machine-translation,8,similar approach of aligning an output symbol with an input symbol was proposed recently by in the context of handwriting synthesis .,system description,LEARNING TO ALIGN,0,180,2,2,0,system description : LEARNING TO ALIGN,0.5438066465256798,0.10526315789473684,0.2
machine-translation,8,Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters .,system description,LEARNING TO ALIGN,0,181,3,3,0,system description : LEARNING TO ALIGN,0.5468277945619335,0.15789473684210525,0.3
machine-translation,8,"In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .",system description,LEARNING TO ALIGN,0,182,4,4,0,system description : LEARNING TO ALIGN,0.5498489425981873,0.21052631578947367,0.4
machine-translation,8,"More specifically , his alignment was restricted to predict the location such that the location increases monotonically .",system description,LEARNING TO ALIGN,0,183,5,5,0,system description : LEARNING TO ALIGN,0.552870090634441,0.2631578947368421,0.5
machine-translation,8,"The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .",system description,LEARNING TO ALIGN,0,184,6,6,0,system description : LEARNING TO ALIGN,0.5558912386706949,0.3157894736842105,0.6
machine-translation,8,"In the context of machine translation , this is a severe limitation , as ( long - distance ) reordering is often needed to generate a grammatically correct translation ( for instance , English - to - German ) .",system description,LEARNING TO ALIGN,0,185,7,7,0,system description : LEARNING TO ALIGN,0.5589123867069486,0.3684210526315789,0.7
machine-translation,8,"Our approach , on the other hand , requires computing the annotation weight of every word in the source sentence for each word in the translation .",system description,LEARNING TO ALIGN,0,186,8,8,0,system description : LEARNING TO ALIGN,0.5619335347432024,0.42105263157894735,0.8
machine-translation,8,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .,system description,LEARNING TO ALIGN,0,187,9,9,0,system description : LEARNING TO ALIGN,0.5649546827794562,0.47368421052631576,0.9
machine-translation,8,"However , this may limit the applicability of the proposed scheme to other tasks .",system description,LEARNING TO ALIGN,0,188,10,10,0,system description : LEARNING TO ALIGN,0.56797583081571,0.5263157894736842,1.0
machine-translation,8,NEURAL NETWORKS FOR MACHINE TRANSLATION,system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,189,11,1,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5709969788519638,0.5789473684210527,0.1111111111111111
machine-translation,8,"Since introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words , neural networks have widely been used in machine translation .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,190,12,2,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5740181268882175,0.631578947368421,0.2222222222222222
machine-translation,8,"However , the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,191,13,3,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5770392749244713,0.6842105263157895,0.3333333333333333
machine-translation,8,"For instance , proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase - based statistical machine translation system .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,192,14,4,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5800604229607251,0.7368421052631579,0.4444444444444444
machine-translation,8,"More recently , and reported the successful use of the neural networks as a sub-component of the existing translation system .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,193,15,5,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5830815709969789,0.7894736842105263,0.5555555555555556
machine-translation,8,"Traditionally , a neural network trained as a target - side language model has been used to rescore or rerank a list of candidate translations ( see , e.g. , .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,194,16,6,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5861027190332326,0.8421052631578947,0.6666666666666666
machine-translation,8,"Although the above approaches were shown to improve the translation performance over the stateof - the - art machine translation systems , we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,195,17,7,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5891238670694864,0.8947368421052632,0.7777777777777778
machine-translation,8,The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works .,system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,196,18,8,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.5921450151057401,0.9473684210526315,0.8888888888888888
machine-translation,8,"Rather than using a neural network as apart of the existing system , our model works on its own and generates a translation from a source sentence directly .",system description,NEURAL NETWORKS FOR MACHINE TRANSLATION,0,197,19,9,0,system description : NEURAL NETWORKS FOR MACHINE TRANSLATION,0.595166163141994,1.0,1.0
machine-translation,8,CONCLUSION,conclusion,CONCLUSION,0,198,1,1,0,conclusion : CONCLUSION,0.5981873111782477,0.0625,0.0625
machine-translation,8,"The conventional approach to neural machine translation , called an encoder - decoder approach , encodes a whole input sentence into a fixed - length vector from which a translation will be decoded .",conclusion,CONCLUSION,0,199,2,2,0,conclusion : CONCLUSION,0.6012084592145015,0.125,0.125
machine-translation,8,"We conjectured that the use of a fixed - length context vector is problematic for translating long sentences , based on a recent empirical study reported by and .",conclusion,CONCLUSION,0,200,3,3,0,conclusion : CONCLUSION,0.6042296072507553,0.1875,0.1875
machine-translation,8,"In this paper , we proposed a novel architecture that addresses this issue .",conclusion,CONCLUSION,0,201,4,4,0,conclusion : CONCLUSION,0.6072507552870091,0.25,0.25
machine-translation,8,"We extended the basic encoder - decoder by letting a model ( soft - ) search for a set of input words , or their annotations computed by an encoder , when generating each target word .",conclusion,CONCLUSION,0,202,5,5,0,conclusion : CONCLUSION,0.6102719033232629,0.3125,0.3125
machine-translation,8,"This frees the model from having to encode a whole source sentence into a fixed - length vector , and also lets the model focus only on information relevant to the generation of the next target word .",conclusion,CONCLUSION,0,203,6,6,0,conclusion : CONCLUSION,0.6132930513595166,0.375,0.375
machine-translation,8,This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences .,conclusion,CONCLUSION,0,204,7,7,0,conclusion : CONCLUSION,0.6163141993957704,0.4375,0.4375
machine-translation,8,"Unlike with the traditional machine translation systems , all of the pieces of the translation system , including the alignment mechanism , are jointly trained towards a better log-probability of producing correct translations .",conclusion,CONCLUSION,0,205,8,8,0,conclusion : CONCLUSION,0.6193353474320241,0.5,0.5
machine-translation,8,"We tested the proposed model , called RNNsearch , on the task of English - to - French translation .",conclusion,CONCLUSION,0,206,9,9,0,conclusion : CONCLUSION,0.622356495468278,0.5625,0.5625
machine-translation,8,"The experiment revealed that the proposed RNNsearch outperforms the conventional encoder - decoder model ( RNNencdec ) significantly , regardless of the sentence length and that it is much more robust to the length of a source sentence .",conclusion,CONCLUSION,0,207,10,10,0,conclusion : CONCLUSION,0.6253776435045317,0.625,0.625
machine-translation,8,"From the qualitative analysis where we investigated the ( soft - ) alignment generated by the RNNsearch , we were able to conclude that the model can correctly align each target word with the relevant words , or their annotations , in the source sentence as it generated a correct translation .",conclusion,CONCLUSION,0,208,11,11,0,conclusion : CONCLUSION,0.6283987915407855,0.6875,0.6875
machine-translation,8,"Perhaps more importantly , the proposed approach achieved a translation performance comparable to the existing phrase - based statistical machine translation .",conclusion,CONCLUSION,0,209,12,12,0,conclusion : CONCLUSION,0.6314199395770392,0.75,0.75
machine-translation,8,"It is a striking result , considering that the proposed architecture , or the whole family of neural machine translation , has only been proposed as recently as this year .",conclusion,CONCLUSION,0,210,13,13,0,conclusion : CONCLUSION,0.6344410876132931,0.8125,0.8125
machine-translation,8,We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general .,conclusion,CONCLUSION,0,211,14,14,0,conclusion : CONCLUSION,0.6374622356495468,0.875,0.875
machine-translation,8,"One of challenges left for the future is to better handle unknown , or rare words .",conclusion,CONCLUSION,0,212,15,15,0,conclusion : CONCLUSION,0.6404833836858006,0.9375,0.9375
machine-translation,8,This will be required for the model to be more widely used and to match the performance of current state - of - the - art machine translation systems in all contexts .,conclusion,CONCLUSION,0,213,16,16,0,conclusion : CONCLUSION,0.6435045317220544,1.0,1.0
machine-translation,8,MODEL ARCHITECTURE,model,MODEL ARCHITECTURE,0,214,1,1,0,model : MODEL ARCHITECTURE,0.6465256797583081,0.00847457627118644,0.25
machine-translation,8,ARCHITECTURAL CHOICES,model,MODEL ARCHITECTURE,0,215,2,2,0,model : MODEL ARCHITECTURE,0.649546827794562,0.01694915254237288,0.5
machine-translation,8,"The proposed scheme in Section 3 is a general framework where one can freely define , for instance , the activation functions f of recurrent neural networks ( RNN ) and the alignment model a .",model,MODEL ARCHITECTURE,0,216,3,3,0,model : MODEL ARCHITECTURE,0.6525679758308157,0.025423728813559324,0.75
machine-translation,8,"Here , we describe the choices we made for the experiments in this paper .",model,MODEL ARCHITECTURE,0,217,4,4,0,model : MODEL ARCHITECTURE,0.6555891238670695,0.03389830508474576,1.0
machine-translation,8,RECURRENT NEURAL NETWORK,model,RECURRENT NEURAL NETWORK,0,218,5,1,0,model : RECURRENT NEURAL NETWORK,0.6586102719033232,0.0423728813559322,0.047619047619047616
machine-translation,8,"For the activation function f of an RNN , we use the gated hidden unit recently proposed by .",model,RECURRENT NEURAL NETWORK,0,219,6,2,0,model : RECURRENT NEURAL NETWORK,0.6616314199395771,0.05084745762711865,0.09523809523809523
machine-translation,8,The gated hidden unit is an alternative to the conventional simple units such as an element - wise tanh .,model,RECURRENT NEURAL NETWORK,0,220,7,3,0,model : RECURRENT NEURAL NETWORK,0.6646525679758308,0.059322033898305086,0.14285714285714285
machine-translation,8,"This gated unit is similar to along short - term memory ( LSTM ) unit proposed earlier by , sharing with it the ability to better model and learn long - term dependencies .",model,RECURRENT NEURAL NETWORK,0,221,8,4,0,model : RECURRENT NEURAL NETWORK,0.6676737160120846,0.06779661016949153,0.19047619047619047
machine-translation,8,This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1 .,model,RECURRENT NEURAL NETWORK,0,222,9,5,0,model : RECURRENT NEURAL NETWORK,0.6706948640483383,0.07627118644067797,0.23809523809523808
machine-translation,8,These paths allow gradients to flow backward easily without suffering too much from the vanishing effect .,model,RECURRENT NEURAL NETWORK,0,223,10,6,0,model : RECURRENT NEURAL NETWORK,0.6737160120845922,0.0847457627118644,0.2857142857142857
machine-translation,8,"It is therefore possible to use LSTM units instead of the gated hidden unit described here , as was done in a similar context by .",model,RECURRENT NEURAL NETWORK,0,224,11,7,0,model : RECURRENT NEURAL NETWORK,0.676737160120846,0.09322033898305085,0.3333333333333333
machine-translation,8,The new state s i of the RNN employing n gated hidden units 8 is computed by,model,RECURRENT NEURAL NETWORK,0,225,12,8,0,model : RECURRENT NEURAL NETWORK,0.6797583081570997,0.1016949152542373,0.38095238095238093
machine-translation,8,"where is an element - wise multiplication , and z i is the output of the update gates ( see below ) .",model,RECURRENT NEURAL NETWORK,0,226,13,9,0,model : RECURRENT NEURAL NETWORK,0.6827794561933535,0.11016949152542373,0.42857142857142855
machine-translation,8,The proposed updated states i is computed b ? where e ( y,model,RECURRENT NEURAL NETWORK,0,227,14,10,0,model : RECURRENT NEURAL NETWORK,0.6858006042296072,0.11864406779661017,0.47619047619047616
machine-translation,8,The proposed updated states i is computed b ? where e ( y,model,RECURRENT NEURAL NETWORK,0,228,15,11,0,model : RECURRENT NEURAL NETWORK,0.6888217522658611,0.1271186440677966,0.5238095238095238
machine-translation,8,"i?1 ) ? R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .",model,RECURRENT NEURAL NETWORK,0,229,16,12,0,model : RECURRENT NEURAL NETWORK,0.6918429003021148,0.13559322033898305,0.5714285714285714
machine-translation,8,"i?1 ) ? R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .",model,RECURRENT NEURAL NETWORK,0,230,17,13,0,model : RECURRENT NEURAL NETWORK,0.6948640483383686,0.1440677966101695,0.6190476190476191
machine-translation,8,"When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ? R mK .",model,RECURRENT NEURAL NETWORK,0,231,18,14,0,model : RECURRENT NEURAL NETWORK,0.6978851963746223,0.15254237288135594,0.6666666666666666
machine-translation,8,"When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ? R mK .",model,RECURRENT NEURAL NETWORK,0,232,19,15,0,model : RECURRENT NEURAL NETWORK,0.7009063444108762,0.16101694915254236,0.7142857142857143
machine-translation,8,"Whenever possible , we omit bias terms to make the equations less cluttered .",model,RECURRENT NEURAL NETWORK,0,233,20,16,0,model : RECURRENT NEURAL NETWORK,0.7039274924471299,0.1694915254237288,0.7619047619047619
machine-translation,8,"The update gates z i allow each hidden unit to maintain its previous activation , and the reset gates r i control how much and what information from the previous state should be reset .",model,RECURRENT NEURAL NETWORK,0,234,21,17,0,model : RECURRENT NEURAL NETWORK,0.7069486404833837,0.17796610169491525,0.8095238095238095
machine-translation,8,We compute them by,model,RECURRENT NEURAL NETWORK,0,235,22,18,0,model : RECURRENT NEURAL NETWORK,0.7099697885196374,0.1864406779661017,0.8571428571428571
machine-translation,8,where ? ( ) is a logistic sigmoid function .,model,RECURRENT NEURAL NETWORK,0,236,23,19,0,model : RECURRENT NEURAL NETWORK,0.7129909365558912,0.19491525423728814,0.9047619047619048
machine-translation,8,"At each step of the decoder , we compute the output probability ( Eq. ( 4 ) ) as a multi -layered function .",model,RECURRENT NEURAL NETWORK,0,237,24,20,0,model : RECURRENT NEURAL NETWORK,0.716012084592145,0.2033898305084746,0.9523809523809523
machine-translation,8,We use a single hidden layer of maxout units and normalize the output probabilities ( one for each word ) with a softmax function ( see Eq. ) .,model,RECURRENT NEURAL NETWORK,0,238,25,21,0,model : RECURRENT NEURAL NETWORK,0.7190332326283988,0.211864406779661,1.0
machine-translation,8,ALIGNMENT MODEL,model,ALIGNMENT MODEL,0,239,26,1,0,model : ALIGNMENT MODEL,0.7220543806646526,0.22033898305084745,0.1111111111111111
machine-translation,8,The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .,model,ALIGNMENT MODEL,0,240,27,2,0,model : ALIGNMENT MODEL,0.7250755287009063,0.2288135593220339,0.2222222222222222
machine-translation,8,"In order to reduce computation , we use a singlelayer multilayer perceptron such that",model,ALIGNMENT MODEL,0,241,28,3,0,model : ALIGNMENT MODEL,0.7280966767371602,0.23728813559322035,0.3333333333333333
machine-translation,8,"where W a ? R nn , U a ? R n 2n and v a ? Rn are the weight matrices .",model,ALIGNMENT MODEL,0,242,29,4,0,model : ALIGNMENT MODEL,0.7311178247734139,0.2457627118644068,0.4444444444444444
machine-translation,8,"where W a ? R nn , U a ? R n 2n and v a ? Rn are the weight matrices .",model,ALIGNMENT MODEL,0,243,30,5,0,model : ALIGNMENT MODEL,0.7341389728096677,0.2542372881355932,0.5555555555555556
machine-translation,8,"where W a ? R nn , U a ? R n 2n and v a ? Rn are the weight matrices .",model,ALIGNMENT MODEL,0,244,31,6,0,model : ALIGNMENT MODEL,0.7371601208459214,0.2627118644067797,0.6666666666666666
machine-translation,8,"where W a ? R nn , U a ? R n 2n and v a ? Rn are the weight matrices .",model,ALIGNMENT MODEL,0,245,32,7,0,model : ALIGNMENT MODEL,0.7401812688821753,0.2711864406779661,0.7777777777777778
machine-translation,8,Since,model,ALIGNMENT MODEL,0,246,33,8,0,model : ALIGNMENT MODEL,0.743202416918429,0.2796610169491525,0.8888888888888888
machine-translation,8,"ah j does not depend on i , we can pre-compute it in advance to minimize the computational cost .",model,ALIGNMENT MODEL,0,247,34,9,0,model : ALIGNMENT MODEL,0.7462235649546828,0.288135593220339,1.0
machine-translation,8,DETAILED DESCRIPTION OF THE MODEL,model,DETAILED DESCRIPTION OF THE MODEL,0,248,35,1,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7492447129909365,0.2966101694915254,0.06666666666666667
machine-translation,8,ENCODER,model,DETAILED DESCRIPTION OF THE MODEL,0,249,36,2,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7522658610271903,0.3050847457627119,0.13333333333333333
machine-translation,8,"In this section , we describe in detail the architecture of the proposed model ( RNNsearch ) used in the experiments ( see .",model,DETAILED DESCRIPTION OF THE MODEL,0,250,37,3,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7552870090634441,0.3135593220338983,0.2
machine-translation,8,"From hereon , we omit all bias terms in order to increase readability .",model,DETAILED DESCRIPTION OF THE MODEL,0,251,38,4,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7583081570996979,0.3220338983050847,0.26666666666666666
machine-translation,8,The model takes a source sentence of 1 - of - K coded word vectors as input,model,DETAILED DESCRIPTION OF THE MODEL,0,252,39,5,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7613293051359517,0.3305084745762712,0.3333333333333333
machine-translation,8,"and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky , where K x and Ky are the vocabulary sizes of source and target languages , respectively .",model,DETAILED DESCRIPTION OF THE MODEL,0,253,40,6,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7643504531722054,0.3389830508474576,0.4
machine-translation,8,"and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky , where K x and Ky are the vocabulary sizes of source and target languages , respectively .",model,DETAILED DESCRIPTION OF THE MODEL,0,254,41,7,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7673716012084593,0.3474576271186441,0.4666666666666667
machine-translation,8,x and Ty respectively denote the lengths of source and target sentences .,model,DETAILED DESCRIPTION OF THE MODEL,0,255,42,8,0,model : DETAILED DESCRIPTION OF THE MODEL,0.770392749244713,0.3559322033898305,0.5333333333333333
machine-translation,8,"First , the forward states of the bidirectional recurrent neural network ( BiRNN ) are computed :",model,DETAILED DESCRIPTION OF THE MODEL,0,256,43,9,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7734138972809668,0.3644067796610169,0.6
machine-translation,8,are weight matrices .,model,DETAILED DESCRIPTION OF THE MODEL,0,257,44,10,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7764350453172205,0.3728813559322034,0.6666666666666666
machine-translation,8,"and n are the word embedding dimensionality and the number of hidden units , respectively . ? ( ) is as usual a logistic sigmoid function .",model,DETAILED DESCRIPTION OF THE MODEL,0,258,45,11,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7794561933534743,0.3813559322033898,0.7333333333333333
machine-translation,8,"The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .",model,DETAILED DESCRIPTION OF THE MODEL,0,259,46,12,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7824773413897281,0.3898305084745763,0.8
machine-translation,8,"We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .",model,DETAILED DESCRIPTION OF THE MODEL,0,260,47,13,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7854984894259819,0.3983050847457627,0.8666666666666667
machine-translation,8,"We concatenate the forward and backward states to to obtain the annotations ( h 1 , h 2 , , h Tx ) , where",model,DETAILED DESCRIPTION OF THE MODEL,0,261,48,14,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7885196374622356,0.4067796610169492,0.9333333333333333
machine-translation,8,A.,model,DETAILED DESCRIPTION OF THE MODEL,0,262,49,15,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7915407854984894,0.4152542372881356,1.0
machine-translation,8,DECODER,model,DECODER,0,263,50,1,0,model : DECODER,0.7945619335347432,0.423728813559322,0.038461538461538464
machine-translation,8,The hidden state s i of the decoder given the annotations from the encoder is computed by,model,DECODER,0,264,51,2,0,model : DECODER,0.797583081570997,0.4322033898305085,0.07692307692307693
machine-translation,8,is the word embedding matrix for the target language .,model,DECODER,0,265,52,3,0,model : DECODER,0.8006042296072508,0.4406779661016949,0.11538461538461539
machine-translation,8,", W z , W r ? R nm , U , U z , Ur ? R nn , and C , C z , Cr ? R n 2n are weights .",model,DECODER,0,266,53,4,0,model : DECODER,0.8036253776435045,0.4491525423728814,0.15384615384615385
machine-translation,8,", W z , W r ? R nm , U , U z , Ur ? R nn , and C , C z , Cr ? R n 2n are weights .",model,DECODER,0,267,54,5,0,model : DECODER,0.8066465256797583,0.4576271186440678,0.19230769230769232
machine-translation,8,", W z , W r ? R nm , U , U z , Ur ? R nn , and C , C z , Cr ? R n 2n are weights .",model,DECODER,0,268,55,6,0,model : DECODER,0.8096676737160121,0.4661016949152542,0.23076923076923078
machine-translation,8,"Again , m and n are the word embedding dimensionality and the number of hidden units , respectively .",model,DECODER,0,269,56,7,0,model : DECODER,0.8126888217522659,0.4745762711864407,0.2692307692307692
machine-translation,8,The initial hidden state s 0 is computed by,model,DECODER,0,270,57,8,0,model : DECODER,0.8157099697885196,0.4830508474576271,0.3076923076923077
machine-translation,8,The context vector c i are recomputed at each step by the alignment model : :,model,DECODER,0,271,58,9,0,model : DECODER,0.8187311178247734,0.4915254237288136,0.34615384615384615
machine-translation,8,Learning statistics and relevant information .,model,DECODER,0,272,59,10,0,model : DECODER,0.8217522658610272,0.5,0.38461538461538464
machine-translation,8,Each update corresponds to updating the parameters once using a single minibatch .,model,DECODER,0,273,60,11,0,model : DECODER,0.824773413897281,0.5084745762711864,0.4230769230769231
machine-translation,8,One epoch is one pass through the training set .,model,DECODER,0,274,61,12,0,model : DECODER,0.8277945619335347,0.5169491525423728,0.46153846153846156
machine-translation,8,NLL is the average conditional log-probabilities of the sentences in either the training set or the development set .,model,DECODER,0,275,62,13,0,model : DECODER,0.8308157099697885,0.5254237288135594,0.5
machine-translation,8,Note that the lengths of the sentences differ .,model,DECODER,0,276,63,14,0,model : DECODER,0.8338368580060423,0.5338983050847458,0.5384615384615384
machine-translation,8,where,model,DECODER,0,277,64,15,0,model : DECODER,0.8368580060422961,0.5423728813559322,0.5769230769230769
machine-translation,8,and h j is the j - th annotation in the source sentence ( see Eq. ) .,model,DECODER,0,278,65,16,0,model : DECODER,0.8398791540785498,0.5508474576271186,0.6153846153846154
machine-translation,8,"a ? Rn , W a ? Rn n and U a ? Rn 2n are weight matrices .",model,DECODER,0,279,66,17,0,model : DECODER,0.8429003021148036,0.559322033898305,0.6538461538461539
machine-translation,8,"a ? Rn , W a ? Rn n and U a ? Rn 2n are weight matrices .",model,DECODER,0,280,67,18,0,model : DECODER,0.8459214501510574,0.5677966101694916,0.6923076923076923
machine-translation,8,"a ? Rn , W a ? Rn n and U a ? Rn 2n are weight matrices .",model,DECODER,0,281,68,19,0,model : DECODER,0.8489425981873112,0.576271186440678,0.7307692307692307
machine-translation,8,"a ? Rn , W a ? Rn n and U a ? Rn 2n are weight matrices .",model,DECODER,0,282,69,20,0,model : DECODER,0.851963746223565,0.5847457627118644,0.7692307692307693
machine-translation,8,Note that the model becomes RNN Encoder - Decoder,model,DECODER,0,283,70,21,0,model : DECODER,0.8549848942598187,0.5932203389830508,0.8076923076923077
machine-translation,8,"With the decoder state s i ?1 , the context c i and the last generated wordy i ? 1 , we define the probability of a target wordy i as",model,DECODER,0,284,71,22,0,model : DECODER,0.8580060422960725,0.6016949152542372,0.8461538461538461
machine-translation,8,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ? and Co ? R 2 l 2n are weight matrices .",model,DECODER,0,285,72,23,0,model : DECODER,0.8610271903323263,0.6101694915254238,0.8846153846153846
machine-translation,8,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ? and Co ? R 2 l 2n are weight matrices .",model,DECODER,0,286,73,24,0,model : DECODER,0.8640483383685801,0.6186440677966102,0.9230769230769231
machine-translation,8,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ? and Co ? R 2 l 2n are weight matrices .",model,DECODER,0,287,74,25,0,model : DECODER,0.8670694864048338,0.6271186440677966,0.9615384615384616
machine-translation,8,This can be understood as having a deep output with a single maxout hidden layer .,model,DECODER,0,288,75,26,0,model : DECODER,0.8700906344410876,0.635593220338983,1.0
machine-translation,8,MODEL SIZE,model,MODEL SIZE,0,289,76,1,0,model : MODEL SIZE,0.8731117824773413,0.6440677966101694,0.125
machine-translation,8,"For all the models used in this paper , the size of a hidden layer n is 1000 , the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500 .",model,MODEL SIZE,0,290,77,2,0,model : MODEL SIZE,0.8761329305135952,0.652542372881356,0.25
machine-translation,8,The number of hidden units in the alignment model n is 1000 .,model,MODEL SIZE,0,291,78,3,0,model : MODEL SIZE,0.879154078549849,0.6610169491525424,0.375
machine-translation,8,and ? ? Ur as random orthogonal matrices .,model,MODEL SIZE,0,292,79,4,0,model : MODEL SIZE,0.8821752265861027,0.6694915254237288,0.5
machine-translation,8,and ? ? Ur as random orthogonal matrices .,model,MODEL SIZE,0,293,80,5,0,model : MODEL SIZE,0.8851963746223565,0.6779661016949152,0.625
machine-translation,8,"For W a and U a , we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.001 2 .",model,MODEL SIZE,0,294,81,6,0,model : MODEL SIZE,0.8882175226586103,0.6864406779661016,0.75
machine-translation,8,All the elements of Va and all the bias vectors were initialized to zero .,model,MODEL SIZE,0,295,82,7,0,model : MODEL SIZE,0.8912386706948641,0.6949152542372882,0.875
machine-translation,8,Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.01 2 .,model,MODEL SIZE,0,296,83,8,0,model : MODEL SIZE,0.8942598187311178,0.7033898305084746,1.0
machine-translation,8,TRAINING,model,TRAINING,0,297,84,1,0,model : TRAINING,0.8972809667673716,0.711864406779661,0.1111111111111111
machine-translation,8,We used the stochastic gradient descent ( SGD ) algorithm .,model,TRAINING,0,298,85,2,0,model : TRAINING,0.9003021148036254,0.7203389830508474,0.2222222222222222
machine-translation,8,Adadelta was used to automatically adapt the learning rate of each parameter ( = 10 ?6 and ? = 0.95 ) .,model,TRAINING,0,299,86,3,0,model : TRAINING,0.9033232628398792,0.7288135593220338,0.3333333333333333
machine-translation,8,"We explicitly normalized the L 2 - norm of the gradient of the cost function each time to beat most a predefined threshold of 1 , when the norm was larger than the threshold .",model,TRAINING,0,300,87,4,0,model : TRAINING,0.9063444108761329,0.7372881355932204,0.4444444444444444
machine-translation,8,Each SGD update direction was computed with a minibatch of 80 sentences .,model,TRAINING,0,301,88,5,0,model : TRAINING,0.9093655589123867,0.7457627118644068,0.5555555555555556
machine-translation,8,At each update our implementation requires time proportional to the length of the longest sentence in a minibatch .,model,TRAINING,0,302,89,6,0,model : TRAINING,0.9123867069486404,0.7542372881355932,0.6666666666666666
machine-translation,8,"Hence , to minimize the waste of computation , before every 20 - th update , we retrieved 1600 sentence pairs , sorted them according to the lengths and split them into 20 minibatches .",model,TRAINING,0,303,90,7,0,model : TRAINING,0.9154078549848943,0.7627118644067796,0.7777777777777778
machine-translation,8,The training data was shuffled once before training and was traversed sequentially in this manner .,model,TRAINING,0,304,91,8,0,model : TRAINING,0.918429003021148,0.7711864406779662,0.8888888888888888
machine-translation,8,In Tables 2 we present the statistics related to training all the models used in the experiments .,model,TRAINING,0,305,92,9,0,model : TRAINING,0.9214501510574018,0.7796610169491526,1.0
machine-translation,8,TRANSLATIONS OF LONG SENTENCES,model,TRANSLATIONS OF LONG SENTENCES Source,0,306,93,1,0,model : TRANSLATIONS OF LONG SENTENCES Source,0.9244712990936556,0.788135593220339,0.2
machine-translation,8,Source,model,TRANSLATIONS OF LONG SENTENCES Source,0,307,94,2,0,model : TRANSLATIONS OF LONG SENTENCES Source,0.9274924471299094,0.7966101694915254,0.4
machine-translation,8,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",model,TRANSLATIONS OF LONG SENTENCES Source,0,308,95,3,0,model : TRANSLATIONS OF LONG SENTENCES Source,0.9305135951661632,0.8050847457627118,0.6
machine-translation,8,Reference,model,TRANSLATIONS OF LONG SENTENCES Source,0,309,96,4,0,model : TRANSLATIONS OF LONG SENTENCES Source,0.9335347432024169,0.8135593220338984,0.8
machine-translation,8,"Le privilge d'admission est le droit d'un mdecin , en vertu de son statut de membre soignant d'un hpital , d'admettre un patient dans un hpital ou un centre mdical afin d 'y dlivrer un diagnostic ou un traitement .",model,TRANSLATIONS OF LONG SENTENCES Source,0,310,97,5,0,model : TRANSLATIONS OF LONG SENTENCES Source,0.9365558912386707,0.8220338983050848,1.0
machine-translation,8,RNNenc - 50,model,RNNenc-50,0,311,98,1,0,model : RNNenc-50,0.9395770392749244,0.8305084745762712,0.16666666666666666
machine-translation,8,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",model,RNNenc-50,0,312,99,2,0,model : RNNenc-50,0.9425981873111783,0.8389830508474576,0.3333333333333333
machine-translation,8,RNNsearch - 50,model,RNNenc-50,0,313,100,3,0,model : RNNenc-50,0.945619335347432,0.847457627118644,0.5
machine-translation,8,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",model,RNNenc-50,0,314,101,4,0,model : RNNenc-50,0.9486404833836858,0.8559322033898306,0.6666666666666666
machine-translation,8,Google Translate,model,RNNenc-50,0,315,102,5,0,model : RNNenc-50,0.9516616314199395,0.864406779661017,0.8333333333333334
machine-translation,8,"Ce genre d'exprience fait partie des efforts de Disney "" tendre la dure de vie de sa srie et construire de nouvelles relations avec le public par le biais des plates - formes numriques qui deviennent de plus en plus important "" , at - il ajout .",model,RNNenc-50,0,316,103,6,0,model : RNNenc-50,0.9546827794561934,0.8728813559322034,1.0
machine-translation,8,Source,model,Source,0,317,104,1,0,model : Source,0.9577039274924471,0.8813559322033898,0.5
machine-translation,8,"In a press conference on Thursday , Mr Blair stated that there was nothing in this video that might constitute a "" reasonable motive "" that could lead to criminal charges being brought against the mayor .",model,Source,0,318,105,2,0,model : Source,0.9607250755287009,0.8898305084745762,1.0
machine-translation,8,Reference,model,Reference,0,319,106,1,0,model : Reference,0.9637462235649547,0.8983050847457628,0.07692307692307693
machine-translation,8,"En confrence de presse , jeudi , M. Blair a affirm qu'il n'y avait rien dans cette vido qui puisse constituer des "" motifs raisonnables "" pouvant mener au dpt d'une accusation criminelle contre le maire .",model,Reference,0,320,107,2,0,model : Reference,0.9667673716012085,0.9067796610169492,0.15384615384615385
machine-translation,8,RNNenc - 50,model,Reference,0,321,108,3,0,model : Reference,0.9697885196374623,0.9152542372881356,0.23076923076923078
machine-translation,8,"Lors de la confrence de presse de jeudi , M. Blair a dit qu'il n'y avait rien dans cette vido qui pourrait constituer une "" motivation raisonnable "" pouvant entraner des accusations criminelles portes contre le maire .",model,Reference,0,322,109,4,0,model : Reference,0.972809667673716,0.923728813559322,0.3076923076923077
machine-translation,8,RNNsearch - 50,model,Reference,0,323,110,5,0,model : Reference,0.9758308157099698,0.9322033898305084,0.38461538461538464
machine-translation,8,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait conduire des accusations criminelles contre le maire .",model,Reference,0,324,111,6,0,model : Reference,0.9788519637462235,0.940677966101695,0.46153846153846156
machine-translation,8,Google Translate,model,Reference,0,325,112,7,0,model : Reference,0.9818731117824774,0.9491525423728814,0.5384615384615384
machine-translation,8,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait mener des accusations criminelles portes contre le maire . :",model,Reference,0,326,113,8,0,model : Reference,0.9848942598187311,0.9576271186440678,0.6153846153846154
machine-translation,8,The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .,model,Reference,0,327,114,9,0,model : Reference,0.9879154078549849,0.9661016949152542,0.6923076923076923
machine-translation,8,"For each source sentence , we also show the goldstandard translation .",model,Reference,0,328,115,10,0,model : Reference,0.9909365558912386,0.9745762711864406,0.7692307692307693
machine-translation,8,The translations by Google Translate were made on 27 August 2014 .,model,Reference,0,329,116,11,0,model : Reference,0.9939577039274925,0.9830508474576272,0.8461538461538461
machine-translation,8,Reference,model,Reference,0,330,117,12,0,model : Reference,0.9969788519637462,0.9915254237288136,0.9230769230769231
machine-translation,8,"Ce type d'exprience entre dans le cadre des efforts de Disney pour "" tendre la dure de vie de ses sries et construire de nouvelles relations avec son public grce des plateformes numriques qui so nt de plus en plus importantes "" , a-t - il ajout .",model,Reference,0,331,118,13,0,model : Reference,1.0,1.0,1.0
machine-translation,9,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,title,title,1,2,1,1,0,title : title,0.006968641114982578,1.0,1.0
machine-translation,9,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.010452961672473868,0.09090909090909091,0.09090909090909091
machine-translation,9,"Natural language processing ( NLP ) models often require a massive number of parameters for word embeddings , resulting in a large storage or memory footprint .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.013937282229965157,0.18181818181818182,0.18181818181818182
machine-translation,9,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,abstract,abstract,1,5,3,3,0,abstract : abstract,0.017421602787456445,0.2727272727272727,0.2727272727272727
machine-translation,9,"For this purpose , we propose to construct the embeddings with few basis vectors .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.020905923344947737,0.36363636363636365,0.36363636363636365
machine-translation,9,"For each word , the composition of basis vectors is determined by a hash code .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.024390243902439025,0.45454545454545453,0.45454545454545453
machine-translation,9,"To maximize the compression rate , we adopt the multi-codebook quantization approach instead of binary coding scheme .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.027874564459930314,0.5454545454545454,0.5454545454545454
machine-translation,9,"Each code is composed of multiple discrete numbers , such as ( 3 , 2 , 1 , 8 ) , where the value of each component is limited to a fixed range .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.0313588850174216,0.6363636363636364,0.6363636363636364
machine-translation,9,We propose to directly learn the discrete codes in an end - to - end neural network by applying the Gumbel - softmax trick .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.03484320557491289,0.7272727272727273,0.7272727272727273
machine-translation,9,Experiments show the compression rate achieves 98 % in a sentiment analysis task and 94 % ? 99 % in machine translation tasks without performance loss .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.03832752613240418,0.8181818181818182,0.8181818181818182
machine-translation,9,"In both tasks , the proposed method can improve the model performance by slightly lowering the compression rate .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.041811846689895474,0.9090909090909091,0.9090909090909091
machine-translation,9,"Compared to other approaches such as character - level segmentation , the proposed method is language - independent and does not require modifications to the network architecture .",abstract,abstract,0,13,11,11,0,abstract : abstract,0.04529616724738676,1.0,1.0
machine-translation,9,INTRODUCTION,introduction,introduction,0,14,1,1,0,introduction : introduction,0.04878048780487805,0.018867924528301886,0.018867924528301886
machine-translation,9,Word embeddings play an important role in neural - based natural language processing ( NLP ) models .,introduction,introduction,0,15,2,2,0,introduction : introduction,0.05226480836236934,0.03773584905660377,0.03773584905660377
machine-translation,9,Neural word embeddings encapsulate the linguistic information of words in continuous vectors .,introduction,introduction,0,16,3,3,0,introduction : introduction,0.05574912891986063,0.05660377358490566,0.05660377358490566
machine-translation,9,"However , as each word is assigned an independent embedding vector , the number of parameters in the embedding matrix can be huge .",introduction,introduction,0,17,4,4,0,introduction : introduction,0.059233449477351915,0.07547169811320754,0.07547169811320754
machine-translation,9,"For example , when each embedding has 500 dimensions , the network has to hold 100M embedding parameters to represent 200K words .",introduction,introduction,0,18,5,5,0,introduction : introduction,0.0627177700348432,0.09433962264150944,0.09433962264150944
machine-translation,9,"In practice , for a simple sentiment analysis model , the word embedding parameters account for 98.8 % of the total parameters .",introduction,introduction,0,19,6,6,0,introduction : introduction,0.06620209059233449,0.11320754716981132,0.11320754716981132
machine-translation,9,"As only a small portion of the word embeddings is selected in the forward pass , the giant embedding matrix usually does not cause a speed issue .",introduction,introduction,0,20,7,7,0,introduction : introduction,0.06968641114982578,0.1320754716981132,0.1320754716981132
machine-translation,9,"However , the massive number of parameters in the neural network results in a large storage or memory footprint .",introduction,introduction,0,21,8,8,0,introduction : introduction,0.07317073170731707,0.1509433962264151,0.1509433962264151
machine-translation,9,"When other components of the neural network are also large , the model may fail to fit into GPU memory during training .",introduction,introduction,0,22,9,9,0,introduction : introduction,0.07665505226480836,0.16981132075471697,0.16981132075471697
machine-translation,9,"Moreover , as the demand for low - latency neural computation for mobile platforms increases , some neural - based models are expected to run on mobile devices .",introduction,introduction,0,23,10,10,0,introduction : introduction,0.08013937282229965,0.18867924528301888,0.18867924528301888
machine-translation,9,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",introduction,introduction,1,24,11,11,0,introduction : introduction,0.08362369337979095,0.20754716981132076,0.20754716981132076
machine-translation,9,"In this study , we attempt to reduce the number of parameters used in word embeddings without hurting the model performance .",introduction,introduction,0,25,12,12,0,introduction : introduction,0.08710801393728224,0.22641509433962265,0.22641509433962265
machine-translation,9,Neural networks are known for the significant redundancy in the connections .,introduction,introduction,0,26,13,13,0,introduction : introduction,0.09059233449477352,0.24528301886792453,0.24528301886792453
machine-translation,9,"In this work , we further hypothesize that learning independent embeddings causes more redundancy in the embedding vectors , as the inter-similarity among words is ignored .",introduction,introduction,0,27,14,14,0,introduction : introduction,0.09407665505226481,0.2641509433962264,0.2641509433962264
machine-translation,9,Some words are very similar regarding the semantics .,introduction,introduction,0,28,15,15,0,introduction : introduction,0.0975609756097561,0.2830188679245283,0.2830188679245283
machine-translation,9,"For example , "" dog "" and "" dogs "" have almost the same meaning , except one is plural .",introduction,introduction,0,29,16,16,0,introduction : introduction,0.10104529616724739,0.3018867924528302,0.3018867924528302
machine-translation,9,"To efficiently represent these two words , it is desirable to share information between the two embeddings .",introduction,introduction,0,30,17,17,0,introduction : introduction,0.10452961672473868,0.32075471698113206,0.32075471698113206
machine-translation,9,"However , a small portion in both vectors still has to be trained independently to capture the syntactic difference .",introduction,introduction,0,31,18,18,0,introduction : introduction,0.10801393728222997,0.33962264150943394,0.33962264150943394
machine-translation,9,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .",introduction,introduction,1,32,19,19,0,introduction : introduction,0.11149825783972125,0.3584905660377358,0.3584905660377358
machine-translation,9,Each component,introduction,introduction,0,33,20,20,0,introduction : introduction,0.11498257839721254,0.37735849056603776,0.37735849056603776
machine-translation,9,Ci w is an integer number in .,introduction,introduction,0,34,21,21,0,introduction : introduction,0.11846689895470383,0.39622641509433965,0.39622641509433965
machine-translation,9,"Ideally , similar words should have similar codes .",introduction,introduction,0,35,22,22,0,introduction : introduction,0.12195121951219512,0.41509433962264153,0.41509433962264153
machine-translation,9,"For example , we may desire C dog = ( 3 , 2 , 4 , 1 ) and C dogs = ( 3 , 2 , 4 , 2 ) .",introduction,introduction,0,36,23,23,0,introduction : introduction,0.1254355400696864,0.4339622641509434,0.4339622641509434
machine-translation,9,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .",introduction,introduction,1,37,24,24,0,introduction : introduction,0.1289198606271777,0.4528301886792453,0.4528301886792453
machine-translation,9,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .",introduction,introduction,1,38,25,25,0,introduction : introduction,0.13240418118466898,0.4716981132075472,0.4716981132075472
machine-translation,9,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,introduction,introduction,1,39,26,26,0,introduction : introduction,0.13588850174216027,0.49056603773584906,0.49056603773584906
machine-translation,9,where E i ( C i w ) is the Ci w - th codeword in the codebook E i .,introduction,introduction,0,40,27,27,0,introduction : introduction,0.13937282229965156,0.5094339622641509,0.5094339622641509
machine-translation,9,"In this way , the number of vectors in the embedding matrix will be M K , which is usually much smaller than the vocabulary size .",introduction,introduction,0,41,28,28,0,introduction : introduction,0.14285714285714285,0.5283018867924528,0.5283018867924528
machine-translation,9,gives an intuitive comparison between the compositional approach and the conventional approach ( assigning unique IDs ) .,introduction,introduction,0,42,29,29,0,introduction : introduction,0.14634146341463414,0.5471698113207547,0.5471698113207547
machine-translation,9,"The codes of all the words can be stored in an integer matrix , denoted by C. Thus , the storage footprint of the embedding layer now depends on the total size of the combined codebook E and the code matrix C.",introduction,introduction,0,43,30,30,0,introduction : introduction,0.14982578397212543,0.5660377358490566,0.5660377358490566
machine-translation,9,"Although the number of embedding vectors can be greatly reduced by using such coding approach , we want to prevent any serious degradation in performance compared to the models using normal embeddings .",introduction,introduction,0,44,31,31,0,introduction : introduction,0.15331010452961671,0.5849056603773585,0.5849056603773585
machine-translation,9,"In other words , given a set of baseline word embeddings ? ( w ) , we wish to find a set of codes ? and combined codebook that can produce the embeddings with the same effectiveness as ? ( w ) .",introduction,introduction,0,45,32,32,0,introduction : introduction,0.156794425087108,0.6037735849056604,0.6037735849056604
machine-translation,9,"In other words , given a set of baseline word embeddings ? ( w ) , we wish to find a set of codes ? and combined codebook that can produce the embeddings with the same effectiveness as ? ( w ) .",introduction,introduction,0,46,33,33,0,introduction : introduction,0.1602787456445993,0.6226415094339622,0.6226415094339622
machine-translation,9,safe and straight - forward way is to minimize the squared distance between the baseline embeddings and the composed embeddings as,introduction,introduction,0,47,34,34,0,introduction : introduction,0.16376306620209058,0.6415094339622641,0.6415094339622641
machine-translation,9,where | V | is the vocabulary size .,introduction,introduction,0,48,35,35,0,introduction : introduction,0.1672473867595819,0.660377358490566,0.660377358490566
machine-translation,9,The baseline embeddings can be a set of pre-trained vectors such as word2vec or GloVe embeddings .,introduction,introduction,0,49,36,36,0,introduction : introduction,0.17073170731707318,0.6792452830188679,0.6792452830188679
machine-translation,9,"In Eq. 3 , the baseline embedding matrix ? is approximated by M codewords selected from M codebooks .",introduction,introduction,0,50,37,37,0,introduction : introduction,0.17421602787456447,0.6981132075471698,0.6981132075471698
machine-translation,9,"In Eq. 3 , the baseline embedding matrix ? is approximated by M codewords selected from M codebooks .",introduction,introduction,0,51,38,38,0,introduction : introduction,0.17770034843205576,0.7169811320754716,0.7169811320754716
machine-translation,9,The selection of codewords is controlled by the code C w .,introduction,introduction,0,52,39,39,0,introduction : introduction,0.18118466898954705,0.7358490566037735,0.7358490566037735
machine-translation,9,"Such problem of learning compact codes with multiple codebooks is formalized and discussed in the research field of compressionbased source coding , known as product quantization and additive quantization .",introduction,introduction,0,53,40,40,0,introduction : introduction,0.18466898954703834,0.7547169811320755,0.7547169811320755
machine-translation,9,Previous works learn compositional codes so as to enable an efficient similarity search of vectors .,introduction,introduction,0,54,41,41,0,introduction : introduction,0.18815331010452963,0.7735849056603774,0.7735849056603774
machine-translation,9,"In this work , we utilize such codes for a different purpose , that is , constructing word embeddings with drastically fewer parameters .",introduction,introduction,1,55,42,42,0,introduction : introduction,0.1916376306620209,0.7924528301886793,0.7924528301886793
machine-translation,9,"Due to the discreteness in the hash codes , it is usually difficult to directly optimize the objective function in Eq.",introduction,introduction,0,56,43,43,0,introduction : introduction,0.1951219512195122,0.8113207547169812,0.8113207547169812
machine-translation,9,3 .,introduction,introduction,0,57,44,44,0,introduction : introduction,0.1986062717770035,0.8301886792452831,0.8301886792452831
machine-translation,9,"In this paper , we propose a simple and straight - forward method to learn the codes in an end - to - end neural network .",introduction,introduction,0,58,45,45,0,introduction : introduction,0.20209059233449478,0.8490566037735849,0.8490566037735849
machine-translation,9,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,introduction,introduction,1,59,46,46,0,introduction : introduction,0.20557491289198607,0.8679245283018868,0.8679245283018868
machine-translation,9,"Besides the simplicity , this approach also allows one to use any arbitrary differentiable loss function , such as cosine similarity .",introduction,introduction,0,60,47,47,0,introduction : introduction,0.20905923344947736,0.8867924528301887,0.8867924528301887
machine-translation,9,The contribution of this work can be summarized as follows :,introduction,introduction,0,61,48,48,0,introduction : introduction,0.21254355400696864,0.9056603773584906,0.9056603773584906
machine-translation,9,We propose to utilize the compositional coding approach for constructing the word embeddings with significantly fewer parameters .,introduction,introduction,0,62,49,49,0,introduction : introduction,0.21602787456445993,0.9245283018867925,0.9245283018867925
machine-translation,9,"In the experiments , we show that over 98 % of the embedding parameters can be eliminated in sentiment analysis task without affecting performance .",introduction,introduction,0,63,50,50,0,introduction : introduction,0.21951219512195122,0.9433962264150944,0.9433962264150944
machine-translation,9,"In machine translation tasks , the loss - free compression rate reaches 94 % ? 99 % . We propose a direct learning approach for the codes in an end - to - end neural network , with a Gumbel - softmax layer to encourage the discreteness .",introduction,introduction,0,64,51,51,0,introduction : introduction,0.2229965156794425,0.9622641509433962,0.9622641509433962
machine-translation,9,The neural network for learning codes will be packaged into a tool .,introduction,introduction,0,65,52,52,0,introduction : introduction,0.2264808362369338,0.9811320754716981,0.9811320754716981
machine-translation,9,"With the learned codes and basis vectors , the computation graph for composing embeddings is fairly easy to implement , and does not require modifications to other parts in the neural network .",introduction,introduction,0,66,53,53,0,introduction : introduction,0.22996515679442509,1.0,1.0
machine-translation,9,RELATED WORK,related work,RELATED WORK,0,67,1,1,0,related work : RELATED WORK,0.23344947735191637,0.03333333333333333,0.03333333333333333
machine-translation,9,"Existing works for compressing neural networks include low - precision computation , quantization and knowledge distillation .",related work,RELATED WORK,0,68,2,2,0,related work : RELATED WORK,0.23693379790940766,0.06666666666666667,0.06666666666666667
machine-translation,9,"Network quantization such as HashedNet forces the weight matrix to have few real weights , with a hash function to determine the weight assignment .",related work,RELATED WORK,0,69,3,3,0,related work : RELATED WORK,0.24041811846689895,0.1,0.1
machine-translation,9,"To capture the non-uniform nature of the networks , DeepCompression groups weight values into clusters based on pre-trained weight matrices .",related work,RELATED WORK,0,70,4,4,0,related work : RELATED WORK,0.24390243902439024,0.13333333333333333,0.13333333333333333
machine-translation,9,The weight assignment for each value is stored in the form of Huffman codes .,related work,RELATED WORK,0,71,5,5,0,related work : RELATED WORK,0.24738675958188153,0.16666666666666666,0.16666666666666666
machine-translation,9,"However , as the embedding matrix is tremendously big , the number of hash codes a model need to maintain is still large even with Huffman coding .",related work,RELATED WORK,0,72,6,6,0,related work : RELATED WORK,0.2508710801393728,0.2,0.2
machine-translation,9,Network pruning works in a different way that makes a network sparse .,related work,RELATED WORK,0,73,7,7,0,related work : RELATED WORK,0.25435540069686413,0.23333333333333334,0.23333333333333334
machine-translation,9,Iterative pruning prunes a weight value if its absolute value is smaller than a threshold .,related work,RELATED WORK,0,74,8,8,0,related work : RELATED WORK,0.2578397212543554,0.26666666666666666,0.26666666666666666
machine-translation,9,The remaining network weights are retrained after pruning .,related work,RELATED WORK,0,75,9,9,0,related work : RELATED WORK,0.2613240418118467,0.3,0.3
machine-translation,9,Some recent works also apply iterative pruning to prune 80 % of the connections for neural machine translation models .,related work,RELATED WORK,0,76,10,10,0,related work : RELATED WORK,0.26480836236933797,0.3333333333333333,0.3333333333333333
machine-translation,9,"In this paper , we compare the proposed method with iterative pruning .",related work,RELATED WORK,0,77,11,11,0,related work : RELATED WORK,0.2682926829268293,0.36666666666666664,0.36666666666666664
machine-translation,9,"The problem of learning compact codes considered in this paper is closely related to learning to hash , which aims to learn the hash codes for vectors to facilitate the approximate nearest neighbor search .",related work,RELATED WORK,0,78,12,12,0,related work : RELATED WORK,0.27177700348432055,0.4,0.4
machine-translation,9,"Initiated byproduct quantization , subsequent works such as additive quantization explore the use of multiple codebooks for source coding , resulting in compositional codes .",related work,RELATED WORK,0,79,13,13,0,related work : RELATED WORK,0.27526132404181186,0.43333333333333335,0.43333333333333335
machine-translation,9,We also adopt the coding scheme of additive quantization for its storage efficiency .,related work,RELATED WORK,0,80,14,14,0,related work : RELATED WORK,0.2787456445993031,0.4666666666666667,0.4666666666666667
machine-translation,9,Previous works mainly focus on performing efficient similarity search of image descriptors .,related work,RELATED WORK,0,81,15,15,0,related work : RELATED WORK,0.28222996515679444,0.5,0.5
machine-translation,9,"In this work , we put more focus on reducing the codebook sizes and learning efficient codes to avoid performance loss .",related work,RELATED WORK,0,82,16,16,0,related work : RELATED WORK,0.2857142857142857,0.5333333333333333,0.5333333333333333
machine-translation,9,utilizes an improved version of product quantization to compress text classification models .,related work,RELATED WORK,0,83,17,17,0,related work : RELATED WORK,0.289198606271777,0.5666666666666667,0.5666666666666667
machine-translation,9,"However , to match the baseline performance , much longer hash codes are required byproduct quantization .",related work,RELATED WORK,0,84,18,18,0,related work : RELATED WORK,0.2926829268292683,0.6,0.6
machine-translation,9,This will be detailed in Section 5.2 .,related work,RELATED WORK,0,85,19,19,0,related work : RELATED WORK,0.2961672473867596,0.6333333333333333,0.6333333333333333
machine-translation,9,"To learn the codebooks and code assignment , additive quantization alternatively optimizes the codebooks and the discrete codes .",related work,RELATED WORK,0,86,20,20,0,related work : RELATED WORK,0.29965156794425085,0.6666666666666666,0.6666666666666666
machine-translation,9,The learning of code assignment is performed by Beam Search algorithm when the codebooks are fixed .,related work,RELATED WORK,0,87,21,21,0,related work : RELATED WORK,0.30313588850174217,0.7,0.7
machine-translation,9,"In this work , we propose a straight - forward method to directly learn the code assignment and codebooks simutaneously in an end - to - end neural network .",related work,RELATED WORK,0,88,22,22,0,related work : RELATED WORK,0.30662020905923343,0.7333333333333333,0.7333333333333333
machine-translation,9,"Some recent works in learning to hash also utilize neural networks to produce binary codes by applying binary constrains ( e.g. , sigmoid function ) .",related work,RELATED WORK,0,89,23,23,0,related work : RELATED WORK,0.31010452961672474,0.7666666666666667,0.7666666666666667
machine-translation,9,"In this work , we encourage the discreteness with the Gumbel - Softmax trick for producing compositional codes .",related work,RELATED WORK,0,90,24,24,0,related work : RELATED WORK,0.313588850174216,0.8,0.8
machine-translation,9,"As an alternative to our approach , one can also reduce the number of unique word types by forcing a character - level segmentation .",related work,RELATED WORK,0,91,25,25,0,related work : RELATED WORK,0.3170731707317073,0.8333333333333334,0.8333333333333334
machine-translation,9,"proposed a character - based neural language model , which applies a convolutional layer after the character embeddings .",related work,RELATED WORK,0,92,26,26,0,related work : RELATED WORK,0.3205574912891986,0.8666666666666667,0.8666666666666667
machine-translation,9,"propose to use char-gram as input features , which are further hashed to save space .",related work,RELATED WORK,0,93,27,27,0,related work : RELATED WORK,0.3240418118466899,0.9,0.9
machine-translation,9,"Generally , using characterlevel inputs requires modifications to the model architecture .",related work,RELATED WORK,0,94,28,28,0,related work : RELATED WORK,0.32752613240418116,0.9333333333333333,0.9333333333333333
machine-translation,9,"Moreover , some Asian languages such as Japanese and Chinese retain a large vocabulary at the character level , which makes the character - based approach difficult to be applied .",related work,RELATED WORK,0,95,29,29,0,related work : RELATED WORK,0.3310104529616725,0.9666666666666667,0.9666666666666667
machine-translation,9,"In contrast , our approach does not suffer from these limitations .",related work,RELATED WORK,0,96,30,30,0,related work : RELATED WORK,0.3344947735191638,1.0,1.0
machine-translation,9,ADVANTAGE OF COMPOSITIONAL CODES,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,97,1,1,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.33797909407665505,0.016666666666666666,0.037037037037037035
machine-translation,9,"In this section , we formally describe the compositional coding approach and analyze its merits for compressing word embeddings .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,98,2,2,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.34146341463414637,0.03333333333333333,0.07407407407407407
machine-translation,9,The coding approach follows the scheme in additive quantization .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,99,3,3,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.34494773519163763,0.05,0.1111111111111111
machine-translation,9,"We represent each word w with a compact code C w that is composed of M components such that , which also indicates that M log 2 K bits are required to store each code .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,100,4,4,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.34843205574912894,0.06666666666666667,0.14814814814814814
machine-translation,9,"For convenience , K is selected to be a number of a multiple of 2 , so that the codes can be efficiently stored .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,101,5,5,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3519163763066202,0.08333333333333333,0.18518518518518517
machine-translation,9,If we restrict each component,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,102,6,6,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3554006968641115,0.1,0.2222222222222222
machine-translation,9,"Ci w to values of 0 or 1 , the code for each word C w will be a binary code .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,103,7,7,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3588850174216028,0.11666666666666667,0.25925925925925924
machine-translation,9,"In this case , the code learning problem is equivalent to a matrix factorization problem with binary components .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,104,8,8,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3623693379790941,0.13333333333333333,0.2962962962962963
machine-translation,9,"Forcing the compact codes to be binary numbers can be beneficial , as the learning problem is usually easier to solve in the binary case , and some existing optimization algorithms in learning to hash can be reused .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,105,9,9,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.36585365853658536,0.15,0.3333333333333333
machine-translation,9,"However , the compositional coding approach produces shorter codes and is thus more storage efficient .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,106,10,10,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3693379790940767,0.16666666666666666,0.37037037037037035
machine-translation,9,"As the number of basis vectors is M K regardless of the vocabulary size , the only uncertain factor contributing to the model size is the size of the hash codes , which is proportional to the vocabulary size .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,107,11,11,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.37282229965156793,0.18333333333333332,0.4074074074074074
machine-translation,9,"Therefore , maintaining short codes is cruicial in our work .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,108,12,12,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.37630662020905925,0.2,0.4444444444444444
machine-translation,9,Suppose we wish the model to have a set of N basis vectors .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,109,13,13,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3797909407665505,0.21666666666666667,0.48148148148148145
machine-translation,9,"Then in the binary case , each code will have N / 2 bits .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,110,14,14,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3832752613240418,0.23333333333333334,0.5185185185185185
machine-translation,9,"For the compositional coding approach , if we can find a M K decomposition such that M K = N , then each code will have M log 2 K bits .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,111,15,15,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3867595818815331,0.25,0.5555555555555556
machine-translation,9,"For example , a binary code will have a length of 256 bits to support 512 basis vectors .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,112,16,16,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3902439024390244,0.26666666666666666,0.5925925925925926
machine-translation,9,"In contrast , a 32 16 compositional coding scheme will produce codes of only 128 bits . :",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,113,17,17,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.39372822299651566,0.2833333333333333,0.6296296296296297
machine-translation,9,Comparison of different coding approaches .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,114,18,18,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.397212543554007,0.3,0.6666666666666666
machine-translation,9,"To support N basis vectors , a binary code will have N / 2 bits and the embedding computation is a summation over N / 2 vectors .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,115,19,19,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.40069686411149824,0.31666666666666665,0.7037037037037037
machine-translation,9,"For the compositional approach with M codebooks and K codewords in each codebook , each code has M log 2 K bits , and the computation is a summation over M vectors .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,116,20,20,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.40418118466898956,0.3333333333333333,0.7407407407407407
machine-translation,9,comparison of different coding approaches is summarized in .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,117,21,21,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.4076655052264808,0.35,0.7777777777777778
machine-translation,9,We also report the number of basis vectors required to compute an embedding as a measure of computational cost .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,118,22,22,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.41114982578397213,0.36666666666666664,0.8148148148148148
machine-translation,9,"For the conventional approach , the number of vectors is identical to the vocabulary size and the computation is basically a single indexing operation .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,119,23,23,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.4146341463414634,0.38333333333333336,0.8518518518518519
machine-translation,9,"In the case of binary codes , the computation for constructing an embedding involves a summation over N / 2 basis vectors .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,120,24,24,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.4181184668989547,0.4,0.8888888888888888
machine-translation,9,"For the compositional approach , the number of vectors required to construct an embedding vector is M .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,121,25,25,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.42160278745644597,0.4166666666666667,0.9259259259259259
machine-translation,9,Both the binary and compositional approaches have significantly fewer vectors in the embedding matrix .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,122,26,26,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.4250871080139373,0.43333333333333335,0.9629629629629629
machine-translation,9,The compositional coding approach provides a better balance with shorter codes and lower computational cost .,system description,ADVANTAGE OF COMPOSITIONAL CODES,0,123,27,27,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.42857142857142855,0.45,1.0
machine-translation,9,CODE LEARNING WITH GUMBEL - SOFTMAX,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,124,28,1,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.43205574912891986,0.4666666666666667,0.030303030303030304
machine-translation,9,"Let ? ? R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,125,29,2,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4355400696864111,0.48333333333333334,0.06060606060606061
machine-translation,9,"Let ? ? R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,126,30,3,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.43902439024390244,0.5,0.09090909090909091
machine-translation,9,"By using the reconstruction loss as the objective function in Eq. 3 , we are actually finding an approximate matrix factorization ? is a basis matrix for the i - th component .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,127,31,4,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4425087108013937,0.5166666666666667,0.12121212121212122
machine-translation,9,"By using the reconstruction loss as the objective function in Eq. 3 , we are actually finding an approximate matrix factorization ? is a basis matrix for the i - th component .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,128,32,5,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.445993031358885,0.5333333333333333,0.15151515151515152
machine-translation,9,Di is a | V | K code matrix in which each row is an K-dimensional one - hot vector .,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,129,33,6,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.44947735191637633,0.55,0.18181818181818182
machine-translation,9,If we let d i w be the one - hot vector corresponding to the code component,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,130,34,7,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4529616724738676,0.5666666666666667,0.21212121212121213
machine-translation,9,"Ci w for word w , the computation of the word embeddings can be reformulated as",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,131,35,8,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4564459930313589,0.5833333333333334,0.24242424242424243
machine-translation,9,"Therefore , the problem of learning discrete codes C w can be converted to a problem of finding a set of optimal one - hot vectors d 1 w , ... , d M wand source dictionaries A 1 , ... , AM , that minimize the reconstruction loss .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,132,36,9,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.45993031358885017,0.6,0.2727272727272727
machine-translation,9,The Gumbel - softmax reparameterization trick is useful for parameterizing a discrete distribution such as the K-dimensional one - hot vectors d i win Eq.,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,133,37,10,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4634146341463415,0.6166666666666667,0.30303030303030304
machine-translation,9,". By applying the Gumbel - softmax trick , the k - th elemement ind i w is computed as",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,134,38,11,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.46689895470383275,0.6333333333333333,0.3333333333333333
machine-translation,9,where,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,135,39,12,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.47038327526132406,0.65,0.36363636363636365
machine-translation,9,"Gk is a noise term that is sampled from the Gumbel distribution ? log ( ? log ( Uniform [ 0 , 1 ] ) ) , whereas ? is the temperature of the softmax .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,136,40,13,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4738675958188153,0.6666666666666666,0.3939393939393939
machine-translation,9,"Gk is a noise term that is sampled from the Gumbel distribution ? log ( ? log ( Uniform [ 0 , 1 ] ) ) , whereas ? is the temperature of the softmax .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,137,41,14,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.47735191637630664,0.6833333333333333,0.42424242424242425
machine-translation,9,"Gk is a noise term that is sampled from the Gumbel distribution ? log ( ? log ( Uniform [ 0 , 1 ] ) ) , whereas ? is the temperature of the softmax .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,138,42,15,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4808362369337979,0.7,0.45454545454545453
machine-translation,9,"In our model , the vector ? i w is computed by a simple neural network with a single hidden layer as",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,139,43,16,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4843205574912892,0.7166666666666667,0.48484848484848486
machine-translation,9,"In our model , the vector ? i w is computed by a simple neural network with a single hidden layer as",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,140,44,17,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4878048780487805,0.7333333333333333,0.5151515151515151
machine-translation,9,"In our experiments , the hidden layer h w always has a size of M K /2 .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,141,45,18,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4912891986062718,0.75,0.5454545454545454
machine-translation,9,We found that a fixed temperature of ? = 1 just works well .,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,142,46,19,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.49477351916376305,0.7666666666666667,0.5757575757575758
machine-translation,9,The Gumbel - softmax trick is applied to ? i w to obtain d i w .,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,143,47,20,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.49825783972125437,0.7833333333333333,0.6060606060606061
machine-translation,9,The Gumbel - softmax trick is applied to ? i w to obtain d i w .,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,144,48,21,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5017421602787456,0.8,0.6363636363636364
machine-translation,9,"Then , the model reconstructs the embedding E(C w ) with Eq. 5 and computes the reconstruction loss with Eq.",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,145,49,22,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5052264808362369,0.8166666666666667,0.6666666666666666
machine-translation,9,3 .,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,146,50,23,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5087108013937283,0.8333333333333334,0.696969696969697
machine-translation,9,"The model architecture of the end - to - end neural network is illustrated in , which is effectively an auto - encoder with a Gumbel - softmax middle layer .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,147,51,24,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5121951219512195,0.85,0.7272727272727273
machine-translation,9,"The whole neural network for coding learning has five parameters ( ? , b , ? , b , A ) .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,148,52,25,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5156794425087108,0.8666666666666667,0.7575757575757576
machine-translation,9,"Once the coding learning model is trained , the code C w for each word can be easily obtained by applying argmax to the one - hot vectors d 1 w , ... , d M w .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,149,53,26,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.519163763066202,0.8833333333333333,0.7878787878787878
machine-translation,9,The basis vectors ( codewords ) for composing the embeddings can be found as the row vectors in the weight matrix A.,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,150,54,27,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5226480836236934,0.9,0.8181818181818182
machine-translation,9,"For general NLP tasks , one can learn the compositional codes from publicly available word vectors such as GloVe vectors .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,151,55,28,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5261324041811847,0.9166666666666666,0.8484848484848485
machine-translation,9,"However , for some tasks such as machine translation , the word embeddings are usually jointly learned with other parts of the neural network .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,152,56,29,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5296167247386759,0.9333333333333333,0.8787878787878788
machine-translation,9,"For such tasks , one has to first train a normal model to obtain the baseline embeddings .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,153,57,30,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5331010452961672,0.95,0.9090909090909091
machine-translation,9,"Then , based on the trained embedding matrix , one can learn a set of task - specific codes .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,154,58,31,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5365853658536586,0.9666666666666667,0.9393939393939394
machine-translation,9,"As the reconstructed embeddings E( C w ) are not identical to the original embeddings ? ( w ) , the model parameters other than the embedding matrix have to be retrained again .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,155,59,32,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5400696864111498,0.9833333333333333,0.9696969696969697
machine-translation,9,The code learning model can not be jointly trained with the machine translation model as it takes far more iterations for the coding layer to converge to one - hot vectors .,system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,156,60,33,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.5435540069686411,1.0,1.0
machine-translation,9,EXPERIMENTS,experiment,EXPERIMENTS,0,157,1,1,0,experiment : EXPERIMENTS,0.5470383275261324,0.047619047619047616,0.125
machine-translation,9,"In our experiments , we focus on evaluating the maximum loss - free compression rate of word embeddings on two typical NLP tasks : sentiment analysis and machine translation .",experiment,EXPERIMENTS,0,158,2,2,0,experiment : EXPERIMENTS,0.5505226480836237,0.09523809523809523,0.25
machine-translation,9,We compare the model performance and the size of embedding layer with the baseline model and the iterative pruning method .,experiment,EXPERIMENTS,0,159,3,3,0,experiment : EXPERIMENTS,0.554006968641115,0.14285714285714285,0.375
machine-translation,9,Please note that the sizes of other parts in the neural networks are not included in our results .,experiment,EXPERIMENTS,0,160,4,4,0,experiment : EXPERIMENTS,0.5574912891986062,0.19047619047619047,0.5
machine-translation,9,"For dense matrices , we report the size of dumped numpy arrays .",experiment,EXPERIMENTS,0,161,5,5,0,experiment : EXPERIMENTS,0.5609756097560976,0.23809523809523808,0.625
machine-translation,9,"For the sparse matrices , we report the size of dumped compressed sparse column matrices ( csc matrix ) in scipy .",experiment,EXPERIMENTS,0,162,6,6,0,experiment : EXPERIMENTS,0.5644599303135889,0.2857142857142857,0.75
machine-translation,9,All float numbers take 32 bits storage .,experiment,EXPERIMENTS,0,163,7,7,0,experiment : EXPERIMENTS,0.5679442508710801,0.3333333333333333,0.875
machine-translation,9,"We enable the "" compressed "" option when dumping the matrices , without this option , the file size is about 1.1 times bigger .",experiment,EXPERIMENTS,0,164,8,8,0,experiment : EXPERIMENTS,0.5714285714285714,0.38095238095238093,1.0
machine-translation,9,CODE LEARNING,experiment,CODE LEARNING,1,165,9,1,0,experiment : CODE LEARNING,0.5749128919860628,0.42857142857142855,0.07692307692307693
machine-translation,9,"To learn efficient compact codes for each word , our proposed method requires a set of baseline embedding vectors .",experiment,CODE LEARNING,0,166,10,2,0,experiment : CODE LEARNING,0.578397212543554,0.47619047619047616,0.15384615384615385
machine-translation,9,"For the sentiment analysis task , we learn the codes based on the publicly available GloVe vectors .",experiment,CODE LEARNING,0,167,11,3,0,experiment : CODE LEARNING,0.5818815331010453,0.5238095238095238,0.23076923076923078
machine-translation,9,"For the machine translation task , we first train a normal neural machine translation ( NMT ) model to obtain task - specific word embeddings .",experiment,CODE LEARNING,0,168,12,4,0,experiment : CODE LEARNING,0.5853658536585366,0.5714285714285714,0.3076923076923077
machine-translation,9,Then we learn the codes using the pre-trained embeddings .,experiment,CODE LEARNING,0,169,13,5,0,experiment : CODE LEARNING,0.5888501742160279,0.6190476190476191,0.38461538461538464
machine-translation,9,We train the end - to - end network described in Section 4 to learn the codes automatically .,experiment,CODE LEARNING,0,170,14,6,0,experiment : CODE LEARNING,0.5923344947735192,0.6666666666666666,0.46153846153846156
machine-translation,9,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .",experiment,CODE LEARNING,1,171,15,7,0,experiment : CODE LEARNING,0.5958188153310104,0.7142857142857143,0.5384615384615384
machine-translation,9,The network parameters are optimized to minimize the reconstruction loss of the sampled embeddings .,experiment,CODE LEARNING,0,172,16,8,0,experiment : CODE LEARNING,0.5993031358885017,0.7619047619047619,0.6153846153846154
machine-translation,9,"In our experiments , the batch size is set to 128 .",experiment,CODE LEARNING,1,173,17,9,0,experiment : CODE LEARNING,0.6027874564459931,0.8095238095238095,0.6923076923076923
machine-translation,9,We use Adam optimizer with a fixed learning rate of 0.0001 .,experiment,CODE LEARNING,1,174,18,10,0,experiment : CODE LEARNING,0.6062717770034843,0.8571428571428571,0.7692307692307693
machine-translation,9,The training is run for 200K iterations .,experiment,CODE LEARNING,1,175,19,11,0,experiment : CODE LEARNING,0.6097560975609756,0.9047619047619048,0.8461538461538461
machine-translation,9,"Every 1,000 iterations , we examine the loss on a fixed validation set and save the parameters if the loss decreases .",experiment,CODE LEARNING,0,176,20,12,0,experiment : CODE LEARNING,0.6132404181184669,0.9523809523809523,0.9230769230769231
machine-translation,9,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .",experiment,CODE LEARNING,1,177,21,13,0,experiment : CODE LEARNING,0.6167247386759582,1.0,1.0
machine-translation,9,SENTIMENT ANALYSIS,analysis,SENTIMENT ANALYSIS,1,178,1,1,0,analysis : SENTIMENT ANALYSIS,0.6202090592334495,0.011363636363636364,0.03125
machine-translation,9,"Dataset : For sentiment analysis , we use a standard separation of IMDB movie review dataset , which contains 25 k reviews for training and 25 K reviews for testing purpose .",analysis,SENTIMENT ANALYSIS,0,179,2,2,0,analysis : SENTIMENT ANALYSIS,0.6236933797909407,0.022727272727272728,0.0625
machine-translation,9,We lowercase and tokenize all texts with the nltk package .,analysis,SENTIMENT ANALYSIS,0,180,3,3,0,analysis : SENTIMENT ANALYSIS,0.627177700348432,0.03409090909090909,0.09375
machine-translation,9,We choose the 300 - dimensional uncased Glo Ve word vectors ( trained on 42B tokens of Common Crawl data ) as our baseline embeddings .,analysis,SENTIMENT ANALYSIS,0,181,4,4,0,analysis : SENTIMENT ANALYSIS,0.6306620209059234,0.045454545454545456,0.125
machine-translation,9,"The vocabulary for the model training contains all words appears both in the IMDB dataset and the GloVe vocabulary , which results in around 75 K words .",analysis,SENTIMENT ANALYSIS,0,182,5,5,0,analysis : SENTIMENT ANALYSIS,0.6341463414634146,0.056818181818181816,0.15625
machine-translation,9,We truncate the texts of reviews to assure they are not longer than 400 words .,analysis,SENTIMENT ANALYSIS,0,183,6,6,0,analysis : SENTIMENT ANALYSIS,0.6376306620209059,0.06818181818181818,0.1875
machine-translation,9,Model architecture :,analysis,SENTIMENT ANALYSIS,0,184,7,7,0,analysis : SENTIMENT ANALYSIS,0.6411149825783972,0.07954545454545454,0.21875
machine-translation,9,Both the baseline model and the compressed models have the same computational graph except the embedding layer .,analysis,SENTIMENT ANALYSIS,0,185,8,8,0,analysis : SENTIMENT ANALYSIS,0.6445993031358885,0.09090909090909091,0.25
machine-translation,9,The model is composed of a single LSTM layer with 150 hidden units and a softmax layer for predicting the binary label .,analysis,SENTIMENT ANALYSIS,0,186,9,9,0,analysis : SENTIMENT ANALYSIS,0.6480836236933798,0.10227272727272728,0.28125
machine-translation,9,"For the baseline model , the embedding layer contains a large 75 K 300 embedding matrix initialized by GloVe embeddings .",analysis,SENTIMENT ANALYSIS,0,187,10,10,0,analysis : SENTIMENT ANALYSIS,0.6515679442508711,0.11363636363636363,0.3125
machine-translation,9,"For the compressed models based on the compositional coding , the embedding layer maintains a matrix of basis vectors .",analysis,SENTIMENT ANALYSIS,0,188,11,11,0,analysis : SENTIMENT ANALYSIS,0.6550522648083623,0.125,0.34375
machine-translation,9,"Suppose we use a 32 16 coding scheme , the basis matrix will then have a shape of 512 300 , which is initialized by the concatenated weight matrices [ A 1 ; A 2 ; ... ; AM ] in the code learning model .",analysis,SENTIMENT ANALYSIS,0,189,12,12,0,analysis : SENTIMENT ANALYSIS,0.6585365853658537,0.13636363636363635,0.375
machine-translation,9,The embedding parameters for both models remain fixed during the training .,analysis,SENTIMENT ANALYSIS,0,190,13,13,0,analysis : SENTIMENT ANALYSIS,0.662020905923345,0.14772727272727273,0.40625
machine-translation,9,"For the models with network pruning , the sparse embedding matrix is finetuned during training .",analysis,SENTIMENT ANALYSIS,0,191,14,14,0,analysis : SENTIMENT ANALYSIS,0.6655052264808362,0.1590909090909091,0.4375
machine-translation,9,Training details :,analysis,SENTIMENT ANALYSIS,0,192,15,15,0,analysis : SENTIMENT ANALYSIS,0.6689895470383276,0.17045454545454544,0.46875
machine-translation,9,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,analysis,SENTIMENT ANALYSIS,1,193,16,16,0,analysis : SENTIMENT ANALYSIS,0.6724738675958188,0.18181818181818182,0.5
machine-translation,9,"At the end of each epoch , we evaluate the loss on a small validation set .",analysis,SENTIMENT ANALYSIS,0,194,17,17,0,analysis : SENTIMENT ANALYSIS,0.6759581881533101,0.19318181818181818,0.53125
machine-translation,9,The parameters with lowest validation loss are saved .,analysis,SENTIMENT ANALYSIS,0,195,18,18,0,analysis : SENTIMENT ANALYSIS,0.6794425087108014,0.20454545454545456,0.5625
machine-translation,9,"Results : For different settings of the number of components M and the number of codewords K , we train the code learning network .",analysis,SENTIMENT ANALYSIS,0,196,19,19,0,analysis : SENTIMENT ANALYSIS,0.6829268292682927,0.2159090909090909,0.59375
machine-translation,9,The average reconstruction loss on a fixed validation set is summarized in the left of .,analysis,SENTIMENT ANALYSIS,0,197,20,20,0,analysis : SENTIMENT ANALYSIS,0.686411149825784,0.22727272727272727,0.625
machine-translation,9,"For reference , we also report the total size ( MB ) of the embedding layer in the right table , which includes the sizes of the basis matrix and the hash table .",analysis,SENTIMENT ANALYSIS,0,198,21,21,0,analysis : SENTIMENT ANALYSIS,0.6898954703832753,0.23863636363636365,0.65625
machine-translation,9,We can see that increasing either M or K can effectively decrease the reconstruction loss .,analysis,SENTIMENT ANALYSIS,0,199,22,22,0,analysis : SENTIMENT ANALYSIS,0.6933797909407665,0.25,0.6875
machine-translation,9,"However , setting M to a large number will result in longer hash codes , thus significantly increase the size of the embedding layer .",analysis,SENTIMENT ANALYSIS,0,200,23,23,0,analysis : SENTIMENT ANALYSIS,0.6968641114982579,0.26136363636363635,0.71875
machine-translation,9,"Hence , it is important to choose correct numbers for M and K to balance the performance and model size .",analysis,SENTIMENT ANALYSIS,0,201,24,24,0,analysis : SENTIMENT ANALYSIS,0.7003484320557491,0.2727272727272727,0.75
machine-translation,9,"To see how the reconstructed loss translates to the classification accuracy , we train the sentiment analysis model for different settings of code schemes and report the results in : Trade - off between the model performance and the size of embedding layer on IMDB sentiment analysis task",analysis,SENTIMENT ANALYSIS,0,202,25,25,0,analysis : SENTIMENT ANALYSIS,0.7038327526132404,0.2840909090909091,0.78125
machine-translation,9,We also show the results using normalized product quantization ( NPQ ) .,analysis,SENTIMENT ANALYSIS,0,203,26,26,0,analysis : SENTIMENT ANALYSIS,0.7073170731707317,0.29545454545454547,0.8125
machine-translation,9,"We quantize the filtered Glo Ve embeddings with the codes provided by the authors , and train the models based on the quantized embeddings .",analysis,SENTIMENT ANALYSIS,0,204,27,27,0,analysis : SENTIMENT ANALYSIS,0.710801393728223,0.3068181818181818,0.84375
machine-translation,9,"To make the results comparable , we report the codebook size in numpy format .",analysis,SENTIMENT ANALYSIS,0,205,28,28,0,analysis : SENTIMENT ANALYSIS,0.7142857142857143,0.3181818181818182,0.875
machine-translation,9,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",analysis,SENTIMENT ANALYSIS,1,206,29,29,0,analysis : SENTIMENT ANALYSIS,0.7177700348432056,0.32954545454545453,0.90625
machine-translation,9,"In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",analysis,SENTIMENT ANALYSIS,0,207,30,30,0,analysis : SENTIMENT ANALYSIS,0.7212543554006968,0.3409090909090909,0.9375
machine-translation,9,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,analysis,SENTIMENT ANALYSIS,1,208,31,31,0,analysis : SENTIMENT ANALYSIS,0.7247386759581882,0.3522727272727273,0.96875
machine-translation,9,The improved model performance maybe a byproduct of the strong regularization .,analysis,SENTIMENT ANALYSIS,0,209,32,32,0,analysis : SENTIMENT ANALYSIS,0.7282229965156795,0.36363636363636365,1.0
machine-translation,9,MACHINE TRANSLATION,analysis,MACHINE TRANSLATION,1,210,33,1,0,analysis : MACHINE TRANSLATION,0.7317073170731707,0.375,0.03125
machine-translation,9,"Dataset : For machine translation tasks , we experiment on IWSLT 2014 German - to - English translation task and ASPEC English - to - Japanese translation task .",analysis,MACHINE TRANSLATION,0,211,34,2,0,analysis : MACHINE TRANSLATION,0.735191637630662,0.38636363636363635,0.0625
machine-translation,9,"The IWSLT14 training data contains 178K sentence pairs , which is a small dataset for machine translation .",analysis,MACHINE TRANSLATION,0,212,35,3,0,analysis : MACHINE TRANSLATION,0.7386759581881533,0.3977272727272727,0.09375
machine-translation,9,We utilize moses toolkit to tokenize and lowercase both sides of the texts .,analysis,MACHINE TRANSLATION,0,213,36,4,0,analysis : MACHINE TRANSLATION,0.7421602787456446,0.4090909090909091,0.125
machine-translation,9,Then we concatenate all five TED / TEDx development and test corpus to form a test set containing 6750 sentence pairs .,analysis,MACHINE TRANSLATION,0,214,37,5,0,analysis : MACHINE TRANSLATION,0.7456445993031359,0.42045454545454547,0.15625
machine-translation,9,We apply byte - pair encoding to transform the texts to subword level so that the vocabulary has a size of 20 K for each language .,analysis,MACHINE TRANSLATION,0,215,38,6,0,analysis : MACHINE TRANSLATION,0.7491289198606271,0.4318181818181818,0.1875
machine-translation,9,"For evaluation , we report tokenized BLEU using "" multi -bleu.perl "" .",analysis,MACHINE TRANSLATION,0,216,39,7,0,analysis : MACHINE TRANSLATION,0.7526132404181185,0.4431818181818182,0.21875
machine-translation,9,The ASPEC dataset contains 300M bilingual pairs in the training data with the automatically estimated quality scores provided for each pair .,analysis,MACHINE TRANSLATION,0,217,40,8,0,analysis : MACHINE TRANSLATION,0.7560975609756098,0.45454545454545453,0.25
machine-translation,9,We only use the first 150M pairs for training the models .,analysis,MACHINE TRANSLATION,0,218,41,9,0,analysis : MACHINE TRANSLATION,0.759581881533101,0.4659090909090909,0.28125
machine-translation,9,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea .,analysis,MACHINE TRANSLATION,0,219,42,10,0,analysis : MACHINE TRANSLATION,0.7630662020905923,0.4772727272727273,0.3125
machine-translation,9,The vocabulary size for each language is reduced to 40K using byte - pair encoding .,analysis,MACHINE TRANSLATION,0,220,43,11,0,analysis : MACHINE TRANSLATION,0.7665505226480837,0.48863636363636365,0.34375
machine-translation,9,The evaluation is performed using a standard kytea - based post -processing script for this dataset .,analysis,MACHINE TRANSLATION,0,221,44,12,0,analysis : MACHINE TRANSLATION,0.7700348432055749,0.5,0.375
machine-translation,9,Model architecture :,analysis,MACHINE TRANSLATION,0,222,45,13,0,analysis : MACHINE TRANSLATION,0.7735191637630662,0.5113636363636364,0.40625
machine-translation,9,"In our preliminary experiments , we found a 32 16 coding works well for a vanilla NMT model .",analysis,MACHINE TRANSLATION,0,223,46,14,0,analysis : MACHINE TRANSLATION,0.7770034843205574,0.5227272727272727,0.4375
machine-translation,9,"As it is more meaningful to test on a high - performance model , we applied several techniques to improve the performance .",analysis,MACHINE TRANSLATION,0,224,47,15,0,analysis : MACHINE TRANSLATION,0.7804878048780488,0.5340909090909091,0.46875
machine-translation,9,The model has a standard bi-directional encoder composed of two LSTM layers similar to .,analysis,MACHINE TRANSLATION,0,225,48,16,0,analysis : MACHINE TRANSLATION,0.7839721254355401,0.5454545454545454,0.5
machine-translation,9,The decoder contains two LSTM layers .,analysis,MACHINE TRANSLATION,0,226,49,17,0,analysis : MACHINE TRANSLATION,0.7874564459930313,0.5568181818181818,0.53125
machine-translation,9,Residual connection with a scaling factor of 1 / 2 is applied to the two decoder states to compute the outputs .,analysis,MACHINE TRANSLATION,0,227,50,18,0,analysis : MACHINE TRANSLATION,0.7909407665505227,0.5681818181818182,0.5625
machine-translation,9,All LSTMs and embeddings have 256 hidden units in the IWSLT14 task and 1000 hidden units in ASPEC task .,analysis,MACHINE TRANSLATION,0,228,51,19,0,analysis : MACHINE TRANSLATION,0.794425087108014,0.5795454545454546,0.59375
machine-translation,9,The decoder states are firstly linearly transformed to 600 - dimensional vectors before computing the final softmax .,analysis,MACHINE TRANSLATION,0,229,52,20,0,analysis : MACHINE TRANSLATION,0.7979094076655052,0.5909090909090909,0.625
machine-translation,9,Dropout with a rate of 0.2 is applied everywhere except the recurrent computation .,analysis,MACHINE TRANSLATION,0,230,53,21,0,analysis : MACHINE TRANSLATION,0.8013937282229965,0.6022727272727273,0.65625
machine-translation,9,"We apply Key - Value Attention to the first decoder , where the query is the sum of the feedback embedding and the previous decoder state and the keys are computed by linear transformation of encoder states .",analysis,MACHINE TRANSLATION,0,231,54,22,0,analysis : MACHINE TRANSLATION,0.8048780487804879,0.6136363636363636,0.6875
machine-translation,9,Training details :,analysis,MACHINE TRANSLATION,0,232,55,23,0,analysis : MACHINE TRANSLATION,0.8083623693379791,0.625,0.71875
machine-translation,9,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,analysis,MACHINE TRANSLATION,1,233,56,24,0,analysis : MACHINE TRANSLATION,0.8118466898954704,0.6363636363636364,0.75
machine-translation,9,"We evaluate the smoothed BLEU ) on a validation set composed of 50 batches every 7,000 iterations .",analysis,MACHINE TRANSLATION,0,234,57,25,0,analysis : MACHINE TRANSLATION,0.8153310104529616,0.6477272727272727,0.78125
machine-translation,9,The learning rate is reduced by a factor of 10 if no improvement is observed in 3 validation runs .,analysis,MACHINE TRANSLATION,0,235,58,26,0,analysis : MACHINE TRANSLATION,0.818815331010453,0.6590909090909091,0.8125
machine-translation,9,The training ends after the learning rate is reduced three times .,analysis,MACHINE TRANSLATION,0,236,59,27,0,analysis : MACHINE TRANSLATION,0.8222996515679443,0.6704545454545454,0.84375
machine-translation,9,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .",analysis,MACHINE TRANSLATION,1,237,60,28,0,analysis : MACHINE TRANSLATION,0.8257839721254355,0.6818181818181818,0.875
machine-translation,9,We firstly train a baseline NMT model to obtain the task - specific embeddings for all in - vocabulary words in both languages .,analysis,MACHINE TRANSLATION,0,238,61,29,0,analysis : MACHINE TRANSLATION,0.8292682926829268,0.6931818181818182,0.90625
machine-translation,9,"Then based on these baseline embeddings , we obtain the hash codes and basis vectors by training the code learning model .",analysis,MACHINE TRANSLATION,0,239,62,30,0,analysis : MACHINE TRANSLATION,0.8327526132404182,0.7045454545454546,0.9375
machine-translation,9,"Finally , the NMT models using compositional coding are retrained by plugging in the reconstructed embeddings .",analysis,MACHINE TRANSLATION,0,240,63,31,0,analysis : MACHINE TRANSLATION,0.8362369337979094,0.7159090909090909,0.96875
machine-translation,9,"Note that the embedding layer is fixed in this phase , other parameters are retrained from random initial values .",analysis,MACHINE TRANSLATION,0,241,64,32,0,analysis : MACHINE TRANSLATION,0.8397212543554007,0.7272727272727273,1.0
machine-translation,9,Results :,analysis,Results:,0,242,65,1,0,analysis : Results:,0.8432055749128919,0.7386363636363636,0.09090909090909091
machine-translation,9,The experimental results are summarized in .,analysis,Results:,0,243,66,2,0,analysis : Results:,0.8466898954703833,0.75,0.18181818181818182
machine-translation,9,All translations are decoded by the beam search with a beam size of 5 .,analysis,Results:,0,244,67,3,0,analysis : Results:,0.8501742160278746,0.7613636363636364,0.2727272727272727
machine-translation,9,The performance of iterative pruning varies between tasks .,analysis,Results:,0,245,68,4,0,analysis : Results:,0.8536585365853658,0.7727272727272727,0.36363636363636365
machine-translation,9,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,analysis,Results:,1,246,69,5,0,analysis : Results:,0.8571428571428571,0.7840909090909091,0.45454545454545453
machine-translation,9,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .",analysis,Results:,1,247,70,6,0,analysis : Results:,0.8606271777003485,0.7954545454545454,0.5454545454545454
machine-translation,9,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .",analysis,Results:,1,248,71,7,0,analysis : Results:,0.8641114982578397,0.8068181818181818,0.6363636363636364
machine-translation,9,"Similar to the sentiment analysis task , a significant performance improvement can be observed by slightly lowering the compression rate .",analysis,Results:,0,249,72,8,0,analysis : Results:,0.867595818815331,0.8181818181818182,0.7272727272727273
machine-translation,9,"Note that the sizes of NMT models are still quite large due to the big softmax layer and the recurrent layers , which are not reported in the , we show some examples of learned codes based on the 300 - dimensional uncased GloVe embeddings used in the sentiment analysis task .",analysis,Results:,0,250,73,9,0,analysis : Results:,0.8710801393728222,0.8295454545454546,0.8181818181818182
machine-translation,9,We can see that the model learned to assign similar codes to the words with similar meanings .,analysis,Results:,0,251,74,10,0,analysis : Results:,0.8745644599303136,0.8409090909090909,0.9090909090909091
machine-translation,9,"Such a code - sharing mechanism can significantly reduce the redundancy of the word embeddings , thus helping to achieve a high compression rate .",analysis,Results:,0,252,75,11,0,analysis : Results:,0.8780487804878049,0.8522727272727273,1.0
machine-translation,9,ANALYSIS OF CODE EFFICIENCY,analysis,ANALYSIS OF CODE EFFICIENCY,0,253,76,1,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.8815331010452961,0.8636363636363636,0.07692307692307693
machine-translation,9,"Besides the performance , we also care about the storage efficiency of the codes .",analysis,ANALYSIS OF CODE EFFICIENCY,0,254,77,2,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.8850174216027874,0.875,0.15384615384615385
machine-translation,9,"In the ideal situation , all codewords shall be fully utilized to convey a fraction of meaning .",analysis,ANALYSIS OF CODE EFFICIENCY,0,255,78,3,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.8885017421602788,0.8863636363636364,0.23076923076923078
machine-translation,9,"However , as the codes are category word 8 8 code 16 16 code dog 0 7 0 1 7 3 7 0 7 7 0 8 3 5 8 5 B 2 E E 0 B 0 A animal cat 7 7 0 1 7 3 7 0 7 7 2 8 B 5 8 CB 2 E E 4 B 0 A penguin 0 7 0 1 7 3 6 0 7 7 E 8 7 6 4 CF DE 3 D 8 0 A go 7 7 0 6 4 3 3 0 2 C C 8 2 C 1 1 B D 0 E 0 B 5 8 verb went 4 0 7 6 4 3 2 0 BC C 6 BC 7 5 B 8 6 E 0 D 0 4 gone 7 7 0 6 4 3 3 0 2 C C 8 0 B 1 5 B D 6 E 0 2 5 A : Examples of learned compositional codes based on Glo Ve embedding vectors automatically learned , it is possible that some codewords are abandoned during the training .",analysis,ANALYSIS OF CODE EFFICIENCY,0,256,79,4,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.89198606271777,0.8977272727272727,0.3076923076923077
machine-translation,9,"In extreme cases , some "" dead "" codewords can be used by none of the words .",analysis,ANALYSIS OF CODE EFFICIENCY,0,257,80,5,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.8954703832752613,0.9090909090909091,0.38461538461538464
machine-translation,9,"To analyze the code efficiency , we count the number of words that contain a specific subcode in each component .",analysis,ANALYSIS OF CODE EFFICIENCY,0,258,81,6,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.8989547038327527,0.9204545454545454,0.46153846153846156
machine-translation,9,gives a visualization of the code balance for three coding schemes .,analysis,ANALYSIS OF CODE EFFICIENCY,0,259,82,7,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9024390243902439,0.9318181818181818,0.5384615384615384
machine-translation,9,Each column shows the counts of the subcodes of a specific component .,analysis,ANALYSIS OF CODE EFFICIENCY,0,260,83,8,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9059233449477352,0.9431818181818182,0.6153846153846154
machine-translation,9,"In our experiments , when using a 8 8 coding scheme , we found 31 % of the words have a subcode "" 0 "" for the first component , while the subcode "" 1 "" is only used by 5 % of the words .",analysis,ANALYSIS OF CODE EFFICIENCY,0,261,84,9,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9094076655052264,0.9545454545454546,0.6923076923076923
machine-translation,9,The assignment of codes is more balanced for larger coding schemes .,analysis,ANALYSIS OF CODE EFFICIENCY,0,262,85,10,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9128919860627178,0.9659090909090909,0.7692307692307693
machine-translation,9,"In any coding scheme , even the most unpopular codeword is used by about 1000 words .",analysis,ANALYSIS OF CODE EFFICIENCY,0,263,86,11,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9163763066202091,0.9772727272727273,0.8461538461538461
machine-translation,9,This result indicates that the code learning model is capable of assigning codes efficiently without wasting a codeword .,analysis,ANALYSIS OF CODE EFFICIENCY,0,264,87,12,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9198606271777003,0.9886363636363636,0.9230769230769231
machine-translation,9,The results show that any codeword is assigned to more than 1000 words without wasting .,analysis,ANALYSIS OF CODE EFFICIENCY,0,265,88,13,0,analysis : ANALYSIS OF CODE EFFICIENCY,0.9233449477351916,1.0,1.0
machine-translation,9,CONCLUSION,conclusion,CONCLUSION,0,266,1,1,0,conclusion : CONCLUSION,0.926829268292683,0.045454545454545456,0.045454545454545456
machine-translation,9,"In this work , we propose a novel method for reducing the number of parameters required in word embeddings .",conclusion,CONCLUSION,0,267,2,2,0,conclusion : CONCLUSION,0.9303135888501742,0.09090909090909091,0.09090909090909091
machine-translation,9,"Instead of assigning each unique word an embedding vector , we compose the embedding vectors using a small set of basis vectors .",conclusion,CONCLUSION,0,268,3,3,0,conclusion : CONCLUSION,0.9337979094076655,0.13636363636363635,0.13636363636363635
machine-translation,9,The selection of basis vectors is governed by the hash code of each word .,conclusion,CONCLUSION,0,269,4,4,0,conclusion : CONCLUSION,0.9372822299651568,0.18181818181818182,0.18181818181818182
machine-translation,9,We apply the compositional coding approach to maximize the storage efficiency .,conclusion,CONCLUSION,0,270,5,5,0,conclusion : CONCLUSION,0.9407665505226481,0.22727272727272727,0.22727272727272727
machine-translation,9,The proposed method works by eliminating the redundancy inherent in representing similar words with independent embeddings .,conclusion,CONCLUSION,0,271,6,6,0,conclusion : CONCLUSION,0.9442508710801394,0.2727272727272727,0.2727272727272727
machine-translation,9,"In our work , we propose a simple way to directly learn the discrete codes in a neural network with Gumbel - softmax trick .",conclusion,CONCLUSION,0,272,7,7,0,conclusion : CONCLUSION,0.9477351916376306,0.3181818181818182,0.3181818181818182
machine-translation,9,The results show that the size of the embedding layer was reduced by 98 % in IMDB sentiment analysis task and 94 % ? 99 % in machine translation tasks without affecting the performance .,conclusion,CONCLUSION,0,273,8,8,0,conclusion : CONCLUSION,0.9512195121951219,0.36363636363636365,0.36363636363636365
machine-translation,9,Our approach achieves a high loss - free compression rate by considering the semantic inter-similarity among different words .,conclusion,CONCLUSION,0,274,9,9,0,conclusion : CONCLUSION,0.9547038327526133,0.4090909090909091,0.4090909090909091
machine-translation,9,"In qualitative analysis , we found the learned codes of similar words are very close in Hamming space .",conclusion,CONCLUSION,0,275,10,10,0,conclusion : CONCLUSION,0.9581881533101045,0.45454545454545453,0.45454545454545453
machine-translation,9,"As our approach maintains a dense basis matrix , it has the potential to be further compressed by applying pruning techniques to the dense matrix .",conclusion,CONCLUSION,0,276,11,11,0,conclusion : CONCLUSION,0.9616724738675958,0.5,0.5
machine-translation,9,The advantage of compositional coding approach will be more significant if the size of embedding layer is dominated by the hash codes .,conclusion,CONCLUSION,0,277,12,12,0,conclusion : CONCLUSION,0.9651567944250871,0.5454545454545454,0.5454545454545454
machine-translation,9,"Huei - Fang Yang , Kevin Lin , and Chu - Song Chen .",conclusion,CONCLUSION,0,278,13,13,0,conclusion : CONCLUSION,0.9686411149825784,0.5909090909090909,0.5909090909090909
machine-translation,9,Supervised learning of semantics - preserving hash via deep convolutional neural networks .,conclusion,CONCLUSION,0,279,14,14,0,conclusion : CONCLUSION,0.9721254355400697,0.6363636363636364,0.6363636363636364
machine-translation,9,"IEEE transactions on pattern analysis and machine intelligence , 2017 .",conclusion,CONCLUSION,0,280,15,15,0,conclusion : CONCLUSION,0.975609756097561,0.6818181818181818,0.6818181818181818
machine-translation,9,"Xiaowei Zhang , Wei Chen , Feng Wang , Shuang Xu , and Bo Xu.",conclusion,CONCLUSION,0,281,16,16,0,conclusion : CONCLUSION,0.9790940766550522,0.7272727272727273,0.7272727272727273
machine-translation,9,Towards compact and fast neural machine translation using a combined method .,conclusion,CONCLUSION,0,282,17,17,0,conclusion : CONCLUSION,0.9825783972125436,0.7727272727272727,0.7727272727272727
machine-translation,9,"In EMNLP , 2017 .",conclusion,CONCLUSION,0,283,18,18,0,conclusion : CONCLUSION,0.9860627177700348,0.8181818181818182,0.8181818181818182
machine-translation,9,"Aojun Zhou , Anbang Yao , Yiwen Guo , Lin Xu , and Yurong Chen .",conclusion,CONCLUSION,0,284,19,19,0,conclusion : CONCLUSION,0.9895470383275261,0.8636363636363636,0.8636363636363636
machine-translation,9,Incremental network quantization :,conclusion,CONCLUSION,0,285,20,20,0,conclusion : CONCLUSION,0.9930313588850174,0.9090909090909091,0.9090909090909091
machine-translation,9,Towards lossless cnns with low - precision weights .,conclusion,CONCLUSION,0,286,21,21,0,conclusion : CONCLUSION,0.9965156794425087,0.9545454545454546,0.9545454545454546
machine-translation,9,"CoRR , abs /1702.03044 , 2017 .",conclusion,CONCLUSION,0,287,22,22,0,conclusion : CONCLUSION,1.0,1.0,1.0
named-entity-recognition,0,"0,000 + Times Accelerated Robust Subset Selection ( ARSS )",title,title,1,2,1,1,0,title : title,0.007380073800738007,1.0,1.0
named-entity-recognition,0,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01107011070110701,0.125,0.125
named-entity-recognition,0,Subset selection from massive data with noised information is increasingly popular for various applications .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.014760147601476014,0.25,0.25
named-entity-recognition,0,This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.01845018450184502,0.375,0.375
named-entity-recognition,0,"To address the above two issues , we propose an accelerated robust subset selection ( ARSS ) method .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02214022140221402,0.5,0.5
named-entity-recognition,0,"Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.025830258302583026,0.625,0.625
named-entity-recognition,0,"As a result , the robustness against outlier elements is greatly enhanced .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02952029520295203,0.75,0.75
named-entity-recognition,0,"Actually , data size is generally much larger than feature length , i.e. N L. Based on this observation , we propose a speedup solver ( via ALM and equivalent derivations ) to highly reduce the computational cost , theoretically from ON 4 to ON 2 L .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.033210332103321034,0.875,0.875
named-entity-recognition,0,"Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods , but also runs 10,000 + times faster than the most related method .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.03690036900369004,1.0,1.0
named-entity-recognition,0,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.04059040590405904,0.08333333333333333,0.08333333333333333
named-entity-recognition,0,"Due to the explosive growth of data , subset selection methods are increasingly popular for a wide range of machine learning and computer vision applications .",introduction,introduction,0,12,2,2,0,introduction : introduction,0.04428044280442804,0.16666666666666666,0.16666666666666666
named-entity-recognition,0,This kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.04797047970479705,0.25,0.25
named-entity-recognition,0,"By analyzing a few , we can roughly know all .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05166051660516605,0.3333333333333333,0.3333333333333333
named-entity-recognition,0,"Such case is very important to summarize and visualize huge datasets of texts , images and videos etc . .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.055350553505535055,0.4166666666666667,0.4166666666666667
named-entity-recognition,0,"Besides , by only using the selected exemplars for succeeding tasks , the cost of memories and computational time will be greatly reduced .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.05904059040590406,0.5,0.5
named-entity-recognition,0,"Additionally , as outliers are generally less representative , the side effect of outliers will be reduced , thus boosting the performance of subsequent applications .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.06273062730627306,0.5833333333333334,0.5833333333333334
named-entity-recognition,0,There have been several subset selection methods .,introduction,introduction,0,18,8,8,0,introduction : introduction,0.06642066420664207,0.6666666666666666,0.6666666666666666
named-entity-recognition,0,The most intuitional method is to randomly select a fixed number of samples .,introduction,introduction,0,19,9,9,0,introduction : introduction,0.07011070110701106,0.75,0.75
named-entity-recognition,0,"Although highly efficient , there is no guarantee for an effective selection .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.07380073800738007,0.8333333333333334,0.8333333333333334
named-entity-recognition,0,"For the other methods , depending on the mechanism of representative exemplars , there are mainly three categories of selection methods .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.07749077490774908,0.9166666666666666,0.9166666666666666
named-entity-recognition,0,One category Data size ( N ) Selection Time,introduction,introduction,0,22,12,12,0,introduction : introduction,0.08118081180811808,1.0,1.0
named-entity-recognition,0,Classifiers,system description,Classifiers,0,23,1,1,0,system description : Classifiers,0.08487084870848709,0.0056179775280898875,0.02564102564102564
named-entity-recognition,0,Classification Accuracy Performance TED RRSSNie RRSSour ARSSour : Comparisons of four algorithms on Optdigit .,system description,Classifiers,0,24,2,2,0,system description : Classifiers,0.08856088560885608,0.011235955056179775,0.05128205128205128
named-entity-recognition,0,Two conclusions can be drawn .,system description,Classifiers,0,25,3,3,0,system description : Classifiers,0.09225092250922509,0.016853932584269662,0.07692307692307693
named-entity-recognition,0,"First , our method ( ARSSour ) is highly faster than all others ; with the help of an elegant new theorem , RRSSour is significantly faster than the authorial algorithm RRSSNie.",system description,Classifiers,0,26,4,4,0,system description : Classifiers,0.0959409594095941,0.02247191011235955,0.10256410256410256
named-entity-recognition,0,"Second , ARSSour achieves highly promising prediction accuracies .",system description,Classifiers,0,27,5,5,0,system description : Classifiers,0.0996309963099631,0.028089887640449437,0.1282051282051282
named-entity-recognition,0,relies on the assumption that the data points lie in one or multiple low - dimensional subspaces .,system description,Classifiers,0,28,6,6,0,system description : Classifiers,0.1033210332103321,0.033707865168539325,0.15384615384615385
named-entity-recognition,0,"Specifically , the Rank Revealing QR ( RRQR ) selects the subsets that give the best conditional sub-matrix .",system description,Classifiers,0,29,7,7,0,system description : Classifiers,0.1070110701107011,0.03932584269662921,0.1794871794871795
named-entity-recognition,0,"Unfortunately , this method has suboptimal properties , as it is not assured to find the globally optimum in polynomial time .",system description,Classifiers,0,30,8,8,0,system description : Classifiers,0.11070110701107011,0.0449438202247191,0.20512820512820512
named-entity-recognition,0,Another category assumes that the samples are distributed around centers .,system description,Classifiers,0,31,9,9,0,system description : Classifiers,0.11439114391143912,0.05056179775280899,0.23076923076923078
named-entity-recognition,0,The center or its nearest neighbour are selected as exemplars .,system description,Classifiers,0,32,10,10,0,system description : Classifiers,0.11808118081180811,0.056179775280898875,0.2564102564102564
named-entity-recognition,0,"Perhaps , Kmeans and Kmedoids are the most typical methods ( Kmedoids is a variant of Kmeans ) .",system description,Classifiers,0,33,11,11,0,system description : Classifiers,0.12177121771217712,0.06179775280898876,0.28205128205128205
named-entity-recognition,0,Both methods employ an EM - like algorithm .,system description,Classifiers,0,34,12,12,0,system description : Classifiers,0.12546125461254612,0.06741573033707865,0.3076923076923077
named-entity-recognition,0,"Thus , the results depend tightly on the initialization , and they are highly unstable for large K ( i.e. the number of centers or selected samples ) .",system description,Classifiers,0,35,13,13,0,system description : Classifiers,0.12915129151291513,0.07303370786516854,0.3333333333333333
named-entity-recognition,0,"Recently , there are a few methods that assume exemplars are the samples that can best represent the whole dataset .",system description,Classifiers,0,36,14,14,0,system description : Classifiers,0.13284132841328414,0.07865168539325842,0.358974358974359
named-entity-recognition,0,"However , for , the optimization is a combinatorial problem ( NP - hard ) , which is computationally intractable to solve .",system description,Classifiers,0,37,15,15,0,system description : Classifiers,0.13653136531365315,0.08426966292134831,0.38461538461538464
named-entity-recognition,0,"Besides , the representation loss is measured by the least square measure , which is sensitive to outliers in data .",system description,Classifiers,0,38,16,16,0,system description : Classifiers,0.14022140221402213,0.0898876404494382,0.41025641025641024
named-entity-recognition,0,"Then improves by employing a robust loss via the 2 , 1 - norm ; the 1 - norm is applied to samples , and the 2 - norm is used for features .",system description,Classifiers,0,39,17,17,0,system description : Classifiers,0.14391143911439114,0.09550561797752809,0.4358974358974359
named-entity-recognition,0,"In this way , the side effect of outlier samples is relieved .",system description,Classifiers,0,40,18,18,0,system description : Classifiers,0.14760147601476015,0.10112359550561797,0.46153846153846156
named-entity-recognition,0,The solver of ) is theoretically perfect due to its ability of convergence to global optima .,system description,Classifiers,0,41,19,19,0,system description : Classifiers,0.15129151291512916,0.10674157303370786,0.48717948717948717
named-entity-recognition,0,"Unfortunately , in terms of computational costs , the solver is highly complex .",system description,Classifiers,0,42,20,20,0,system description : Classifiers,0.15498154981549817,0.11235955056179775,0.5128205128205128
named-entity-recognition,0,It takes ON 4 for one iteration as shown in .,system description,Classifiers,0,43,21,21,0,system description : Classifiers,0.15867158671586715,0.11797752808988764,0.5384615384615384
named-entity-recognition,0,This is infeasible for the case of large N ( e.g. it takes 2000 + hours for a case of N = 13000 ) .,system description,Classifiers,0,44,22,22,0,system description : Classifiers,0.16236162361623616,0.12359550561797752,0.5641025641025641
named-entity-recognition,0,"Moreover , the representation loss is only robust against outlier samples .",system description,Classifiers,0,45,23,23,0,system description : Classifiers,0.16605166051660517,0.12921348314606743,0.5897435897435898
named-entity-recognition,0,"Such case is worth improvement , as there may exist outlier elements in real data .",system description,Classifiers,0,46,24,24,0,system description : Classifiers,0.16974169741697417,0.1348314606741573,0.6153846153846154
named-entity-recognition,0,Contributions .,system description,Classifiers,0,47,25,25,0,system description : Classifiers,0.17343173431734318,0.1404494382022472,0.6410256410256411
named-entity-recognition,0,"In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .",system description,Classifiers,1,48,26,26,0,system description : Classifiers,0.17712177121771217,0.14606741573033707,0.6666666666666666
named-entity-recognition,0,"To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .",system description,Classifiers,1,49,27,27,0,system description : Classifiers,0.18081180811808117,0.15168539325842698,0.6923076923076923
named-entity-recognition,0,"As a result , the robustness against outliers is greatly boosted .",system description,Classifiers,0,50,28,28,0,system description : Classifiers,0.18450184501845018,0.15730337078651685,0.717948717948718
named-entity-recognition,0,"Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .",system description,Classifiers,1,51,29,29,0,system description : Classifiers,0.1881918819188192,0.16292134831460675,0.7435897435897436
named-entity-recognition,0,The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .,system description,Classifiers,1,52,30,30,0,system description : Classifiers,0.1918819188191882,0.16853932584269662,0.7692307692307693
named-entity-recognition,0,"Via them , we reduce the computational complexity from ON 4 to ON 2 L .",system description,Classifiers,0,53,31,31,0,system description : Classifiers,0.19557195571955718,0.17415730337078653,0.7948717948717948
named-entity-recognition,0,"Extensive results on ten benchmark datasets demonstrate that in average , our method is 10,000 + times faster than Nie 's method .",system description,Classifiers,0,54,32,32,0,system description : Classifiers,0.1992619926199262,0.1797752808988764,0.8205128205128205
named-entity-recognition,0,The selection quality is highly encouraging as shown in .,system description,Classifiers,0,55,33,33,0,system description : Classifiers,0.2029520295202952,0.1853932584269663,0.8461538461538461
named-entity-recognition,0,"Additionally , via another equivalent derivation , we give an accelerated solver for Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 as listed in , empirically obtaining a 500 + times speedup compared with the authorial solver .",system description,Classifiers,0,56,34,34,0,system description : Classifiers,0.2066420664206642,0.19101123595505617,0.8717948717948718
named-entity-recognition,0,Notations .,system description,Classifiers,0,57,35,35,0,system description : Classifiers,0.21033210332103322,0.19662921348314608,0.8974358974358975
named-entity-recognition,0,We use boldface uppercase letters to denote matrices and boldface lowercase letters to represent vectors .,system description,Classifiers,0,58,36,36,0,system description : Classifiers,0.2140221402214022,0.20224719101123595,0.9230769230769231
named-entity-recognition,0,"For a matrix Y = [ Y ln ] ? R LN , we denote it s l throw and n th column as y land y n respectively .",system description,Classifiers,0,59,37,37,0,system description : Classifiers,0.2177121771217712,0.20786516853932585,0.9487179487179487
named-entity-recognition,0,"For a matrix Y = [ Y ln ] ? R LN , we denote it s l throw and n th column as y land y n respectively .",system description,Classifiers,0,60,38,38,0,system description : Classifiers,0.22140221402214022,0.21348314606741572,0.9743589743589743
named-entity-recognition,0,"The 2 ,1 - norm of a matrix is defined as",system description,Classifiers,0,61,39,39,0,system description : Classifiers,0.22509225092250923,0.21910112359550563,1.0
named-entity-recognition,0,Subset Selection via Self - Representation,system description,Subset Selection via Self-Representation,0,62,40,1,0,system description : Subset Selection via Self-Representation,0.22878228782287824,0.2247191011235955,0.02702702702702703
named-entity-recognition,0,"In the problem of subset selection , we are often given a set of N unlabelled points X = x 1 , x 2 , , x N | x n ? R L , where L is the feature length .",system description,Subset Selection via Self-Representation,0,63,41,2,0,system description : Subset Selection via Self-Representation,0.23247232472324722,0.2303370786516854,0.05405405405405406
named-entity-recognition,0,"In the problem of subset selection , we are often given a set of N unlabelled points X = x 1 , x 2 , , x N | x n ? R L , where L is the feature length .",system description,Subset Selection via Self-Representation,0,64,42,3,0,system description : Subset Selection via Self-Representation,0.23616236162361623,0.23595505617977527,0.08108108108108109
named-entity-recognition,0,The goal is to select the top K ( K N ) most representative and informative samples ( i.e. exemplars ) to effectively describe the entire dataset X .,system description,Subset Selection via Self-Representation,0,65,43,4,0,system description : Subset Selection via Self-Representation,0.23985239852398524,0.24157303370786518,0.10810810810810811
named-entity-recognition,0,"By solely using these K exemplars for subsequent tasks , we could greatly reduce the computational costs and largely alleviate the side effects of outlier elements in data .",system description,Subset Selection via Self-Representation,0,66,44,5,0,system description : Subset Selection via Self-Representation,0.24354243542435425,0.24719101123595505,0.13513513513513514
named-entity-recognition,0,Such a motivation could be formulated as the Transductive Experimental Design ( TED ) model :,system description,Subset Selection via Self-Representation,0,67,45,6,0,system description : Subset Selection via Self-Representation,0.24723247232472326,0.25280898876404495,0.16216216216216217
named-entity-recognition,0,"where Q ? R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ? X , ?k ? { 1 , , K} ; A = [ a 1 , , a N ] ? R KN is the corresponding linear combination coefficients .",system description,Subset Selection via Self-Representation,0,68,46,7,0,system description : Subset Selection via Self-Representation,0.25092250922509224,0.25842696629213485,0.1891891891891892
named-entity-recognition,0,"where Q ? R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ? X , ?k ? { 1 , , K} ; A = [ a 1 , , a N ] ? R KN is the corresponding linear combination coefficients .",system description,Subset Selection via Self-Representation,0,69,47,8,0,system description : Subset Selection via Self-Representation,0.25461254612546125,0.2640449438202247,0.21621621621621623
named-entity-recognition,0,"where Q ? R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ? X , ?k ? { 1 , , K} ; A = [ a 1 , , a N ] ? R KN is the corresponding linear combination coefficients .",system description,Subset Selection via Self-Representation,0,70,48,9,0,system description : Subset Selection via Self-Representation,0.25830258302583026,0.2696629213483146,0.24324324324324326
named-entity-recognition,0,"where Q ? R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ? X , ?k ? { 1 , , K} ; A = [ a 1 , , a N ] ? R KN is the corresponding linear combination coefficients .",system description,Subset Selection via Self-Representation,0,71,49,10,0,system description : Subset Selection via Self-Representation,0.26199261992619927,0.2752808988764045,0.2702702702702703
named-entity-recognition,0,"where Q ? R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ? X , ?k ? { 1 , , K} ; A = [ a 1 , , a N ] ? R KN is the corresponding linear combination coefficients .",system description,Subset Selection via Self-Representation,0,72,50,11,0,system description : Subset Selection via Self-Representation,0.2656826568265683,0.2808988764044944,0.2972972972972973
named-entity-recognition,0,"By minimizing ( 1 ) , TED could select the highly informative and representative samples , as they have to well represent all the samples in X .",system description,Subset Selection via Self-Representation,0,73,51,12,0,system description : Subset Selection via Self-Representation,0.2693726937269373,0.28651685393258425,0.32432432432432434
named-entity-recognition,0,"Although TED ( 1 ) is well modeled - very accurate and intuitive , there are two bottlenecks .",system description,Subset Selection via Self-Representation,0,74,52,13,0,system description : Subset Selection via Self-Representation,0.2730627306273063,0.29213483146067415,0.35135135135135137
named-entity-recognition,0,"First , the objective is a combinatorial optimization problem .",system description,Subset Selection via Self-Representation,0,75,53,14,0,system description : Subset Selection via Self-Representation,0.2767527675276753,0.29775280898876405,0.3783783783783784
named-entity-recognition,0,It is NP - hard to exhaustively search the optimal subset Q from X .,system description,Subset Selection via Self-Representation,0,76,54,15,0,system description : Subset Selection via Self-Representation,0.28044280442804426,0.30337078651685395,0.40540540540540543
named-entity-recognition,0,"For this reason , the author approximate ( 1 ) via a sequential optimization problem , which is solved by an inefficient greedy optimization algorithm .",system description,Subset Selection via Self-Representation,0,77,55,16,0,system description : Subset Selection via Self-Representation,0.28413284132841327,0.3089887640449438,0.43243243243243246
named-entity-recognition,0,"Second , similar to the existing least square loss based models in machine learning and statistics , ( 1 ) is sensitive to the presence of outliers",system description,Subset Selection via Self-Representation,0,78,56,17,0,system description : Subset Selection via Self-Representation,0.2878228782287823,0.3146067415730337,0.4594594594594595
named-entity-recognition,0,"where ? is a nonnegative parameter ; A is constrained to be row - sparse , and thus to select the most representative and informative samples .",system description,Subset Selection via Self-Representation,0,79,57,18,0,system description : Subset Selection via Self-Representation,0.2915129151291513,0.3202247191011236,0.4864864864864865
named-entity-recognition,0,"where ? is a nonnegative parameter ; A is constrained to be row - sparse , and thus to select the most representative and informative samples .",system description,Subset Selection via Self-Representation,0,80,58,19,0,system description : Subset Selection via Self-Representation,0.2952029520295203,0.3258426966292135,0.5135135135135135
named-entity-recognition,0,"As the representation loss is accumulated via the 1 - norm among samples , compared with ( 1 ) , the robustness against outlier samples is enhanced .",system description,Subset Selection via Self-Representation,0,81,59,20,0,system description : Subset Selection via Self-Representation,0.2988929889298893,0.33146067415730335,0.5405405405405406
named-entity-recognition,0,"Equivalently , ( 2 ) is rewritten in the matrix format :",system description,Subset Selection via Self-Representation,0,82,60,21,0,system description : Subset Selection via Self-Representation,0.3025830258302583,0.33707865168539325,0.5675675675675675
named-entity-recognition,0,"Since the objective is convex in A , the global minimum maybe found by differentiating ( 3 ) and setting the derivative to zero , resulting in a linear system 1",system description,Subset Selection via Self-Representation,0,83,61,22,0,system description : Subset Selection via Self-Representation,0.3062730627306273,0.34269662921348315,0.5945945945945946
named-entity-recognition,0,where V ? RN,system description,Subset Selection via Self-Representation,0,84,62,23,0,system description : Subset Selection via Self-Representation,0.30996309963099633,0.34831460674157305,0.6216216216216216
named-entity-recognition,0,where V ? RN,system description,Subset Selection via Self-Representation,0,85,63,24,0,system description : Subset Selection via Self-Representation,0.31365313653136534,0.3539325842696629,0.6486486486486487
named-entity-recognition,0,is a diagonal matrix with then th diagonal entry as V nn = 1 2 an 2 and U nn = 1 2 xn ? Xan 2 .,system description,Subset Selection via Self-Representation,0,86,64,25,0,system description : Subset Selection via Self-Representation,0.3173431734317343,0.3595505617977528,0.6756756756756757
named-entity-recognition,0,is a diagonal matrix with then th diagonal entry as V nn = 1 2 an 2 and U nn = 1 2 xn ? Xan 2 .,system description,Subset Selection via Self-Representation,0,87,65,26,0,system description : Subset Selection via Self-Representation,0.3210332103321033,0.3651685393258427,0.7027027027027027
named-entity-recognition,0,"It seems perfect to use ( 4 ) to solve the objective ( 3 ) , because ( 4 ) looks simple and the global optimum is theoretically guaranteed .",system description,Subset Selection via Self-Representation,0,88,66,27,0,system description : Subset Selection via Self-Representation,0.3247232472324723,0.3707865168539326,0.7297297297297297
named-entity-recognition,0,"Unfortunately , in terms of speed , ( 4 ) is usually infeasible due to the incredible computational demand in the case of large N ( the number of samples ) .",system description,Subset Selection via Self-Representation,0,89,67,28,0,system description : Subset Selection via Self-Representation,0.3284132841328413,0.37640449438202245,0.7567567567567568
named-entity-recognition,0,"At each iteration , the computational complexity of ( 4 ) is up to ON 4 , as analyzed in Remark 1 .",system description,Subset Selection via Self-Representation,0,90,68,29,0,system description : Subset Selection via Self-Representation,0.33210332103321033,0.38202247191011235,0.7837837837837838
named-entity-recognition,0,"According to our experiments , the time cost is up to 2088 hours ( i.e. 87 days ) for a subset selection problem of 13000 samples .",system description,Subset Selection via Self-Representation,0,91,69,30,0,system description : Subset Selection via Self-Representation,0.33579335793357934,0.38764044943820225,0.8108108108108109
named-entity-recognition,0,Remark,system description,Subset Selection via Self-Representation,0,92,70,31,0,system description : Subset Selection via Self-Representation,0.33948339483394835,0.39325842696629215,0.8378378378378378
named-entity-recognition,0,". Since U nn X T X +? V ? RN N , the major computational cost of ( 4 ) focuses on a N N linear system .",system description,Subset Selection via Self-Representation,0,93,71,32,0,system description : Subset Selection via Self-Representation,0.34317343173431736,0.398876404494382,0.8648648648648649
named-entity-recognition,0,". Since U nn X T X +? V ? RN N , the major computational cost of ( 4 ) focuses on a N N linear system .",system description,Subset Selection via Self-Representation,0,94,72,33,0,system description : Subset Selection via Self-Representation,0.34686346863468637,0.4044943820224719,0.8918918918918919
named-entity-recognition,0,"If solved by the Cholesky factorization method , it costs 1 3 N 3 for factorization as well as 2N 2 for forward and backward substitution .",system description,Subset Selection via Self-Representation,0,95,73,34,0,system description : Subset Selection via Self-Representation,0.3505535055350554,0.4101123595505618,0.918918918918919
named-entity-recognition,0,This amounts to ON 3 in total .,system description,Subset Selection via Self-Representation,0,96,74,35,0,system description : Subset Selection via Self-Representation,0.35424354243542433,0.4157303370786517,0.9459459459459459
named-entity-recognition,0,"By now , we only solve an .",system description,Subset Selection via Self-Representation,0,97,75,36,0,system description : Subset Selection via Self-Representation,0.35793357933579334,0.42134831460674155,0.972972972972973
named-entity-recognition,0,"Once solving all the set of {a n } N n= 1 , the total complexity amounts to ON 4 for one iteration step .",system description,Subset Selection via Self-Representation,0,98,76,37,0,system description : Subset Selection via Self-Representation,0.36162361623616235,0.42696629213483145,1.0
named-entity-recognition,0,Accelerated Robust Subset Selection ( ARSS ),system description,Accelerated Robust Subset Selection (ARSS),0,99,77,1,0,system description : Accelerated Robust Subset Selection (ARSS),0.36531365313653136,0.43258426966292135,0.00980392156862745
named-entity-recognition,0,"Due to the huge computational costs , Nie 's method is infeasible for the case of large N - the computational time is up to 2088 hours for a case of 13000 samples .",system description,Accelerated Robust Subset Selection (ARSS),0,100,78,2,0,system description : Accelerated Robust Subset Selection (ARSS),0.36900369003690037,0.43820224719101125,0.0196078431372549
named-entity-recognition,0,"Besides , Nie 's model imposes the 2 - norm among features , which is prone to outliers in features .",system description,Accelerated Robust Subset Selection (ARSS),0,101,79,3,0,system description : Accelerated Robust Subset Selection (ARSS),0.3726937269372694,0.4438202247191011,0.029411764705882353
named-entity-recognition,0,"To tackle the above two issues , we propose a more robust model in the p ( 0 < p ? 1 ) norm .",system description,Accelerated Robust Subset Selection (ARSS),0,102,80,4,0,system description : Accelerated Robust Subset Selection (ARSS),0.3763837638376384,0.449438202247191,0.0392156862745098
named-entity-recognition,0,"Although the resulted objective is challenging to solve , a speedup algorithm is proposed to dramatically save the computational costs .",system description,Accelerated Robust Subset Selection (ARSS),0,103,81,5,0,system description : Accelerated Robust Subset Selection (ARSS),0.3800738007380074,0.4550561797752809,0.049019607843137254
named-entity-recognition,0,"For the same task of N = 13000 , it costs our method 1.8 minutes , achieving a 68429 times acceleration compared with the speed of Nie 's method .",system description,Accelerated Robust Subset Selection (ARSS),0,104,82,6,0,system description : Accelerated Robust Subset Selection (ARSS),0.3837638376383764,0.4606741573033708,0.058823529411764705
named-entity-recognition,0,Modeling .,system description,Accelerated Robust Subset Selection (ARSS),0,105,83,7,0,system description : Accelerated Robust Subset Selection (ARSS),0.3874538745387454,0.46629213483146065,0.06862745098039216
named-entity-recognition,0,"To boost the robustness against outliers in both samples and features , we formulate the discrepancy between X and XA via the p ( 0 < p < 1 ) - norm .",system description,Accelerated Robust Subset Selection (ARSS),0,106,84,8,0,system description : Accelerated Robust Subset Selection (ARSS),0.39114391143911437,0.47191011235955055,0.0784313725490196
named-entity-recognition,0,"There are theoretical and empirical evidences to verify that compared with 2 or 1 norms , the p - norm is more able to prevent outlier elements from dominating the objective , enhancing the robustness ) .",system description,Accelerated Robust Subset Selection (ARSS),0,107,85,9,0,system description : Accelerated Robust Subset Selection (ARSS),0.3948339483394834,0.47752808988764045,0.08823529411764706
named-entity-recognition,0,"Thus , we have the following objective min",system description,Accelerated Robust Subset Selection (ARSS),0,108,86,10,0,system description : Accelerated Robust Subset Selection (ARSS),0.3985239852398524,0.48314606741573035,0.09803921568627451
named-entity-recognition,0,"where ? is a balancing parameter ; A is a row sparse matrix , used to select the most informative and representative samples .",system description,Accelerated Robust Subset Selection (ARSS),0,109,87,11,0,system description : Accelerated Robust Subset Selection (ARSS),0.4022140221402214,0.4887640449438202,0.10784313725490197
named-entity-recognition,0,"where ? is a balancing parameter ; A is a row sparse matrix , used to select the most informative and representative samples .",system description,Accelerated Robust Subset Selection (ARSS),0,110,88,12,0,system description : Accelerated Robust Subset Selection (ARSS),0.4059040590405904,0.4943820224719101,0.11764705882352941
named-entity-recognition,0,"By minimizing the energy of ( 5 ) , we could capture the most essential properties of the dataset X.",system description,Accelerated Robust Subset Selection (ARSS),0,111,89,13,0,system description : Accelerated Robust Subset Selection (ARSS),0.4095940959409594,0.5,0.12745098039215685
named-entity-recognition,0,"After obtaining the optimal A , the row indexes are sorted by the row - sum value of the absolute A in decreasing order .",system description,Accelerated Robust Subset Selection (ARSS),0,112,90,14,0,system description : Accelerated Robust Subset Selection (ARSS),0.4132841328413284,0.5056179775280899,0.13725490196078433
named-entity-recognition,0,The samples specified by the top K indexes are selected as exemplars .,system description,Accelerated Robust Subset Selection (ARSS),0,113,91,15,0,system description : Accelerated Robust Subset Selection (ARSS),0.41697416974169743,0.5112359550561798,0.14705882352941177
named-entity-recognition,0,Note that the model ( 5 ) could be applied to the unsupervised feature selection problem by only transposing the data matrix X .,system description,Accelerated Robust Subset Selection (ARSS),0,114,92,16,0,system description : Accelerated Robust Subset Selection (ARSS),0.42066420664206644,0.5168539325842697,0.1568627450980392
named-entity-recognition,0,"In this case , A is a L L row sparse matrix , used to select the most representative features .",system description,Accelerated Robust Subset Selection (ARSS),0,115,93,17,0,system description : Accelerated Robust Subset Selection (ARSS),0.42435424354243545,0.5224719101123596,0.16666666666666666
named-entity-recognition,0,"Accelerated Solver for the ARSS Objective in Although objective ( 5 ) is challenging to solve , we propose an effective and highly efficient solver .",system description,Accelerated Robust Subset Selection (ARSS),0,116,94,18,0,system description : Accelerated Robust Subset Selection (ARSS),0.4280442804428044,0.5280898876404494,0.17647058823529413
named-entity-recognition,0,The acceleration owes to the ALM and an equivalent derivation .,system description,Accelerated Robust Subset Selection (ARSS),0,117,95,19,0,system description : Accelerated Robust Subset Selection (ARSS),0.4317343173431734,0.5337078651685393,0.18627450980392157
named-entity-recognition,0,ALM,system description,Accelerated Robust Subset Selection (ARSS),0,118,96,20,0,system description : Accelerated Robust Subset Selection (ARSS),0.4354243542435424,0.5393258426966292,0.19607843137254902
named-entity-recognition,0,"The most intractable challenge of ( 5 ) is that , the p ( 0 < p ? 1 ) - norm is non-convex , non-smooth and notdifferentiable at the zero point .",system description,Accelerated Robust Subset Selection (ARSS),0,119,97,21,0,system description : Accelerated Robust Subset Selection (ARSS),0.43911439114391143,0.5449438202247191,0.20588235294117646
named-entity-recognition,0,"Therefore , it is beneficial to use the Augmented Lagrangian Method ( ALM ) to solve ( 5 ) , resulting in several easily tackled unconstrained subproblems .",system description,Accelerated Robust Subset Selection (ARSS),0,120,98,22,0,system description : Accelerated Robust Subset Selection (ARSS),0.44280442804428044,0.550561797752809,0.21568627450980393
named-entity-recognition,0,"By solving them iteratively , the solutions of subproblems could eventually converge to a minimum .",system description,Accelerated Robust Subset Selection (ARSS),0,121,99,23,0,system description : Accelerated Robust Subset Selection (ARSS),0.44649446494464945,0.5561797752808989,0.22549019607843138
named-entity-recognition,0,"Specifically , we introduce an auxiliary variable E = X ? XA ? R LN .",system description,Accelerated Robust Subset Selection (ARSS),0,122,100,24,0,system description : Accelerated Robust Subset Selection (ARSS),0.45018450184501846,0.5617977528089888,0.23529411764705882
named-entity-recognition,0,"Specifically , we introduce an auxiliary variable E = X ? XA ? R LN .",system description,Accelerated Robust Subset Selection (ARSS),0,123,101,25,0,system description : Accelerated Robust Subset Selection (ARSS),0.45387453874538747,0.5674157303370787,0.24509803921568626
named-entity-recognition,0,"Thus , the objective ( 5 ) becomes : min",system description,Accelerated Robust Subset Selection (ARSS),0,124,102,26,0,system description : Accelerated Robust Subset Selection (ARSS),0.4575645756457565,0.5730337078651685,0.2549019607843137
named-entity-recognition,0,"To deal with the equality constraint in , the most convenient method is to add a penalty , resulting in",system description,Accelerated Robust Subset Selection (ARSS),0,125,103,27,0,system description : Accelerated Robust Subset Selection (ARSS),0.4612546125461255,0.5786516853932584,0.2647058823529412
named-entity-recognition,0,where is a penalty parameter .,system description,Accelerated Robust Subset Selection (ARSS),0,126,104,28,0,system description : Accelerated Robust Subset Selection (ARSS),0.46494464944649444,0.5842696629213483,0.27450980392156865
named-entity-recognition,0,"To guarantee the equality constraint , it requires approaching infinity , which may cause bad numerical conditions .",system description,Accelerated Robust Subset Selection (ARSS),0,127,105,29,0,system description : Accelerated Robust Subset Selection (ARSS),0.46863468634686345,0.5898876404494382,0.28431372549019607
named-entity-recognition,0,"Instead , once introducing a Lagrangian multiplier , it is no longer requiring ? ?.",system description,Accelerated Robust Subset Selection (ARSS),0,128,106,30,0,system description : Accelerated Robust Subset Selection (ARSS),0.47232472324723246,0.5955056179775281,0.29411764705882354
named-entity-recognition,0,"Thus , we rewrite into the standard ALM formulation as :",system description,Accelerated Robust Subset Selection (ARSS),0,129,107,31,0,system description : Accelerated Robust Subset Selection (ARSS),0.47601476014760147,0.601123595505618,0.30392156862745096
named-entity-recognition,0,where ? consists of L N Lagrangian multipliers .,system description,Accelerated Robust Subset Selection (ARSS),0,130,108,32,0,system description : Accelerated Robust Subset Selection (ARSS),0.4797047970479705,0.6067415730337079,0.3137254901960784
named-entity-recognition,0,where ? consists of L N Lagrangian multipliers .,system description,Accelerated Robust Subset Selection (ARSS),0,131,109,33,0,system description : Accelerated Robust Subset Selection (ARSS),0.4833948339483395,0.6123595505617978,0.3235294117647059
named-entity-recognition,0,"In the following , a highly efficient solver will be given .",system description,Accelerated Robust Subset Selection (ARSS),0,132,110,34,0,system description : Accelerated Robust Subset Selection (ARSS),0.4870848708487085,0.6179775280898876,0.3333333333333333
named-entity-recognition,0,"The updating rule for ? Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",system description,Accelerated Robust Subset Selection (ARSS),0,133,111,35,0,system description : Accelerated Robust Subset Selection (ARSS),0.4907749077490775,0.6235955056179775,0.3431372549019608
named-entity-recognition,0,"The updating rule for ? Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",system description,Accelerated Robust Subset Selection (ARSS),0,134,112,36,0,system description : Accelerated Robust Subset Selection (ARSS),0.4944649446494465,0.6292134831460674,0.35294117647058826
named-entity-recognition,0,where is a monotonically increasing parameter over iteration steps .,system description,Accelerated Robust Subset Selection (ARSS),0,135,113,37,0,system description : Accelerated Robust Subset Selection (ARSS),0.4981549815498155,0.6348314606741573,0.3627450980392157
named-entity-recognition,0,"For example , ? ? , where 1 < ? < 2 is a predefined parameter .",system description,Accelerated Robust Subset Selection (ARSS),0,136,114,38,0,system description : Accelerated Robust Subset Selection (ARSS),0.5018450184501845,0.6404494382022472,0.37254901960784315
named-entity-recognition,0,"Efficient solver for E Removing irrelevant terms with E from ( 8 ) , we have",system description,Accelerated Robust Subset Selection (ARSS),0,137,115,39,0,system description : Accelerated Robust Subset Selection (ARSS),0.5055350553505535,0.6460674157303371,0.38235294117647056
named-entity-recognition,0,where H = X ? XA ? ? ? R LN .,system description,Accelerated Robust Subset Selection (ARSS),0,138,116,40,0,system description : Accelerated Robust Subset Selection (ARSS),0.5092250922509225,0.651685393258427,0.39215686274509803
named-entity-recognition,0,where H = X ? XA ? ? ? R LN .,system description,Accelerated Robust Subset Selection (ARSS),0,139,117,41,0,system description : Accelerated Robust Subset Selection (ARSS),0.5129151291512916,0.6573033707865169,0.4019607843137255
named-entity-recognition,0,"According to the definition of the p - norm and the Frobenius - norm , ( 10 ) could be decoupled into L N independent and unconstrained subproblems .",system description,Accelerated Robust Subset Selection (ARSS),0,140,118,42,0,system description : Accelerated Robust Subset Selection (ARSS),0.5166051660516605,0.6629213483146067,0.4117647058823529
named-entity-recognition,0,The standard form of these subproblems is,system description,Accelerated Robust Subset Selection (ARSS),0,141,119,43,0,system description : Accelerated Robust Subset Selection (ARSS),0.5202952029520295,0.6685393258426966,0.4215686274509804
named-entity-recognition,0,"where ? = 1 is a given positive parameter , y is the scalar variable need to deal with , c is a known scalar constant .",system description,Accelerated Robust Subset Selection (ARSS),0,142,120,44,0,system description : Accelerated Robust Subset Selection (ARSS),0.5239852398523985,0.6741573033707865,0.43137254901960786
named-entity-recognition,0,Zuo et al. ) has recently proposed a generalized iterative shrinkage algorithm to solve ( 11 ) .,system description,Accelerated Robust Subset Selection (ARSS),0,143,121,45,0,system description : Accelerated Robust Subset Selection (ARSS),0.5276752767527675,0.6797752808988764,0.4411764705882353
named-entity-recognition,0,This algorithm is easy to implement and able to achieve more accurate solutions than current methods .,system description,Accelerated Robust Subset Selection (ARSS),0,144,122,46,0,system description : Accelerated Robust Subset Selection (ARSS),0.5313653136531366,0.6853932584269663,0.45098039215686275
named-entity-recognition,0,"Thus , we use it for our problem as :",system description,Accelerated Robust Subset Selection (ARSS),0,145,123,47,0,system description : Accelerated Robust Subset Selection (ARSS),0.5350553505535055,0.6910112359550562,0.46078431372549017
named-entity-recognition,0,is obtained by solving the following equation :,system description,Accelerated Robust Subset Selection (ARSS),0,146,124,48,0,system description : Accelerated Robust Subset Selection (ARSS),0.5387453874538746,0.6966292134831461,0.47058823529411764
named-entity-recognition,0,which could be solved efficiently via an iterative algorithm .,system description,Accelerated Robust Subset Selection (ARSS),0,147,125,49,0,system description : Accelerated Robust Subset Selection (ARSS),0.5424354243542435,0.702247191011236,0.4803921568627451
named-entity-recognition,0,"In this manner , ( 10 ) could be sovled extremely fast .",system description,Accelerated Robust Subset Selection (ARSS),0,148,126,50,0,system description : Accelerated Robust Subset Selection (ARSS),0.5461254612546126,0.7078651685393258,0.49019607843137253
named-entity-recognition,0,"Accelerated solver for A The main acceleration focuses on the solver of A. Removing irrelevant terms with A from ( 8 ) , we have",system description,Accelerated Robust Subset Selection (ARSS),0,149,127,51,0,system description : Accelerated Robust Subset Selection (ARSS),0.5498154981549815,0.7134831460674157,0.5
named-entity-recognition,0,"where ? = ? is a nonnegative parameter , P = X ? E ? ? ? R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .",system description,Accelerated Robust Subset Selection (ARSS),0,150,128,52,0,system description : Accelerated Robust Subset Selection (ARSS),0.5535055350553506,0.7191011235955056,0.5098039215686274
named-entity-recognition,0,"where ? = ? is a nonnegative parameter , P = X ? E ? ? ? R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .",system description,Accelerated Robust Subset Selection (ARSS),0,151,129,53,0,system description : Accelerated Robust Subset Selection (ARSS),0.5571955719557196,0.7247191011235955,0.5196078431372549
named-entity-recognition,0,"where ? = ? is a nonnegative parameter , P = X ? E ? ? ? R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .",system description,Accelerated Robust Subset Selection (ARSS),0,152,130,54,0,system description : Accelerated Robust Subset Selection (ARSS),0.5608856088560885,0.7303370786516854,0.5294117647058824
named-entity-recognition,0,This amounts to tackling the following linear system 2 :,system description,Accelerated Robust Subset Selection (ARSS),0,153,131,55,0,system description : Accelerated Robust Subset Selection (ARSS),0.5645756457564576,0.7359550561797753,0.5392156862745098
named-entity-recognition,0,"As V + ? X TX ? RN N , is mainly a N N linear system .",system description,Accelerated Robust Subset Selection (ARSS),0,154,132,56,0,system description : Accelerated Robust Subset Selection (ARSS),0.5682656826568265,0.7415730337078652,0.5490196078431373
named-entity-recognition,0,"As V + ? X TX ? RN N , is mainly a N N linear system .",system description,Accelerated Robust Subset Selection (ARSS),0,155,133,57,0,system description : Accelerated Robust Subset Selection (ARSS),0.5719557195571956,0.7471910112359551,0.5588235294117647
named-entity-recognition,0,"Once solved by the Cholesky factorization , the computational complexity is highly up to ON 3 .",system description,Accelerated Robust Subset Selection (ARSS),0,156,134,58,0,system description : Accelerated Robust Subset Selection (ARSS),0.5756457564575646,0.7528089887640449,0.5686274509803921
named-entity-recognition,0,This is by no means a good choice for real applications with large N .,system description,Accelerated Robust Subset Selection (ARSS),0,157,135,59,0,system description : Accelerated Robust Subset Selection (ARSS),0.5793357933579336,0.7584269662921348,0.5784313725490197
named-entity-recognition,0,"In the following , an equivalent derivation of ( 14 ) will be proposed to significantly save the computational complexity .",system description,Accelerated Robust Subset Selection (ARSS),0,158,136,60,0,system description : Accelerated Robust Subset Selection (ARSS),0.5830258302583026,0.7640449438202247,0.5882352941176471
named-entity-recognition,0,Theorem,system description,Accelerated Robust Subset Selection (ARSS),0,159,137,61,0,system description : Accelerated Robust Subset Selection (ARSS),0.5867158671586716,0.7696629213483146,0.5980392156862745
named-entity-recognition,0,. The N N linear system is equivalent to the following L L linear system :,system description,Accelerated Robust Subset Selection (ARSS),0,160,138,62,0,system description : Accelerated Robust Subset Selection (ARSS),0.5904059040590406,0.7752808988764045,0.6078431372549019
named-entity-recognition,0,where IL is a L L identity matrix .,system description,Accelerated Robust Subset Selection (ARSS),0,161,139,63,0,system description : Accelerated Robust Subset Selection (ARSS),0.5940959409594095,0.7808988764044944,0.6176470588235294
named-entity-recognition,0,Proof .,system description,Accelerated Robust Subset Selection (ARSS),0,162,140,64,0,system description : Accelerated Robust Subset Selection (ARSS),0.5977859778597786,0.7865168539325843,0.6274509803921569
named-entity-recognition,0,"Note that V is a N N diagonal and positive - definite matrix , the exponent of V is efficient to achieve , i.e. V ? = { V ? nn } N n=1 , ?? ? R.",system description,Accelerated Robust Subset Selection (ARSS),0,163,141,65,0,system description : Accelerated Robust Subset Selection (ARSS),0.6014760147601476,0.7921348314606742,0.6372549019607843
named-entity-recognition,0,"Note that V is a N N diagonal and positive - definite matrix , the exponent of V is efficient to achieve , i.e. V ? = { V ? nn } N n=1 , ?? ? R.",system description,Accelerated Robust Subset Selection (ARSS),0,164,142,66,0,system description : Accelerated Robust Subset Selection (ARSS),0.6051660516605166,0.797752808988764,0.6470588235294118
named-entity-recognition,0,We have the following equations,system description,Accelerated Robust Subset Selection (ARSS),0,165,143,67,0,system description : Accelerated Robust Subset Selection (ARSS),0.6088560885608856,0.8033707865168539,0.6568627450980392
named-entity-recognition,0,"where Z = XV ? 1 2 , IN is a N N identity matrix .",system description,Accelerated Robust Subset Selection (ARSS),0,166,144,68,0,system description : Accelerated Robust Subset Selection (ARSS),0.6125461254612546,0.8089887640449438,0.6666666666666666
named-entity-recognition,0,The following equation holds for any conditions,system description,Accelerated Robust Subset Selection (ARSS),0,167,145,69,0,system description : Accelerated Robust Subset Selection (ARSS),0.6162361623616236,0.8146067415730337,0.6764705882352942
named-entity-recognition,0,"Multiplying with IN + ?Z T Z ?1 on the left and IL + ? ZZ T ? 1 on the right of both sides of the equal - sign , we have the equation as :",system description,Accelerated Robust Subset Selection (ARSS),0,168,146,70,0,system description : Accelerated Robust Subset Selection (ARSS),0.6199261992619927,0.8202247191011236,0.6862745098039216
named-entity-recognition,0,"Therefore , substituting ( 18 ) and Z = XV ? 1 2 into ( 16 ) , we have the simplified updating rule as :",system description,Accelerated Robust Subset Selection (ARSS),0,169,147,71,0,system description : Accelerated Robust Subset Selection (ARSS),0.6236162361623616,0.8258426966292135,0.696078431372549
named-entity-recognition,0,"When N L , the most complex operation is the matrix multiplications , not the L L linear system .",system description,Accelerated Robust Subset Selection (ARSS),0,170,148,72,0,system description : Accelerated Robust Subset Selection (ARSS),0.6273062730627307,0.8314606741573034,0.7058823529411765
named-entity-recognition,0,Corollary,system description,Accelerated Robust Subset Selection (ARSS),0,171,149,73,0,system description : Accelerated Robust Subset Selection (ARSS),0.6309963099630996,0.8370786516853933,0.7156862745098039
named-entity-recognition,0,. We have two equivalent updating rules and ( 15 ) for the objective ( 13 ) .,system description,Accelerated Robust Subset Selection (ARSS),0,172,150,74,0,system description : Accelerated Robust Subset Selection (ARSS),0.6346863468634686,0.8426966292134831,0.7254901960784313
named-entity-recognition,0,"If using ( 14 ) when N ? L , and otherwise using ( 15 ) as shown in Algorithm 1 , the computational complexity of solvers for ( 13 ) is ON 2 L .",system description,Accelerated Robust Subset Selection (ARSS),0,173,151,75,0,system description : Accelerated Robust Subset Selection (ARSS),0.6383763837638377,0.848314606741573,0.7352941176470589
named-entity-recognition,0,"If using ( 14 ) when N ? L , and otherwise using ( 15 ) as shown in Algorithm 1 , the computational complexity of solvers for ( 13 ) is ON 2 L .",system description,Accelerated Robust Subset Selection (ARSS),0,174,152,76,0,system description : Accelerated Robust Subset Selection (ARSS),0.6420664206642066,0.8539325842696629,0.7450980392156863
named-entity-recognition,0,"Due to N L , we have highly reduced the complexity from ON 4 to ON 2 L compared with Nie 's method .",system description,Accelerated Robust Subset Selection (ARSS),0,175,153,77,0,system description : Accelerated Robust Subset Selection (ARSS),0.6457564575645757,0.8595505617977528,0.7549019607843137
named-entity-recognition,0,"Algorithm 1 for ( 13 ) : A * = ARSS A ( X , V , P , IL , ?)",system description,Accelerated Robust Subset Selection (ARSS),0,176,154,78,0,system description : Accelerated Robust Subset Selection (ARSS),0.6494464944649446,0.8651685393258427,0.7647058823529411
named-entity-recognition,0,"Input : X , V , P , IL , ? 1 : if N ? L then 2 :",system description,Accelerated Robust Subset Selection (ARSS),0,177,155,79,0,system description : Accelerated Robust Subset Selection (ARSS),0.6531365313653137,0.8707865168539326,0.7745098039215687
named-entity-recognition,0,"Input : X , V , P , IL , ? 1 : if N ? L then 2 :",system description,Accelerated Robust Subset Selection (ARSS),0,178,156,80,0,system description : Accelerated Robust Subset Selection (ARSS),0.6568265682656826,0.8764044943820225,0.7843137254901961
named-entity-recognition,0,"update A via the updating rule , that is 3 : update ? by the updating rule ( 9 ) , ? ?.",system description,Accelerated Robust Subset Selection (ARSS),0,179,157,81,0,system description : Accelerated Robust Subset Selection (ARSS),0.6605166051660517,0.8820224719101124,0.7941176470588235
named-entity-recognition,0,: until convergence,system description,Accelerated Robust Subset Selection (ARSS),0,180,158,82,0,system description : Accelerated Robust Subset Selection (ARSS),0.6642066420664207,0.8876404494382022,0.803921568627451
named-entity-recognition,0,Output : A The solver to update A is given in Algorithm,system description,Accelerated Robust Subset Selection (ARSS),0,181,159,83,0,system description : Accelerated Robust Subset Selection (ARSS),0.6678966789667896,0.8932584269662921,0.8137254901960784
named-entity-recognition,0,. The over all solver for our model ( 5 ) is summarized in Algorithm,system description,Accelerated Robust Subset Selection (ARSS),0,182,160,84,0,system description : Accelerated Robust Subset Selection (ARSS),0.6715867158671587,0.898876404494382,0.8235294117647058
named-entity-recognition,0,2 .,system description,Accelerated Robust Subset Selection (ARSS),0,183,161,85,0,system description : Accelerated Robust Subset Selection (ARSS),0.6752767527675276,0.9044943820224719,0.8333333333333334
named-entity-recognition,0,"According to Theorem 2 and Corollary 3 , the solver for our model ( 13 ) is highly simplified , as feature length is generally much smaller than data size , i.e L N .",system description,Accelerated Robust Subset Selection (ARSS),0,184,162,86,0,system description : Accelerated Robust Subset Selection (ARSS),0.6789667896678967,0.9101123595505618,0.8431372549019608
named-entity-recognition,0,"Similarly , Nie 's method could be highly accelerated by Theorem 4 , obtaining 500 + times speedup , as shown in and .",system description,Accelerated Robust Subset Selection (ARSS),0,185,163,87,0,system description : Accelerated Robust Subset Selection (ARSS),0.6826568265682657,0.9157303370786517,0.8529411764705882
named-entity-recognition,0,"Similarly , Nie 's method could be highly accelerated by Theorem 4 , obtaining 500 + times speedup , as shown in and .",system description,Accelerated Robust Subset Selection (ARSS),0,186,164,88,0,system description : Accelerated Robust Subset Selection (ARSS),0.6863468634686347,0.9213483146067416,0.8627450980392157
named-entity-recognition,0,Theorem,system description,Accelerated Robust Subset Selection (ARSS),0,187,165,89,0,system description : Accelerated Robust Subset Selection (ARSS),0.6900369003690037,0.9269662921348315,0.8725490196078431
named-entity-recognition,0,. Nie 's N N solver ( 20 ) ) is equivalent to the following L L linear system ( 21 ) an = U nn U nn X TX + ? V,system description,Accelerated Robust Subset Selection (ARSS),0,188,166,90,0,system description : Accelerated Robust Subset Selection (ARSS),0.6937269372693727,0.9325842696629213,0.8823529411764706
named-entity-recognition,0,?1 X T x n ( 20 ),system description,Accelerated Robust Subset Selection (ARSS),0,189,167,91,0,system description : Accelerated Robust Subset Selection (ARSS),0.6974169741697417,0.9382022471910112,0.8921568627450981
named-entity-recognition,0,"?n ? { 1 , 2 , , N } , where IL is a L L identity matrix .",system description,Accelerated Robust Subset Selection (ARSS),0,190,168,92,0,system description : Accelerated Robust Subset Selection (ARSS),0.7011070110701108,0.9438202247191011,0.9019607843137255
named-entity-recognition,0,Proof .,system description,Accelerated Robust Subset Selection (ARSS),0,191,169,93,0,system description : Accelerated Robust Subset Selection (ARSS),0.7047970479704797,0.949438202247191,0.9117647058823529
named-entity-recognition,0,"Based on ( 20 ) , we have the following equalities :",system description,Accelerated Robust Subset Selection (ARSS),0,192,170,94,0,system description : Accelerated Robust Subset Selection (ARSS),0.7084870848708487,0.9550561797752809,0.9215686274509803
named-entity-recognition,0,The derivations are equivalent ; their results are equal .,system description,Accelerated Robust Subset Selection (ARSS),0,193,171,95,0,system description : Accelerated Robust Subset Selection (ARSS),0.7121771217712177,0.9606741573033708,0.9313725490196079
named-entity-recognition,0,V ? RN,system description,Accelerated Robust Subset Selection (ARSS),0,194,172,96,0,system description : Accelerated Robust Subset Selection (ARSS),0.7158671586715867,0.9662921348314607,0.9411764705882353
named-entity-recognition,0,V ? RN,system description,Accelerated Robust Subset Selection (ARSS),0,195,173,97,0,system description : Accelerated Robust Subset Selection (ARSS),0.7195571955719557,0.9719101123595506,0.9509803921568627
named-entity-recognition,0,"is a positive and diagonal matrix with then th diagonal entry as Vnn = 1 ? an 2 2 + > 0 , where is a small value to avoid singular failures Corollary 5 .",system description,Accelerated Robust Subset Selection (ARSS),0,196,174,98,0,system description : Accelerated Robust Subset Selection (ARSS),0.7232472324723247,0.9775280898876404,0.9607843137254902
named-entity-recognition,0,"Since feature length is generally much smaller than data size , i.e. L N , our accelerated solver ( 20 ) for Nie 's model ( 3 ) is highly faster than the authorial solver ( 21 ) .",system description,Accelerated Robust Subset Selection (ARSS),0,197,175,99,0,system description : Accelerated Robust Subset Selection (ARSS),0.7269372693726938,0.9831460674157303,0.9705882352941176
named-entity-recognition,0,"Theoretically , we reduce the computational complexity from ON 4 to ON 2 L + N L 3 , while maintaining the same solution .",system description,Accelerated Robust Subset Selection (ARSS),0,198,176,100,0,system description : Accelerated Robust Subset Selection (ARSS),0.7306273062730627,0.9887640449438202,0.9803921568627451
named-entity-recognition,0,"That is , like Nie 's solver ( 20 ) , our speedup solver ( 21 ) can reach the global optimum .",system description,Accelerated Robust Subset Selection (ARSS),0,199,177,101,0,system description : Accelerated Robust Subset Selection (ARSS),0.7343173431734318,0.9943820224719101,0.9901960784313726
named-entity-recognition,0,Extensive empirical results will verify the huge acceleration,system description,Accelerated Robust Subset Selection (ARSS),0,200,178,102,0,system description : Accelerated Robust Subset Selection (ARSS),0.7380073800738007,1.0,1.0
named-entity-recognition,0,Experiments Experimental Settings,experiment,Experiments Experimental Settings,0,201,1,1,0,experiment : Experiments Experimental Settings,0.7416974169741697,0.030303030303030304,0.1111111111111111
named-entity-recognition,0,"In this part , the experimental settings are introduced .",experiment,Experiments Experimental Settings,0,202,2,2,0,experiment : Experiments Experimental Settings,0.7453874538745388,0.06060606060606061,0.2222222222222222
named-entity-recognition,0,"All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .",experiment,Experiments Experimental Settings,1,203,3,3,0,experiment : Experiments Experimental Settings,0.7490774907749077,0.09090909090909091,0.3333333333333333
named-entity-recognition,0,"Brief descriptions often benchmark datasets are summarized in , where ' Total ( N * ) ' denotes the total set of samples in each data .",experiment,Experiments Experimental Settings,0,204,4,4,0,experiment : Experiments Experimental Settings,0.7527675276752768,0.12121212121212122,0.4444444444444444
named-entity-recognition,0,"Due to the high computational complexity , other methods can only handle small datasets ( while our method can handle the total set ) .",experiment,Experiments Experimental Settings,0,205,5,5,0,experiment : Experiments Experimental Settings,0.7564575645756457,0.15151515151515152,0.5555555555555556
named-entity-recognition,0,"Thus , we randomly choose the candidate set from the total set to reduce the sample size , i.e. N < N * ( cf. ' Total ( N * ) ' and ' candid . ( N ) ' in ) .",experiment,Experiments Experimental Settings,0,206,6,6,0,experiment : Experiments Experimental Settings,0.7601476014760148,0.18181818181818182,0.6666666666666666
named-entity-recognition,0,The remainder ( except candidate set ) are used for test .,experiment,Experiments Experimental Settings,0,207,7,7,0,experiment : Experiments Experimental Settings,0.7638376383763837,0.21212121212121213,0.7777777777777778
named-entity-recognition,0,"Specifically , to simulate the varying quality of samples , ten percentage of candidate samples from each class are randomly selected and arbitrarily added one of the following three kinds of noise : "" Gaussian "" , "" Laplace "" and "" Salt & pepper "" respectively .",experiment,Experiments Experimental Settings,0,208,8,8,0,experiment : Experiments Experimental Settings,0.7675276752767528,0.24242424242424243,0.8888888888888888
named-entity-recognition,0,"In a word , all experiment settings are same and fair for all the methods .",experiment,Experiments Experimental Settings,0,209,9,9,0,experiment : Experiments Experimental Settings,0.7712177121771218,0.2727272727272727,1.0
named-entity-recognition,0,Speed Comparisons,experiment,Speed Comparisons,1,210,10,1,0,experiment : Speed Comparisons,0.7749077490774908,0.30303030303030304,0.041666666666666664
named-entity-recognition,0,There are two parts of speed comparisons .,experiment,Speed Comparisons,0,211,11,2,0,experiment : Speed Comparisons,0.7785977859778598,0.3333333333333333,0.08333333333333333
named-entity-recognition,0,"First , how speed varies with increasing N is illustrated in .",experiment,Speed Comparisons,0,212,12,3,0,experiment : Speed Comparisons,0.7822878228782287,0.36363636363636365,0.125
named-entity-recognition,0,Then the comparison of specific speed is summarized in .,experiment,Speed Comparisons,0,213,13,4,0,experiment : Speed Comparisons,0.7859778597785978,0.3939393939393939,0.16666666666666666
named-entity-recognition,0,Note that TED and RRSS Nie denote the authorial solver ( via authorial codes ) ; RRSS our is our accelerated solver for Nie 's model via Theorem 4 ; ARSS is the proposed method .,experiment,Speed Comparisons,0,214,14,5,0,experiment : Speed Comparisons,0.7896678966789668,0.42424242424242425,0.20833333333333334
named-entity-recognition,0,"Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .",experiment,Speed Comparisons,1,215,15,6,0,experiment : Speed Comparisons,0.7933579335793358,0.45454545454545453,0.25
named-entity-recognition,0,"The results are illustrated in , where there are three sub-figures showing the speed of four methods on the benchmark datasets of Letter , MNIST and Waveform respectively .",experiment,Speed Comparisons,0,216,16,7,0,experiment : Speed Comparisons,0.7970479704797048,0.48484848484848486,0.2916666666666667
named-entity-recognition,0,"As we shall see , both selection time of TED and RRSS Nie increases dramatically as N increases .",experiment,Speed Comparisons,0,217,17,8,0,experiment : Speed Comparisons,0.8007380073800738,0.5151515151515151,0.3333333333333333
named-entity-recognition,0,"No surprisingly , RRSS Nie is incredibly time - consuming as N grows the order of curves looks higher than quadratic .",experiment,Speed Comparisons,0,218,18,9,0,experiment : Speed Comparisons,0.8044280442804428,0.5454545454545454,0.375
named-entity-recognition,0,"Actually , the theoretical complexity of RRSS Nie is highly up to ON 4 as analyzed in Remark",experiment,Speed Comparisons,0,219,19,10,0,experiment : Speed Comparisons,0.8081180811808119,0.5757575757575758,0.4166666666666667
named-entity-recognition,0,1 .,experiment,Speed Comparisons,0,220,20,11,0,experiment : Speed Comparisons,0.8118081180811808,0.6060606060606061,0.4583333333333333
named-entity-recognition,0,"Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .",experiment,Speed Comparisons,1,221,21,12,0,experiment : Speed Comparisons,0.8154981549815498,0.6363636363636364,0.5
named-entity-recognition,0,This is owing to the speedup techniques of ALM and equivalent derivations .,experiment,Speed Comparisons,0,222,22,13,0,experiment : Speed Comparisons,0.8191881918819188,0.6666666666666666,0.5416666666666666
named-entity-recognition,0,"Via them , we reduce the computational cost from ON 4 to ON 2 L , as analyzed in Theorem 2 and Corollary",experiment,Speed Comparisons,0,223,23,14,0,experiment : Speed Comparisons,0.8228782287822878,0.696969696969697,0.5833333333333334
named-entity-recognition,0,3 .,experiment,Speed Comparisons,0,224,24,15,0,experiment : Speed Comparisons,0.8265682656826568,0.7272727272727273,0.625
named-entity-recognition,0,"Moreover , with the help of Theorem 4 , RRSS our is the second faster algorithm that is significantly accelerated compared with the authorial algorithm RRSS Nie .",experiment,Speed Comparisons,0,225,25,16,0,experiment : Speed Comparisons,0.8302583025830258,0.7575757575757576,0.6666666666666666
named-entity-recognition,0,"Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .",experiment,Speed Comparisons,1,226,26,17,0,experiment : Speed Comparisons,0.8339483394833949,0.7878787878787878,0.7083333333333334
named-entity-recognition,0,Four conclusions can be drawn from .,experiment,Speed Comparisons,0,227,27,18,0,experiment : Speed Comparisons,0.8376383763837638,0.8181818181818182,0.75
named-entity-recognition,0,"First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .",experiment,Speed Comparisons,1,228,28,19,0,experiment : Speed Comparisons,0.8413284132841329,0.8484848484848485,0.7916666666666666
named-entity-recognition,0,"Second , with the help of Theorem 4 , RRSS our is highly faster than RRSS Nie , averagely obtaining a 559 times acceleration .",experiment,Speed Comparisons,0,229,29,20,0,experiment : Speed Comparisons,0.8450184501845018,0.8787878787878788,0.8333333333333334
named-entity-recognition,0,"Third , ARSS is dramatically faster than :",experiment,Speed Comparisons,0,230,30,21,0,experiment : Speed Comparisons,0.8487084870848709,0.9090909090909091,0.875
named-entity-recognition,0,"Performances of TED , RRSS and ARSS : ( left - a ) speed in seconds , prediction accuracies .",experiment,Speed Comparisons,0,231,31,22,0,experiment : Speed Comparisons,0.8523985239852399,0.9393939393939394,0.9166666666666666
named-entity-recognition,0,"In terms of speed , with the help of Theorem 4 , RRSSour is averagely 559 + times faster than the authorial algorithm , i.e. RRSSNie ; ARSS achieves surprisingly 23275 + times acceleration compared with RRSSNie .",experiment,Speed Comparisons,0,232,32,23,0,experiment : Speed Comparisons,0.8560885608856088,0.9696969696969697,0.9583333333333334
named-entity-recognition,0,"Due to the more robust loss in the p -norm , the prediction accuracy of ARSS is highly encouraging .",experiment,Speed Comparisons,0,233,33,24,0,experiment : Speed Comparisons,0.8597785977859779,1.0,1.0
named-entity-recognition,0,Datasets,dataset,Datasets,0,234,1,1,0,dataset : Datasets,0.8634686346863468,0.14285714285714285,0.14285714285714285
named-entity-recognition,0,"Speed 1 ' ARSS ( N * ) ' means the task of selecting samples from the whole dataset ( with N * samples as shown in the 2 nd column in ) , while ' TED ' to ' ARSS ' indicate the problem of dealing with the candidate sample sets ( with N samples as shown in the 3 rd column in ) .",dataset,Datasets,0,235,2,2,0,dataset : Datasets,0.8671586715867159,0.2857142857142857,0.2857142857142857
named-entity-recognition,0,verify an average acceleration of 23275 times faster than RRSS Nie and 281 times faster than TED .,dataset,Datasets,0,236,3,3,0,dataset : Datasets,0.8708487084870848,0.42857142857142855,0.42857142857142855
named-entity-recognition,0,"This means that for example if it takes RRSS Nie 100 years to do a subset selection task , it only takes our method 1.6 days to address the same problem .",dataset,Datasets,0,237,4,4,0,dataset : Datasets,0.8745387453874539,0.5714285714285714,0.5714285714285714
named-entity-recognition,0,"Finally , we apply ARSS to the whole sample set of each data .",dataset,Datasets,0,238,5,5,0,dataset : Datasets,0.8782287822878229,0.7142857142857143,0.7142857142857143
named-entity-recognition,0,RRSS Nie and TED ; the results in,dataset,Datasets,0,239,6,6,0,dataset : Datasets,0.8819188191881919,0.8571428571428571,0.8571428571428571
named-entity-recognition,0,"The results are displayed in the 6 th column in , showing its capability to process very large datasets .",dataset,Datasets,0,240,7,7,0,dataset : Datasets,0.8856088560885609,1.0,1.0
named-entity-recognition,0,Prediction Accuracy,system description,Prediction Accuracy,1,241,1,1,0,system description : Prediction Accuracy,0.8892988929889298,0.05263157894736842,0.05263157894736842
named-entity-recognition,0,Accuracy comparison,system description,Prediction Accuracy,1,242,2,2,0,system description : Prediction Accuracy,0.8929889298892989,0.10526315789473684,0.10526315789473684
named-entity-recognition,0,We conduct experiments on ten benchmark datasets .,system description,Prediction Accuracy,0,243,3,3,0,system description : Prediction Accuracy,0.8966789667896679,0.15789473684210525,0.15789473684210525
named-entity-recognition,0,"For each dataset , the top 200 representative samples are selected for training .",system description,Prediction Accuracy,0,244,4,4,0,system description : Prediction Accuracy,0.9003690036900369,0.21052631578947367,0.21052631578947367
named-entity-recognition,0,"The prediction accuracies are reported in , including the results of two popular classifiers .",system description,Prediction Accuracy,0,245,5,5,0,system description : Prediction Accuracy,0.9040590405904059,0.2631578947368421,0.2631578947368421
named-entity-recognition,0,Three observations can be drawn from this table .,system description,Prediction Accuracy,0,246,6,6,0,system description : Prediction Accuracy,0.9077490774907749,0.3157894736842105,0.3157894736842105
named-entity-recognition,0,"First , Linear SVM generally outperforms KNN .",system description,Prediction Accuracy,0,247,7,7,0,system description : Prediction Accuracy,0.9114391143911439,0.3684210526315789,0.3684210526315789
named-entity-recognition,0,"Second , in general , our method performs the best ; for a few cases , our method achieves comparable results with the best performances .",system description,Prediction Accuracy,0,248,8,8,0,system description : Prediction Accuracy,0.915129151291513,0.42105263157894735,0.42105263157894735
named-entity-recognition,0,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .",system description,Prediction Accuracy,1,249,9,9,0,system description : Prediction Accuracy,0.9188191881918819,0.47368421052631576,0.47368421052631576
named-entity-recognition,0,The above analyses are better illustrated in the last row of .,system description,Prediction Accuracy,0,250,10,10,0,system description : Prediction Accuracy,0.922509225092251,0.5263157894736842,0.5263157894736842
named-entity-recognition,0,These results demonstrate that the p loss in our model is well suited to select exemplars from the sample sets of various quality .,system description,Prediction Accuracy,0,251,11,11,0,system description : Prediction Accuracy,0.9261992619926199,0.5789473684210527,0.5789473684210527
named-entity-recognition,0,"Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .",system description,Prediction Accuracy,1,252,12,12,0,system description : Prediction Accuracy,0.9298892988929889,0.631578947368421,0.631578947368421
named-entity-recognition,0,There are two rows and four columns of sub-figures .,system description,Prediction Accuracy,0,253,13,13,0,system description : Prediction Accuracy,0.933579335793358,0.6842105263157895,0.6842105263157895
named-entity-recognition,0,"The top row shows the results of KNN , and the bottom one shows results of SVM .",system description,Prediction Accuracy,0,254,14,14,0,system description : Prediction Accuracy,0.9372693726937269,0.7368421052631579,0.7368421052631579
named-entity-recognition,0,Each column gives the result on one dataset .,system description,Prediction Accuracy,0,255,15,15,0,system description : Prediction Accuracy,0.940959409594096,0.7894736842105263,0.7894736842105263
named-entity-recognition,0,"As we shall see , the prediction accuracies generally increase as K increases .",system description,Prediction Accuracy,1,256,16,16,0,system description : Prediction Accuracy,0.9446494464944649,0.8421052631578947,0.8421052631578947
named-entity-recognition,0,Such case is consistent with the common view that more training data will boost the prediction accuracy .,system description,Prediction Accuracy,0,257,17,17,0,system description : Prediction Accuracy,0.948339483394834,0.8947368421052632,0.8947368421052632
named-entity-recognition,0,"For each sub-figure , ARSS is generally among the best .",system description,Prediction Accuracy,1,258,18,18,0,system description : Prediction Accuracy,0.9520295202952029,0.9473684210526315,0.9473684210526315
named-entity-recognition,0,This case implies that our robust objective ( 5 ) via the p - norm is feasible to select subsets from the data of varying qualities .,system description,Prediction Accuracy,0,259,19,19,0,system description : Prediction Accuracy,0.955719557195572,1.0,1.0
named-entity-recognition,0,Conclusion,conclusion,Conclusion,0,260,1,1,0,conclusion : Conclusion,0.959409594095941,0.08333333333333333,0.08333333333333333
named-entity-recognition,0,"To deal with tremendous data of varying quality , we propose an accelerated robust subset selection ( ARSS ) method .",conclusion,Conclusion,0,261,2,2,0,conclusion : Conclusion,0.9630996309963099,0.16666666666666666,0.16666666666666666
named-entity-recognition,0,The p - norm is exploited to enhance the robustness against both outlier samples and outlier features .,conclusion,Conclusion,0,262,3,3,0,conclusion : Conclusion,0.966789667896679,0.25,0.25
named-entity-recognition,0,"Although the resulted objective is complex to solve , we propose a highly efficient solver via two techniques : ALM and equivalent derivations .",conclusion,Conclusion,0,263,4,4,0,conclusion : Conclusion,0.9704797047970479,0.3333333333333333,0.3333333333333333
named-entity-recognition,0,"Via them , we greatly reduce the computational complexity from ON 4 to ON 2 L .",conclusion,Conclusion,0,264,5,5,0,conclusion : Conclusion,0.974169741697417,0.4166666666666667,0.4166666666666667
named-entity-recognition,0,"Here feature length L is much smaller than data size N , i.e. L N .",conclusion,Conclusion,0,265,6,6,0,conclusion : Conclusion,0.977859778597786,0.5,0.5
named-entity-recognition,0,"Extensive results on ten benchmark datasets verify that our method not only runs 10,000 + times faster than the most related method , but also outperforms state of the art methods .",conclusion,Conclusion,0,266,7,7,0,conclusion : Conclusion,0.981549815498155,0.5833333333333334,0.5833333333333334
named-entity-recognition,0,"Moreover , we propose an accelerated solver to highly speedup Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 .",conclusion,Conclusion,0,267,8,8,0,conclusion : Conclusion,0.985239852398524,0.6666666666666666,0.6666666666666666
named-entity-recognition,0,"Empirically , our accelerated solver could achieve equal results and 500 + times acceleration compared with the authorial solver .",conclusion,Conclusion,0,268,9,9,0,conclusion : Conclusion,0.988929889298893,0.75,0.75
named-entity-recognition,0,Limitation .,conclusion,Conclusion,0,269,10,10,0,conclusion : Conclusion,0.992619926199262,0.8333333333333334,0.8333333333333334
named-entity-recognition,0,"Our efficient algorithm build on the observation that the number of samples is generally larger than feature length , i.e. N > L. For the case of N ? L , the acceleration will be inapparent .",conclusion,Conclusion,0,270,11,11,0,conclusion : Conclusion,0.996309963099631,0.9166666666666666,0.9166666666666666
named-entity-recognition,0,"Our efficient algorithm build on the observation that the number of samples is generally larger than feature length , i.e. N > L. For the case of N ? L , the acceleration will be inapparent .",conclusion,Conclusion,0,271,12,12,0,conclusion : Conclusion,1.0,1.0,1.0
named-entity-recognition,1,Neural Architectures for Named Entity Recognition,title,title,1,2,1,1,0,title : title,0.00966183574879227,1.0,1.0
named-entity-recognition,1,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.014492753623188406,0.16666666666666666,0.16666666666666666
named-entity-recognition,1,"State - of - the - art named entity recognition systems rely heavily on hand - crafted features and domain - specific knowledge in order to learn effectively from the small , supervised training corpora thatare available .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.01932367149758454,0.3333333333333333,0.3333333333333333
named-entity-recognition,1,"In this paper , we introduce two new neural architectures - one based on bidirectional LSTMs and conditional random fields , and the other that constructs and labels segments using a transition - based approach inspired by shift - reduce parsers .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.024154589371980676,0.5,0.5
named-entity-recognition,1,Our models rely on two sources of information about words : character - based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.028985507246376812,0.6666666666666666,0.6666666666666666
named-entity-recognition,1,Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,abstract,abstract,1,7,5,5,0,abstract : abstract,0.033816425120772944,0.8333333333333334,0.8333333333333334
named-entity-recognition,1,1,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03864734299516908,1.0,1.0
named-entity-recognition,1,Introduction,introduction,introduction,0,9,1,1,0,introduction : introduction,0.043478260869565216,0.058823529411764705,0.058823529411764705
named-entity-recognition,1,Named entity recognition ( NER ) is a challenging learning problem .,introduction,introduction,0,10,2,2,0,introduction : introduction,0.04830917874396135,0.11764705882352941,0.11764705882352941
named-entity-recognition,1,"One the one hand , in most languages and domains , there is only a very small amount of supervised training data available .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.05314009661835749,0.17647058823529413,0.17647058823529413
named-entity-recognition,1,"On the other , there are few constraints on the kinds of words that can be names , so generalizing from this small sample of data is difficult .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.057971014492753624,0.23529411764705882,0.23529411764705882
named-entity-recognition,1,"As a result , carefully constructed orthographic features and language - specific knowledge resources , such as gazetteers , are widely used for solving this task .",introduction,introduction,0,13,5,5,0,introduction : introduction,0.06280193236714976,0.29411764705882354,0.29411764705882354
named-entity-recognition,1,"Unfortunately , languagespecific resources and features are costly to develop in new languages and new domains , making NER a challenge to adapt .",introduction,introduction,0,14,6,6,0,introduction : introduction,0.06763285024154589,0.35294117647058826,0.35294117647058826
named-entity-recognition,1,Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision .,introduction,introduction,0,15,7,7,0,introduction : introduction,0.07246376811594203,0.4117647058823529,0.4117647058823529
named-entity-recognition,1,"However , even systems that have relied extensively on unsupervised features have used these to augment , rather than replace , hand - engineered features ( e.g. , knowledge about capitalization patterns and character classes in a particular language ) and specialized knowledge resources ( e.g. , gazetteers ) .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.07729468599033816,0.47058823529411764,0.47058823529411764
named-entity-recognition,1,"In this paper , we present neural architectures for NER that use no language - specific resources or features beyond a small amount of supervised training data and unlabeled corpora .",introduction,introduction,0,17,9,9,0,introduction : introduction,0.0821256038647343,0.5294117647058824,0.5294117647058824
named-entity-recognition,1,Our models are designed to capture two intuitions .,introduction,introduction,0,18,10,10,0,introduction : introduction,0.08695652173913043,0.5882352941176471,0.5882352941176471
named-entity-recognition,1,"First , since names often consist of multiple tokens , reasoning jointly over tagging decisions for each token is important .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.09178743961352658,0.6470588235294118,0.6470588235294118
named-entity-recognition,1,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",introduction,introduction,1,20,12,12,0,introduction : introduction,0.0966183574879227,0.7058823529411765,0.7058823529411765
named-entity-recognition,1,"Second , token - level evidence for "" being a name "" includes both orthographic evidence ( what does the word being tagged as a name look like ? ) and distributional evidence ( where does the word being tagged tend to occur in a corpus ? ) .",introduction,introduction,0,21,13,13,0,introduction : introduction,0.10144927536231885,0.7647058823529411,0.7647058823529411
named-entity-recognition,1,"To capture orthographic sensitivity , we use character - based word representation model to capture distributional sensitivity , we combine these representations with distributional representations .",introduction,introduction,1,22,14,14,0,introduction : introduction,0.10628019323671498,0.8235294117647058,0.8235294117647058
named-entity-recognition,1,"Our word representations combine both of these , and dropout training is used to encourage the model to learn to trust both sources of evidence ( 4 ) .",introduction,introduction,0,23,15,15,0,introduction : introduction,0.1111111111111111,0.8823529411764706,0.8823529411764706
named-entity-recognition,1,"Experiments in English , Dutch , German , and Spanish show that we are able to obtain state - of - the - art NER performance with the LSTM - CRF model in Dutch , German , and Spanish , and very near the state - of - the - art in English without any hand - engineered features or gazetteers ( 5 ) .",introduction,introduction,0,24,16,16,0,introduction : introduction,0.11594202898550725,0.9411764705882353,0.9411764705882353
named-entity-recognition,1,"The transition - based algorithm likewise surpasses the best previously published results in several languages , although it performs less well than the LSTM - CRF model .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.12077294685990338,1.0,1.0
named-entity-recognition,1,LSTM - CRF,system description,LSTM-CRF Model,0,26,1,1,0,system description : LSTM-CRF Model,0.12560386473429952,1.0,1.0
named-entity-recognition,1,Model,model,model,0,27,1,1,0,model : model,0.13043478260869565,0.043478260869565216,0.3333333333333333
named-entity-recognition,1,"We provide a brief description of LSTMs and CRFs , and present a hybrid tagging architecture .",model,model,0,28,2,2,0,model : model,0.13526570048309178,0.08695652173913043,0.6666666666666666
named-entity-recognition,1,This architecture is similar to the ones presented by .,model,model,0,29,3,3,0,model : model,0.14009661835748793,0.13043478260869565,1.0
named-entity-recognition,1,LSTM,model,LSTM,0,30,4,1,0,model : LSTM,0.14492753623188406,0.17391304347826086,0.05263157894736842
named-entity-recognition,1,Recurrent neural networks ( RNNs ) are a family of neural networks that operate on sequential data .,model,LSTM,0,31,5,2,0,model : LSTM,0.1497584541062802,0.21739130434782608,0.10526315789473684
named-entity-recognition,1,"They take as input a sequence of vectors ( x 1 , x 2 , . . . , x n ) and return another sequence ( h 1 , h 2 , . . . , h n ) that represents some information about the sequence at every step in the input .",model,LSTM,0,32,6,3,0,model : LSTM,0.15458937198067632,0.2608695652173913,0.15789473684210525
named-entity-recognition,1,"Although RNNs can , in theory , learn long dependencies , in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence .",model,LSTM,0,33,7,4,0,model : LSTM,0.15942028985507245,0.30434782608695654,0.21052631578947367
named-entity-recognition,1,Long Short - term Memory Networks ( LSTMs ) have been designed to combat this issue by incorporating a memory - cell and have been shown to capture long - range dependencies .,model,LSTM,0,34,8,5,0,model : LSTM,0.1642512077294686,0.34782608695652173,0.2631578947368421
named-entity-recognition,1,"They do so using several gates that control the proportion of the input to give to the memory cell , and the proportion from the previous state to forget .",model,LSTM,0,35,9,6,0,model : LSTM,0.16908212560386474,0.391304347826087,0.3157894736842105
named-entity-recognition,1,We use the following implementation :,model,LSTM,0,36,10,7,0,model : LSTM,0.17391304347826086,0.43478260869565216,0.3684210526315789
named-entity-recognition,1,"where ? is the element - wise sigmoid function , and is the element - wise product .",model,LSTM,0,37,11,8,0,model : LSTM,0.178743961352657,0.4782608695652174,0.42105263157894735
named-entity-recognition,1,"where ? is the element - wise sigmoid function , and is the element - wise product .",model,LSTM,0,38,12,9,0,model : LSTM,0.18357487922705315,0.5217391304347826,0.47368421052631576
named-entity-recognition,1,"For a given sentence ( x 1 , x 2 , . . . , x n ) containing n words , each represented as a d-dimensional vector , an LSTM computes a representation ? ? ht of the left context of the sentence at every word t.",model,LSTM,0,39,13,10,0,model : LSTM,0.18840579710144928,0.5652173913043478,0.5263157894736842
named-entity-recognition,1,"For a given sentence ( x 1 , x 2 , . . . , x n ) containing n words , each represented as a d-dimensional vector , an LSTM computes a representation ? ? ht of the left context of the sentence at every word t.",model,LSTM,0,40,14,11,0,model : LSTM,0.1932367149758454,0.6086956521739131,0.5789473684210527
named-entity-recognition,1,"Naturally , generating a representation of the right context ? ? ht as well should add useful information .",model,LSTM,0,41,15,12,0,model : LSTM,0.19806763285024154,0.6521739130434783,0.631578947368421
named-entity-recognition,1,"Naturally , generating a representation of the right context ? ? ht as well should add useful information .",model,LSTM,0,42,16,13,0,model : LSTM,0.2028985507246377,0.6956521739130435,0.6842105263157895
named-entity-recognition,1,This can be achieved using a second LSTM that reads the same sequence in reverse .,model,LSTM,0,43,17,14,0,model : LSTM,0.20772946859903382,0.7391304347826086,0.7368421052631579
named-entity-recognition,1,We will refer to the former as the forward LSTM and the latter as the backward LSTM .,model,LSTM,0,44,18,15,0,model : LSTM,0.21256038647342995,0.782608695652174,0.7894736842105263
named-entity-recognition,1,These are two distinct networks with different parameters .,model,LSTM,0,45,19,16,0,model : LSTM,0.21739130434782608,0.8260869565217391,0.8421052631578947
named-entity-recognition,1,This forward and backward LSTM pair is referred to as a bidirectional LSTM .,model,LSTM,0,46,20,17,0,model : LSTM,0.2222222222222222,0.8695652173913043,0.8947368421052632
named-entity-recognition,1,"The representation of a word using this model is obtained by concatenating its left and right context representations , These representations effectively include a representation of a word in context , which is useful for numerous tagging applications .",model,LSTM,0,47,21,18,0,model : LSTM,0.22705314009661837,0.9130434782608695,0.9473684210526315
named-entity-recognition,1,"The representation of a word using this model is obtained by concatenating its left and right context representations , These representations effectively include a representation of a word in context , which is useful for numerous tagging applications .",model,LSTM,0,48,22,19,0,model : LSTM,0.2318840579710145,0.9565217391304348,1.0
named-entity-recognition,1,CRF,model,CRF Tagging Models,0,49,23,1,0,model : CRF Tagging Models,0.23671497584541062,1.0,1.0
named-entity-recognition,1,Tagging Models,model,model,0,50,1,1,0,model : model,0.24154589371980675,0.05263157894736842,0.05263157894736842
named-entity-recognition,1,Avery simple - but surprisingly effective - tagging model is to use the ht 's as features to make independent tagging decisions for each output y t.,model,model,0,51,2,2,0,model : model,0.2463768115942029,0.10526315789473684,0.10526315789473684
named-entity-recognition,1,"Despite this model 's success in simple problems like POS tagging , its independent classification decisions are limiting when there are strong dependencies across output labels .",model,model,0,52,3,3,0,model : model,0.25120772946859904,0.15789473684210525,0.15789473684210525
named-entity-recognition,1,"NER is one such task , since the "" grammar "" that characterizes interpretable sequences of tags imposes several hard constraints ( e.g. , I - PER can not follow B - LOC ; see 2.4 for details ) that would be impossible to model with independence assumptions .",model,model,0,53,4,4,0,model : model,0.2560386473429952,0.21052631578947367,0.21052631578947367
named-entity-recognition,1,"Therefore , instead of modeling tagging decisions independently , we model them jointly using a conditional random field .",model,model,0,54,5,5,0,model : model,0.2608695652173913,0.2631578947368421,0.2631578947368421
named-entity-recognition,1,For an input sentence,model,model,0,55,6,6,0,model : model,0.26570048309178745,0.3157894736842105,0.3157894736842105
named-entity-recognition,1,we consider P to be the matrix of scores output by the bidirectional LSTM network .,model,model,0,56,7,7,0,model : model,0.27053140096618356,0.3684210526315789,0.3684210526315789
named-entity-recognition,1,"is of size n k , where k is the number of distinct tags , and P i , j corresponds to the score of the j th tag of the i th word in a sentence .",model,model,0,57,8,8,0,model : model,0.2753623188405797,0.42105263157894735,0.42105263157894735
named-entity-recognition,1,"For a sequence of predictions y = ( y 1 , y 2 , . . . , y n ) , we define it s score to be",model,model,0,58,9,9,0,model : model,0.28019323671497587,0.47368421052631576,0.47368421052631576
named-entity-recognition,1,"where A is a matrix of transition scores such that A i , j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence , that we add to the set of possible tags .",model,model,0,59,10,10,0,model : model,0.28502415458937197,0.5263157894736842,0.5263157894736842
named-entity-recognition,1,is therefore a square matrix of size k + 2 .,model,model,0,60,11,11,0,model : model,0.2898550724637681,0.5789473684210527,0.5789473684210527
named-entity-recognition,1,softmax over all possible tag sequences yields a probability for the sequence y:,model,model,0,61,12,12,0,model : model,0.2946859903381642,0.631578947368421,0.631578947368421
named-entity-recognition,1,"?Y Xe s ( X , y) .",model,model,0,62,13,13,0,model : model,0.2995169082125604,0.6842105263157895,0.6842105263157895
named-entity-recognition,1,"During training , we maximize the log-probability of the correct tag sequence :",model,model,0,63,14,14,0,model : model,0.30434782608695654,0.7368421052631579,0.7368421052631579
named-entity-recognition,1,where Y X represents all possible tag sequences ( even those that do not verify the IOB format ) for a sentence X .,model,model,0,64,15,15,0,model : model,0.30917874396135264,0.7894736842105263,0.7894736842105263
named-entity-recognition,1,"From the formulation above , it is evident that we encourage our network to produce a valid sequence of output labels .",model,model,0,65,16,16,0,model : model,0.3140096618357488,0.8421052631578947,0.8421052631578947
named-entity-recognition,1,"While decoding , we predict the output sequence that obtains the maximum score given by :",model,model,0,66,17,17,0,model : model,0.3188405797101449,0.8947368421052632,0.8947368421052632
named-entity-recognition,1,"Since we are only modeling bigram interactions between outputs , both the summation in Eq. 1 and the maximum a posteriori sequence y * in Eq.",model,model,0,67,18,18,0,model : model,0.32367149758454106,0.9473684210526315,0.9473684210526315
named-entity-recognition,1,can be computed using dynamic programming .,model,model,0,68,19,19,0,model : model,0.3285024154589372,1.0,1.0
named-entity-recognition,1,Parameterization and Training,training,Parameterization and Training,0,69,1,1,0,training : Parameterization and Training,0.3333333333333333,0.011764705882352941,0.07692307692307693
named-entity-recognition,1,"The scores associated with each tagging decision for each token ( i.e. , the P i , y 's ) are defined to be the dot product between the embedding of a wordin - context computed with a bidirectional LSTMexactly the same as the POS tagging model of and these are combined with bigram compatibility scores ( i.e. , the A y , y 's ) .",training,Parameterization and Training,0,70,2,2,0,training : Parameterization and Training,0.33816425120772947,0.023529411764705882,0.15384615384615385
named-entity-recognition,1,This architecture is shown in figure,training,Parameterization and Training,0,71,3,3,0,training : Parameterization and Training,0.34299516908212563,0.03529411764705882,0.23076923076923078
named-entity-recognition,1,". Circles represent observed variables , diamonds are deterministic functions of their parents , and double circles are random variables .",training,Parameterization and Training,0,72,4,4,0,training : Parameterization and Training,0.34782608695652173,0.047058823529411764,0.3076923076923077
named-entity-recognition,1,"The parameters of this model are thus the matrix of bigram compatibility scores A , and the parameters that give rise to the matrix P , namely the parameters of the bidirectional LSTM , the linear feature weights , and the word embeddings .",training,Parameterization and Training,0,73,5,5,0,training : Parameterization and Training,0.3526570048309179,0.058823529411764705,0.38461538461538464
named-entity-recognition,1,"As in part 2.2 , let x i denote the sequence of word embeddings for every word in a sentence , and y i be their associated tags .",training,Parameterization and Training,0,74,6,6,0,training : Parameterization and Training,0.357487922705314,0.07058823529411765,0.46153846153846156
named-entity-recognition,1,We return to a discussion of how the embeddings xi are modeled in Section 4 .,training,Parameterization and Training,0,75,7,7,0,training : Parameterization and Training,0.36231884057971014,0.08235294117647059,0.5384615384615384
named-entity-recognition,1,"The sequence of word embeddings is given as input to a bidirectional LSTM , which returns a representation of the left and right context for each word as explained in 2.1 .",training,Parameterization and Training,0,76,8,8,0,training : Parameterization and Training,0.3671497584541063,0.09411764705882353,0.6153846153846154
named-entity-recognition,1,These representations are concatenated ( c i ) and linearly projected onto a layer whose size is equal to the number of distinct tags .,training,Parameterization and Training,0,77,9,9,0,training : Parameterization and Training,0.3719806763285024,0.10588235294117647,0.6923076923076923
named-entity-recognition,1,"Instead of using the softmax output from this layer , we use a CRF as previously described to take into account neighboring tags , yielding the final predictions for every wordy i .",training,Parameterization and Training,0,78,10,10,0,training : Parameterization and Training,0.37681159420289856,0.11764705882352941,0.7692307692307693
named-entity-recognition,1,"Additionally , we observed that adding a hidden layer between c i and the CRF layer marginally improved our results .",training,Parameterization and Training,0,79,11,11,0,training : Parameterization and Training,0.38164251207729466,0.12941176470588237,0.8461538461538461
named-entity-recognition,1,All results reported with this model incorporate this extra-layer .,training,Parameterization and Training,0,80,12,12,0,training : Parameterization and Training,0.3864734299516908,0.1411764705882353,0.9230769230769231
named-entity-recognition,1,"The parameters are trained to maximize Eq. 1 of observed sequences of NER tags in an annotated corpus , given the observed words .",training,Parameterization and Training,0,81,13,13,0,training : Parameterization and Training,0.391304347826087,0.15294117647058825,1.0
named-entity-recognition,1,Tagging Schemes,training,Tagging Schemes,0,82,14,1,0,training : Tagging Schemes,0.3961352657004831,0.16470588235294117,0.125
named-entity-recognition,1,The task of named entity recognition is to assign a named entity label to every word in a sentence .,training,Tagging Schemes,0,83,15,2,0,training : Tagging Schemes,0.40096618357487923,0.17647058823529413,0.25
named-entity-recognition,1,single named entity could span several tokens within a sentence .,training,Tagging Schemes,0,84,16,3,0,training : Tagging Schemes,0.4057971014492754,0.18823529411764706,0.375
named-entity-recognition,1,"Sentences are usually represented in the IOB format ( Inside , Outside , Beginning ) where every token is labeled as B- label if the token is the beginning of a named entity , I-label if it is inside a named entity but not the first token within the named entity , or O otherwise .",training,Tagging Schemes,0,85,17,4,0,training : Tagging Schemes,0.4106280193236715,0.2,0.5
named-entity-recognition,1,"However , we decided to use the IOBES tagging scheme , a variant of IOB commonly used for named entity recognition , which encodes information about singleton entities ( S ) and explicitly marks the end of named entities ( E ) .",training,Tagging Schemes,0,86,18,5,0,training : Tagging Schemes,0.41545893719806765,0.21176470588235294,0.625
named-entity-recognition,1,"Using this scheme , tagging a word as I-label with high - confidence narrows down the choices for the subsequent word to I-label or E-label , however , the IOB scheme is only capable of determining that the subsequent word can not be the interior of another label .",training,Tagging Schemes,0,87,19,6,0,training : Tagging Schemes,0.42028985507246375,0.2235294117647059,0.75
named-entity-recognition,1,Ratinov and Roth and showed that using a more expressive tagging scheme like IOBES improves model performance marginally .,training,Tagging Schemes,0,88,20,7,0,training : Tagging Schemes,0.4251207729468599,0.23529411764705882,0.875
named-entity-recognition,1,"However , we did not observe a significant improvement over the IOB tagging scheme .",training,Tagging Schemes,0,89,21,8,0,training : Tagging Schemes,0.42995169082125606,0.24705882352941178,1.0
named-entity-recognition,1,Transition - Based Chunking Model,training,Transition-Based Chunking Model,0,90,22,1,0,training : Transition-Based Chunking Model,0.43478260869565216,0.25882352941176473,0.1111111111111111
named-entity-recognition,1,"As an alternative to the LSTM - CRF discussed in the previous section , we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition - based dependency parsing .",training,Transition-Based Chunking Model,0,91,23,2,0,training : Transition-Based Chunking Model,0.4396135265700483,0.27058823529411763,0.2222222222222222
named-entity-recognition,1,"This model directly constructs representations of the multi-token names ( e.g. , the name Mark Watney is composed into a single representation ) .",training,Transition-Based Chunking Model,0,92,24,3,0,training : Transition-Based Chunking Model,0.4444444444444444,0.2823529411764706,0.3333333333333333
named-entity-recognition,1,This model relies on a stack data structure to incrementally construct chunks of the input .,training,Transition-Based Chunking Model,0,93,25,4,0,training : Transition-Based Chunking Model,0.4492753623188406,0.29411764705882354,0.4444444444444444
named-entity-recognition,1,"To obtain representations of this stack used for predicting subsequent actions , we use the Stack - LSTM presented by , in which the LSTM is augmented with a "" stack pointer . """,training,Transition-Based Chunking Model,0,94,26,5,0,training : Transition-Based Chunking Model,0.45410628019323673,0.3058823529411765,0.5555555555555556
named-entity-recognition,1,"While sequential LSTMs model sequences from left to right , stack LSTMs permit embedding of a stack of objects thatare both added to ( using a push operation ) and removed from ( using a pop operation ) .",training,Transition-Based Chunking Model,0,95,27,6,0,training : Transition-Based Chunking Model,0.45893719806763283,0.3176470588235294,0.6666666666666666
named-entity-recognition,1,"This allows the Stack - LSTM to work like a stack that maintains a "" summary embedding "" of its contents .",training,Transition-Based Chunking Model,0,96,28,7,0,training : Transition-Based Chunking Model,0.463768115942029,0.32941176470588235,0.7777777777777778
named-entity-recognition,1,We refer to this model as Stack - LSTM or S - LSTM model for simplicity .,training,Transition-Based Chunking Model,0,97,29,8,0,training : Transition-Based Chunking Model,0.46859903381642515,0.3411764705882353,0.8888888888888888
named-entity-recognition,1,"Finally , we refer interested readers to the original paper for details about the Stack - LSTM model since in this paper we merely use the same architecture through a new transition - based algorithm presented in the following Section .",training,Transition-Based Chunking Model,0,98,30,9,0,training : Transition-Based Chunking Model,0.47342995169082125,0.35294117647058826,1.0
named-entity-recognition,1,Chunking Algorithm,training,Chunking Algorithm,0,99,31,1,0,training : Chunking Algorithm,0.4782608695652174,0.36470588235294116,0.06666666666666667
named-entity-recognition,1,"We designed a transition inventory which is given in that is inspired by transition - based parsers , in particular the arc-standard parser of .",training,Chunking Algorithm,0,100,32,2,0,training : Chunking Algorithm,0.4830917874396135,0.3764705882352941,0.13333333333333333
named-entity-recognition,1,"In this algorithm , we make use of two stacks ( designated output and stack representing , respectively , completed chunks and scratch space ) and a buffer that contains the words that have yet to be processed .",training,Chunking Algorithm,0,101,33,3,0,training : Chunking Algorithm,0.48792270531400966,0.38823529411764707,0.2
named-entity-recognition,1,The transition inventory contains the following transitions :,training,Chunking Algorithm,0,102,34,4,0,training : Chunking Algorithm,0.4927536231884058,0.4,0.26666666666666666
named-entity-recognition,1,"The SHIFT transition moves a word from the buffer to the stack , the OUT transition moves a word from the buffer directly into the output stack while the REDUCE ( y ) transition pops all items from the top of the stack creating a "" chunk , "" labels this with label y , and pushes a representation of this chunk onto the output stack .",training,Chunking Algorithm,0,103,35,5,0,training : Chunking Algorithm,0.4975845410628019,0.4117647058823529,0.3333333333333333
named-entity-recognition,1,The algorithm completes when the stack and buffer are both empty .,training,Chunking Algorithm,0,104,36,6,0,training : Chunking Algorithm,0.5024154589371981,0.4235294117647059,0.4
named-entity-recognition,1,"The algorithm is depicted in , which shows the sequence of operations required to process the sentence Mark Watney visited Mars .",training,Chunking Algorithm,0,105,37,7,0,training : Chunking Algorithm,0.5072463768115942,0.43529411764705883,0.4666666666666667
named-entity-recognition,1,"The model is parameterized by defining a probability distribution over actions at each time step , given the current contents of the stack , buffer , and output , as well as the history of actions taken .",training,Chunking Algorithm,0,106,38,8,0,training : Chunking Algorithm,0.5120772946859904,0.4470588235294118,0.5333333333333333
named-entity-recognition,1,"Following , we use stack LSTMs to compute a fixed dimensional embedding of each of these , and take a concatenation of these to obtain the full algorithm state .",training,Chunking Algorithm,0,107,39,9,0,training : Chunking Algorithm,0.5169082125603864,0.4588235294117647,0.6
named-entity-recognition,1,This representation is used to define a distribution over the possible actions that can betaken at each time step .,training,Chunking Algorithm,0,108,40,10,0,training : Chunking Algorithm,0.5217391304347826,0.47058823529411764,0.6666666666666666
named-entity-recognition,1,The model is trained to maximize the conditional probability of sequences of reference actions ( extracted from a labeled training corpus ) given the input sentences .,training,Chunking Algorithm,0,109,41,11,0,training : Chunking Algorithm,0.5265700483091788,0.4823529411764706,0.7333333333333333
named-entity-recognition,1,"To label a new input sequence at test time , the maximum probability action is chosen greedily until the algorithm reaches a termination state .",training,Chunking Algorithm,0,110,42,12,0,training : Chunking Algorithm,0.5314009661835749,0.49411764705882355,0.8
named-entity-recognition,1,"Although this is not guaranteed to find a global optimum , it is effective in practice .",training,Chunking Algorithm,0,111,43,13,0,training : Chunking Algorithm,0.5362318840579711,0.5058823529411764,0.8666666666666667
named-entity-recognition,1,"Since each token is either moved directly to the output ( 1 action ) or first to the stack and then the output ( 2 actions ) , the total number of actions for a sequence of length n is maximally 2n .",training,Chunking Algorithm,0,112,44,14,0,training : Chunking Algorithm,0.5410628019323671,0.5176470588235295,0.9333333333333333
named-entity-recognition,1,It is worth noting that the nature of this algorithm,training,Chunking Algorithm,0,113,45,15,0,training : Chunking Algorithm,0.5458937198067633,0.5294117647058824,1.0
named-entity-recognition,1,Representing Labeled Chunks,training,Representing Labeled Chunks,0,114,46,1,0,training : Representing Labeled Chunks,0.5507246376811594,0.5411764705882353,0.2
named-entity-recognition,1,"When the REDUCE ( y ) operation is executed , the algorithm shifts a sequence of tokens ( together with their vector embeddings ) from the stack to the output buffer as a single completed chunk .",training,Representing Labeled Chunks,0,115,47,2,0,training : Representing Labeled Chunks,0.5555555555555556,0.5529411764705883,0.4
named-entity-recognition,1,"To compute an embedding of this sequence , we run a bidirectional LSTM over the embeddings of its constituent tokens together with a token representing the type of the chunk being identified ( i.e. , y ) .",training,Representing Labeled Chunks,0,116,48,3,0,training : Representing Labeled Chunks,0.5603864734299517,0.5647058823529412,0.6
named-entity-recognition,1,"This function is given as g ( u , . . . , v , r y ) , where r y is a learned embedding of a label type .",training,Representing Labeled Chunks,0,117,49,4,0,training : Representing Labeled Chunks,0.5652173913043478,0.5764705882352941,0.8
named-entity-recognition,1,"Thus , the output buffer contains a single vector representation for each labeled chunk that is generated , regardless of its length .",training,Representing Labeled Chunks,0,118,50,5,0,training : Representing Labeled Chunks,0.5700483091787439,0.5882352941176471,1.0
named-entity-recognition,1,Input Word Embeddings,training,Input Word Embeddings,0,119,51,1,0,training : Input Word Embeddings,0.5748792270531401,0.6,0.125
named-entity-recognition,1,The input layers to both of our models are vector representations of individual words .,training,Input Word Embeddings,0,120,52,2,0,training : Input Word Embeddings,0.5797101449275363,0.611764705882353,0.25
named-entity-recognition,1,Learning independent representations for word types from the limited NER training data is a difficult problem : there are simply too many parameters to reliably estimate .,training,Input Word Embeddings,0,121,53,3,0,training : Input Word Embeddings,0.5845410628019324,0.6235294117647059,0.375
named-entity-recognition,1,"Since many languages have orthographic or morphological evidence that something is a name ( or not a name ) , we want representations thatare sensitive to the spelling of words .",training,Input Word Embeddings,0,122,54,4,0,training : Input Word Embeddings,0.5893719806763285,0.6352941176470588,0.5
named-entity-recognition,1,We therefore use a model that constructs representations of words from representations of the characters they are composed of ( 4.1 ) .,training,Input Word Embeddings,0,123,55,5,0,training : Input Word Embeddings,0.5942028985507246,0.6470588235294118,0.625
named-entity-recognition,1,"Our second intuition is that names , which may individually be quite varied , appear in regular contexts in large corpora .",training,Input Word Embeddings,0,124,56,6,0,training : Input Word Embeddings,0.5990338164251208,0.6588235294117647,0.75
named-entity-recognition,1,Therefore we use embed - dings learned from a large corpus thatare sensitive to word order ( 4.2 ) .,training,Input Word Embeddings,0,125,57,7,0,training : Input Word Embeddings,0.6038647342995169,0.6705882352941176,0.875
named-entity-recognition,1,"Finally , to prevent the models from depending on one representation or the other too strongly , we use dropout training and find this is crucial for good generalization performance ( 4.3 ) .",training,Input Word Embeddings,0,126,58,8,0,training : Input Word Embeddings,0.6086956521739131,0.6823529411764706,1.0
named-entity-recognition,1,Character - based models of words,training,Character-based models of words,0,127,59,1,0,training : Character-based models of words,0.6135265700483091,0.6941176470588235,0.0625
named-entity-recognition,1,An important distinction of our work from most previous approaches is that we learn character - level features while training instead of hand - engineering prefix and suffix information about words .,training,Character-based models of words,0,128,60,2,0,training : Character-based models of words,0.6183574879227053,0.7058823529411765,0.125
named-entity-recognition,1,Learning character - level embeddings has the advantage of learning representations specific to the task and domain at hand .,training,Character-based models of words,0,129,61,3,0,training : Character-based models of words,0.6231884057971014,0.7176470588235294,0.1875
named-entity-recognition,1,They have been found useful for morphologically rich languages and to handle the outof - vocabulary problem for tasks like part - of - speech tagging and language modeling or dependency parsing . describes our architecture to generate a word embedding for a word from its characters .,training,Character-based models of words,0,130,62,4,0,training : Character-based models of words,0.6280193236714976,0.7294117647058823,0.25
named-entity-recognition,1,character lookup table initialized at random contains an embedding for every character .,training,Character-based models of words,0,131,63,5,0,training : Character-based models of words,0.6328502415458938,0.7411764705882353,0.3125
named-entity-recognition,1,The character embeddings corresponding to every character in a word are given indirect and reverse order to a forward and a backward LSTM .,training,Character-based models of words,0,132,64,6,0,training : Character-based models of words,0.6376811594202898,0.7529411764705882,0.375
named-entity-recognition,1,The embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional LSTM .,training,Character-based models of words,0,133,65,7,0,training : Character-based models of words,0.642512077294686,0.7647058823529411,0.4375
named-entity-recognition,1,This character - level representation is then concatenated with a word - level representation from a word lookup - table .,training,Character-based models of words,0,134,66,8,0,training : Character-based models of words,0.6473429951690821,0.7764705882352941,0.5
named-entity-recognition,1,"During testing , words that do not have an embedding in the lookup table are mapped to a UNK embedding .",training,Character-based models of words,0,135,67,9,0,training : Character-based models of words,0.6521739130434783,0.788235294117647,0.5625
named-entity-recognition,1,"To train the UNK embedding , we replace singletons with the UNK embedding with a probability 0.5 .",training,Character-based models of words,0,136,68,10,0,training : Character-based models of words,0.6570048309178744,0.8,0.625
named-entity-recognition,1,"In all our experiments , the hidden dimension of the forward and backward character LSTMs are 25 each , which results in our character - based representation of words being of dimension 50 .",training,Character-based models of words,0,137,69,11,0,training : Character-based models of words,0.6618357487922706,0.8117647058823529,0.6875
named-entity-recognition,1,"Recurrent models like RNNs and LSTMs are capable of encoding very long sequences , however , they have a representation biased towards their most recent inputs .",training,Character-based models of words,0,138,70,12,0,training : Character-based models of words,0.6666666666666666,0.8235294117647058,0.75
named-entity-recognition,1,"As a result , we expect the final representation of the forward LSTM to bean accurate representation of the suffix of the word , and the final state of the backward LSTM to be a better representation of its prefix .",training,Character-based models of words,0,139,71,13,0,training : Character-based models of words,0.6714975845410628,0.8352941176470589,0.8125
named-entity-recognition,1,Alternative approachesmost notably like convolutional networks - have been proposed to learn representations of words from their characters .,training,Character-based models of words,0,140,72,14,0,training : Character-based models of words,0.6763285024154589,0.8470588235294118,0.875
named-entity-recognition,1,"However , convnets are designed to discover position - invariant features of their inputs .",training,Character-based models of words,0,141,73,15,0,training : Character-based models of words,0.6811594202898551,0.8588235294117647,0.9375
named-entity-recognition,1,"While this is appropriate for many problems , e.g. , image recognition ( a cat can appear anywhere in a picture ) , we argue that important information is position dependent ( e.g. , prefixes and suffixes encode different information than stems ) , making LSTMs an a priori better function class for modeling the relationship between words and their characters .",training,Character-based models of words,0,142,74,16,0,training : Character-based models of words,0.6859903381642513,0.8705882352941177,1.0
named-entity-recognition,1,Pretrained embeddings,training,Pretrained embeddings,0,143,75,1,0,training : Pretrained embeddings,0.6908212560386473,0.8823529411764706,0.14285714285714285
named-entity-recognition,1,"As in , we use pretrained word embeddings to initialize our lookup table .",training,Pretrained embeddings,0,144,76,2,0,training : Pretrained embeddings,0.6956521739130435,0.8941176470588236,0.2857142857142857
named-entity-recognition,1,We observe significant improvements using pretrained word embeddings over randomly initialized ones .,training,Pretrained embeddings,0,145,77,3,0,training : Pretrained embeddings,0.7004830917874396,0.9058823529411765,0.42857142857142855
named-entity-recognition,1,"Embeddings are pretrained using skip - n- gram ( Ling et al. , 2015 a ) , a variation of word2vec that accounts for word order .",training,Pretrained embeddings,0,146,78,4,0,training : Pretrained embeddings,0.7053140096618358,0.9176470588235294,0.5714285714285714
named-entity-recognition,1,These embeddings are fine - tuned during training .,training,Pretrained embeddings,0,147,79,5,0,training : Pretrained embeddings,0.7101449275362319,0.9294117647058824,0.7142857142857143
named-entity-recognition,1,"Word embeddings for Spanish , Dutch , German and English are trained using the Spanish Gigaword version 3 , the Leipzig corpora collection , the German monolingual training data from the 2010 Machine Translation Workshop and the English Gigaword version 4 ( with the LA Times and NY Times portions removed ) respectively .",training,Pretrained embeddings,0,148,80,6,0,training : Pretrained embeddings,0.714975845410628,0.9411764705882353,0.8571428571428571
named-entity-recognition,1,"We use an embedding dimension of 100 for English , 64 for other languages , a minimum word frequency cutoff of 4 , and a window size of 8 .",training,Pretrained embeddings,0,149,81,7,0,training : Pretrained embeddings,0.7198067632850241,0.9529411764705882,1.0
named-entity-recognition,1,Dropout training,training,Dropout training,0,150,82,1,0,training : Dropout training,0.7246376811594203,0.9647058823529412,0.25
named-entity-recognition,1,Initial experiments showed that character - level embeddings did not improve our over all performance when used in conjunction with pretrained word representations .,training,Dropout training,0,151,83,2,0,training : Dropout training,0.7294685990338164,0.9764705882352941,0.5
named-entity-recognition,1,"To encourage the model to depend on both representations , we use dropout training , applying a dropout mask to the final embedding layer just before the input to the bidirectional LSTM in .",training,Dropout training,0,152,84,3,0,training : Dropout training,0.7342995169082126,0.9882352941176471,0.75
named-entity-recognition,1,We observe a significant improvement in our model 's performance after using dropout ( see ) .,training,Dropout training,0,153,85,4,0,training : Dropout training,0.7391304347826086,1.0,1.0
named-entity-recognition,1,Experiments,experiment,Experiments,0,154,1,1,0,experiment : Experiments,0.7439613526570048,0.5,0.5
named-entity-recognition,1,"This section presents the methods we use to train our models , the results we obtained on various tasks and the impact of our networks ' configuration on model performance .",experiment,Experiments,0,155,2,2,0,experiment : Experiments,0.748792270531401,1.0,1.0
named-entity-recognition,1,Training,training,Training,0,156,1,1,0,training : Training,0.7536231884057971,0.07692307692307693,0.07692307692307693
named-entity-recognition,1,"For both models presented , we train our networks using the back - propagation algorithm updating our parameters on every training example , one at a time , using stochastic gradient descent ( SGD ) with a learning rate of 0.01 and a gradient clipping of 5.0 .",training,Training,1,157,2,2,0,training : Training,0.7584541062801933,0.15384615384615385,0.15384615384615385
named-entity-recognition,1,"Several methods have been proposed to enhance the performance of SGD , such as Adadelta or Adam ( Kingma and Ba , 2014 ) .",training,Training,0,158,3,3,0,training : Training,0.7632850241545893,0.23076923076923078,0.23076923076923078
named-entity-recognition,1,"Although we observe faster convergence using these methods , none of them perform as well as SGD with gradient clipping .",training,Training,0,159,4,4,0,training : Training,0.7681159420289855,0.3076923076923077,0.3076923076923077
named-entity-recognition,1,Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,training,Training,1,160,5,5,0,training : Training,0.7729468599033816,0.38461538461538464,0.38461538461538464
named-entity-recognition,1,Tuning this dimension did not significantly impact model performance .,training,Training,0,161,6,6,0,training : Training,0.7777777777777778,0.46153846153846156,0.46153846153846156
named-entity-recognition,1,We set the dropout rate to 0.5 .,training,Training,1,162,7,7,0,training : Training,0.782608695652174,0.5384615384615384,0.5384615384615384
named-entity-recognition,1,"Using higher rates negatively impacted our results , while smaller rates led to longer training time .",training,Training,0,163,8,8,0,training : Training,0.7874396135265701,0.6153846153846154,0.6153846153846154
named-entity-recognition,1,The stack - LSTM model uses two layers each of dimension 100 for each stack .,training,Training,1,164,9,9,0,training : Training,0.7922705314009661,0.6923076923076923,0.6923076923076923
named-entity-recognition,1,"The embeddings of the actions used in the composition functions have 16 dimensions each , and the output embedding is of dimension 20 .",training,Training,1,165,10,10,0,training : Training,0.7971014492753623,0.7692307692307693,0.7692307692307693
named-entity-recognition,1,We experimented with different dropout rates and reported the scores using the best dropout rate for each language .,training,Training,0,166,11,11,0,training : Training,0.8019323671497585,0.8461538461538461,0.8461538461538461
named-entity-recognition,1,3,training,Training,0,167,12,12,0,training : Training,0.8067632850241546,0.9230769230769231,0.9230769230769231
named-entity-recognition,1,"It is a greedy model that apply locally optimal actions until the entire sentence is processed , further improvements might be obtained with beam search or training with exploration .",training,Training,0,168,13,13,0,training : Training,0.8115942028985508,1.0,1.0
named-entity-recognition,1,Data Sets,data set,Data Sets,0,169,1,1,0,data set : Data Sets,0.8164251207729468,0.05555555555555555,0.05555555555555555
named-entity-recognition,1,We test our model on different datasets for named entity recognition .,data set,Data Sets,0,170,2,2,0,data set : Data Sets,0.821256038647343,0.1111111111111111,0.1111111111111111
named-entity-recognition,1,"To demonstrate our model 's ability to generalize to different languages , we present results on the ) that contain independent named entity labels for English , Spanish , German and Dutch .",data set,Data Sets,0,171,3,3,0,data set : Data Sets,0.8260869565217391,0.16666666666666666,0.16666666666666666
named-entity-recognition,1,"All datasets contain four different types of named entities : locations , persons , organizations , and miscellaneous entities that do not belong in any of the three previous categories .",data set,Data Sets,0,172,4,4,0,data set : Data Sets,0.8309178743961353,0.2222222222222222,0.2222222222222222
named-entity-recognition,1,"Although POS tags were made available for all datasets , we did not include them in our models .",data set,Data Sets,0,173,5,5,0,data set : Data Sets,0.8357487922705314,0.2777777777777778,0.2777777777777778
named-entity-recognition,1,"We did not perform any dataset preprocessing , apart from replacing every digit with a zero in the English NER dataset .",data set,Data Sets,0,174,6,6,0,data set : Data Sets,0.8405797101449275,0.3333333333333333,0.3333333333333333
named-entity-recognition,1,presents our comparisons with other models for named entity recognition in English .,data set,Data Sets,0,175,7,7,0,data set : Data Sets,0.8454106280193237,0.3888888888888889,0.3888888888888889
named-entity-recognition,1,"To make the comparison between our model and others fair , we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases .",data set,Data Sets,0,176,8,8,0,data set : Data Sets,0.8502415458937198,0.4444444444444444,0.4444444444444444
named-entity-recognition,1,Our models do not use gazetteers or any external labeled resources .,data set,Data Sets,0,177,9,9,0,data set : Data Sets,0.855072463768116,0.5,0.5
named-entity-recognition,1,The best score reported on this task is by .,data set,Data Sets,0,178,10,10,0,data set : Data Sets,0.8599033816425121,0.5555555555555556,0.5555555555555556
named-entity-recognition,1,They obtained a F 1 of 91.2 by jointly modeling the NER and entity linking tasks .,data set,Data Sets,0,179,11,11,0,data set : Data Sets,0.8647342995169082,0.6111111111111112,0.6111111111111112
named-entity-recognition,1,"Their model uses a lot of hand - engineered features including spelling features , WordNet clusters , Brown clusters , POS tags , chunks tags , as well as stemming and external knowledge bases like Freebase and Wikipedia .",data set,Data Sets,0,180,12,12,0,data set : Data Sets,0.8695652173913043,0.6666666666666666,0.6666666666666666
named-entity-recognition,1,"Our LSTM - CRF model outperforms all other systems , including the ones using external labeled data like gazetteers .",data set,Data Sets,1,181,13,13,0,data set : Data Sets,0.8743961352657005,0.7222222222222222,0.7222222222222222
named-entity-recognition,1,"Our Stack - LSTM model also outperforms all previous models that do not incorporate external features , apart from the one presented by .",data set,Data Sets,1,182,14,14,0,data set : Data Sets,0.8792270531400966,0.7777777777777778,0.7777777777777778
named-entity-recognition,1,"and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .",data set,Data Sets,1,183,15,15,0,data set : Data Sets,0.8840579710144928,0.8333333333333334,0.8333333333333334
named-entity-recognition,1,"On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .",data set,Data Sets,1,184,16,16,0,data set : Data Sets,0.8888888888888888,0.8888888888888888,0.8888888888888888
named-entity-recognition,1,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .",data set,Data Sets,1,185,17,17,0,data set : Data Sets,0.893719806763285,0.9444444444444444,0.9444444444444444
named-entity-recognition,1,The Stack - LSTM also consistently presents statethe - art ( or close to ) results compared to systems that do not use external data .,data set,Data Sets,1,186,18,18,0,data set : Data Sets,0.8985507246376812,1.0,1.0
named-entity-recognition,1,Results,result,Results,0,187,1,1,0,result : Results,0.9033816425120773,0.5,0.5
named-entity-recognition,1,"As we can see in the tables , the Stack - LSTM model is more dependent on character - based representations to achieve competitive performance ; we hypothesize that the LSTM - CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs ; however , the Stack - LSTM model consumes the words one by one and it just relies on the word representations when it chunks words .",result,Results,0,188,2,2,0,result : Results,0.9082125603864735,1.0,1.0
named-entity-recognition,1,Network architectures,architecture,Network architectures,0,189,1,1,0,architecture : Network architectures,0.9130434782608695,0.07142857142857142,0.07142857142857142
named-entity-recognition,1,Our models had several components that we could tweak to understand their impact on the over all performance .,architecture,Network architectures,0,190,2,2,0,architecture : Network architectures,0.9178743961352657,0.14285714285714285,0.14285714285714285
named-entity-recognition,1,"We explored the impact that the CRF , the character - level representations , pretraining of our presented a model similar to our LSTM - CRF , but using hand - crafted spelling features .",architecture,Network architectures,0,191,3,3,0,architecture : Network architectures,0.9227053140096618,0.21428571428571427,0.21428571428571427
named-entity-recognition,1,also used a similar model and adapted it to the semantic role labeling task .,architecture,Network architectures,0,192,4,4,0,architecture : Network architectures,0.927536231884058,0.2857142857142857,0.2857142857142857
named-entity-recognition,1,"used a linear chain CRF with L 2 regularization , they added phrase cluster features extracted from the web data and spelling features .",architecture,Network architectures,0,193,5,5,0,architecture : Network architectures,0.9323671497584541,0.35714285714285715,0.35714285714285715
named-entity-recognition,1,also used a linear chain CRF with spelling features and gazetteers .,architecture,Network architectures,0,194,6,6,0,architecture : Network architectures,0.9371980676328503,0.42857142857142855,0.42857142857142855
named-entity-recognition,1,Language independent,architecture,Network architectures,0,195,7,7,0,architecture : Network architectures,0.9420289855072463,0.5,0.5
named-entity-recognition,1,NER models like ours have also been proposed in the past .,architecture,Network architectures,0,196,8,8,0,architecture : Network architectures,0.9468599033816425,0.5714285714285714,0.5714285714285714
named-entity-recognition,1,present semi-supervised bootstrapping algorithms for named entity recognition by co-training character - level ( word - internal ) and token - level ( context ) features .,architecture,Network architectures,0,197,9,9,0,architecture : Network architectures,0.9516908212560387,0.6428571428571429,0.6428571428571429
named-entity-recognition,1,use Bayesian nonparametrics to construct a data base of named entities in an almost unsupervised setting .,architecture,Network architectures,0,198,10,10,0,architecture : Network architectures,0.9565217391304348,0.7142857142857143,0.7142857142857143
named-entity-recognition,1,Ratinov and Roth quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information .,architecture,Network architectures,0,199,11,11,0,architecture : Network architectures,0.961352657004831,0.7857142857142857,0.7857142857142857
named-entity-recognition,1,"Finally , there is currently a lot of interest in models for NER that use letter - based representations .",architecture,Network architectures,0,200,12,12,0,architecture : Network architectures,0.966183574879227,0.8571428571428571,0.8571428571428571
named-entity-recognition,1,model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character - based representations into their encoder model .,architecture,Network architectures,0,201,13,13,0,architecture : Network architectures,0.9710144927536232,0.9285714285714286,0.9285714285714286
named-entity-recognition,1,"employ an architecture similar to ours , but instead use CNNs to learn character - level features , in a way similar to the work by .",architecture,Network architectures,0,202,14,14,0,architecture : Network architectures,0.9758454106280193,1.0,1.0
named-entity-recognition,1,Conclusion,conclusion,Conclusion,0,203,1,1,0,conclusion : Conclusion,0.9806763285024155,0.2,0.2
named-entity-recognition,1,"This paper presents two neural architectures for sequence labeling that provide the best NER results ever reported in standard evaluation settings , even compared with models that use external resources , such as gazetteers .",conclusion,Conclusion,0,204,2,2,0,conclusion : Conclusion,0.9855072463768116,0.4,0.4
named-entity-recognition,1,"key aspect of our models are that they model output label dependencies , either via a simple CRF architecture , or using a transition - based algorithm to explicitly construct and label chunks of the input .",conclusion,Conclusion,0,205,3,3,0,conclusion : Conclusion,0.9903381642512077,0.6,0.6
named-entity-recognition,1,"Word representations are also crucially important for success : we use both pre-trained word representations and "" character - based "" representations that capture morphological and orthographic information .",conclusion,Conclusion,0,206,4,4,0,conclusion : Conclusion,0.9951690821256038,0.8,0.8
named-entity-recognition,1,"To prevent the learner from depending too heavily on one representation class , dropout is used .",conclusion,Conclusion,0,207,5,5,0,conclusion : Conclusion,1.0,1.0,1.0
named-entity-recognition,2,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,title,title,1,2,1,1,0,title : title,0.009389671361502348,1.0,1.0
named-entity-recognition,2,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.014084507042253521,0.125,0.125
named-entity-recognition,2,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.018779342723004695,0.25,0.25
named-entity-recognition,2,Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.023474178403755867,0.375,0.375
named-entity-recognition,2,"Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.028169014084507043,0.5,0.5
named-entity-recognition,2,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.03286384976525822,0.625,0.625
named-entity-recognition,2,"Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03755868544600939,0.75,0.75
named-entity-recognition,2,"We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.04225352112676056,0.875,0.875
named-entity-recognition,2,"Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.046948356807511735,1.0,1.0
named-entity-recognition,2,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.051643192488262914,0.029411764705882353,0.029411764705882353
named-entity-recognition,2,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.056338028169014086,0.058823529411764705,0.058823529411764705
named-entity-recognition,2,Speed is not sufficient of course : they must also be expressive enough to tolerate the tremendous lexical variation in input data .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.06103286384976526,0.08823529411764706,0.08823529411764706
named-entity-recognition,2,The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling .,introduction,introduction,0,14,4,4,0,introduction : introduction,0.06572769953051644,0.11764705882352941,0.11764705882352941
named-entity-recognition,2,"While these models are expressive and accurate , they fail to fully exploit the parallelism opportunities of a GPU , and thus their speed is limited .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.07042253521126761,0.14705882352941177,0.14705882352941177
named-entity-recognition,2,"Specifically , they employ either recurrent neural networks ( RNNs ) for feature extraction , or Viterbi inference in a structured output model , both of which require sequential computation across the length of the input .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.07511737089201878,0.17647058823529413,0.17647058823529413
named-entity-recognition,2,"Instead , parallelized runtime independent of the length of the sequence saves time and energy costs , maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.07981220657276995,0.20588235294117646,0.20588235294117646
named-entity-recognition,2,Convolutional neural networks ( CNNs ) provide exactly this property .,introduction,introduction,0,18,8,8,0,introduction : introduction,0.08450704225352113,0.23529411764705882,0.23529411764705882
named-entity-recognition,2,"Rather than composing representations incrementally over each token in a sequence , they apply filters in parallel across the entire sequence at once .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.0892018779342723,0.2647058823529412,0.2647058823529412
named-entity-recognition,2,"Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.09389671361502347,0.29411764705882354,0.29411764705882354
named-entity-recognition,2,"This provides , for example , audio generation models that can be trained in parallel ( van den .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.09859154929577464,0.3235294117647059,0.3235294117647059
named-entity-recognition,2,"Despite the clear computational advantages of CNNs , RNNs have become the standard method for composing deep representations of text .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.10328638497652583,0.35294117647058826,0.35294117647058826
named-entity-recognition,2,"This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence , but the CNN 's representation is limited by the effective input width 1 of the network : the size of the input context which is observed , directly or indirectly , by the representation of a token at a given layer in the network .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.107981220657277,0.38235294117647056,0.38235294117647056
named-entity-recognition,2,"Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w ? 1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.11267605633802817,0.4117647058823529,0.4117647058823529
named-entity-recognition,2,"Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w ? 1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.11737089201877934,0.4411764705882353,0.4411764705882353
named-entity-recognition,2,"To avoid this scaling , one could pool representations across the sequence , but this is not appropriate for sequence labeling , since it reduces the output resolution of the representation .",introduction,introduction,0,26,16,16,0,introduction : introduction,0.12206572769953052,0.47058823529411764,0.47058823529411764
named-entity-recognition,2,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .",introduction,introduction,1,27,17,17,0,introduction : introduction,0.1267605633802817,0.5,0.5
named-entity-recognition,2,"For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .",introduction,introduction,0,28,18,18,0,introduction : introduction,0.13145539906103287,0.5294117647058824,0.5294117647058824
named-entity-recognition,2,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",introduction,introduction,1,29,19,19,0,introduction : introduction,0.13615023474178403,0.5588235294117647,0.5588235294117647
named-entity-recognition,2,"By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers :",introduction,introduction,1,30,20,20,0,introduction : introduction,0.14084507042253522,0.5882352941176471,0.5882352941176471
named-entity-recognition,2,The size of the effective input width for a token at layer l is now given by 2 l +1 ?1 .,introduction,introduction,0,31,21,21,0,introduction : introduction,0.14553990610328638,0.6176470588235294,0.6176470588235294
named-entity-recognition,2,"More concretely , just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .",introduction,introduction,0,32,22,22,0,introduction : introduction,0.15023474178403756,0.6470588235294118,0.6470588235294118
named-entity-recognition,2,Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,introduction,introduction,1,33,23,23,0,introduction : introduction,0.15492957746478872,0.6764705882352942,0.6764705882352942
named-entity-recognition,2,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,introduction,introduction,1,34,24,24,0,introduction : introduction,0.1596244131455399,0.7058823529411765,0.7058823529411765
named-entity-recognition,2,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .",introduction,introduction,1,35,25,25,0,introduction : introduction,0.1643192488262911,0.7352941176470589,0.7352941176470589
named-entity-recognition,2,"In experiments on CoNLL 2003 and OntoNotes 1 What we call effective input width here is known as the receptive field in the vision literature , drawing an analogy to the visual receptive field of a neuron in the retina . :",introduction,introduction,0,36,26,26,0,introduction : introduction,0.16901408450704225,0.7647058823529411,0.7647058823529411
named-entity-recognition,2,dilated CNN block with maximum dilation width 4 and filter width 3 .,introduction,introduction,0,37,27,27,0,introduction : introduction,0.17370892018779344,0.7941176470588235,0.7941176470588235
named-entity-recognition,2,Neurons contributing to a single highlighted neuron in the last layer are also highlighted .,introduction,introduction,0,38,28,28,0,introduction : introduction,0.1784037558685446,0.8235294117647058,0.8235294117647058
named-entity-recognition,2,"English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance .",introduction,introduction,0,39,29,29,0,introduction : introduction,0.18309859154929578,0.8529411764705882,0.8529411764705882
named-entity-recognition,2,"When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .",introduction,introduction,0,40,30,30,0,introduction : introduction,0.18779342723004694,0.8823529411764706,0.8823529411764706
named-entity-recognition,2,"As an extractor of per-token logits for a CRF , our model out - performs the Bi - LSTM - CRF .",introduction,introduction,0,41,31,31,0,introduction : introduction,0.19248826291079812,0.9117647058823529,0.9117647058823529
named-entity-recognition,2,"We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8 faster .",introduction,introduction,0,42,32,32,0,introduction : introduction,0.19718309859154928,0.9411764705882353,0.9411764705882353
named-entity-recognition,2,The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .,introduction,introduction,0,43,33,33,0,introduction : introduction,0.20187793427230047,0.9705882352941176,0.9705882352941176
named-entity-recognition,2,2 Background,introduction,introduction,0,44,34,34,0,introduction : introduction,0.20657276995305165,1.0,1.0
named-entity-recognition,2,Conditional Probability,system description,Conditional Probability Models for Tagging,0,45,1,1,0,system description : Conditional Probability Models for Tagging,0.2112676056338028,1.0,1.0
named-entity-recognition,2,Models for Tagging,model,model,0,46,1,1,0,model : model,0.215962441314554,0.01694915254237288,0.043478260869565216
named-entity-recognition,2,"Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per-token output tags .",model,model,0,47,2,2,0,model : model,0.22065727699530516,0.03389830508474576,0.08695652173913043
named-entity-recognition,2,Let D be the domain size of each y i .,model,model,0,48,3,3,0,model : model,0.22535211267605634,0.05084745762711865,0.13043478260869565
named-entity-recognition,2,"We predict the most likely y , given a conditional model P ( y|x ) .",model,model,0,49,4,4,0,model : model,0.2300469483568075,0.06779661016949153,0.17391304347826086
named-entity-recognition,2,This paper considers two factorizations of the conditional distribution .,model,model,0,50,5,5,0,model : model,0.2347417840375587,0.0847457627118644,0.21739130434782608
named-entity-recognition,2,"First , we have",model,model,0,51,6,6,0,model : model,0.23943661971830985,0.1016949152542373,0.2608695652173913
named-entity-recognition,2,where the tags are conditionally independent given some features for x .,model,model,0,52,7,7,0,model : model,0.24413145539906103,0.11864406779661017,0.30434782608695654
named-entity-recognition,2,"Given these features , O ( D ) prediction is simple and parallelizable across the length of the sequence .",model,model,0,53,8,8,0,model : model,0.24882629107981222,0.13559322033898305,0.34782608695652173
named-entity-recognition,2,"However , feature extraction may not necessarily be parallelizable .",model,model,0,54,9,9,0,model : model,0.2535211267605634,0.15254237288135594,0.391304347826087
named-entity-recognition,2,"For example , RNN - based features require iterative passes along the length of x .",model,model,0,55,10,10,0,model : model,0.25821596244131456,0.1694915254237288,0.43478260869565216
named-entity-recognition,2,We also consider a linear - chain CRF model that couples all of y together :,model,model,0,56,11,11,0,model : model,0.26291079812206575,0.1864406779661017,0.4782608695652174
named-entity-recognition,2,"where ? t is a local factor , ? p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",model,model,0,57,12,12,0,model : model,0.2676056338028169,0.2033898305084746,0.5217391304347826
named-entity-recognition,2,"where ? t is a local factor , ? p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",model,model,0,58,13,13,0,model : model,0.27230046948356806,0.22033898305084745,0.5652173913043478
named-entity-recognition,2,"where ? t is a local factor , ? p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",model,model,0,59,14,14,0,model : model,0.27699530516431925,0.23728813559322035,0.6086956521739131
named-entity-recognition,2,"To avoid overfitting , ? p does not depend on the timestep tor the input x in our experiments .",model,model,0,60,15,15,0,model : model,0.28169014084507044,0.2542372881355932,0.6521739130434783
named-entity-recognition,2,"To avoid overfitting , ? p does not depend on the timestep tor the input x in our experiments .",model,model,0,61,16,16,0,model : model,0.2863849765258216,0.2711864406779661,0.6956521739130435
named-entity-recognition,2,Prediction in this model requires global search using the O ( D 2 T ),model,model,0,62,17,17,0,model : model,0.29107981220657275,0.288135593220339,0.7391304347826086
named-entity-recognition,2,Viterbi algorithm .,model,model,0,63,18,18,0,model : model,0.29577464788732394,0.3050847457627119,0.782608695652174
named-entity-recognition,2,"CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step .",model,model,0,64,19,19,0,model : model,0.3004694835680751,0.3220338983050847,0.8260869565217391
named-entity-recognition,2,The suitability of such compilation depends on the properties and quantity of the data .,model,model,0,65,20,20,0,model : model,0.3051643192488263,0.3389830508474576,0.8695652173913043
named-entity-recognition,2,"While CRF prediction requires non-trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging , will always be satisfied .",model,model,0,66,21,21,0,model : model,0.30985915492957744,0.3559322033898305,0.9130434782608695
named-entity-recognition,2,"It may also have better sample complexity , as it imposes more prior knowledge about the structure of the interactions among the tags .",model,model,0,67,22,22,0,model : model,0.3145539906103286,0.3728813559322034,0.9565217391304348
named-entity-recognition,2,"However , it has worse computational complexity than independent prediction .",model,model,0,68,23,23,0,model : model,0.3192488262910798,0.3898305084745763,1.0
named-entity-recognition,2,Dilated Convolutions,model,Dilated Convolutions,0,69,24,1,0,model : Dilated Convolutions,0.323943661971831,0.4067796610169492,0.07692307692307693
named-entity-recognition,2,"CNNs in NLP are typically one - dimensional , applied to a sequence of vectors representing tokens rather than to a two -dimensional grid of vectors representing pixels .",model,Dilated Convolutions,0,70,25,2,0,model : Dilated Convolutions,0.3286384976525822,0.423728813559322,0.15384615384615385
named-entity-recognition,2,"In this setting , a convolutional neural network layer is equivalent to applying an affine transformation , W c to a sliding window of width r tokens on either side of each token in the sequence .",model,Dilated Convolutions,0,71,26,3,0,model : Dilated Convolutions,0.3333333333333333,0.4406779661016949,0.23076923076923078
named-entity-recognition,2,"Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations .",model,Dilated Convolutions,0,72,27,4,0,model : Dilated Convolutions,0.3380281690140845,0.4576271186440678,0.3076923076923077
named-entity-recognition,2,The convolutional operator applied to each token x t with output ct is defined as :,model,Dilated Convolutions,0,73,28,5,0,model : Dilated Convolutions,0.3427230046948357,0.4745762711864407,0.38461538461538464
named-entity-recognition,2,where ? is vector concatenation .,model,Dilated Convolutions,0,74,29,6,0,model : Dilated Convolutions,0.3474178403755869,0.4915254237288136,0.46153846153846156
named-entity-recognition,2,where ? is vector concatenation .,model,Dilated Convolutions,0,75,30,7,0,model : Dilated Convolutions,0.352112676056338,0.5084745762711864,0.5384615384615384
named-entity-recognition,2,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ? inputs at a time , where ? is the dilation width .",model,Dilated Convolutions,0,76,31,8,0,model : Dilated Convolutions,0.3568075117370892,0.5254237288135594,0.6153846153846154
named-entity-recognition,2,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ? inputs at a time , where ? is the dilation width .",model,Dilated Convolutions,0,77,32,9,0,model : Dilated Convolutions,0.3615023474178404,0.5423728813559322,0.6923076923076923
named-entity-recognition,2,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ? inputs at a time , where ? is the dilation width .",model,Dilated Convolutions,0,78,33,10,0,model : Dilated Convolutions,0.36619718309859156,0.559322033898305,0.7692307692307693
named-entity-recognition,2,We define the dilated convolution operator :,model,Dilated Convolutions,0,79,34,11,0,model : Dilated Convolutions,0.37089201877934275,0.576271186440678,0.8461538461538461
named-entity-recognition,2,dilated convolution of width 1 is equivalent to a simple convolution .,model,Dilated Convolutions,0,80,35,12,0,model : Dilated Convolutions,0.3755868544600939,0.5932203389830508,0.9230769230769231
named-entity-recognition,2,"Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the ? > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .",model,Dilated Convolutions,0,81,36,13,0,model : Dilated Convolutions,0.38028169014084506,0.6101694915254238,1.0
named-entity-recognition,2,Multi - Scale Context Aggregation,model,Multi-Scale Context Aggregation,0,82,37,1,0,model : Multi-Scale Context Aggregation,0.38497652582159625,0.6271186440677966,0.1
named-entity-recognition,2,We can leverage the ability of dilated convolutions to incorporate global context without losing important local information by stacking dilated convolutions of increasing width .,model,Multi-Scale Context Aggregation,0,83,38,2,0,model : Multi-Scale Context Aggregation,0.38967136150234744,0.6440677966101694,0.2
named-entity-recognition,2,"First described for pixel classification in computer vision , achieve state - of - the - art results on image segmentation benchmarks by stacking dilated convolutions with exponentially increasing rates of dilation , a technique they refer to as multiscale context aggregation .",model,Multi-Scale Context Aggregation,0,84,39,3,0,model : Multi-Scale Context Aggregation,0.39436619718309857,0.6610169491525424,0.3
named-entity-recognition,2,"By feeding the outputs of each dilated convolution as the input to the next , increasingly non-local information is incorporated into each pixel 's representation .",model,Multi-Scale Context Aggregation,0,85,40,4,0,model : Multi-Scale Context Aggregation,0.39906103286384975,0.6779661016949152,0.4
named-entity-recognition,2,Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .,model,Multi-Scale Context Aggregation,0,86,41,5,0,model : Multi-Scale Context Aggregation,0.40375586854460094,0.6949152542372882,0.5
named-entity-recognition,2,"By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .",model,Multi-Scale Context Aggregation,0,87,42,6,0,model : Multi-Scale Context Aggregation,0.4084507042253521,0.711864406779661,0.6
named-entity-recognition,2,"Unfortunately , simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments .",model,Multi-Scale Context Aggregation,0,88,43,7,0,model : Multi-Scale Context Aggregation,0.4131455399061033,0.7288135593220338,0.7
named-entity-recognition,2,"In response , we present Iterated Dilated CNNs ( ID - CNNs ) , which instead apply the same small stack of dilated convolutions multiple times , each iterate taking as input the result of the last application .",model,Multi-Scale Context Aggregation,0,89,44,8,0,model : Multi-Scale Context Aggregation,0.41784037558685444,0.7457627118644068,0.8
named-entity-recognition,2,Repeatedly employing the same parameters in a recurrent fashion provides both broad effective input width and desirable generalization capabilities .,model,Multi-Scale Context Aggregation,0,90,45,9,0,model : Multi-Scale Context Aggregation,0.4225352112676056,0.7627118644067796,0.9
named-entity-recognition,2,"We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .",model,Multi-Scale Context Aggregation,0,91,46,10,0,model : Multi-Scale Context Aggregation,0.4272300469483568,0.7796610169491526,1.0
named-entity-recognition,2,Model Architecture,model,Model Architecture,0,92,47,1,0,model : Model Architecture,0.431924882629108,0.7966101694915254,0.07692307692307693
named-entity-recognition,2,"The network takes as input a sequence of T vectors x t , and outputs a sequence of per-class scores ht , which serve either as the local conditional distributions of Eqn. ( 1 ) or the local factors ? t of Eqn..",model,Model Architecture,0,93,48,2,0,model : Model Architecture,0.43661971830985913,0.8135593220338984,0.15384615384615385
named-entity-recognition,2,"The network takes as input a sequence of T vectors x t , and outputs a sequence of per-class scores ht , which serve either as the local conditional distributions of Eqn. ( 1 ) or the local factors ? t of Eqn..",model,Model Architecture,0,94,49,3,0,model : Model Architecture,0.4413145539906103,0.8305084745762712,0.23076923076923078
named-entity-recognition,2,We denote the jth dilated convolutional layer of dilation width ? as D ( j ) ? .,model,Model Architecture,0,95,50,4,0,model : Model Architecture,0.4460093896713615,0.847457627118644,0.3076923076923077
named-entity-recognition,2,The first layer in the network is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation it :,model,Model Architecture,0,96,51,5,0,model : Model Architecture,0.4507042253521127,0.864406779661017,0.38461538461538464
named-entity-recognition,2,"Next , L c layers of dilated convolutions of exponentially increasing dilation width are applied to it , folding in increasingly broader context into the embedded representation of x tat each layer .",model,Model Architecture,0,97,52,6,0,model : Model Architecture,0.45539906103286387,0.8813559322033898,0.46153846153846156
named-entity-recognition,2,Let r ( ) denote the ReLU activation function .,model,Model Architecture,0,98,53,7,0,model : Model Architecture,0.460093896713615,0.8983050847457628,0.5384615384615384
named-entity-recognition,2,Beginning with ct ( 0 ) = it we define the stack of layers with the following recurrence :,model,Model Architecture,0,99,54,8,0,model : Model Architecture,0.4647887323943662,0.9152542372881356,0.6153846153846154
named-entity-recognition,2,and add a final dilation - 1 layer to the stack :,model,Model Architecture,0,100,55,9,0,model : Model Architecture,0.4694835680751174,0.9322033898305084,0.6923076923076923
named-entity-recognition,2,"We refer to this stack of dilated convolutions as a block B ( ) , which has output resolution equal to its input resolution .",model,Model Architecture,0,101,56,10,0,model : Model Architecture,0.47417840375586856,0.9491525423728814,0.7692307692307693
named-entity-recognition,2,"To incorporate even broader context without over - fitting , we avoid making B deeper , and instead iteratively apply B L b times , introducing no extra parameters .",model,Model Architecture,0,102,57,11,0,model : Model Architecture,0.4788732394366197,0.9661016949152542,0.8461538461538461
named-entity-recognition,2,Starting,model,Model Architecture,0,103,58,12,0,model : Model Architecture,0.4835680751173709,0.9830508474576272,0.9230769230769231
named-entity-recognition,2,We apply a simple affine transformation W o to this final representation to obtain per-class scores for each token x t :,model,Model Architecture,0,104,59,13,0,model : Model Architecture,0.48826291079812206,1.0,1.0
named-entity-recognition,2,Training,training,Training,0,105,1,1,0,training : Training,0.49295774647887325,0.047619047619047616,0.047619047619047616
named-entity-recognition,2,"Our main focus is to apply the ID - CNN an encoder to produce per-token logits for the first conditional model described in Sec. 2.1 , where tags are conditionally independent given deep features , since this will enable prediction that is parallelizable across the length of the input sequence .",training,Training,0,106,2,2,0,training : Training,0.49765258215962443,0.09523809523809523,0.09523809523809523
named-entity-recognition,2,"Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn. :",training,Training,0,107,3,3,0,training : Training,0.5023474178403756,0.14285714285714285,0.14285714285714285
named-entity-recognition,2,"We can also use the ID - CNN as logits for the CRF model ( Eqn. ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .",training,Training,0,108,4,4,0,training : Training,0.5070422535211268,0.19047619047619047,0.19047619047619047
named-entity-recognition,2,We next present an alternative training method that helps bridge the gap between these two techniques .,training,Training,0,109,5,5,0,training : Training,0.5117370892018779,0.23809523809523808,0.23809523809523808
named-entity-recognition,2,Sec. 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .,training,Training,0,110,6,6,0,training : Training,0.5164319248826291,0.2857142857142857,0.2857142857142857
named-entity-recognition,2,"In response , we compile some of this reasoning in output space into ID - CNN feature extraction .",training,Training,0,111,7,7,0,training : Training,0.5211267605633803,0.3333333333333333,0.3333333333333333
named-entity-recognition,2,"Instead of explicit reasoning over output labels during inference , we train the network such that each block is predictive of output labels .",training,Training,0,112,8,8,0,training : Training,0.5258215962441315,0.38095238095238093,0.38095238095238093
named-entity-recognition,2,"Subsequent blocks learn to correct dependency violations of their predecessors , refining the final sequence prediction .",training,Training,0,113,9,9,0,training : Training,0.5305164319248826,0.42857142857142855,0.42857142857142855
named-entity-recognition,2,"To do so , we first define predictions of the model after each of the L b applications of the block .",training,Training,0,114,10,10,0,training : Training,0.5352112676056338,0.47619047619047616,0.47619047619047616
named-entity-recognition,2,"Let ht ( k ) be the result of applying the matrix W o from ( 9 ) to b t ( k ) , the output of block k.",training,Training,0,115,11,11,0,training : Training,0.539906103286385,0.5238095238095238,0.5238095238095238
named-entity-recognition,2,We minimize the average of the losses for each application of the block :,training,Training,0,116,12,12,0,training : Training,0.5446009389671361,0.5714285714285714,0.5714285714285714
named-entity-recognition,2,"By rewarding accurate predictions after each application of the block , we learn a model where later blocks are used to refine initial predictions .",training,Training,0,117,13,13,0,training : Training,0.5492957746478874,0.6190476190476191,0.6190476190476191
named-entity-recognition,2,The loss also helps reduce the vanishing gradient problem for deep architectures .,training,Training,0,118,14,14,0,training : Training,0.5539906103286385,0.6666666666666666,0.6666666666666666
named-entity-recognition,2,"Such an approach has been applied in a variety of contexts for training very deep networks in computer vision , but not to our knowledge in NLP .",training,Training,0,119,15,15,0,training : Training,0.5586854460093896,0.7142857142857143,0.7142857142857143
named-entity-recognition,2,We apply dropout to the raw inputs x t and to each block 's output b t ( b ) to help prevent overfitting .,training,Training,0,120,16,16,0,training : Training,0.5633802816901409,0.7619047619047619,0.7619047619047619
named-entity-recognition,2,The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used at test time .,training,Training,0,121,17,17,0,training : Training,0.568075117370892,0.8095238095238095,0.8095238095238095
named-entity-recognition,2,"present dropout with expectationlinear regularization , which explicitly regularizes these two predictors to behave similarly .",training,Training,0,122,18,18,0,training : Training,0.5727699530516432,0.8571428571428571,0.8571428571428571
named-entity-recognition,2,All of our best reported results include such regularization .,training,Training,0,123,19,19,0,training : Training,0.5774647887323944,0.9047619047619048,0.9047619047619048
named-entity-recognition,2,"This is the first investigation of the technique 's effectiveness for NLP , including for RNNs .",training,Training,0,124,20,20,0,training : Training,0.5821596244131455,0.9523809523809523,0.9523809523809523
named-entity-recognition,2,We encourage its further application .,training,Training,0,125,21,21,0,training : Training,0.5868544600938967,1.0,1.0
named-entity-recognition,2,Related work,related work,Related work,0,126,1,1,0,related work : Related work,0.5915492957746479,0.041666666666666664,0.041666666666666664
named-entity-recognition,2,"The state - of - the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain - structured graphical model , or approximates this search with a beam .",related work,Related work,0,127,2,2,0,related work : Related work,0.596244131455399,0.08333333333333333,0.08333333333333333
named-entity-recognition,2,"These outperform similar systems that use the same features , but independent local predictions .",related work,Related work,0,128,3,3,0,related work : Related work,0.6009389671361502,0.125,0.125
named-entity-recognition,2,"On the other hand , the greedy sequential prediction ) approach of , which employs lexicalized features , gazetteers , and word clusters , outperforms CRFs with similar features .",related work,Related work,0,129,4,4,0,related work : Related work,0.6056338028169014,0.16666666666666666,0.16666666666666666
named-entity-recognition,2,LSTMs were used for NER as early as the CoNLL shared task in 2003 .,related work,Related work,0,130,5,5,0,related work : Related work,0.6103286384976526,0.20833333333333334,0.20833333333333334
named-entity-recognition,2,"More recently , a wide variety of neural network architectures for NER have been proposed .",related work,Related work,0,131,6,6,0,related work : Related work,0.6150234741784038,0.25,0.25
named-entity-recognition,2,"employ a one - layer CNN with pre-trained word embeddings , capitalization and lexicon features , and CRF - based prediction .",related work,Related work,0,132,7,7,0,related work : Related work,0.6197183098591549,0.2916666666666667,0.2916666666666667
named-entity-recognition,2,"achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF. proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .",related work,Related work,0,133,8,8,0,related work : Related work,0.6244131455399061,0.3333333333333333,0.3333333333333333
named-entity-recognition,2,Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .,related work,Related work,0,134,9,9,0,related work : Related work,0.6291079812206573,0.375,0.375
named-entity-recognition,2,"use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - of the - art performance on part - of - speech tagging and CoNLL NER without lexicons .",related work,Related work,0,135,10,10,0,related work : Related work,0.6338028169014085,0.4166666666666667,0.4166666666666667
named-entity-recognition,2,"Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .",related work,Related work,0,136,11,11,0,related work : Related work,0.6384976525821596,0.4583333333333333,0.4583333333333333
named-entity-recognition,2,use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .,related work,Related work,0,137,12,12,0,related work : Related work,0.6431924882629108,0.5,0.5
named-entity-recognition,2,"In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre-training of distributed word representations .",related work,Related work,0,138,13,13,0,related work : Related work,0.647887323943662,0.5416666666666666,0.5416666666666666
named-entity-recognition,2,"Though our models would also likely benefit from additional features such as character representations and lexicons , we focus on simpler models which use word - embeddings alone , leaving more elaborate input representations to future work .",related work,Related work,0,139,14,14,0,related work : Related work,0.6525821596244131,0.5833333333333334,0.5833333333333334
named-entity-recognition,2,"In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .",related work,Related work,0,140,15,15,0,related work : Related work,0.6572769953051644,0.625,0.625
named-entity-recognition,2,"Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim Our work draws on the use of dilated convolutions for image segmentation in the computer vision community .",related work,Related work,0,141,16,16,0,related work : Related work,0.6619718309859155,0.6666666666666666,0.6666666666666666
named-entity-recognition,2,"Similar to our block , Yu and Koltun ( 2016 ) employ a context - module of stacked dilated convolutions of exponentially increasing dilation width .",related work,Related work,0,142,17,17,0,related work : Related work,0.6666666666666666,0.7083333333333334,0.7083333333333334
named-entity-recognition,2,"Dilated convolutions were recently applied to the task of speech generation , and concurrent with this work , posted a pre-print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .",related work,Related work,0,143,18,18,0,related work : Related work,0.6713615023474179,0.75,0.75
named-entity-recognition,2,"Our basic model architecture is similar to that of the ByteNet encoder , except that the inputs to our model are tokens and not bytes .",related work,Related work,0,144,19,19,0,related work : Related work,0.676056338028169,0.7916666666666666,0.7916666666666666
named-entity-recognition,2,"Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .",related work,Related work,0,145,20,20,0,related work : Related work,0.6807511737089202,0.8333333333333334,0.8333333333333334
named-entity-recognition,2,We are the first to use dilated convolutions for sequence labeling .,related work,Related work,0,146,21,21,0,related work : Related work,0.6854460093896714,0.875,0.875
named-entity-recognition,2,The broad effective input width of the ID - CNN helps aggregate document - level context .,related work,Related work,0,147,22,22,0,related work : Related work,0.6901408450704225,0.9166666666666666,0.9166666666666666
named-entity-recognition,2,"incorporate document context in their greedy model by adding features based on tagged entities within a large , fixed window of tokens .",related work,Related work,0,148,23,23,0,related work : Related work,0.6948356807511737,0.9583333333333334,0.9583333333333334
named-entity-recognition,2,Prior work has also posed a structured model that couples predictions across the whole document .,related work,Related work,0,149,24,24,0,related work : Related work,0.6995305164319249,1.0,1.0
named-entity-recognition,2,Experimental Results,experiment,Experimental Results,0,150,1,1,0,experiment : Experimental Results,0.704225352112676,0.25,0.25
named-entity-recognition,2,We describe experiments on two benchmark English named entity recognition datasets .,experiment,Experimental Results,0,151,2,2,0,experiment : Experimental Results,0.7089201877934272,0.5,0.5
named-entity-recognition,2,"On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per-token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .",experiment,Experimental Results,0,152,3,3,0,experiment : Experimental Results,0.7136150234741784,0.75,0.75
named-entity-recognition,2,"We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .",experiment,Experimental Results,0,153,4,4,0,experiment : Experimental Results,0.7183098591549296,1.0,1.0
named-entity-recognition,2,Data and Evaluation,evaluation,Data and Evaluation,0,154,1,1,0,evaluation : Data and Evaluation,0.7230046948356808,0.14285714285714285,0.14285714285714285
named-entity-recognition,2,We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong and OntoNotes 5.0 .,evaluation,Data and Evaluation,0,155,2,2,0,evaluation : Data and Evaluation,0.7276995305164319,0.2857142857142857,0.2857142857142857
named-entity-recognition,2,"Following previous work , we use the same OntoNotes data split used for co-reference resolution in the CoNLL - 2012 shared task .",evaluation,Data and Evaluation,0,156,3,3,0,evaluation : Data and Evaluation,0.7323943661971831,0.42857142857142855,0.42857142857142855
named-entity-recognition,2,"For both datasets , we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance .",evaluation,Data and Evaluation,0,157,4,4,0,evaluation : Data and Evaluation,0.7370892018779343,0.5714285714285714,0.5714285714285714
named-entity-recognition,2,As in previous work we evaluate the performance of our models using segment - level micro -averaged F1 score .,evaluation,Data and Evaluation,0,158,5,5,0,evaluation : Data and Evaluation,0.7417840375586855,0.7142857142857143,0.7142857142857143
named-entity-recognition,2,Hyperparameters that resulted in the best performance on the validation set were selected via grid search .,evaluation,Data and Evaluation,0,159,6,6,0,evaluation : Data and Evaluation,0.7464788732394366,0.8571428571428571,0.8571428571428571
named-entity-recognition,2,"more detailed description of the data , evaluation , optimization and data pre-processing can be found in the Appendix .",evaluation,Data and Evaluation,0,160,7,7,0,evaluation : Data and Evaluation,0.7511737089201878,1.0,1.0
named-entity-recognition,2,Baselines,baseline,Baselines,0,161,1,1,0,baseline : Baselines,0.755868544600939,0.027777777777777776,0.04
named-entity-recognition,2,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .",baseline,Baselines,1,162,2,2,0,baseline : Baselines,0.7605633802816901,0.05555555555555555,0.08
named-entity-recognition,2,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,baseline,Baselines,1,163,3,3,0,baseline : Baselines,0.7652582159624414,0.08333333333333333,0.12
named-entity-recognition,2,"We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .",baseline,Baselines,0,164,4,4,0,baseline : Baselines,0.7699530516431925,0.1111111111111111,0.16
named-entity-recognition,2,We do not compare with deeper or more elaborate CNN architectures for a number of reasons :,baseline,Baselines,0,165,5,5,0,baseline : Baselines,0.7746478873239436,0.1388888888888889,0.2
named-entity-recognition,2,") Fast train and test performance are highly desirable for NLP practitioners , and deeper models require more computation time 2 ) more complicated models tend to over - fit on this relatively small dataset and 3 ) most accurate deep CNN architectures repeatedly up - sample and down - sample the inputs .",baseline,Baselines,0,166,6,6,0,baseline : Baselines,0.7793427230046949,0.16666666666666666,0.24
named-entity-recognition,2,We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .,baseline,Baselines,0,167,7,7,0,baseline : Baselines,0.784037558685446,0.19444444444444445,0.28
named-entity-recognition,2,"Since our task is sequence labeling , we desire a model that maintains the token - level resolution of the input , making dilated convolutions an elegant solution .",baseline,Baselines,0,168,8,8,0,baseline : Baselines,0.7887323943661971,0.2222222222222222,0.32
named-entity-recognition,2,CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,baseline,Baselines,1,169,9,9,0,baseline : Baselines,0.7934272300469484,0.25,0.36
named-entity-recognition,2,"For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .",baseline,Baselines,0,170,10,10,0,baseline : Baselines,0.7981220657276995,0.2777777777777778,0.4
named-entity-recognition,2,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .",baseline,Baselines,1,171,11,11,0,baseline : Baselines,0.8028169014084507,0.3055555555555556,0.44
named-entity-recognition,2,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .",baseline,Baselines,1,172,12,12,0,baseline : Baselines,0.8075117370892019,0.3333333333333333,0.48
named-entity-recognition,2,"All CNN models out - perform the Bi-Model F1 86.96 90.33 Bi-LSTM 89.34 0.28 4 - layer CNN 89.97 0.20 5 - layer CNN 90.23 0.16 ID- CNN 90.32 0.26 88.67 90.05 90.20 Bi-LSTM-CRF ( re-impl ) 90.43 0.12 ID-CNN- CRF 90.54 0.18 LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .",baseline,Baselines,0,173,13,13,0,baseline : Baselines,0.812206572769953,0.3611111111111111,0.52
named-entity-recognition,2,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .",baseline,Baselines,1,174,14,14,0,baseline : Baselines,0.8169014084507042,0.3888888888888889,0.56
named-entity-recognition,2,Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .,baseline,Baselines,0,175,15,15,0,baseline : Baselines,0.8215962441314554,0.4166666666666667,0.6
named-entity-recognition,2,"lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .",baseline,Baselines,0,176,16,16,0,baseline : Baselines,0.8262910798122066,0.4444444444444444,0.64
named-entity-recognition,2,We report decoding times using the fastest batch size for each method .,baseline,Baselines,0,177,17,17,0,baseline : Baselines,0.8309859154929577,0.4722222222222222,0.68
named-entity-recognition,2,The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .,baseline,Baselines,0,178,18,18,0,baseline : Baselines,0.8356807511737089,0.5,0.72
named-entity-recognition,2,"With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .",baseline,Baselines,0,179,19,19,0,baseline : Baselines,0.8403755868544601,0.5277777777777778,0.76
named-entity-recognition,2,"The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF at test time , with comparable accuracy .",baseline,Baselines,0,180,20,20,0,baseline : Baselines,0.8450704225352113,0.5555555555555556,0.8
named-entity-recognition,2,"The 5 - layer CNN , which observes the same effective input width as the ID - CNN but with more parameters , performs at about the same speed as the ID - CNN in our experiments .",baseline,Baselines,0,181,21,21,0,baseline : Baselines,0.8497652582159625,0.5833333333333334,0.84
named-entity-recognition,2,"With a better implementation of dilated convolutions than currently included in TensorFlow , we would expect the ID - CNN to be notably faster than the 5 - layer CNN .",baseline,Baselines,0,182,22,22,0,baseline : Baselines,0.8544600938967136,0.6111111111111112,0.88
named-entity-recognition,2,"We emphasize the importance of the dropout regularizer of in , where we observe increased F1 for every model trained with expectation - linear dropout regularization .",baseline,Baselines,0,183,23,23,0,baseline : Baselines,0.8591549295774648,0.6388888888888888,0.92
named-entity-recognition,2,"Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as .",baseline,Baselines,0,184,24,24,0,baseline : Baselines,0.863849765258216,0.6666666666666666,0.96
named-entity-recognition,2,We recommend this regularizer as a simple and helpful tool for practitioners training neural networks for NLP .,baseline,Baselines,0,185,25,25,0,baseline : Baselines,0.8685446009389671,0.6944444444444444,1.0
named-entity-recognition,2,Document - level prediction,baseline,Document-level prediction,1,186,26,1,0,baseline : Document-level prediction,0.8732394366197183,0.7222222222222222,0.125
named-entity-recognition,2,In we show that adding document - level context improves every model on CoNLL - 2003 .,baseline,Document-level prediction,1,187,27,2,0,baseline : Document-level prediction,0.8779342723004695,0.75,0.25
named-entity-recognition,2,"Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 .",baseline,Document-level prediction,0,188,28,3,0,baseline : Document-level prediction,0.8826291079812206,0.7777777777777778,0.375
named-entity-recognition,2,"We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1,000 tokens long such as entire documents .",baseline,Document-level prediction,0,189,29,4,0,baseline : Document-level prediction,0.8873239436619719,0.8055555555555556,0.5
named-entity-recognition,2,We also note that our combination of training objective ( Eqn. 11 ) and tied parameters ( Eqn. : Comparing ID - CNNs with 1 ) backpropagating loss only from the final layer ( 1 - loss ) and 2 ) untied parameters across blocks ( noshare ) 8 ) more effectively learns to aggregate this broad context than a vanilla cross - entropy loss or deep CNN back - propagated from the final neural network layer .,baseline,Document-level prediction,0,190,30,5,0,baseline : Document-level prediction,0.892018779342723,0.8333333333333334,0.625
named-entity-recognition,2,compares models trained to incorporate entire document context using the document baselines described in Section 6.2 .,baseline,Document-level prediction,0,191,31,6,0,baseline : Document-level prediction,0.8967136150234741,0.8611111111111112,0.75
named-entity-recognition,2,"In we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .",baseline,Document-level prediction,0,192,32,7,0,baseline : Document-level prediction,0.9014084507042254,0.8888888888888888,0.875
named-entity-recognition,2,"On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .",baseline,Document-level prediction,0,193,33,8,0,baseline : Document-level prediction,0.9061032863849765,0.9166666666666666,1.0
named-entity-recognition,2,OntoNotes 5.0 English NER,baseline,OntoNotes 5.0 English NER,1,194,34,1,0,baseline : OntoNotes 5.0 English NER,0.9107981220657277,0.9444444444444444,0.3333333333333333
named-entity-recognition,2,We observe similar patterns on OntoNotes as we do on CoNLL. lists over all F 1 scores of our models compared to those in the existing literature .,baseline,OntoNotes 5.0 English NER,0,195,35,2,0,baseline : OntoNotes 5.0 English NER,0.9154929577464789,0.9722222222222222,0.6666666666666666
named-entity-recognition,2,The greedy Bi - LSTM out - performs the lex -,baseline,OntoNotes 5.0 English NER,0,196,36,3,0,baseline : OntoNotes 5.0 English NER,0.92018779342723,1.0,1.0
named-entity-recognition,2,Model,model,Model,0,197,1,1,0,model : Model,0.9248826291079812,0.07692307692307693,0.07692307692307693
named-entity-recognition,2,Speed Bi-LSTM-CRF 1 Bi-LSTM 4.60 ID- CNN 7.96 85.76 0.13 21.19 ID-CNN 85.27 0.24 13.21 ID - CNN ( 1 block ) 84.28 0.10 26.01 : F1 score of sentence and document models on OntoNotes .,model,Model,0,198,2,2,0,model : Model,0.9295774647887324,0.15384615384615385,0.15384615384615385
named-entity-recognition,2,"icalized greedy model of , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co-reference .",model,Model,0,199,3,3,0,model : Model,0.9342723004694836,0.23076923076923078,0.23076923076923078
named-entity-recognition,2,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .",model,Model,1,200,4,4,0,model : Model,0.9389671361502347,0.3076923076923077,0.3076923076923077
named-entity-recognition,2,The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL .,model,Model,0,201,5,5,0,model : Model,0.9436619718309859,0.38461538461538464,0.38461538461538464
named-entity-recognition,2,"We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi-token named entity segment in CoNLL is about one token shorter than in OntoNotes .",model,Model,0,202,6,6,0,model : Model,0.9483568075117371,0.46153846153846156,0.46153846153846156
named-entity-recognition,2,These long entities benefit more from explicit structured constraints enforced in Viterbi decoding .,model,Model,0,203,7,7,0,model : Model,0.9530516431924883,0.5384615384615384,0.5384615384615384
named-entity-recognition,2,"Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .",model,Model,0,204,8,8,0,model : Model,0.9577464788732394,0.6153846153846154,0.6153846153846154
named-entity-recognition,2,"Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .",model,Model,0,205,9,9,0,model : Model,0.9624413145539906,0.6923076923076923,0.6923076923076923
named-entity-recognition,2,"In , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .",model,Model,0,206,10,10,0,model : Model,0.9671361502347418,0.7692307692307693,0.7692307692307693
named-entity-recognition,2,"For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .",model,Model,0,207,11,11,0,model : Model,0.971830985915493,0.8461538461538461,0.8461538461538461
named-entity-recognition,2,"We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with dis ambiguation .",model,Model,0,208,12,12,0,model : Model,0.9765258215962441,0.9230769230769231,0.9230769230769231
named-entity-recognition,2,"In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .",model,Model,0,209,13,13,0,model : Model,0.9812206572769953,1.0,1.0
named-entity-recognition,2,Conclusion,conclusion,Conclusion,0,210,1,1,0,conclusion : Conclusion,0.9859154929577465,0.25,0.25
named-entity-recognition,2,"We present iterated dilated convolutional neural networks , fast token encoders that efficiently aggregate broad context without losing resolution .",conclusion,Conclusion,0,211,2,2,0,conclusion : Conclusion,0.9906103286384976,0.5,0.5
named-entity-recognition,2,"These provide impressive speed improvements for sequence labeling , particularly when processing entire documents at a time .",conclusion,Conclusion,0,212,3,3,0,conclusion : Conclusion,0.9953051643192489,0.75,0.75
named-entity-recognition,2,"In the future we hope to extend this work to NLP tasks with richer structured output , such as parsing .",conclusion,Conclusion,0,213,4,4,0,conclusion : Conclusion,1.0,1.0,1.0
named-entity-recognition,3,Semi-supervised sequence tagging with bidirectional language models,title,title,1,2,1,1,0,title : title,0.010810810810810811,1.0,1.0
named-entity-recognition,3,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.016216216216216217,0.2,0.2
named-entity-recognition,3,Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.021621621621621623,0.4,0.4
named-entity-recognition,3,"However , in most cases , the recurrent network that operates on word - level representations to produce context sensitive representations is trained on relatively little labeled data .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02702702702702703,0.6,0.6
named-entity-recognition,3,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.032432432432432434,0.8,0.8
named-entity-recognition,3,"We evaluate our model on two standard datasets for named entity recognition ( NER ) and chunking , and in both cases achieve state of the art results , surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03783783783783784,1.0,1.0
named-entity-recognition,3,Introduction,introduction,introduction,0,8,1,1,0,introduction : introduction,0.043243243243243246,0.05263157894736842,0.05263157894736842
named-entity-recognition,3,"Due to their simplicity and efficacy , pre-trained word embedding have become ubiquitous in NLP systems .",introduction,introduction,0,9,2,2,0,introduction : introduction,0.04864864864864865,0.10526315789473684,0.10526315789473684
named-entity-recognition,3,Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks .,introduction,introduction,0,10,3,3,0,introduction : introduction,0.05405405405405406,0.15789473684210525,0.15789473684210525
named-entity-recognition,3,"However , in many NLP tasks it is essential to represent not just the meaning of a word , but also the word in context .",introduction,introduction,0,11,4,4,0,introduction : introduction,0.05945945945945946,0.21052631578947367,0.21052631578947367
named-entity-recognition,3,"For example , in the two phrases "" A Central Bank spokesman "" and "" The Central African Republic "" , the word ' Central ' is used as part of both an Organization and Location .",introduction,introduction,0,12,5,5,0,introduction : introduction,0.06486486486486487,0.2631578947368421,0.2631578947368421
named-entity-recognition,3,"Accordingly , current state of the art sequence tagging models typically include a bidirectional re-current neural network ( RNN ) that encodes token sequences into a context sensitive representation before making token specific predictions .",introduction,introduction,0,13,6,6,0,introduction : introduction,0.07027027027027027,0.3157894736842105,0.3157894736842105
named-entity-recognition,3,"Although the token representation is initialized with pre-trained embeddings , the parameters of the bidirectional RNN are typically learned only on labeled data .",introduction,introduction,0,14,7,7,0,introduction : introduction,0.07567567567567568,0.3684210526315789,0.3684210526315789
named-entity-recognition,3,"Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks ( e.g. , .",introduction,introduction,0,15,8,8,0,introduction : introduction,0.08108108108108109,0.42105263157894735,0.42105263157894735
named-entity-recognition,3,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .",introduction,introduction,1,16,9,9,0,introduction : introduction,0.08648648648648649,0.47368421052631576,0.47368421052631576
named-entity-recognition,3,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .",introduction,introduction,1,17,10,10,0,introduction : introduction,0.0918918918918919,0.5263157894736842,0.5263157894736842
named-entity-recognition,3,"Since the LM embeddings are used to compute the probability of future words in a neural LM , they are likely to encode both the semantic and syntactic roles of words in context .",introduction,introduction,0,18,11,11,0,introduction : introduction,0.0972972972972973,0.5789473684210527,0.5789473684210527
named-entity-recognition,3,Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting .,introduction,introduction,0,19,12,12,0,introduction : introduction,0.10270270270270271,0.631578947368421,0.631578947368421
named-entity-recognition,3,"When we include the LM embeddings in our system over all performance increases from 90. 87 % to 91.93 % F 1 for the CoNLL 2003 NER task , a more then 1 % absolute F1 increase , and a substantial improvement over the previous state of the art .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.10810810810810811,0.6842105263157895,0.6842105263157895
named-entity-recognition,3,We also establish a new state of the art result ( 96.37 % F 1 ) for the CoNLL 2000 Chunking task .,introduction,introduction,0,21,14,14,0,introduction : introduction,0.11351351351351352,0.7368421052631579,0.7368421052631579
named-entity-recognition,3,"As a secondary contribution , we show that using both forward and backward LM embeddings boosts performance over a forward only LM .",introduction,introduction,0,22,15,15,0,introduction : introduction,0.11891891891891893,0.7894736842105263,0.7894736842105263
named-entity-recognition,3,We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers .,introduction,introduction,0,23,16,16,0,introduction : introduction,0.12432432432432433,0.8421052631578947,0.8421052631578947
named-entity-recognition,3,The main components in our language - modelaugmented sequence tagger ( TagLM ) are illustrated in .,introduction,introduction,0,24,17,17,0,introduction : introduction,0.12972972972972974,0.8947368421052632,0.8947368421052632
named-entity-recognition,3,"After pre-training word embeddings and a neural LM on large , unlabeled corpora ( Step 1 ) , we extract the word and LM embeddings for every token in a given input sequence Step 2 ) and use them in the supervised sequence tagging model (",introduction,introduction,0,25,18,18,0,introduction : introduction,0.13513513513513514,0.9473684210526315,0.9473684210526315
named-entity-recognition,3,Step 3 ) .,introduction,introduction,0,26,19,19,0,introduction : introduction,0.14054054054054055,1.0,1.0
named-entity-recognition,3,Baseline sequence tagging model,model,Baseline sequence tagging model,0,27,1,1,0,model : Baseline sequence tagging model,0.14594594594594595,0.023255813953488372,0.023255813953488372
named-entity-recognition,3,"Our baseline sequence tagging model is a hierarchical neural tagging model , closely following a number of recent studies ) ( left side of ) .",model,Baseline sequence tagging model,0,28,2,2,0,model : Baseline sequence tagging model,0.15135135135135136,0.046511627906976744,0.046511627906976744
named-entity-recognition,3,"Given a sentence of tokens ( t 1 , t 2 , . . . , t N ) it first forms a representation , x k , for each token by concatenating a character based representation ck with a token embedding wk :",model,Baseline sequence tagging model,0,29,3,3,0,model : Baseline sequence tagging model,0.15675675675675677,0.06976744186046512,0.06976744186046512
named-entity-recognition,3,The character representation ck captures morphological information and is either a convolutional neural network ( CNN ) or RNN .,model,Baseline sequence tagging model,0,30,4,4,0,model : Baseline sequence tagging model,0.16216216216216217,0.09302325581395349,0.09302325581395349
named-entity-recognition,3,"It is parameterized by C ( , ? c ) with parameters ? c .",model,Baseline sequence tagging model,0,31,5,5,0,model : Baseline sequence tagging model,0.16756756756756758,0.11627906976744186,0.11627906976744186
named-entity-recognition,3,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",model,Baseline sequence tagging model,0,32,6,6,0,model : Baseline sequence tagging model,0.17297297297297298,0.13953488372093023,0.13953488372093023
named-entity-recognition,3,"To learn a context sensitive representation , we employ multiple layers of bidirectional RNNs .",model,Baseline sequence tagging model,0,33,7,7,0,model : Baseline sequence tagging model,0.1783783783783784,0.16279069767441862,0.16279069767441862
named-entity-recognition,3,"For each token position , k , the hidden state h k , i of RNN layer i is formed by concatenating the hidden states from the forward ( ? ? h k , i ) and backward ( ? ? h k , i ) RNNs .",model,Baseline sequence tagging model,0,34,8,8,0,model : Baseline sequence tagging model,0.1837837837837838,0.18604651162790697,0.18604651162790697
named-entity-recognition,3,"As a result , the bidirectional RNN is able to use both past and future information to make a prediction at token k.",model,Baseline sequence tagging model,0,35,9,9,0,model : Baseline sequence tagging model,0.1891891891891892,0.20930232558139536,0.20930232558139536
named-entity-recognition,3,"More formally , for the first RNN layer that operates on x k to output h k,1 :",model,Baseline sequence tagging model,0,36,10,10,0,model : Baseline sequence tagging model,0.1945945945945946,0.23255813953488372,0.23255813953488372
named-entity-recognition,3,Step 2 : Prepare word embedding and LM embedding for each token in the input sequence .,model,Baseline sequence tagging model,0,37,11,11,0,model : Baseline sequence tagging model,0.2,0.2558139534883721,0.2558139534883721
named-entity-recognition,3,"Two representations of the word "" York """,model,Baseline sequence tagging model,0,38,12,12,0,model : Baseline sequence tagging model,0.20540540540540542,0.27906976744186046,0.27906976744186046
named-entity-recognition,3,Step 3 : Use both word embeddings and LM embeddings in the sequence tagging model .,model,Baseline sequence tagging model,0,39,13,13,0,model : Baseline sequence tagging model,0.21081081081081082,0.3023255813953488,0.3023255813953488
named-entity-recognition,3,New York is located :,model,Baseline sequence tagging model,0,40,14,14,0,model : Baseline sequence tagging model,0.21621621621621623,0.32558139534883723,0.32558139534883723
named-entity-recognition,3,"The main components in TagLM , our language - model - augmented sequence tagging system .",model,Baseline sequence tagging model,0,41,15,15,0,model : Baseline sequence tagging model,0.22162162162162163,0.3488372093023256,0.3488372093023256
named-entity-recognition,3,The language model component ( in orange ) is used to augment the input token representation in a traditional sequence tagging models ( in grey ) .,model,Baseline sequence tagging model,0,42,16,16,0,model : Baseline sequence tagging model,0.22702702702702704,0.37209302325581395,0.37209302325581395
named-entity-recognition,3,"The second RNN layer is similar and uses h k , 1 to output h k ,2 .",model,Baseline sequence tagging model,0,43,17,17,0,model : Baseline sequence tagging model,0.23243243243243245,0.3953488372093023,0.3953488372093023
named-entity-recognition,3,"In this paper , we use L = 2 layers of RNNs in all experiments and parameterize R i as either Gated Recurrent Units ( GRU ) or Long Short - Term Memory units ( LSTM ) depending on the task .",model,Baseline sequence tagging model,0,44,18,18,0,model : Baseline sequence tagging model,0.23783783783783785,0.4186046511627907,0.4186046511627907
named-entity-recognition,3,"Finally , the output of the final RNN layer h k,L is used to predict a score for each possible tag using a single dense layer .",model,Baseline sequence tagging model,0,45,19,19,0,model : Baseline sequence tagging model,0.24324324324324326,0.4418604651162791,0.4418604651162791
named-entity-recognition,3,"Due to the dependencies between successive tags in our sequence labeling tasks ( e.g. using the BIOES labeling scheme , it is not possible for I - PER to follow B - LOC ) , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token .",model,Baseline sequence tagging model,0,46,20,20,0,model : Baseline sequence tagging model,0.24864864864864866,0.46511627906976744,0.46511627906976744
named-entity-recognition,3,"Accordingly , we add another layer with parameters for each label bigram , computing the sentence conditional random field ( CRF ) loss using the forward - backward algorithm at training time , and using the Viterbi algorithm to find the most likely tag sequence at test time , similar to Collobert et al . Bidirectional LM",model,Baseline sequence tagging model,0,47,21,21,0,model : Baseline sequence tagging model,0.25405405405405407,0.4883720930232558,0.4883720930232558
named-entity-recognition,3,"Accordingly , we add another layer with parameters for each label bigram , computing the sentence conditional random field ( CRF ) loss using the forward - backward algorithm at training time , and using the Viterbi algorithm to find the most likely tag sequence at test time , similar to Collobert et al . Bidirectional LM",model,Baseline sequence tagging model,0,48,22,22,0,model : Baseline sequence tagging model,0.2594594594594595,0.5116279069767442,0.5116279069767442
named-entity-recognition,3,"language model computes the probability of a token sequence ( t 1 , t 2 , . . . , t N )",model,Baseline sequence tagging model,0,49,23,23,0,model : Baseline sequence tagging model,0.2648648648648649,0.5348837209302325,0.5348837209302325
named-entity-recognition,3,"Recent state of the art neural language models ) use a similar architecture to our baseline sequence tagger where they pass a token representation ( either from a CNN over characters or as token embeddings ) through multiple layers of LSTMs to embed the history ( t 1 , t 2 , . . . , t k ) into a fixed dimensional vector ? ? h LM k .",model,Baseline sequence tagging model,0,50,24,24,0,model : Baseline sequence tagging model,0.2702702702702703,0.5581395348837209,0.5581395348837209
named-entity-recognition,3,This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model .,model,Baseline sequence tagging model,0,51,25,25,0,model : Baseline sequence tagging model,0.2756756756756757,0.5813953488372093,0.5813953488372093
named-entity-recognition,3,"Finally , the language model predicts the probability of token t k + 1 using a softmax layer over words in the vocabulary .",model,Baseline sequence tagging model,0,52,26,26,0,model : Baseline sequence tagging model,0.2810810810810811,0.6046511627906976,0.6046511627906976
named-entity-recognition,3,The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM .,model,Baseline sequence tagging model,0,53,27,27,0,model : Baseline sequence tagging model,0.2864864864864865,0.627906976744186,0.627906976744186
named-entity-recognition,3,backward LM predicts the previous token given the future context .,model,Baseline sequence tagging model,0,54,28,28,0,model : Baseline sequence tagging model,0.2918918918918919,0.6511627906976745,0.6511627906976745
named-entity-recognition,3,"Given a sentence with N tokens , it computes",model,Baseline sequence tagging model,0,55,29,29,0,model : Baseline sequence tagging model,0.2972972972972973,0.6744186046511628,0.6744186046511628
named-entity-recognition,3,"backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ? h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",model,Baseline sequence tagging model,0,56,30,30,0,model : Baseline sequence tagging model,0.3027027027027027,0.6976744186046512,0.6976744186046512
named-entity-recognition,3,"backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ? h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",model,Baseline sequence tagging model,0,57,31,31,0,model : Baseline sequence tagging model,0.3081081081081081,0.7209302325581395,0.7209302325581395
named-entity-recognition,3,"In our final system , after pre-training the forward and backward LMs separately , we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings , i.e. , h LM",model,Baseline sequence tagging model,0,58,32,32,0,model : Baseline sequence tagging model,0.31351351351351353,0.7441860465116279,0.7441860465116279
named-entity-recognition,3,"Note that in our formulation , the forward and backward LMs are independent , without any shared parameters .",model,Baseline sequence tagging model,0,59,33,33,0,model : Baseline sequence tagging model,0.31891891891891894,0.7674418604651163,0.7674418604651163
named-entity-recognition,3,Combining LM with sequence model,model,Baseline sequence tagging model,0,60,34,34,0,model : Baseline sequence tagging model,0.32432432432432434,0.7906976744186046,0.7906976744186046
named-entity-recognition,3,"Our combined system , TagLM , uses the LM embeddings as additional inputs to the sequence tagging model .",model,Baseline sequence tagging model,0,61,35,35,0,model : Baseline sequence tagging model,0.32972972972972975,0.813953488372093,0.813953488372093
named-entity-recognition,3,"In particular , we concatenate the LM embeddings h LM with the output from one of the bidirectional RNN layers in the sequence model .",model,Baseline sequence tagging model,0,62,36,36,0,model : Baseline sequence tagging model,0.33513513513513515,0.8372093023255814,0.8372093023255814
named-entity-recognition,3,"In our experiments , we found that introducing the LM embeddings at the output of the first layer performed the best .",model,Baseline sequence tagging model,0,63,37,37,0,model : Baseline sequence tagging model,0.34054054054054056,0.8604651162790697,0.8604651162790697
named-entity-recognition,3,"More formally , we simply replace ( 2 ) with",model,Baseline sequence tagging model,0,64,38,38,0,model : Baseline sequence tagging model,0.34594594594594597,0.8837209302325582,0.8837209302325582
named-entity-recognition,3,There are alternate possibilities for adding the LM embeddings to the sequence model .,model,Baseline sequence tagging model,0,65,39,39,0,model : Baseline sequence tagging model,0.35135135135135137,0.9069767441860465,0.9069767441860465
named-entity-recognition,3,One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,model,Baseline sequence tagging model,0,66,40,40,0,model : Baseline sequence tagging model,0.3567567567567568,0.9302325581395349,0.9302325581395349
named-entity-recognition,3,where f is a non-linear function ) .,model,Baseline sequence tagging model,0,67,41,41,0,model : Baseline sequence tagging model,0.3621621621621622,0.9534883720930233,0.9534883720930233
named-entity-recognition,3,Another possibility introduces an attention - like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model .,model,Baseline sequence tagging model,0,68,42,42,0,model : Baseline sequence tagging model,0.3675675675675676,0.9767441860465116,0.9767441860465116
named-entity-recognition,3,"Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study , preferring to leave them for future work .",model,Baseline sequence tagging model,0,69,43,43,0,model : Baseline sequence tagging model,0.372972972972973,1.0,1.0
named-entity-recognition,3,Experiments,experiment,experiment,0,70,1,1,0,experiment : experiment,0.3783783783783784,0.023255813953488372,0.023255813953488372
named-entity-recognition,3,"We evaluate our approach on two well benchmarked sequence tagging tasks , the CoNLL 2003 NER task and the CoNLL 2000 Chunking task .",experiment,experiment,0,71,2,2,0,experiment : experiment,0.3837837837837838,0.046511627906976744,0.046511627906976744
named-entity-recognition,3,We report the official evaluation metric ( micro - averaged F 1 ) .,experiment,experiment,0,72,3,3,0,experiment : experiment,0.3891891891891892,0.06976744186046512,0.06976744186046512
named-entity-recognition,3,"In both cases , we use the BIOES labeling scheme for the output tags , following previous work which showed it outperforms other options ( e.g. , ) .",experiment,experiment,0,73,4,4,0,experiment : experiment,0.3945945945945946,0.09302325581395349,0.09302325581395349
named-entity-recognition,3,"Following , we use the Senna word embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0 .",experiment,experiment,0,74,5,5,0,experiment : experiment,0.4,0.11627906976744186,0.11627906976744186
named-entity-recognition,3,CoNLL 2003 NER .,experiment,experiment,1,75,6,6,0,experiment : experiment,0.40540540540540543,0.13953488372093023,0.13953488372093023
named-entity-recognition,3,"The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",experiment,experiment,0,76,7,7,0,experiment : experiment,0.41081081081081083,0.16279069767441862,0.16279069767441862
named-entity-recognition,3,"It includes standard train , development and test sets .",experiment,experiment,0,77,8,8,0,experiment : experiment,0.41621621621621624,0.18604651162790697,0.18604651162790697
named-entity-recognition,3,Following previous work we trained on both the train and development sets after tuning hyperparameters on the development set .,experiment,experiment,0,78,9,9,0,experiment : experiment,0.42162162162162165,0.20930232558139536,0.20930232558139536
named-entity-recognition,3,The hyperparameters for our baseline model are similar to .,experiment,experiment,0,79,10,10,0,experiment : experiment,0.42702702702702705,0.23255813953488372,0.23255813953488372
named-entity-recognition,3,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,experiment,experiment,1,80,11,11,0,experiment : experiment,0.43243243243243246,0.2558139534883721,0.2558139534883721
named-entity-recognition,3,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,experiment,experiment,1,81,12,12,0,experiment : experiment,0.43783783783783786,0.27906976744186046,0.27906976744186046
named-entity-recognition,3,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .",experiment,experiment,1,82,13,13,0,experiment : experiment,0.44324324324324327,0.3023255813953488,0.3023255813953488
named-entity-recognition,3,CoNLL 2000 chunking .,experiment,experiment,1,83,14,14,0,experiment : experiment,0.4486486486486487,0.32558139534883723,0.32558139534883723
named-entity-recognition,3,The CoNLL 2000 chunking task uses sections 15 - 18 from the Wall Street Journal corpus for training and section 20 for testing .,experiment,experiment,0,84,15,15,0,experiment : experiment,0.4540540540540541,0.3488372093023256,0.3488372093023256
named-entity-recognition,3,"It defines 11 syntactic chunk types ( e.g. , NP , VP , ADJP ) in addition to other .",experiment,experiment,0,85,16,16,0,experiment : experiment,0.4594594594594595,0.37209302325581395,0.37209302325581395
named-entity-recognition,3,We randomly sampled 1000 sentences from the training set as a held - out development set .,experiment,experiment,0,86,17,17,0,experiment : experiment,0.4648648648648649,0.3953488372093023,0.3953488372093023
named-entity-recognition,3,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,experiment,experiment,1,87,18,18,0,experiment : experiment,0.4702702702702703,0.4186046511627907,0.4186046511627907
named-entity-recognition,3,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,experiment,experiment,1,88,19,19,0,experiment : experiment,0.4756756756756757,0.4418604651162791,0.4418604651162791
named-entity-recognition,3,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",experiment,experiment,1,89,20,20,0,experiment : experiment,0.4810810810810811,0.46511627906976744,0.46511627906976744
named-entity-recognition,3,Pre-trained language models .,experiment,experiment,0,90,21,21,0,experiment : experiment,0.4864864864864865,0.4883720930232558,0.4883720930232558
named-entity-recognition,3,"The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark , a publicly available benchmark for largescale language modeling .",experiment,experiment,0,91,22,22,0,experiment : experiment,0.4918918918918919,0.5116279069767442,0.5116279069767442
named-entity-recognition,3,"The training split has approximately 800 million tokens , about a 4000X increase over the number training tokens in the CoNLL datasets .",experiment,experiment,0,92,23,23,0,experiment : experiment,0.4972972972972973,0.5348837209302325,0.5348837209302325
named-entity-recognition,3,explored several model architectures and released their best single model and training recipes .,experiment,experiment,0,93,24,24,0,experiment : experiment,0.5027027027027027,0.5581395348837209,0.5581395348837209
named-entity-recognition,3,"Following , they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state .",experiment,experiment,0,94,25,25,0,experiment : experiment,0.5081081081081081,0.5813953488372093,0.5813953488372093
named-entity-recognition,3,Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity .,experiment,experiment,0,95,26,26,0,experiment : experiment,0.5135135135135135,0.6046511627906976,0.6046511627906976
named-entity-recognition,3,"It uses a character CNN with 4096 filters for input , followed by two stacked LSTMs , each with 8192 hidden units and a 1024 dimensional projection layer .",experiment,experiment,0,96,27,27,0,experiment : experiment,0.518918918918919,0.627906976744186,0.627906976744186
named-entity-recognition,3,We use CNN - BIG - LSTM to refer to this language model in our results .,experiment,experiment,0,97,28,28,0,experiment : experiment,0.5243243243243243,0.6511627906976745,0.6511627906976745
named-entity-recognition,3,"In addition to CNN - BIG - LSTM from , 1 we used the same corpus to train two additional language models with fewer parameters : forward LSTM - 2048-512 and backward LSTM - 2048-512 .",experiment,experiment,0,98,29,29,0,experiment : experiment,0.5297297297297298,0.6744186046511628,0.6744186046511628
named-entity-recognition,3,Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer .,experiment,experiment,0,99,30,30,0,experiment : experiment,0.5351351351351351,0.6976744186046512,0.6976744186046512
named-entity-recognition,3,"We closely followed the procedure outlined in , except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs .",experiment,experiment,0,100,31,31,0,experiment : experiment,0.5405405405405406,0.7209302325581395,0.7209302325581395
named-entity-recognition,3,"The test set perplexities for our forward and backward LSTM - 2048 - 512 language models are 47.7 and 47.3 , respectively .",experiment,experiment,0,101,32,32,0,experiment : experiment,0.5459459459459459,0.7441860465116279,0.7441860465116279
named-entity-recognition,3,2,experiment,experiment,0,102,33,33,0,experiment : experiment,0.5513513513513514,0.7674418604651163,0.7674418604651163
named-entity-recognition,3,Model F 1 std 90.91 0.20 90.94 91.37 Our baseline without LM 90.87 0.13 TagLM 91.93 0.19 Training .,experiment,experiment,0,103,34,34,0,experiment : experiment,0.5567567567567567,0.7906976744186046,0.7906976744186046
named-entity-recognition,3,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .",experiment,experiment,1,104,35,35,0,experiment : experiment,0.5621621621621622,0.813953488372093,0.813953488372093
named-entity-recognition,3,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .",experiment,experiment,1,105,36,36,0,experiment : experiment,0.5675675675675675,0.8372093023255814,0.8372093023255814
named-entity-recognition,3,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .",experiment,experiment,1,106,37,37,0,experiment : experiment,0.572972972972973,0.8604651162790697,0.8604651162790697
named-entity-recognition,3,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,experiment,experiment,1,107,38,38,0,experiment : experiment,0.5783783783783784,0.8837209302325582,0.8837209302325582
named-entity-recognition,3,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ? an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ? an order of magnitude again , train for five more epochs and stop .",experiment,experiment,0,108,39,39,0,experiment : experiment,0.5837837837837838,0.9069767441860465,0.9069767441860465
named-entity-recognition,3,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ? an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ? an order of magnitude again , train for five more epochs and stop .",experiment,experiment,0,109,40,40,0,experiment : experiment,0.5891891891891892,0.9302325581395349,0.9302325581395349
named-entity-recognition,3,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ? an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ? an order of magnitude again , train for five more epochs and stop .",experiment,experiment,0,110,41,41,0,experiment : experiment,0.5945945945945946,0.9534883720930233,0.9534883720930233
named-entity-recognition,3,"Following , we train each final model configuration ten times with different random seeds and report the mean and standard deviation F 1 .",experiment,experiment,0,111,42,42,0,experiment : experiment,0.6,0.9767441860465116,0.9767441860465116
named-entity-recognition,3,It is important to estimate the variance of model performance since the test data sets are relatively small .,experiment,experiment,0,112,43,43,0,experiment : experiment,0.6054054054054054,1.0,1.0
named-entity-recognition,3,Overall system results,result,result,0,113,1,1,0,result : result,0.6108108108108108,0.5,0.5
named-entity-recognition,3,Tables 1 and 2 compare results from Tag LM with previously published state of the art results without additional labeled data or task specific gazetteers .,result,result,0,114,2,2,0,result : result,0.6162162162162163,1.0,1.0
named-entity-recognition,3,compare results of,result,result,0,115,1,1,0,result : result,0.6216216216216216,0.08333333333333333,0.08333333333333333
named-entity-recognition,3,Tag LM to other systems that include additional labeled data or gazetteers .,result,result,0,116,2,2,0,result : result,0.6270270270270271,0.16666666666666666,0.16666666666666666
named-entity-recognition,3,"In both tasks , Tag LM establishes a new state of the art using bidirectional LMs ( the forward CNN - BIG - LSTM and the backward LSTM - 2048 - 512 ) .",result,result,0,117,3,3,0,result : result,0.6324324324324324,0.25,0.25
named-entity-recognition,3,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .",result,result,1,118,4,4,0,result : result,0.6378378378378379,0.3333333333333333,0.3333333333333333
named-entity-recognition,3,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .",result,result,1,119,5,5,0,result : result,0.6432432432432432,0.4166666666666667,0.4166666666666667
named-entity-recognition,3,The improvement over the previous best result of 95.77 in that jointly trains with Penn Treebank ( PTB ) POS tags is statistically significant at 95 % ( p < 0.001 assuming standard deviation of 0.1 ) .,result,result,0,120,6,6,0,result : result,0.6486486486486487,0.5,0.5
named-entity-recognition,3,"Importantly , the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F 1 in the NER and Chunking tasks , respectively .",result,result,0,121,7,7,0,result : result,0.654054054054054,0.5833333333333334,0.5833333333333334
named-entity-recognition,3,Adding external resources .,result,result,0,122,8,8,0,result : result,0.6594594594594595,0.6666666666666666,0.6666666666666666
named-entity-recognition,3,"Although we do not use external labeled data or gazetteers , we found that TagLM outperforms previous state of the art results in both tasks when external resources ( labeled data or task specific gazetteers ) are available .",result,result,0,123,9,9,0,result : result,0.6648648648648648,0.75,0.75
named-entity-recognition,3,"Furthermore , show that , in most cases , the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning .",result,result,0,124,10,10,0,result : result,0.6702702702702703,0.8333333333333334,0.8333333333333334
named-entity-recognition,3,"For example , noted an improvement of only 0.06 F 1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and reported an increase of 0.71 F 1 when adding gazetteers to their baseline .",result,result,0,125,11,11,0,result : result,0.6756756756756757,0.9166666666666666,0.9166666666666666
named-entity-recognition,3,"In the Chunking task , previous work has reported from 0.28 to 0.75 improvement in F 1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities .",result,result,0,126,12,12,0,result : result,0.6810810810810811,1.0,1.0
named-entity-recognition,3,Analysis,analysis,analysis,0,127,1,1,0,analysis : analysis,0.6864864864864865,0.04,0.04
named-entity-recognition,3,"To elucidate the characteristics of our LM augmented sequence tagger , we ran a number of additional experiments on the CoNLL 2003 NER task .",analysis,analysis,0,128,2,2,0,analysis : analysis,0.6918918918918919,0.08,0.08
named-entity-recognition,3,"How to use LM embeddings ? In this experiment , we concatenate the LM embeddings at dif - shows that the second alternative performs best .",analysis,analysis,0,129,3,3,0,analysis : analysis,0.6972972972972973,0.12,0.12
named-entity-recognition,3,"How to use LM embeddings ? In this experiment , we concatenate the LM embeddings at dif - shows that the second alternative performs best .",analysis,analysis,0,130,4,4,0,analysis : analysis,0.7027027027027027,0.16,0.16
named-entity-recognition,3,We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves over all system performance .,analysis,analysis,0,131,5,5,0,analysis : analysis,0.7081081081081081,0.2,0.2
named-entity-recognition,3,These results are consistent with who found that chunking performance was sensitive to the level at which additional POS supervision was added .,analysis,analysis,0,132,6,6,0,analysis : analysis,0.7135135135135136,0.24,0.24
named-entity-recognition,3,"Does it matter which language model to use ? In this experiment , we compare six different configurations of the forward and backward language models ( including the baseline model which does not use any language models ) .",analysis,analysis,0,133,7,7,0,analysis : analysis,0.7189189189189189,0.28,0.28
named-entity-recognition,3,"Does it matter which language model to use ? In this experiment , we compare six different configurations of the forward and backward language models ( including the baseline model which does not use any language models ) .",analysis,analysis,0,134,8,8,0,analysis : analysis,0.7243243243243244,0.32,0.32
named-entity-recognition,3,The results are reported in .,analysis,analysis,0,135,9,9,0,analysis : analysis,0.7297297297297297,0.36,0.36
named-entity-recognition,3,"We find that adding backward LM embeddings consistently outperforms forward - only LM embeddings , with F 1 improvements between 0.22 and 0.27 % , even with the relatively small backward LSTM - 2048-512 LM .",analysis,analysis,0,136,10,10,0,analysis : analysis,0.7351351351351352,0.4,0.4
named-entity-recognition,3,"LM size is important , and replacing the forward LSTM - 2048 - 512 with CNN - BIG - LSTM ( test perplexities of 47.7 to 30.0 on 1B Word Benchmark ) improves F 1 by 0.26 - 0.31 % , about as much as adding backward LM .",analysis,analysis,0,137,11,11,0,analysis : analysis,0.7405405405405405,0.44,0.44
named-entity-recognition,3,"Accordingly , we hypothesize ( but have not tested ) that replacing the backward LSTM - 2048 - 512 with a backward LM analogous to the CNN - BIG - LSTM would further improve performance .",analysis,analysis,0,138,12,12,0,analysis : analysis,0.745945945945946,0.48,0.48
named-entity-recognition,3,"To highlight the importance of including language models trained on a large scale data , we also experimented with training a language model on just the CoNLL 2003 training and development data .",analysis,analysis,0,139,13,13,0,analysis : analysis,0.7513513513513513,0.52,0.52
named-entity-recognition,3,Due to the much smaller size of this data Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM .,analysis,analysis,0,140,14,14,0,analysis : analysis,0.7567567567567568,0.56,0.56
named-entity-recognition,3,"This result supports the hypothesis that adding language models help because they learn composition functions ( i.e. , the RNN parameters in the language model ) from much larger data compared to the composition functions in the baseline tagger , which are only learned from labeled data .",analysis,analysis,0,141,15,15,0,analysis : analysis,0.7621621621621621,0.6,0.6
named-entity-recognition,3,Importance of task specific RNN .,analysis,analysis,0,142,16,16,0,analysis : analysis,0.7675675675675676,0.64,0.64
named-entity-recognition,3,To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags .,analysis,analysis,0,143,17,17,0,analysis : analysis,0.772972972972973,0.68,0.68
named-entity-recognition,3,"In this setup , performance was very low , 88.17 F 1 , well below our baseline .",analysis,analysis,0,144,18,18,0,analysis : analysis,0.7783783783783784,0.72,0.72
named-entity-recognition,3,This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings .,analysis,analysis,0,145,19,19,0,analysis : analysis,0.7837837837837838,0.76,0.76
named-entity-recognition,3,Does the LM transfer across domains ? One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,analysis,analysis,0,146,20,20,0,analysis : analysis,0.7891891891891892,0.8,0.8
named-entity-recognition,3,Does the LM transfer across domains ? One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,analysis,analysis,0,147,21,21,0,analysis : analysis,0.7945945945945946,0.84,0.84
named-entity-recognition,3,"To test the sensitivity to the LM training domain , we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 , Science IE .",analysis,analysis,0,148,22,22,0,analysis : analysis,0.8,0.88,0.88
named-entity-recognition,3,"Scien - ce IE requires end - to - end joint entity and relationship extraction from scientific publications across three diverse fields ( computer science , material sciences , and physics ) and defines three broad entity types ( Task , Material and Process ) .",analysis,analysis,0,149,23,23,0,analysis : analysis,0.8054054054054054,0.92,0.92
named-entity-recognition,3,"For this task , Tag LM increased F 1 on the development set by 4.12 % ( from 49.93 to to 54.05 % ) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to Science IE , Scenario 1 .",analysis,analysis,0,150,24,24,0,analysis : analysis,0.8108108108108109,0.96,0.96
named-entity-recognition,3,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain .,analysis,analysis,0,151,25,25,0,analysis : analysis,0.8162162162162162,1.0,1.0
named-entity-recognition,3,Related work,related work,related work,0,152,1,1,0,related work : related work,0.8216216216216217,0.034482758620689655,0.034482758620689655
named-entity-recognition,3,Unlabeled data .,related work,related work,0,153,2,2,0,related work : related work,0.827027027027027,0.06896551724137931,0.06896551724137931
named-entity-recognition,3,Tag LM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models .,related work,related work,0,154,3,3,0,related work : related work,0.8324324324324325,0.10344827586206896,0.10344827586206896
named-entity-recognition,3,"Besides pre-trained word embeddings , our method is most closely related to .",related work,related work,0,155,4,4,0,related work : related work,0.8378378378378378,0.13793103448275862,0.13793103448275862
named-entity-recognition,3,"Instead of using a LM , uses a probabilistic generative model to infer contextsensitive latent variables for each token , which are then used as extra features in a supervised CRF tagger .",related work,related work,0,156,5,5,0,related work : related work,0.8432432432432433,0.1724137931034483,0.1724137931034483
named-entity-recognition,3,"Other semisupervised learning methods for structured prediction problems include co-training , expectation maximization , structural learning ( Ando and Zhang , 2005 ) and maximum discriminant functions .",related work,related work,0,157,6,6,0,related work : related work,0.8486486486486486,0.20689655172413793,0.20689655172413793
named-entity-recognition,3,It is easy to combine Tag LM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model ( except for expectation maximization ) .,related work,related work,0,158,7,7,0,related work : related work,0.8540540540540541,0.2413793103448276,0.2413793103448276
named-entity-recognition,3,detailed discussion of semisupervised learning methods in NLP can be found in .,related work,related work,0,159,8,8,0,related work : related work,0.8594594594594595,0.27586206896551724,0.27586206896551724
named-entity-recognition,3,"learned a context encoder from unlabeled data with an objective function similar to a bi-directional LM and applied it to several NLP tasks closely related to the unlabeled objective function : sentence completion , lexical substitution and word sense dis ambiguation .",related work,related work,0,160,9,9,0,related work : related work,0.8648648648648649,0.3103448275862069,0.3103448275862069
named-entity-recognition,3,"LM embeddings are related to a class of methods ( e.g. , for learning sentence and document encoders from unlabeled data , which can be used for text classification and textual entailment among other tasks .",related work,related work,0,161,10,10,0,related work : related work,0.8702702702702703,0.3448275862068966,0.3448275862068966
named-entity-recognition,3,Dai and Le pre-trained LSTMs using language models and sequence autoencoders then fine tuned the weights for classification tasks .,related work,related work,0,162,11,11,0,related work : related work,0.8756756756756757,0.3793103448275862,0.3793103448275862
named-entity-recognition,3,"In contrast to our method that uses unlabeled data to learn token - in - context embeddings , all of these methods use unlabeled data to learn an encoder for an entire text sequence ( sentence or document ) .",related work,related work,0,163,12,12,0,related work : related work,0.8810810810810811,0.41379310344827586,0.41379310344827586
named-entity-recognition,3,Neural language models .,related work,related work,0,164,13,13,0,related work : related work,0.8864864864864865,0.4482758620689655,0.4482758620689655
named-entity-recognition,3,LMs have always been a critical component in statistical machine translation systems .,related work,related work,0,165,14,14,0,related work : related work,0.8918918918918919,0.4827586206896552,0.4827586206896552
named-entity-recognition,3,"Recently , neural LMs have also been integrated in neural machine translation systems ( e.g. , to score candidate translations .",related work,related work,0,166,15,15,0,related work : related work,0.8972972972972973,0.5172413793103449,0.5172413793103449
named-entity-recognition,3,"In contrast , Tag LM uses neural LMs to encode words in the input sequence .",related work,related work,0,167,16,16,0,related work : related work,0.9027027027027027,0.5517241379310345,0.5517241379310345
named-entity-recognition,3,"Unlike forward LMs , bidirectional LMs have received little prior attention .",related work,related work,0,168,17,17,0,related work : related work,0.9081081081081082,0.5862068965517241,0.5862068965517241
named-entity-recognition,3,"Most similar to our formulation , used a bidirectional neural LM in a statistical machine translation system for instance selection .",related work,related work,0,169,18,18,0,related work : related work,0.9135135135135135,0.6206896551724138,0.6206896551724138
named-entity-recognition,3,"They tied the input token embeddings and softmax weights in the forward and backward directions , unlike our approach which uses two distinct models without any shared parameters .",related work,related work,0,170,19,19,0,related work : related work,0.918918918918919,0.6551724137931034,0.6551724137931034
named-entity-recognition,3,Frinken et al. ( 2012 ) also used a bidirectional n-gram LM for handwriting recognition .,related work,related work,0,171,20,20,0,related work : related work,0.9243243243243243,0.6896551724137931,0.6896551724137931
named-entity-recognition,3,Interpreting RNN states .,related work,related work,0,172,21,21,0,related work : related work,0.9297297297297298,0.7241379310344828,0.7241379310344828
named-entity-recognition,3,"Recently , there has been some interest in interpreting the activations of RNNs .",related work,related work,0,173,22,22,0,related work : related work,0.9351351351351351,0.7586206896551724,0.7586206896551724
named-entity-recognition,3,showed that single LSTM units can learn to predict singular - plural distinctions .,related work,related work,0,174,23,23,0,related work : related work,0.9405405405405406,0.7931034482758621,0.7931034482758621
named-entity-recognition,3,"visualized character level LSTM states and showed that individual cells capture long - range dependencies such as line lengths , quotes and brackets .",related work,related work,0,175,24,24,0,related work : related work,0.9459459459459459,0.8275862068965517,0.8275862068965517
named-entity-recognition,3,Our work complements these studies by showing that LM states are useful for downstream tasks as away of interpreting what they learn .,related work,related work,0,176,25,25,0,related work : related work,0.9513513513513514,0.8620689655172413,0.8620689655172413
named-entity-recognition,3,Other sequence tagging models .,related work,related work,0,177,26,26,0,related work : related work,0.9567567567567568,0.896551724137931,0.896551724137931
named-entity-recognition,3,Current state of the art results in sequence tagging problems are based on bidirectional RNN models .,related work,related work,0,178,27,27,0,related work : related work,0.9621621621621622,0.9310344827586207,0.9310344827586207
named-entity-recognition,3,"However , many other sequence tagging models have been proposed in the literature for this class of problems ( e.g. , .",related work,related work,0,179,28,28,0,related work : related work,0.9675675675675676,0.9655172413793104,0.9655172413793104
named-entity-recognition,3,"LM embeddings could also be used as additional features in other models , although it is not clear whether the model complexity would be sufficient to effectively make use of them .",related work,related work,0,180,29,29,0,related work : related work,0.972972972972973,1.0,1.0
named-entity-recognition,3,Conclusion,conclusion,conclusion,0,181,1,1,0,conclusion : conclusion,0.9783783783783784,0.2,0.2
named-entity-recognition,3,"In this paper , we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models .",conclusion,conclusion,0,182,2,2,0,conclusion : conclusion,0.9837837837837838,0.4,0.4
named-entity-recognition,3,Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking .,conclusion,conclusion,0,183,3,3,0,conclusion : conclusion,0.9891891891891892,0.6,0.6
named-entity-recognition,3,Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance .,conclusion,conclusion,0,184,4,4,0,conclusion : conclusion,0.9945945945945946,0.8,0.8
named-entity-recognition,3,"The proposed method is robust even when the LM is trained on unlabeled data from a different domain , or when the baseline model is trained on a large number of labeled examples .",conclusion,conclusion,0,185,5,5,0,conclusion : conclusion,1.0,1.0,1.0
named-entity-recognition,4,Deep contextualized word representations,title,title,1,2,1,1,0,title : title,0.007352941176470588,1.0,1.0
named-entity-recognition,4,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011029411764705883,0.2,0.2
named-entity-recognition,4,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.014705882352941176,0.4,0.4
named-entity-recognition,4,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01838235294117647,0.6,0.6
named-entity-recognition,4,"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems , including question answering , textual entailment and sentiment analysis .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.022058823529411766,0.8,0.8
named-entity-recognition,4,"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial , allowing downstream models to mix different types of semi-supervision signals .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.025735294117647058,1.0,1.0
named-entity-recognition,4,Introduction,introduction,introduction,0,8,1,1,0,introduction : introduction,0.029411764705882353,0.02564102564102564,0.02564102564102564
named-entity-recognition,4,Pre-trained word representations are a key component in many neural language understanding models .,introduction,introduction,0,9,2,2,0,introduction : introduction,0.03308823529411765,0.05128205128205128,0.05128205128205128
named-entity-recognition,4,"However , learning high quality representations can be challenging .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.03676470588235294,0.07692307692307693,0.07692307692307693
named-entity-recognition,4,"They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",introduction,introduction,0,11,4,4,0,introduction : introduction,0.04044117647058824,0.10256410256410256,0.10256410256410256
named-entity-recognition,4,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",introduction,introduction,0,12,5,5,0,introduction : introduction,0.04411764705882353,0.1282051282051282,0.1282051282051282
named-entity-recognition,4,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,introduction,introduction,1,13,6,6,0,introduction : introduction,0.04779411764705882,0.15384615384615385,0.15384615384615385
named-entity-recognition,4,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,introduction,introduction,1,14,7,7,0,introduction : introduction,0.051470588235294115,0.1794871794871795,0.1794871794871795
named-entity-recognition,4,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",introduction,introduction,1,15,8,8,0,introduction : introduction,0.05514705882352941,0.20512820512820512,0.20512820512820512
named-entity-recognition,4,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",introduction,introduction,0,16,9,9,0,introduction : introduction,0.058823529411764705,0.23076923076923078,0.23076923076923078
named-entity-recognition,4,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",introduction,introduction,1,17,10,10,0,introduction : introduction,0.0625,0.2564102564102564,0.2564102564102564
named-entity-recognition,4,Combining the internal states in this manner allows for very rich word representations .,introduction,introduction,0,18,11,11,0,introduction : introduction,0.0661764705882353,0.28205128205128205,0.28205128205128205
named-entity-recognition,4,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense dis ambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",introduction,introduction,0,19,12,12,0,introduction : introduction,0.06985294117647059,0.3076923076923077,0.3076923076923077
named-entity-recognition,4,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision thatare most useful for each end task .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.07352941176470588,0.3333333333333333,0.3333333333333333
named-entity-recognition,4,Extensive experiments demonstrate that ELMo representations work extremely well in practice .,introduction,introduction,0,21,14,14,0,introduction : introduction,0.07720588235294118,0.358974358974359,0.358974358974359
named-entity-recognition,4,"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems , including textual entailment , question answering and sentiment analysis .",introduction,introduction,0,22,15,15,0,introduction : introduction,0.08088235294117647,0.38461538461538464,0.38461538461538464
named-entity-recognition,4,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .",introduction,introduction,0,23,16,16,0,introduction : introduction,0.08455882352941177,0.41025641025641024,0.41025641025641024
named-entity-recognition,4,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",introduction,introduction,0,24,17,17,0,introduction : introduction,0.08823529411764706,0.4358974358974359,0.4358974358974359
named-entity-recognition,4,"Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform ar Xiv : 1802.05365v2 [ cs. CL ] 22 Mar 2018 those derived from just the top layer of an LSTM .",introduction,introduction,0,25,18,18,0,introduction : introduction,0.09191176470588236,0.46153846153846156,0.46153846153846156
named-entity-recognition,4,"Our trained models and code are publicly available , and we expect that ELMo will provide similar gains for many other NLP problems .",introduction,introduction,0,26,19,19,0,introduction : introduction,0.09558823529411764,0.48717948717948717,0.48717948717948717
named-entity-recognition,4,1,introduction,introduction,0,27,20,20,0,introduction : introduction,0.09926470588235294,0.5128205128205128,0.5128205128205128
named-entity-recognition,4,"Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",introduction,introduction,0,28,21,21,0,introduction : introduction,0.10294117647058823,0.5384615384615384,0.5384615384615384
named-entity-recognition,4,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",introduction,introduction,0,29,22,22,0,introduction : introduction,0.10661764705882353,0.5641025641025641,0.5641025641025641
named-entity-recognition,4,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",introduction,introduction,0,30,23,23,0,introduction : introduction,0.11029411764705882,0.5897435897435898,0.5897435897435898
named-entity-recognition,4,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",introduction,introduction,0,31,24,24,0,introduction : introduction,0.11397058823529412,0.6153846153846154,0.6153846153846154
named-entity-recognition,4,Other recent work has also focused on learning context - dependent representations .,introduction,introduction,0,32,25,25,0,introduction : introduction,0.11764705882352941,0.6410256410256411,0.6410256410256411
named-entity-recognition,4,context2vec,introduction,introduction,0,33,26,26,0,introduction : introduction,0.1213235294117647,0.6666666666666666,0.6666666666666666
named-entity-recognition,4,"Melamud et al. , 2016 ) uses a bidirectional Long Short Term Memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) to encode the context around a pivot word .",introduction,introduction,0,34,27,27,0,introduction : introduction,0.125,0.6923076923076923,0.6923076923076923
named-entity-recognition,4,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,introduction,introduction,0,35,28,28,0,introduction : introduction,0.12867647058823528,0.717948717948718,0.717948717948718
named-entity-recognition,4,"Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .",introduction,introduction,0,36,29,29,0,introduction : introduction,0.1323529411764706,0.7435897435897436,0.7435897435897436
named-entity-recognition,4,"In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences .",introduction,introduction,0,37,30,30,0,introduction : introduction,0.13602941176470587,0.7692307692307693,0.7692307692307693
named-entity-recognition,4,"We also generalize these approaches to deep contextual representations , which we show work well across a broad range of diverse NLP tasks .",introduction,introduction,0,38,31,31,0,introduction : introduction,0.13970588235294118,0.7948717948717948,0.7948717948717948
named-entity-recognition,4,http://allennlp.org/elmo,introduction,introduction,0,39,32,32,0,introduction : introduction,0.14338235294117646,0.8205128205128205,0.8205128205128205
named-entity-recognition,4,Previous work has also shown that different layers of deep biRNNs encode different types of information .,introduction,introduction,0,40,33,33,0,introduction : introduction,0.14705882352941177,0.8461538461538461,0.8461538461538461
named-entity-recognition,4,"For example , introducing multi-task syntactic supervision ( e.g. , part - of - speech tags ) at the lower levels of a deep LSTM can improve over all performance of higher level tasks such as dependency parsing or CCG super tagging .",introduction,introduction,0,41,34,34,0,introduction : introduction,0.15073529411764705,0.8717948717948718,0.8717948717948718
named-entity-recognition,4,"In an RNN - based encoder - decoder machine translation system , showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer .",introduction,introduction,0,42,35,35,0,introduction : introduction,0.15441176470588236,0.8974358974358975,0.8974358974358975
named-entity-recognition,4,"Finally , the top layer of an LSTM for encoding word context has been shown to learn representations of word sense .",introduction,introduction,0,43,36,36,0,introduction : introduction,0.15808823529411764,0.9230769230769231,0.9230769230769231
named-entity-recognition,4,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",introduction,introduction,0,44,37,37,0,introduction : introduction,0.16176470588235295,0.9487179487179487,0.9487179487179487
named-entity-recognition,4,and pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .,introduction,introduction,0,45,38,38,0,introduction : introduction,0.16544117647058823,0.9743589743589743,0.9743589743589743
named-entity-recognition,4,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",introduction,introduction,0,46,39,39,0,introduction : introduction,0.16911764705882354,1.0,1.0
named-entity-recognition,4,Dai and Le,system description,Dai and Le,0,47,1,1,0,system description : Dai and Le,0.17279411764705882,1.0,1.0
named-entity-recognition,4,Bidirectional language models,model,Bidirectional language models,0,48,1,1,0,model : Bidirectional language models,0.17647058823529413,0.017241379310344827,0.09090909090909091
named-entity-recognition,4,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k?1 ) :",model,Bidirectional language models,0,49,2,2,0,model : Bidirectional language models,0.1801470588235294,0.034482758620689655,0.18181818181818182
named-entity-recognition,4,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,model,Bidirectional language models,0,50,3,3,0,model : Bidirectional language models,0.18382352941176472,0.05172413793103448,0.2727272727272727
named-entity-recognition,4,"At each position k , each LSTM layer outputs a context - dependent representation ? ? h LM k , j where j = 1 , . . . , L. The top layer LSTM output , ? ? h LM k , L , is used to predict the next token t k +1 with a Softmax layer .",model,Bidirectional language models,0,51,4,4,0,model : Bidirectional language models,0.1875,0.06896551724137931,0.36363636363636365
named-entity-recognition,4,"backward LM is similar to a forward LM , except it runs over the sequence in reverse , predicting the previous token given the future context :",model,Bidirectional language models,0,52,5,5,0,model : Bidirectional language models,0.19117647058823528,0.08620689655172414,0.45454545454545453
named-entity-recognition,4,"It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations ? ? h LM k , j oft k given ( t k+1 , . . . , t N ) .",model,Bidirectional language models,0,53,6,6,0,model : Bidirectional language models,0.1948529411764706,0.10344827586206896,0.5454545454545454
named-entity-recognition,4,bi LM combines both a forward and backward LM .,model,Bidirectional language models,0,54,7,7,0,model : Bidirectional language models,0.19852941176470587,0.1206896551724138,0.6363636363636364
named-entity-recognition,4,Our formulation jointly maximizes the log likelihood of the forward and backward directions :,model,Bidirectional language models,0,55,8,8,0,model : Bidirectional language models,0.20220588235294118,0.13793103448275862,0.7272727272727273
named-entity-recognition,4,We tie the parameters for both the token representation (? x ) and Softmax layer (? s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction .,model,Bidirectional language models,0,56,9,9,0,model : Bidirectional language models,0.20588235294117646,0.15517241379310345,0.8181818181818182
named-entity-recognition,4,"Overall , this formulation is similar to the approach of Peters et al. , with the exception that we share some weights between directions instead of using completely independent parameters .",model,Bidirectional language models,0,57,10,10,0,model : Bidirectional language models,0.20955882352941177,0.1724137931034483,0.9090909090909091
named-entity-recognition,4,"In the next section , we depart from previous work by introducing a new approach for learning word representations thatare a linear combination of the biLM layers .",model,Bidirectional language models,0,58,11,11,0,model : Bidirectional language models,0.21323529411764705,0.1896551724137931,1.0
named-entity-recognition,4,ELMo,model,ELMo,0,59,12,1,0,model : ELMo,0.21691176470588236,0.20689655172413793,0.08333333333333333
named-entity-recognition,4,ELMo is a task specific combination of the intermediate layer representations in the biLM .,model,ELMo,0,60,13,2,0,model : ELMo,0.22058823529411764,0.22413793103448276,0.16666666666666666
named-entity-recognition,4,"For each token t k , a L-layer biLM computes a set of 2L + 1 representations",model,ELMo,0,61,14,3,0,model : ELMo,0.22426470588235295,0.2413793103448276,0.25
named-entity-recognition,4,"where h LM k ,0 is the token layer and h LM",model,ELMo,0,62,15,4,0,model : ELMo,0.22794117647058823,0.25862068965517243,0.3333333333333333
named-entity-recognition,4,for each biLSTM layer .,model,ELMo,0,63,16,5,0,model : ELMo,0.23161764705882354,0.27586206896551724,0.4166666666666667
named-entity-recognition,4,"For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",model,ELMo,0,64,17,6,0,model : ELMo,0.23529411764705882,0.29310344827586204,0.5
named-entity-recognition,4,"In the simplest case , ELMo just selects the top layer , E( R k ) = h LM k , L , as in Tag LM and CoVe .",model,ELMo,0,65,18,7,0,model : ELMo,0.23897058823529413,0.3103448275862069,0.5833333333333334
named-entity-recognition,4,"More generally , we compute a task specific weighting of all biLM layers :",model,ELMo,0,66,19,8,0,model : ELMo,0.2426470588235294,0.3275862068965517,0.6666666666666666
named-entity-recognition,4,"1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ? task allows the task model to scale the entire ELMo vector . ? is of practical importance to aid the optimization process ( see supplemental material for details ) .",model,ELMo,0,67,20,9,0,model : ELMo,0.24632352941176472,0.3448275862068966,0.75
named-entity-recognition,4,"1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ? task allows the task model to scale the entire ELMo vector . ? is of practical importance to aid the optimization process ( see supplemental material for details ) .",model,ELMo,0,68,21,10,0,model : ELMo,0.25,0.3620689655172414,0.8333333333333334
named-entity-recognition,4,"1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ? task allows the task model to scale the entire ELMo vector . ? is of practical importance to aid the optimization process ( see supplemental material for details ) .",model,ELMo,0,69,22,11,0,model : ELMo,0.2536764705882353,0.3793103448275862,0.9166666666666666
named-entity-recognition,4,"Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization to each biLM layer before weighting .",model,ELMo,0,70,23,12,0,model : ELMo,0.25735294117647056,0.39655172413793105,1.0
named-entity-recognition,4,Using biLMs for supervised NLP tasks,model,Using biLMs for supervised NLP tasks,0,71,24,1,0,model : Using biLMs for supervised NLP tasks,0.2610294117647059,0.41379310344827586,0.06666666666666667
named-entity-recognition,4,"Given a pre-trained biLM and a supervised architecture for a target NLP task , it is a simple process to use the biLM to improve the task model .",model,Using biLMs for supervised NLP tasks,0,72,25,2,0,model : Using biLMs for supervised NLP tasks,0.2647058823529412,0.43103448275862066,0.13333333333333333
named-entity-recognition,4,We simply run the biLM and record all of the layer representations for each word .,model,Using biLMs for supervised NLP tasks,0,73,26,3,0,model : Using biLMs for supervised NLP tasks,0.26838235294117646,0.4482758620689655,0.2
named-entity-recognition,4,"Then , we let the end task model learn a linear combination of these representations , as described below .",model,Using biLMs for supervised NLP tasks,0,74,27,4,0,model : Using biLMs for supervised NLP tasks,0.27205882352941174,0.46551724137931033,0.26666666666666666
named-entity-recognition,4,First consider the lowest layers of the supervised model without the biLM .,model,Using biLMs for supervised NLP tasks,0,75,28,5,0,model : Using biLMs for supervised NLP tasks,0.2757352941176471,0.4827586206896552,0.3333333333333333
named-entity-recognition,4,"Most supervised NLP models share a common architecture at the lowest layers , allowing us to add ELMo in a consistent , unified manner .",model,Using biLMs for supervised NLP tasks,0,76,29,6,0,model : Using biLMs for supervised NLP tasks,0.27941176470588236,0.5,0.4
named-entity-recognition,4,"Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre-trained word embeddings and optionally character - based representations .",model,Using biLMs for supervised NLP tasks,0,77,30,7,0,model : Using biLMs for supervised NLP tasks,0.28308823529411764,0.5172413793103449,0.4666666666666667
named-entity-recognition,4,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .",model,Using biLMs for supervised NLP tasks,0,78,31,8,0,model : Using biLMs for supervised NLP tasks,0.2867647058823529,0.5344827586206896,0.5333333333333333
named-entity-recognition,4,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",model,Using biLMs for supervised NLP tasks,0,79,32,9,0,model : Using biLMs for supervised NLP tasks,0.29044117647058826,0.5517241379310345,0.6
named-entity-recognition,4,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",model,Using biLMs for supervised NLP tasks,0,80,33,10,0,model : Using biLMs for supervised NLP tasks,0.29411764705882354,0.5689655172413793,0.6666666666666666
named-entity-recognition,4,"As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",model,Using biLMs for supervised NLP tasks,0,81,34,11,0,model : Using biLMs for supervised NLP tasks,0.2977941176470588,0.5862068965517241,0.7333333333333333
named-entity-recognition,4,"For example , see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs , or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs .",model,Using biLMs for supervised NLP tasks,0,82,35,12,0,model : Using biLMs for supervised NLP tasks,0.3014705882352941,0.603448275862069,0.8
named-entity-recognition,4,"Finally , we found it beneficial to add a moderate amount of dropout to and in some cases to regularize the ELMo weights by adding ? w 2 2 to the loss .",model,Using biLMs for supervised NLP tasks,0,83,36,13,0,model : Using biLMs for supervised NLP tasks,0.30514705882352944,0.6206896551724138,0.8666666666666667
named-entity-recognition,4,"Finally , we found it beneficial to add a moderate amount of dropout to and in some cases to regularize the ELMo weights by adding ? w 2 2 to the loss .",model,Using biLMs for supervised NLP tasks,0,84,37,14,0,model : Using biLMs for supervised NLP tasks,0.3088235294117647,0.6379310344827587,0.9333333333333333
named-entity-recognition,4,This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers .,model,Using biLMs for supervised NLP tasks,0,85,38,15,0,model : Using biLMs for supervised NLP tasks,0.3125,0.6551724137931034,1.0
named-entity-recognition,4,Pre-trained bidirectional language model architecture,model,Pre-trained bidirectional language model architecture,0,86,39,1,0,model : Pre-trained bidirectional language model architecture,0.3161764705882353,0.6724137931034483,0.05
named-entity-recognition,4,"The pre-trained biLMs in this paper are similar to the architectures in Jzefowicz et al. and , but modified to support joint training of both directions and add a residual connection between LSTM layers .",model,Pre-trained bidirectional language model architecture,0,87,40,2,0,model : Pre-trained bidirectional language model architecture,0.31985294117647056,0.6896551724137931,0.1
named-entity-recognition,4,"We focus on large scale biLMs in this work , as Peters et al . highlighted the importance of using biLMs over forward - only LMs and large scale training .",model,Pre-trained bidirectional language model architecture,0,88,41,3,0,model : Pre-trained bidirectional language model architecture,0.3235294117647059,0.7068965517241379,0.15
named-entity-recognition,4,"We focus on large scale biLMs in this work , as Peters et al . highlighted the importance of using biLMs over forward - only LMs and large scale training .",model,Pre-trained bidirectional language model architecture,0,89,42,4,0,model : Pre-trained bidirectional language model architecture,0.3272058823529412,0.7241379310344828,0.2
named-entity-recognition,4,"To balance over all language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character - based input representation , we halved all embedding and hidden dimensions from the single best model CNN - BIG - LSTM in Jzefowicz et al ..",model,Pre-trained bidirectional language model architecture,0,90,43,5,0,model : Pre-trained bidirectional language model architecture,0.33088235294117646,0.7413793103448276,0.25
named-entity-recognition,4,The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer .,model,Pre-trained bidirectional language model architecture,0,91,44,6,0,model : Pre-trained bidirectional language model architecture,0.33455882352941174,0.7586206896551724,0.3
named-entity-recognition,4,The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation .,model,Pre-trained bidirectional language model architecture,0,92,45,7,0,model : Pre-trained bidirectional language model architecture,0.3382352941176471,0.7758620689655172,0.35
named-entity-recognition,4,"As a result , the biLM provides three layers of representations for each input token , including those outside the training set due to the purely character input .",model,Pre-trained bidirectional language model architecture,0,93,46,8,0,model : Pre-trained bidirectional language model architecture,0.34191176470588236,0.7931034482758621,0.4
named-entity-recognition,4,"In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .",model,Pre-trained bidirectional language model architecture,0,94,47,9,0,model : Pre-trained bidirectional language model architecture,0.34558823529411764,0.8103448275862069,0.45
named-entity-recognition,4,"After training for 10 epochs on the 1B Word Benchmark , the average forward and backward perplexities is 39.7 , compared to 30.0 for the forward CNN - BIG - LSTM .",model,Pre-trained bidirectional language model architecture,0,95,48,10,0,model : Pre-trained bidirectional language model architecture,0.3492647058823529,0.8275862068965517,0.5
named-entity-recognition,4,"Generally , we found the forward and backward perplexities to be approximately equal , with the backward value slightly lower .",model,Pre-trained bidirectional language model architecture,0,96,49,11,0,model : Pre-trained bidirectional language model architecture,0.35294117647058826,0.8448275862068966,0.55
named-entity-recognition,4,"Once pretrained , the biLM can compute representations for any task .",model,Pre-trained bidirectional language model architecture,0,97,50,12,0,model : Pre-trained bidirectional language model architecture,0.35661764705882354,0.8620689655172413,0.6
named-entity-recognition,4,"In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .",model,Pre-trained bidirectional language model architecture,0,98,51,13,0,model : Pre-trained bidirectional language model architecture,0.3602941176470588,0.8793103448275862,0.65
named-entity-recognition,4,This can be seen as a type of domain transfer for the biLM .,model,Pre-trained bidirectional language model architecture,0,99,52,14,0,model : Pre-trained bidirectional language model architecture,0.3639705882352941,0.896551724137931,0.7
named-entity-recognition,4,"As a result , in most cases we used a fine - tuned biLM in the downstream task .",model,Pre-trained bidirectional language model architecture,0,100,53,15,0,model : Pre-trained bidirectional language model architecture,0.36764705882352944,0.9137931034482759,0.75
named-entity-recognition,4,See supplemental material for details .,model,Pre-trained bidirectional language model architecture,0,101,54,16,0,model : Pre-trained bidirectional language model architecture,0.3713235294117647,0.9310344827586207,0.8
named-entity-recognition,4,shows the performance of ELMo across a diverse set of six benchmark NLP tasks .,model,Pre-trained bidirectional language model architecture,0,102,55,17,0,model : Pre-trained bidirectional language model architecture,0.375,0.9482758620689655,0.85
named-entity-recognition,4,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",model,Pre-trained bidirectional language model architecture,0,103,56,18,0,model : Pre-trained bidirectional language model architecture,0.3786764705882353,0.9655172413793104,0.9
named-entity-recognition,4,This is a very general result across a diverse set model architectures and language understanding tasks .,model,Pre-trained bidirectional language model architecture,0,104,57,19,0,model : Pre-trained bidirectional language model architecture,0.38235294117647056,0.9827586206896551,0.95
named-entity-recognition,4,In the remainder of this section we provide high - level sketches of the individual task results ; see the supplemental material for full experimental details .,model,Pre-trained bidirectional language model architecture,0,105,58,20,0,model : Pre-trained bidirectional language model architecture,0.3860294117647059,1.0,1.0
named-entity-recognition,4,Evaluation,evaluation,Evaluation,0,106,1,1,0,evaluation : Evaluation,0.3897058823529412,0.06666666666666667,0.06666666666666667
named-entity-recognition,4,Question Textual entailment,evaluation,Evaluation,0,107,2,2,0,evaluation : Evaluation,0.39338235294117646,0.13333333333333333,0.13333333333333333
named-entity-recognition,4,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",evaluation,Evaluation,1,108,3,3,0,evaluation : Evaluation,0.39705882352941174,0.2,0.2
named-entity-recognition,4,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,evaluation,Evaluation,1,109,4,4,0,evaluation : Evaluation,0.4007352941176471,0.26666666666666666,0.26666666666666666
named-entity-recognition,4,"Our baseline , the ESIM sequence model from Chen et al. , uses a biL - STM to encode the premise and hypothesis , followed by a matrix attention layer , a local inference layer , another biLSTM inference composition layer , and finally a pooling operation before the output layer .",evaluation,Evaluation,0,110,5,5,0,evaluation : Evaluation,0.40441176470588236,0.3333333333333333,0.3333333333333333
named-entity-recognition,4,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",evaluation,Evaluation,1,111,6,6,0,evaluation : Evaluation,0.40808823529411764,0.4,0.4
named-entity-recognition,4,"five member ensemble pushes the over all accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .",evaluation,Evaluation,0,112,7,7,0,evaluation : Evaluation,0.4117647058823529,0.4666666666666667,0.4666666666666667
named-entity-recognition,4,Semantic role labeling,evaluation,Evaluation,0,113,8,8,0,evaluation : Evaluation,0.41544117647058826,0.5333333333333333,0.5333333333333333
named-entity-recognition,4,"semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",evaluation,Evaluation,0,114,9,9,0,evaluation : Evaluation,0.41911764705882354,0.6,0.6
named-entity-recognition,4,"He et al . ( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",evaluation,Evaluation,0,115,10,10,0,evaluation : Evaluation,0.4227941176470588,0.6666666666666666,0.6666666666666666
named-entity-recognition,4,"He et al . ( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",evaluation,Evaluation,0,116,11,11,0,evaluation : Evaluation,0.4264705882352941,0.7333333333333333,0.7333333333333333
named-entity-recognition,4,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,evaluation,Evaluation,1,117,12,12,0,evaluation : Evaluation,0.43014705882352944,0.8,0.8
named-entity-recognition,4,Our baseline model is the end - to - end span - based neural model of .,evaluation,Evaluation,0,118,13,13,0,evaluation : Evaluation,0.4338235294117647,0.8666666666666667,0.8666666666666667
named-entity-recognition,4,It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .,evaluation,Evaluation,0,119,14,14,0,evaluation : Evaluation,0.4375,0.9333333333333333,0.9333333333333333
named-entity-recognition,4,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",evaluation,Evaluation,1,120,15,15,0,evaluation : Evaluation,0.4411764705882353,1.0,1.0
named-entity-recognition,4,Analysis,analysis,Analysis,0,121,1,1,0,analysis : Analysis,0.44485294117647056,0.014705882352941176,0.16666666666666666
named-entity-recognition,4,This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations .,analysis,Analysis,0,122,2,2,0,analysis : Analysis,0.4485294117647059,0.029411764705882353,0.3333333333333333
named-entity-recognition,4,"Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer , regardless of whether they are produced from a biLM or MT encoder , and that ELMo representations provide the best over all performance .",analysis,Analysis,0,123,3,3,0,analysis : Analysis,0.4522058823529412,0.04411764705882353,0.5
named-entity-recognition,4,"Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers , consistent with MT encoders .",analysis,Analysis,0,124,4,4,0,analysis : Analysis,0.45588235294117646,0.058823529411764705,0.6666666666666666
named-entity-recognition,4,It also shows that our biLM consistently provides richer representations then CoVe .,analysis,Analysis,0,125,5,5,0,analysis : Analysis,0.45955882352941174,0.07352941176470588,0.8333333333333334
named-entity-recognition,4,"Additionally , we analyze the sensitivity to where ELMo is included in the task model ( Sec. 5.2 ) , training set size ( Sec. 5.4 ) , and visualize the ELMo learned weights across the tasks ( Sec. 5.5 ) .",analysis,Analysis,0,126,6,6,0,analysis : Analysis,0.4632352941176471,0.08823529411764706,1.0
named-entity-recognition,4,Alternate layer weighting schemes,analysis,Alternate layer weighting schemes,0,127,7,1,0,analysis : Alternate layer weighting schemes,0.46691176470588236,0.10294117647058823,0.016129032258064516
named-entity-recognition,4,There are many alternatives to Equation 1 for combining the biLM layers .,analysis,Alternate layer weighting schemes,0,128,8,2,0,analysis : Alternate layer weighting schemes,0.47058823529411764,0.11764705882352941,0.03225806451612903
named-entity-recognition,4,"Previous work on contextual representations used only the last layer , whether it be from a biLM or an MT encoder ( CoVe ; .",analysis,Alternate layer weighting schemes,0,129,9,3,0,analysis : Alternate layer weighting schemes,0.4742647058823529,0.1323529411764706,0.04838709677419355
named-entity-recognition,4,"The choice of the regularization parameter ? is also important , as large values such as ? = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , ? = 0.001 ) allow the layer weights to vary .",analysis,Alternate layer weighting schemes,0,130,10,4,0,analysis : Alternate layer weighting schemes,0.47794117647058826,0.14705882352941177,0.06451612903225806
named-entity-recognition,4,"The choice of the regularization parameter ? is also important , as large values such as ? = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , ? = 0.001 ) allow the layer weights to vary .",analysis,Alternate layer weighting schemes,0,131,11,5,0,analysis : Alternate layer weighting schemes,0.48161764705882354,0.16176470588235295,0.08064516129032258
named-entity-recognition,4,"compares these alternatives for SQuAD , SNLI and SRL .",analysis,Alternate layer weighting schemes,0,132,12,6,0,analysis : Alternate layer weighting schemes,0.4852941176470588,0.17647058823529413,0.0967741935483871
named-entity-recognition,4,"Including representations from all layers improves over all performance over just using the last layer , and including contextual representations from the last layer improves performance over the baseline .",analysis,Alternate layer weighting schemes,0,133,13,7,0,analysis : Alternate layer weighting schemes,0.4889705882352941,0.19117647058823528,0.11290322580645161
named-entity-recognition,4,"For example , in the case of SQuAD , using just the last biLM layer improves development F 1 by 3.9 % over the baseline .",analysis,Alternate layer weighting schemes,0,134,14,8,0,analysis : Alternate layer weighting schemes,0.49264705882352944,0.20588235294117646,0.12903225806451613
named-entity-recognition,4,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to ?= 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( ?= 1 vs. ?= 0.001 ) .",analysis,Alternate layer weighting schemes,0,135,15,9,0,analysis : Alternate layer weighting schemes,0.4963235294117647,0.22058823529411764,0.14516129032258066
named-entity-recognition,4,"small ? is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ? ( not shown ) .",analysis,Alternate layer weighting schemes,0,136,16,10,0,analysis : Alternate layer weighting schemes,0.5,0.23529411764705882,0.16129032258064516
named-entity-recognition,4,"small ? is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ? ( not shown ) .",analysis,Alternate layer weighting schemes,0,137,17,11,0,analysis : Alternate layer weighting schemes,0.5036764705882353,0.25,0.1774193548387097
named-entity-recognition,4,"small ? is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ? ( not shown ) .",analysis,Alternate layer weighting schemes,0,138,18,12,0,analysis : Alternate layer weighting schemes,0.5073529411764706,0.2647058823529412,0.1935483870967742
named-entity-recognition,4,The over all trend is similar with CoVe but with smaller increases over the baseline .,analysis,Alternate layer weighting schemes,0,139,19,13,0,analysis : Alternate layer weighting schemes,0.5110294117647058,0.27941176470588236,0.20967741935483872
named-entity-recognition,4,"For SNLI , averaging all layers with ?= 1 improves development accuracy from 88.2 to 88.7 % over using just the last layer .",analysis,Alternate layer weighting schemes,0,140,20,14,0,analysis : Alternate layer weighting schemes,0.5147058823529411,0.29411764705882354,0.22580645161290322
named-entity-recognition,4,SRL F 1 increased a marginal 0.1 % to 82.2 for the ?= 1 case compared to using the last layer only .,analysis,Alternate layer weighting schemes,0,141,21,15,0,analysis : Alternate layer weighting schemes,0.5183823529411765,0.3088235294117647,0.24193548387096775
named-entity-recognition,4,Where to include ELMo ? All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,analysis,Alternate layer weighting schemes,0,142,22,16,0,analysis : Alternate layer weighting schemes,0.5220588235294118,0.3235294117647059,0.25806451612903225
named-entity-recognition,4,Where to include ELMo ? All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,analysis,Alternate layer weighting schemes,0,143,23,17,0,analysis : Alternate layer weighting schemes,0.5257352941176471,0.3382352941176471,0.27419354838709675
named-entity-recognition,4,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves over all results for some tasks .",analysis,Alternate layer weighting schemes,0,144,24,18,0,analysis : Alternate layer weighting schemes,0.5294117647058824,0.35294117647058826,0.2903225806451613
named-entity-recognition,4,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .",analysis,Alternate layer weighting schemes,0,145,25,19,0,analysis : Alternate layer weighting schemes,0.5330882352941176,0.36764705882352944,0.3064516129032258
named-entity-recognition,4,"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM 's internal representations .",analysis,Alternate layer weighting schemes,0,146,26,20,0,analysis : Alternate layer weighting schemes,0.5367647058823529,0.38235294117647056,0.3225806451612903
named-entity-recognition,4,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement . the task - specific context representations are likely more important than those from the biLM .",analysis,Alternate layer weighting schemes,0,147,27,21,0,analysis : Alternate layer weighting schemes,0.5404411764705882,0.39705882352941174,0.3387096774193548
named-entity-recognition,4,What information is captured by the biLM 's representations ? Since adding,analysis,Alternate layer weighting schemes,0,148,28,22,0,analysis : Alternate layer weighting schemes,0.5441176470588235,0.4117647058823529,0.3548387096774194
named-entity-recognition,4,What information is captured by the biLM 's representations ? Since adding,analysis,Alternate layer weighting schemes,0,149,29,23,0,analysis : Alternate layer weighting schemes,0.5477941176470589,0.4264705882352941,0.3709677419354839
named-entity-recognition,4,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .",analysis,Alternate layer weighting schemes,0,150,30,24,0,analysis : Alternate layer weighting schemes,0.5514705882352942,0.4411764705882353,0.3870967741935484
named-entity-recognition,4,"Intuitively , the biLM must be dis ambiguating the meaning of words using their context .",analysis,Alternate layer weighting schemes,0,151,31,25,0,analysis : Alternate layer weighting schemes,0.5551470588235294,0.45588235294117646,0.4032258064516129
named-entity-recognition,4,"Consider "" play "" , a highly polysemous word .",analysis,Alternate layer weighting schemes,0,152,32,26,0,analysis : Alternate layer weighting schemes,0.5588235294117647,0.47058823529411764,0.41935483870967744
named-entity-recognition,4,"The top of lists nearest neighbors to "" play "" using GloVe vectors .",analysis,Alternate layer weighting schemes,0,153,33,27,0,analysis : Alternate layer weighting schemes,0.5625,0.4852941176470588,0.43548387096774194
named-entity-recognition,4,"They are spread across several parts of speech ( e.g. , "" played "" , "" playing "" as verbs , and "" player "" , "" game "" as nouns ) but concentrated in the sportsrelated senses of "" play "" .",analysis,Alternate layer weighting schemes,0,154,34,28,0,analysis : Alternate layer weighting schemes,0.5661764705882353,0.5,0.45161290322580644
named-entity-recognition,4,"In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM 's context representation of "" play "" in the source sentence .",analysis,Alternate layer weighting schemes,0,155,35,29,0,analysis : Alternate layer weighting schemes,0.5698529411764706,0.5147058823529411,0.46774193548387094
named-entity-recognition,4,"In these cases , the biLM is able to dis ambiguate both the part of speech and word sense in the source sentence .",analysis,Alternate layer weighting schemes,0,156,36,30,0,analysis : Alternate layer weighting schemes,0.5735294117647058,0.5294117647058824,0.4838709677419355
named-entity-recognition,4,These observations can be quantified using an intrinsic evaluation of the contextual representations similar to .,analysis,Alternate layer weighting schemes,0,157,37,31,0,analysis : Alternate layer weighting schemes,0.5772058823529411,0.5441176470588235,0.5
named-entity-recognition,4,"To isolate the information encoded by the biLM , the representations are used to directly make predictions for a fine grained word sense dis ambiguation ( WSD ) task and a POS tagging task .",analysis,Alternate layer weighting schemes,0,158,38,32,0,analysis : Alternate layer weighting schemes,0.5808823529411765,0.5588235294117647,0.5161290322580645
named-entity-recognition,4,"Using this approach , it is also possible to compare to CoVe , and across each of the individual layers .",analysis,Alternate layer weighting schemes,0,159,39,33,0,analysis : Alternate layer weighting schemes,0.5845588235294118,0.5735294117647058,0.532258064516129
named-entity-recognition,4,"Word sense dis ambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach , similar to .",analysis,Alternate layer weighting schemes,0,160,40,34,0,analysis : Alternate layer weighting schemes,0.5882352941176471,0.5882352941176471,0.5483870967741935
named-entity-recognition,4,"To do so , we first use the biLM to compute representations for all words in Sem - Cor 3.0 , our training corpus , and then take the average representation for each sense .",analysis,Alternate layer weighting schemes,0,161,41,35,0,analysis : Alternate layer weighting schemes,0.5919117647058824,0.6029411764705882,0.5645161290322581
named-entity-recognition,4,"At test time , we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set , falling back to the first sense from WordNet for lemmas not observed during training .",analysis,Alternate layer weighting schemes,0,162,42,36,0,analysis : Alternate layer weighting schemes,0.5955882352941176,0.6176470588235294,0.5806451612903226
named-entity-recognition,4,compares WSD results using the evaluation framework from across the same suite of four test sets in .,analysis,Alternate layer weighting schemes,0,163,43,37,0,analysis : Alternate layer weighting schemes,0.5992647058823529,0.6323529411764706,0.5967741935483871
named-entity-recognition,4,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .",analysis,Alternate layer weighting schemes,0,164,44,38,0,analysis : Alternate layer weighting schemes,0.6029411764705882,0.6470588235294118,0.6129032258064516
named-entity-recognition,4,This is competitive with a state - of - the - art WSD - specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse - grained semantic labels and POS tags .,analysis,Alternate layer weighting schemes,0,165,45,39,0,analysis : Alternate layer weighting schemes,0.6066176470588235,0.6617647058823529,0.6290322580645161
named-entity-recognition,4,"The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher over all performance at the second layer compared to the first ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet first sense baseline .",analysis,Alternate layer weighting schemes,0,166,46,40,0,analysis : Alternate layer weighting schemes,0.6102941176470589,0.6764705882352942,0.6451612903225806
named-entity-recognition,4,POS tagging,analysis,Alternate layer weighting schemes,0,167,47,41,0,analysis : Alternate layer weighting schemes,0.6139705882352942,0.6911764705882353,0.6612903225806451
named-entity-recognition,4,"To examine whether the biLM captures basic syntax , we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) .",analysis,Alternate layer weighting schemes,0,168,48,42,0,analysis : Alternate layer weighting schemes,0.6176470588235294,0.7058823529411765,0.6774193548387096
named-entity-recognition,4,"As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .",analysis,Alternate layer weighting schemes,0,169,49,43,0,analysis : Alternate layer weighting schemes,0.6213235294117647,0.7205882352941176,0.6935483870967742
named-entity-recognition,4,"Similar to WSD , the biLM representations are competitive with carefully tuned , task specific biLSTMs .",analysis,Alternate layer weighting schemes,0,170,50,44,0,analysis : Alternate layer weighting schemes,0.625,0.7352941176470589,0.7096774193548387
named-entity-recognition,4,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .",analysis,Alternate layer weighting schemes,0,171,51,45,0,analysis : Alternate layer weighting schemes,0.6286764705882353,0.75,0.7258064516129032
named-entity-recognition,4,"CoVe POS tagging accuracies follow the same pattern as those from the biLM , and just like for WSD , the biLM achieves higher accuracies than the CoVe encoder .",analysis,Alternate layer weighting schemes,0,172,52,46,0,analysis : Alternate layer weighting schemes,0.6323529411764706,0.7647058823529411,0.7419354838709677
named-entity-recognition,4,"Implications for supervised tasks Taken together , these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks .",analysis,Alternate layer weighting schemes,0,173,53,47,0,analysis : Alternate layer weighting schemes,0.6360294117647058,0.7794117647058824,0.7580645161290323
named-entity-recognition,4,"In addition , the biLM 's representations are more transferable to WSD and POS tagging than those in CoVe , helping to illustrate why ELMo outperforms CoVe in downstream tasks .",analysis,Alternate layer weighting schemes,0,174,54,48,0,analysis : Alternate layer weighting schemes,0.6397058823529411,0.7941176470588235,0.7741935483870968
named-entity-recognition,4,Sample efficiency,analysis,Alternate layer weighting schemes,0,175,55,49,0,analysis : Alternate layer weighting schemes,0.6433823529411765,0.8088235294117647,0.7903225806451613
named-entity-recognition,4,"Adding ELMo to a model increases the sample efficiency considerably , both in terms of number of parameter updates to reach state - of - the - art performance and the over all training set size .",analysis,Alternate layer weighting schemes,0,176,56,50,0,analysis : Alternate layer weighting schemes,0.6470588235294118,0.8235294117647058,0.8064516129032258
named-entity-recognition,4,"For example , the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo .",analysis,Alternate layer weighting schemes,0,177,57,51,0,analysis : Alternate layer weighting schemes,0.6507352941176471,0.8382352941176471,0.8225806451612904
named-entity-recognition,4,"After adding ELMo , the model exceeds the baseline maximum at epoch 10 , a 98 % relative decrease in the number of updates needed to reach In addition , ELMo - enhanced models use smaller training sets more efficiently than models without ELMo.",analysis,Alternate layer weighting schemes,0,178,58,52,0,analysis : Alternate layer weighting schemes,0.6544117647058824,0.8529411764705882,0.8387096774193549
named-entity-recognition,4,compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1 % to 100 % .,analysis,Alternate layer weighting schemes,0,179,59,53,0,analysis : Alternate layer weighting schemes,0.6580882352941176,0.8676470588235294,0.8548387096774194
named-entity-recognition,4,Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance .,analysis,Alternate layer weighting schemes,0,180,60,54,0,analysis : Alternate layer weighting schemes,0.6617647058823529,0.8823529411764706,0.8709677419354839
named-entity-recognition,4,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .",analysis,Alternate layer weighting schemes,0,181,61,55,0,analysis : Alternate layer weighting schemes,0.6654411764705882,0.8970588235294118,0.8870967741935484
named-entity-recognition,4,visualizes the softmax - normalized learned layer weights .,analysis,Alternate layer weighting schemes,0,182,62,56,0,analysis : Alternate layer weighting schemes,0.6691176470588235,0.9117647058823529,0.9032258064516129
named-entity-recognition,4,"At the input layer , the task model favors the first biLSTM layer .",analysis,Alternate layer weighting schemes,0,183,63,57,0,analysis : Alternate layer weighting schemes,0.6727941176470589,0.9264705882352942,0.9193548387096774
named-entity-recognition,4,"For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks .",analysis,Alternate layer weighting schemes,0,184,64,58,0,analysis : Alternate layer weighting schemes,0.6764705882352942,0.9411764705882353,0.9354838709677419
named-entity-recognition,4,"The output layer weights are relatively balanced , with a slight preference for the lower layers .",analysis,Alternate layer weighting schemes,0,185,65,59,0,analysis : Alternate layer weighting schemes,0.6801470588235294,0.9558823529411765,0.9516129032258065
named-entity-recognition,4,Visualization of learned weights,analysis,Alternate layer weighting schemes,0,186,66,60,0,analysis : Alternate layer weighting schemes,0.6838235294117647,0.9705882352941176,0.967741935483871
named-entity-recognition,4,"We have introduced a general approach for learning high - quality deep context - dependent representations from biLMs , and shown large improvements when applying ELMo to a broad range of NLP tasks .",analysis,Alternate layer weighting schemes,0,187,67,61,0,analysis : Alternate layer weighting schemes,0.6875,0.9852941176470589,0.9838709677419355
named-entity-recognition,4,"Through ablations and other controlled experiments , we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin - context , and that using all layers improves over all task performance .",analysis,Alternate layer weighting schemes,0,188,68,62,0,analysis : Alternate layer weighting schemes,0.6911764705882353,1.0,1.0
named-entity-recognition,4,Supplemental Material to accompany Deep contextualized word representations,system description,system description,0,189,1,1,0,system description : system description,0.6948529411764706,0.030303030303030304,0.030303030303030304
named-entity-recognition,4,"This supplement contains details of the model architectures , training routines and hyper - parameter choices for the state - of - the - art models in Section 4 .",system description,system description,0,190,2,2,0,system description : system description,0.6985294117647058,0.06060606060606061,0.06060606060606061
named-entity-recognition,4,All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs .,system description,system description,0,191,3,3,0,system description : system description,0.7022058823529411,0.09090909090909091,0.09090909090909091
named-entity-recognition,4,A.1,system description,system description,0,192,4,4,0,system description : system description,0.7058823529411765,0.12121212121212122,0.12121212121212122
named-entity-recognition,4,Fine tuning biLM,system description,system description,0,193,5,5,0,system description : system description,0.7095588235294118,0.15151515151515152,0.15151515151515152
named-entity-recognition,4,"As noted in Sec. 3.4 , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .",system description,system description,0,194,6,6,0,system description : system description,0.7132352941176471,0.18181818181818182,0.18181818181818182
named-entity-recognition,4,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",system description,system description,0,195,7,7,0,system description : system description,0.7169117647058824,0.21212121212121213,0.21212121212121213
named-entity-recognition,4,"Once fine tuned , the biLM weights were fixed during task training .",system description,system description,0,196,8,8,0,system description : system description,0.7205882352941176,0.24242424242424243,0.24242424242424243
named-entity-recognition,4,lists the development set perplexities for the considered tasks .,system description,system description,0,197,9,9,0,system description : system description,0.7242647058823529,0.2727272727272727,0.2727272727272727
named-entity-recognition,4,"In every case except CoNLL 2012 , fine tuning results in a large improvement in perplexity , e.g. , from 72.1 to 16.8 for SNLI .",system description,system description,0,198,10,10,0,system description : system description,0.7279411764705882,0.30303030303030304,0.30303030303030304
named-entity-recognition,4,The impact of fine tuning on supervised performance is task dependent .,system description,system description,0,199,11,11,0,system description : system description,0.7316176470588235,0.3333333333333333,0.3333333333333333
named-entity-recognition,4,"In the case of SNLI , fine tuning the biLM increased development accuracy 0.6 % from 88.9 % to 89.5 % for our single best model .",system description,system description,0,200,12,12,0,system description : system description,0.7352941176470589,0.36363636363636365,0.36363636363636365
named-entity-recognition,4,"However , for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used .",system description,system description,0,201,13,13,0,system description : system description,0.7389705882352942,0.3939393939393939,0.3939393939393939
named-entity-recognition,4,Importance of ? in Eqn .,system description,system description,0,202,14,14,0,system description : system description,0.7426470588235294,0.42424242424242425,0.42424242424242425
named-entity-recognition,4,"The ? parameter in Eqn. ( 1 ) was of practical importance to aid optimization , due to the different distributions between the biLM internal representations and the task specific representations .",system description,system description,0,203,15,15,0,system description : system description,0.7463235294117647,0.45454545454545453,0.45454545454545453
named-entity-recognition,4,It is especially important in the last - only casein Sec. 5.1 .,system description,system description,0,204,16,16,0,system description : system description,0.75,0.48484848484848486,0.48484848484848486
named-entity-recognition,4,"Without this parameter , the last - only case performed poorly ( well below the baseline ) for SNLI and training failed completely for SRL .",system description,system description,0,205,17,17,0,system description : system description,0.7536764705882353,0.5151515151515151,0.5151515151515151
named-entity-recognition,4,Textual Entailment,system description,system description,0,206,18,18,0,system description : system description,0.7573529411764706,0.5454545454545454,0.5454545454545454
named-entity-recognition,4,Our baseline SNLI model is the ESIM sequence model from .,system description,system description,0,207,19,19,0,system description : system description,0.7610294117647058,0.5757575757575758,0.5757575757575758
named-entity-recognition,4,"Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training .",system description,system description,0,208,20,20,0,system description : system description,0.7647058823529411,0.6060606060606061,0.6060606060606061
named-entity-recognition,4,"For regularization , we added 50 % variational dropout to the input of each LSTM layer and 50 % dropout at the input to the final two fully connected layers .",system description,system description,0,209,21,21,0,system description : system description,0.7683823529411765,0.6363636363636364,0.6363636363636364
named-entity-recognition,4,All feed forward layers use ReLU activations .,system description,system description,0,210,22,22,0,system description : system description,0.7720588235294118,0.6666666666666666,0.6666666666666666
named-entity-recognition,4,"Parameters were optimized using Adam ( Kingma and with gradient norms clipped at 5.0 and initial learning rate 0.0004 , decreasing by half each time accuracy on the development set did not increase in subsequent epochs .",system description,system description,0,211,23,23,0,system description : system description,0.7757352941176471,0.696969696969697,0.696969696969697
named-entity-recognition,4,The batch size was 32 .,system description,system description,0,212,24,24,0,system description : system description,0.7794117647058824,0.7272727272727273,0.7272727272727273
named-entity-recognition,4,"The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM , using ( 1 ) with layer normalization and ? = 0.001 .",system description,system description,0,213,25,25,0,system description : system description,0.7830882352941176,0.7575757575757576,0.7575757575757576
named-entity-recognition,4,"Due to the increased number of parameters in the ELMo model , we added 2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50 % dropout after the attention layer .",system description,system description,0,214,26,26,0,system description : system description,0.7867647058823529,0.7878787878787878,0.7878787878787878
named-entity-recognition,4,compares test set accuracy of our system to previously published systems .,system description,system description,0,215,27,27,0,system description : system description,0.7904411764705882,0.8181818181818182,0.8181818181818182
named-entity-recognition,4,"Overall , adding ELMo to the ESIM model improved accuracy by 0.7 % establishing a new single model state - of - the - art of 88.7 % , and a five member ensemble pushes the over all accuracy to 89.3 % .",system description,system description,0,216,28,28,0,system description : system description,0.7941176470588235,0.8484848484848485,0.8484848484848485
named-entity-recognition,4,Question Answering,system description,system description,0,217,29,29,0,system description : system description,0.7977941176470589,0.8787878787878788,0.8787878787878788
named-entity-recognition,4,Our QA model is a simplified version of the model from .,system description,system description,0,218,30,30,0,system description : system description,0.8014705882352942,0.9090909090909091,0.9090909090909091
named-entity-recognition,4,It embeds tokens by concatenating each token 's case - sensitive 300 dimensional Glo Ve word vector with a character - derived embedding produced using a convolutional neural network followed by max - pooling on learned character embeddings .,system description,system description,0,219,31,31,0,system description : system description,0.8051470588235294,0.9393939393939394,0.9393939393939394
named-entity-recognition,4,"The token embeddings are passed through a shared bi-directional GRU , and then the bi-directional attention mechanism from .",system description,system description,0,220,32,32,0,system description : system description,0.8088235294117647,0.9696969696969697,0.9696969696969697
named-entity-recognition,4,The augmented con-,system description,system description,0,221,33,33,0,system description : system description,0.8125,1.0,1.0
named-entity-recognition,4,Model,model,model,0,222,1,1,0,model : model,0.8161764705882353,0.0196078431372549,0.0196078431372549
named-entity-recognition,4,Acc.,model,model,0,223,2,2,0,model : model,0.8198529411764706,0.0392156862745098,0.0392156862745098
named-entity-recognition,4,"Feature based 78.2 DIIN 88.0 BCN+Char+CoVe 88.0 ESIM + TreeLSTM 88.6 ESIM+ELMo 88.7 0.17 DIIN ensemble 88.9 ESIM + ELMo ensemble 89.3 text vectors are then passed through a linear layer with ReLU activations , a residual self - attention layer that uses a GRU followed by the same attention mechanism applied context - to - context , and another linear layer with ReLU activations .",model,model,0,224,3,3,0,model : model,0.8235294117647058,0.058823529411764705,0.058823529411764705
named-entity-recognition,4,"Finally , the results are fed through linear layers to predict the start and end token of the answer .",model,model,0,225,4,4,0,model : model,0.8272058823529411,0.0784313725490196,0.0784313725490196
named-entity-recognition,4,Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2 .,model,model,0,226,5,5,0,model : model,0.8308823529411765,0.09803921568627451,0.09803921568627451
named-entity-recognition,4,"dimensionality of 90 is used for the GRUs , and 180 for the linear layers .",model,model,0,227,6,6,0,model : model,0.8345588235294118,0.11764705882352941,0.11764705882352941
named-entity-recognition,4,We optimize the model using Adadelta with a batch size of 45 .,model,model,0,228,7,7,0,model : model,0.8382352941176471,0.13725490196078433,0.13725490196078433
named-entity-recognition,4,At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17 .,model,model,0,229,8,8,0,model : model,0.8419117647058824,0.1568627450980392,0.1568627450980392
named-entity-recognition,4,We do not update the word vectors during training .,model,model,0,230,9,9,0,model : model,0.8455882352941176,0.17647058823529413,0.17647058823529413
named-entity-recognition,4,Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized (? = 0 ) .,model,model,0,231,10,10,0,model : model,0.8492647058823529,0.19607843137254902,0.19607843137254902
named-entity-recognition,4,"compares test set results from the SQuAD leaderboard as of November 17 , 2017 when we submitted our system .",model,model,0,232,11,11,0,model : model,0.8529411764705882,0.21568627450980393,0.21568627450980393
named-entity-recognition,4,"Overall , our submission had the highest single model and ensemble results , improving the previous single model result ( SAN ) by 1.4 % F 1 and our baseline by 4.2 % .",model,model,0,233,12,12,0,model : model,0.8566176470588235,0.23529411764705882,0.23529411764705882
named-entity-recognition,4,"11 member ensemble pushes F 1 to 87.4 % , 1.0 % increase over the previous ensemble best .",model,model,0,234,13,13,0,model : model,0.8602941176470589,0.2549019607843137,0.2549019607843137
named-entity-recognition,4,Semantic Role Labeling,model,model,0,235,14,14,0,model : model,0.8639705882352942,0.27450980392156865,0.27450980392156865
named-entity-recognition,4,Our baseline SRL model is an exact reimplementation of .,model,model,0,236,15,15,0,model : model,0.8676470588235294,0.29411764705882354,0.29411764705882354
named-entity-recognition,4,"Words are represented using a concatenation of 100 dimensional vector representations , initialized using GloVe and a binary , per-word predicate feature , represented using an 100 dimensional em-bedding .",model,model,0,237,16,16,0,model : model,0.8713235294117647,0.3137254901960784,0.3137254901960784
named-entity-recognition,4,"This 200 dimensional token representation is then passed through an 8 layer "" interleaved "" biLSTM with a 300 dimensional hidden size , in which the directions of the LSTM layers alternate per layer .",model,model,0,238,17,17,0,model : model,0.875,0.3333333333333333,0.3333333333333333
named-entity-recognition,4,This deep LSTM uses Highway connections between layers and variational recurrent dropout .,model,model,0,239,18,18,0,model : model,0.8786764705882353,0.35294117647058826,0.35294117647058826
named-entity-recognition,4,This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags .,model,model,0,240,19,19,0,model : model,0.8823529411764706,0.37254901960784315,0.37254901960784315
named-entity-recognition,4,Labels consist of semantic roles from PropBank augmented with a BIO labeling scheme to represent argument spans .,model,model,0,241,20,20,0,model : model,0.8860294117647058,0.39215686274509803,0.39215686274509803
named-entity-recognition,4,"During training , we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ? = 0.95 .",model,model,0,242,21,21,0,model : model,0.8897058823529411,0.4117647058823529,0.4117647058823529
named-entity-recognition,4,"At test time , we perform Viterbi decoding to enforce valid spans using BIO constraints .",model,model,0,243,22,22,0,model : model,0.8933823529411765,0.43137254901960786,0.43137254901960786
named-entity-recognition,4,Variational dropout of 10 % is added to all LSTM hidden layers .,model,model,0,244,23,23,0,model : model,0.8970588235294118,0.45098039215686275,0.45098039215686275
named-entity-recognition,4,Gradients are clipped if their value exceeds 1.0 .,model,model,0,245,24,24,0,model : model,0.9007352941176471,0.47058823529411764,0.47058823529411764
named-entity-recognition,4,"Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs , whichever is sooner .",model,model,0,246,25,25,0,model : model,0.9044117647058824,0.49019607843137253,0.49019607843137253
named-entity-recognition,4,The pretrained Glo Ve vectors are fine - tuned during training .,model,model,0,247,26,26,0,model : model,0.9080882352941176,0.5098039215686274,0.5098039215686274
named-entity-recognition,4,The final dense layer and all cells of all LSTMs are initialized to be orthogonal .,model,model,0,248,27,27,0,model : model,0.9117647058823529,0.5294117647058824,0.5294117647058824
named-entity-recognition,4,"The forget gate bias is initialized to 1 for all LSTMs , with all other gates initialized to 0 , as per . perparameters exactly following the original implementation .",model,model,0,249,28,28,0,model : model,0.9154411764705882,0.5490196078431373,0.5490196078431373
named-entity-recognition,4,The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using ( 1 ) without any regularization ( ? = 0 ) or layer normalization .,model,model,0,250,29,29,0,model : model,0.9191176470588235,0.5686274509803921,0.5686274509803921
named-entity-recognition,4,50 % dropout was added to the ELMo representations .,model,model,0,251,30,30,0,model : model,0.9227941176470589,0.5882352941176471,0.5882352941176471
named-entity-recognition,4,compares our results with previously published results .,model,model,0,252,31,31,0,model : model,0.9264705882352942,0.6078431372549019,0.6078431372549019
named-entity-recognition,4,"Overall , we improve the single model state - of - the - art by 3.2 % average F 1 , and our single model result improves the previous ensemble best by 1.6 % F 1 .",model,model,0,253,32,32,0,model : model,0.9301470588235294,0.6274509803921569,0.6274509803921569
named-entity-recognition,4,Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F 1 by approximately 0.7 % ( not shown ) .,model,model,0,254,33,33,0,model : model,0.9338235294117647,0.6470588235294118,0.6470588235294118
named-entity-recognition,4,Named Entity Recognition,model,model,0,255,34,34,0,model : model,0.9375,0.6666666666666666,0.6666666666666666
named-entity-recognition,4,Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors with a CNN character based representation .,model,model,0,256,35,35,0,model : model,0.9411764705882353,0.6862745098039216,0.6862745098039216
named-entity-recognition,4,"The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters , a ReLU activation and by max pooling .",model,model,0,257,36,36,0,model : model,0.9448529411764706,0.7058823529411765,0.7058823529411765
named-entity-recognition,4,"The token representation is passed through two biLSTM layers , the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer .",model,model,0,258,37,37,0,model : model,0.9485294117647058,0.7254901960784313,0.7254901960784313
named-entity-recognition,4,"During training , we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid .",model,model,0,259,38,38,0,model : model,0.9522058823529411,0.7450980392156863,0.7450980392156863
named-entity-recognition,4,Variational dropout is added to the input of both biLSTM layers .,model,model,0,260,39,39,0,model : model,0.9558823529411765,0.7647058823529411,0.7647058823529411
named-entity-recognition,4,During training the gradients are rescaled if their 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001 .,model,model,0,261,40,40,0,model : model,0.9595588235294118,0.7843137254901961,0.7843137254901961
named-entity-recognition,4,The pre-trained Senna embeddings are fine tuned during training .,model,model,0,262,41,41,0,model : model,0.9632352941176471,0.803921568627451,0.803921568627451
named-entity-recognition,4,We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds .,model,model,0,263,42,42,0,model : model,0.9669117647058824,0.8235294117647058,0.8235294117647058
named-entity-recognition,4,ELMo was added to the input of the lowest layer task biLSTM .,model,model,0,264,43,43,0,model : model,0.9705882352941176,0.8431372549019608,0.8431372549019608
named-entity-recognition,4,"As the CoNLL 2003 NER data set is relatively small , we found the best performance by constraining the trainable layer weights to be effectively constant by setting ? = 0.1 with ( 1 ) .",model,model,0,265,44,44,0,model : model,0.9742647058823529,0.8627450980392157,0.8627450980392157
named-entity-recognition,4,from all layers of the biLM provides a modest improvement .,model,model,0,266,45,45,0,model : model,0.9779411764705882,0.8823529411764706,0.8823529411764706
named-entity-recognition,4,Sentiment classification,model,model,0,267,46,46,0,model : model,0.9816176470588235,0.9019607843137255,0.9019607843137255
named-entity-recognition,4,"We use almost the same biattention classification network architecture described in , with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout .",model,model,0,268,47,47,0,model : model,0.9852941176470589,0.9215686274509803,0.9215686274509803
named-entity-recognition,4,"BCN model with a batch - normalized maxout network reached significantly lower validation accuracies in our experiments , although there maybe discrepancies between our implementation and that of .",model,model,0,269,48,48,0,model : model,0.9889705882352942,0.9411764705882353,0.9411764705882353
named-entity-recognition,4,"To match the CoVe training setup , we only train on phrases that contain four or more tokens .",model,model,0,270,49,49,0,model : model,0.9926470588235294,0.9607843137254902,0.9607843137254902
named-entity-recognition,4,We use 300 -d hidden states for the biLSTM and optimize the model parameters with Adam ( Kingma and using a learning rate of 0.0001 .,model,model,0,271,50,50,0,model : model,0.9963235294117647,0.9803921568627451,0.9803921568627451
named-entity-recognition,4,"The trainable biLM layer weights are regularized by ? = 0.001 , and we add ELMo to both the input and output of the biLSTM ; the output ELMo vectors are computed with a second biLSTM and concatenated to the input .",model,model,0,272,51,51,0,model : model,1.0,1.0,1.0
named-entity-recognition,5,Sentence - State LSTM for Text Representation,title,title,1,2,1,1,0,title : title,0.009569377990430622,1.0,1.0
named-entity-recognition,5,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.014354066985645933,0.14285714285714285,0.14285714285714285
named-entity-recognition,5,Bi-directional,abstract,abstract,0,4,2,2,0,abstract : abstract,0.019138755980861243,0.2857142857142857,0.2857142857142857
named-entity-recognition,5,LSTMs are a powerful tool for text representation .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.023923444976076555,0.42857142857142855,0.42857142857142855
named-entity-recognition,5,"On the other hand , they have been shown to suffer various limitations due to their sequential nature .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.028708133971291867,0.5714285714285714,0.5714285714285714
named-entity-recognition,5,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.03349282296650718,0.7142857142857143,0.7142857142857143
named-entity-recognition,5,"Recurrent steps are used to perform local and global information exchange between words simultaneously , rather than incremental reading of a sequence of words .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03827751196172249,0.8571428571428571,0.8571428571428571
named-entity-recognition,5,"Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power , giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.0430622009569378,1.0,1.0
named-entity-recognition,5,Introduction,introduction,introduction,0,10,1,1,0,introduction : introduction,0.04784688995215311,0.045454545454545456,0.045454545454545456
named-entity-recognition,5,Neural models have become the dominant approach in the NLP literature .,introduction,introduction,0,11,2,2,0,introduction : introduction,0.05263157894736842,0.09090909090909091,0.09090909090909091
named-entity-recognition,5,"Compared to handcrafted indicator features , neural sentence representations are less sparse , and more flexible in encoding intricate syntactic and semantic information .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.05741626794258373,0.13636363636363635,0.13636363636363635
named-entity-recognition,5,"Among various neural networks for encoding sentences , bi-directional LSTMs ( BiLSTM ) have been a dominant method , giving state - of - the - art results in language modelling , machine translation , syntactic parsing and question answering .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.06220095693779904,0.18181818181818182,0.18181818181818182
named-entity-recognition,5,"Despite their success , BiLSTMs have been shown to suffer several limitations .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.06698564593301436,0.22727272727272727,0.22727272727272727
named-entity-recognition,5,"For example , their inherently sequential nature endows computation non-parallel within the same sentence , which can lead to a computational bottleneck , hindering their use in the in - dustry .",introduction,introduction,0,15,6,6,0,introduction : introduction,0.07177033492822966,0.2727272727272727,0.2727272727272727
named-entity-recognition,5,"In addition , local ngrams , which have been shown a highly useful source of contextual information for NLP , are not explicitly modelled .",introduction,introduction,0,16,7,7,0,introduction : introduction,0.07655502392344497,0.3181818181818182,0.3181818181818182
named-entity-recognition,5,"Finally , sequential information flow leads to relatively weaker power in capturing longrange dependencies , which results in lower performance in encoding longer sentences .",introduction,introduction,0,17,8,8,0,introduction : introduction,0.08133971291866028,0.36363636363636365,0.36363636363636365
named-entity-recognition,5,We investigate an alternative recurrent neural network structure for addressing these issues .,introduction,introduction,1,18,9,9,0,introduction : introduction,0.0861244019138756,0.4090909090909091,0.4090909090909091
named-entity-recognition,5,"As shown in , the main idea is to model the hidden states of all words simultaneously at each recurrent step , rather than one word at a time .",introduction,introduction,1,19,10,10,0,introduction : introduction,0.09090909090909091,0.45454545454545453,0.45454545454545453
named-entity-recognition,5,"In particular , we view the whole sentence as a single state , which consists of sub-states for individual words and an over all sentence - level state .",introduction,introduction,1,20,11,11,0,introduction : introduction,0.09569377990430622,0.5,0.5
named-entity-recognition,5,"To capture local and non-local contexts , states are updated recurrently by exchanging information between each other .",introduction,introduction,1,21,12,12,0,introduction : introduction,0.10047846889952153,0.5454545454545454,0.5454545454545454
named-entity-recognition,5,"Consequently , we refer to our model as sentence - state LSTM , or S - LSTM in short .",introduction,introduction,0,22,13,13,0,introduction : introduction,0.10526315789473684,0.5909090909090909,0.5909090909090909
named-entity-recognition,5,"Empirically , S - LSTM can give effective sentence encoding after 3 - 6 recurrent steps .",introduction,introduction,0,23,14,14,0,introduction : introduction,0.11004784688995216,0.6363636363636364,0.6363636363636364
named-entity-recognition,5,"In contrast , the number of recurrent steps necessary for BiLSTM scales with the size of the sentence .",introduction,introduction,0,24,15,15,0,introduction : introduction,0.11483253588516747,0.6818181818181818,0.6818181818181818
named-entity-recognition,5,"At each recurrent step , information exchange is conducted between consecutive words in the sentence , and between the sentence - level state and each word .",introduction,introduction,1,25,16,16,0,introduction : introduction,0.11961722488038277,0.7272727272727273,0.7272727272727273
named-entity-recognition,5,"In particular , each word receives information from its predecessor and successor simultaneously .",introduction,introduction,1,26,17,17,0,introduction : introduction,0.12440191387559808,0.7727272727272727,0.7727272727272727
named-entity-recognition,5,"From an initial state without information exchange , each word - level state can obtain 3 - gram , 5 - gram and 7 - gram information after 1 , 2 and 3 recurrent steps , respectively .",introduction,introduction,0,27,18,18,0,introduction : introduction,0.1291866028708134,0.8181818181818182,0.8181818181818182
named-entity-recognition,5,"Being connected with every word , the sentence - level state vector serves to exchange non-local information with each word .",introduction,introduction,0,28,19,19,0,introduction : introduction,0.1339712918660287,0.8636363636363636,0.8636363636363636
named-entity-recognition,5,"In addition , it can also be used as a global sentence - level representation for classification tasks .",introduction,introduction,0,29,20,20,0,introduction : introduction,0.13875598086124402,0.9090909090909091,0.9090909090909091
named-entity-recognition,5,"Results on both classification and sequence labelling show that S - LSTM gives better accuracies compared to BiLSTM using the same number of parameters , while being faster .",introduction,introduction,0,30,21,21,0,introduction : introduction,0.14354066985645933,0.9545454545454546,0.9545454545454546
named-entity-recognition,5,"We release our code and models at https://github.com/ leuchine /S - LSTM , which include all baselines and the final model .",introduction,introduction,1,31,22,22,0,introduction : introduction,0.14832535885167464,1.0,1.0
named-entity-recognition,5,Related Work,related work,Related Work,0,32,1,1,0,related work : Related Work,0.15311004784688995,0.045454545454545456,0.045454545454545456
named-entity-recognition,5,LSTM showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models .,related work,Related Work,0,33,2,2,0,related work : Related Work,0.15789473684210525,0.09090909090909091,0.09090909090909091
named-entity-recognition,5,"LSTM encoders have since been explored for other tasks , including syntactic parsing , text classification and machine reading .",related work,Related Work,0,34,3,3,0,related work : Related Work,0.16267942583732056,0.13636363636363635,0.13636363636363635
named-entity-recognition,5,Bidirectional extensions have become a standard configuration for achieving state - of - the - art accuracies among various tasks .,related work,Related Work,0,35,4,4,0,related work : Related Work,0.1674641148325359,0.18181818181818182,0.18181818181818182
named-entity-recognition,5,"S- LSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words , but different in the design of state transition .",related work,Related Work,0,36,5,5,0,related work : Related Work,0.1722488038277512,0.22727272727272727,0.22727272727272727
named-entity-recognition,5,"CNNs ) also allow better parallelis ation compared to LSTMs for sentence encoding , thanks to parallelism among convolution filters .",related work,Related Work,0,37,6,6,0,related work : Related Work,0.17703349282296652,0.2727272727272727,0.2727272727272727
named-entity-recognition,5,"On the other hand , convolution features embody only fix - sized local ngram information , whereas sentence - level feature aggregation via pooling can lead to loss of information .",related work,Related Work,0,38,7,7,0,related work : Related Work,0.18181818181818182,0.3181818181818182,0.3181818181818182
named-entity-recognition,5,"In contrast , S - LSTM uses a global sentence - level node to assemble and back - distribute local information in the recurrent state transition process , suffering less information loss compared to pooling .",related work,Related Work,0,39,8,8,0,related work : Related Work,0.18660287081339713,0.36363636363636365,0.36363636363636365
named-entity-recognition,5,"Attention has recently been explored as a standalone method for sentence encoding , giving competitive results compared to Bi - LSTM encoders for neural machine translation .",related work,Related Work,0,40,9,9,0,related work : Related Work,0.19138755980861244,0.4090909090909091,0.4090909090909091
named-entity-recognition,5,"The attention mechanism allows parallelis ation , and can play a similar role to the sentence - level state in S - LSTMs , which uses neural gates to integrate word - level information compared to hierarchical attention .",related work,Related Work,0,41,10,10,0,related work : Related Work,0.19617224880382775,0.45454545454545453,0.45454545454545453
named-entity-recognition,5,- LSTM further allows local communication between neighbouring words .,related work,Related Work,0,42,11,11,0,related work : Related Work,0.20095693779904306,0.5,0.5
named-entity-recognition,5,Hierarchical stacking of CNN layers allows better interaction between non-local components in a sentence via incremental levels of abstraction .,related work,Related Work,0,43,12,12,0,related work : Related Work,0.20574162679425836,0.5454545454545454,0.5454545454545454
named-entity-recognition,5,"- LSTM is similar to hierarchical attention and stacked CNN in this respect , incrementally refining sentence representations .",related work,Related Work,0,44,13,13,0,related work : Related Work,0.21052631578947367,0.5909090909090909,0.5909090909090909
named-entity-recognition,5,"However , S - LSTM models hierarchical encoding of sentence structure as a recurrent state transition process .",related work,Related Work,0,45,14,14,0,related work : Related Work,0.215311004784689,0.6363636363636364,0.6363636363636364
named-entity-recognition,5,"In nature , our work belongs to the family of LSTM sentence representations .",related work,Related Work,0,46,15,15,0,related work : Related Work,0.22009569377990432,0.6818181818181818,0.6818181818181818
named-entity-recognition,5,- LSTM is inspired by message passing over graphs ) .,related work,Related Work,0,47,16,16,0,related work : Related Work,0.22488038277511962,0.7272727272727273,0.7272727272727273
named-entity-recognition,5,Graph - structure neural models have been used for computer program verification and image object detection .,related work,Related Work,0,48,17,17,0,related work : Related Work,0.22966507177033493,0.7727272727272727,0.7727272727272727
named-entity-recognition,5,The closest previous work in NLP includes the use of convolutional neural networks and DAG LSTMs for modelling syntactic structures .,related work,Related Work,0,49,18,18,0,related work : Related Work,0.23444976076555024,0.8181818181818182,0.8181818181818182
named-entity-recognition,5,"Compared to our work , their motivations and network structures are highly different .",related work,Related Work,0,50,19,19,0,related work : Related Work,0.23923444976076555,0.8636363636363636,0.8636363636363636
named-entity-recognition,5,"In particular , the DAG LSTM of is a natural extension of tree LSTM , and is sequential rather than parallel in nature .",related work,Related Work,0,51,20,20,0,related work : Related Work,0.24401913875598086,0.9090909090909091,0.9090909090909091
named-entity-recognition,5,"To our knowledge , we are the first to investigate a graph RNN for encoding sentences , proposing parallel graph states for integrating word - level and sentence - level information .",related work,Related Work,0,52,21,21,0,related work : Related Work,0.24880382775119617,0.9545454545454546,0.9545454545454546
named-entity-recognition,5,"In this perspective , our contribution is similar to that of and in introducing a neural representation to the NLP literature .",related work,Related Work,0,53,22,22,0,related work : Related Work,0.2535885167464115,1.0,1.0
named-entity-recognition,5,Model,model,Model,0,54,1,1,0,model : Model,0.2583732057416268,0.05555555555555555,0.05555555555555555
named-entity-recognition,5,Time 4 ) generally results in faster plateauing .,model,Model,0,55,2,2,0,model : Model,0.2631578947368421,0.1111111111111111,0.1111111111111111
named-entity-recognition,5,"This can be be explained by the intuition that information exchange between distant nodes can be achieved using more recurrent steps under a smaller window size , as can be achieved using fewer steps under a larger window size .",model,Model,0,56,3,3,0,model : Model,0.2679425837320574,0.16666666666666666,0.16666666666666666
named-entity-recognition,5,"Considering efficiency , we choose a window size of 1 for the remaining experiments , setting the number of recurrent steps to 9 according to .",model,Model,0,57,4,4,0,model : Model,0.2727272727272727,0.2222222222222222,0.2222222222222222
named-entity-recognition,5,"S- LSTM vs BiLSTM : As shown in , BiLSTM gives significantly better accuracies compared to uni-directional LSTM 2 , with the training time per epoch growing from 67 seconds to 106 seconds .",model,Model,0,58,5,5,0,model : Model,0.27751196172248804,0.2777777777777778,0.2777777777777778
named-entity-recognition,5,"Stacking 2 layers of BiLSTM gives further improvements to development results , with a larger time of 207 seconds .",model,Model,0,59,6,6,0,model : Model,0.2822966507177033,0.3333333333333333,0.3333333333333333
named-entity-recognition,5,layers of stacked BiLSTM does not further improve the results .,model,Model,0,60,7,7,0,model : Model,0.28708133971291866,0.3888888888888889,0.3888888888888889
named-entity-recognition,5,"In contrast , S - LSTM gives a development result of 82 . 64 % , which is significantly better compared to 2 - layer stacked BiLSTM , with a smaller number of model parameters and a shorter time of 65 seconds .",model,Model,0,61,8,8,0,model : Model,0.291866028708134,0.4444444444444444,0.4444444444444444
named-entity-recognition,5,"We additionally make comparisons with stacked CNNs and hierarchical attention , shown in ( the CNN and Transformer rows ) , where N indicates the number of attention layers .",model,Model,0,62,9,9,0,model : Model,0.2966507177033493,0.5,0.5
named-entity-recognition,5,"CNN is the most efficient among all models compared , with the smallest model size .",model,Model,0,63,10,10,0,model : Model,0.3014354066985646,0.5555555555555556,0.5555555555555556
named-entity-recognition,5,"On the other hand , a 3 - layer stacked CNN gives an accuracy of 81 . 46 % , which is also the lowest compared with BiLSTM , hierarchical attention and S - LSTM .",model,Model,0,64,11,11,0,model : Model,0.3062200956937799,0.6111111111111112,0.6111111111111112
named-entity-recognition,5,The best performance of hierarchical attention is between single - layer and two - layer BiLSTMs in terms of both accuracy and efficiency .,model,Model,0,65,12,12,0,model : Model,0.31100478468899523,0.6666666666666666,0.6666666666666666
named-entity-recognition,5,- LSTM gives significantly better accuracies compared with both CNN and hierarchical attention .,model,Model,0,66,13,13,0,model : Model,0.3157894736842105,0.7222222222222222,0.7222222222222222
named-entity-recognition,5,Influence of external attention mechanism .,model,Model,0,67,14,14,0,model : Model,0.32057416267942584,0.7777777777777778,0.7777777777777778
named-entity-recognition,5,additionally shows the results of BiLSTM and S - LSTM when external attention is used as described in Section 3.3 .,model,Model,0,68,15,15,0,model : Model,0.3253588516746411,0.8333333333333334,0.8333333333333334
named-entity-recognition,5,"Attention leads to improved accuracies for both BiLSTM and S - LSTM in classification , with S - LSTM still outperforming BiLSTM significantly .",model,Model,0,69,16,16,0,model : Model,0.33014354066985646,0.8888888888888888,0.8888888888888888
named-entity-recognition,5,"The result suggests that external techniques such as attention can play orthogonal roles compared with internal recurrent structures , therefore benefiting both BiLSTMs and S - LSTMs .",model,Model,0,70,17,17,0,model : Model,0.3349282296650718,0.9444444444444444,0.9444444444444444
named-entity-recognition,5,Similar observations are found using external CRF layers for sequence labelling .,model,Model,0,71,18,18,0,model : Model,0.3397129186602871,1.0,1.0
named-entity-recognition,5,Baseline BiLSTM,baseline,Baseline BiLSTM,0,72,1,1,0,baseline : Baseline BiLSTM,0.3444976076555024,0.013513513513513514,0.058823529411764705
named-entity-recognition,5,"The baseline BiLSTM model consists of two LSTM components , which process the input in the forward left - to - right and the backward rightto - left directions , respectively .",baseline,Baseline BiLSTM,0,73,2,2,0,baseline : Baseline BiLSTM,0.3492822966507177,0.02702702702702703,0.11764705882352941
named-entity-recognition,5,"In each direction , the reading of input words is modelled as a recurrent process with a single hidden state .",baseline,Baseline BiLSTM,0,74,3,3,0,baseline : Baseline BiLSTM,0.35406698564593303,0.04054054054054054,0.17647058823529413
named-entity-recognition,5,"Given an initial value , the state changes its value recurrently , each time consuming an incoming word .",baseline,Baseline BiLSTM,0,75,4,4,0,baseline : Baseline BiLSTM,0.3588516746411483,0.05405405405405406,0.23529411764705882
named-entity-recognition,5,Take the forward LSTM component for example .,baseline,Baseline BiLSTM,0,76,5,5,0,baseline : Baseline BiLSTM,0.36363636363636365,0.06756756756756757,0.29411764705882354
named-entity-recognition,5,"Denoting the initial state as ? ? h 0 , which is a model parameter , the recurrent state transition step for calculating ? ? h 1 , . . . , ? ? h n+1 is defined as follows :",baseline,Baseline BiLSTM,0,77,6,6,0,baseline : Baseline BiLSTM,0.3684210526315789,0.08108108108108109,0.35294117647058826
named-entity-recognition,5,"where x t denotes the word representation of wt ; it , o t , ft and u t represent the values of an input gate , an output gate , a forget gate and an actual input at time step t , respectively , which controls the information flow for a recurrent cell ? ? ct and the state vector",baseline,Baseline BiLSTM,0,78,7,7,0,baseline : Baseline BiLSTM,0.37320574162679426,0.0945945945945946,0.4117647058823529
named-entity-recognition,5,"where x t denotes the word representation of wt ; it , o t , ft and u t represent the values of an input gate , an output gate , a forget gate and an actual input at time step t , respectively , which controls the information flow for a recurrent cell ? ? ct and the state vector",baseline,Baseline BiLSTM,0,79,8,8,0,baseline : Baseline BiLSTM,0.37799043062200954,0.10810810810810811,0.47058823529411764
named-entity-recognition,5,The backward LSTM component follows the same recurrent state transition process as described in Eq 1 .,baseline,Baseline BiLSTM,0,80,9,9,0,baseline : Baseline BiLSTM,0.3827751196172249,0.12162162162162163,0.5294117647058824
named-entity-recognition,5,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n , The BiLSTM model uses the concatenated value of ? ? ht and ? ? ht as the hidden vector for wt :",baseline,Baseline BiLSTM,0,81,10,10,0,baseline : Baseline BiLSTM,0.3875598086124402,0.13513513513513514,0.5882352941176471
named-entity-recognition,5,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n , The BiLSTM model uses the concatenated value of ? ? ht and ? ? ht as the hidden vector for wt :",baseline,Baseline BiLSTM,0,82,11,11,0,baseline : Baseline BiLSTM,0.3923444976076555,0.14864864864864866,0.6470588235294118
named-entity-recognition,5,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n , The BiLSTM model uses the concatenated value of ? ? ht and ? ? ht as the hidden vector for wt :",baseline,Baseline BiLSTM,0,83,12,12,0,baseline : Baseline BiLSTM,0.39712918660287083,0.16216216216216217,0.7058823529411765
named-entity-recognition,5,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n , The BiLSTM model uses the concatenated value of ? ? ht and ? ? ht as the hidden vector for wt :",baseline,Baseline BiLSTM,0,84,13,13,0,baseline : Baseline BiLSTM,0.4019138755980861,0.17567567567567569,0.7647058823529411
named-entity-recognition,5,single hidden vector representation g of the whole input sentence can be obtained using the final state values of the two LSTM components :,baseline,Baseline BiLSTM,0,85,14,14,0,baseline : Baseline BiLSTM,0.40669856459330145,0.1891891891891892,0.8235294117647058
named-entity-recognition,5,Stacked BiLSTM,baseline,Baseline BiLSTM,0,86,15,15,0,baseline : Baseline BiLSTM,0.41148325358851673,0.20270270270270271,0.8823529411764706
named-entity-recognition,5,"Multiple layers of BiLTMs can be stacked for increased representation power , where the hidden vectors of a lower layer are used as inputs for an upper layer .",baseline,Baseline BiLSTM,0,87,16,16,0,baseline : Baseline BiLSTM,0.41626794258373206,0.21621621621621623,0.9411764705882353
named-entity-recognition,5,Different model parameters are used in each stacked BiLSTM layer .,baseline,Baseline BiLSTM,0,88,17,17,0,baseline : Baseline BiLSTM,0.42105263157894735,0.22972972972972974,1.0
named-entity-recognition,5,Sentence - State LSTM,baseline,Sentence-State LSTM,0,89,18,1,0,baseline : Sentence-State LSTM,0.4258373205741627,0.24324324324324326,0.023809523809523808
named-entity-recognition,5,"Formally , an S - LSTM state at time step t can be denoted by :",baseline,Sentence-State LSTM,0,90,19,2,0,baseline : Sentence-State LSTM,0.430622009569378,0.25675675675675674,0.047619047619047616
named-entity-recognition,5,which consists of a sub state ht i for each word w i and a sentence - level sub state gt .,baseline,Sentence-State LSTM,0,91,20,3,0,baseline : Sentence-State LSTM,0.4354066985645933,0.2702702702702703,0.07142857142857142
named-entity-recognition,5,"- LSTM uses a recurrent state transition process to model information exchange between sub states , which enriches state representations incrementally .",baseline,Sentence-State LSTM,0,92,21,4,0,baseline : Sentence-State LSTM,0.44019138755980863,0.28378378378378377,0.09523809523809523
named-entity-recognition,5,"For the initial state H 0 , we set h 0",baseline,Sentence-State LSTM,0,93,22,5,0,baseline : Sentence-State LSTM,0.4449760765550239,0.2972972972972973,0.11904761904761904
named-entity-recognition,5,to ht i and from g t?1 tog t .,baseline,Sentence-State LSTM,0,94,23,6,0,baseline : Sentence-State LSTM,0.44976076555023925,0.3108108108108108,0.14285714285714285
named-entity-recognition,5,"We take an LSTM structure similar to the baseline BiLSTM for modelling state transition , using a recurrent cell ct i for each w i and a cell ct g for g.",baseline,Sentence-State LSTM,0,95,24,7,0,baseline : Sentence-State LSTM,0.45454545454545453,0.32432432432432434,0.16666666666666666
named-entity-recognition,5,"As shown in , the value of each ht i is computed based on the values of",baseline,Sentence-State LSTM,0,96,25,8,0,baseline : Sentence-State LSTM,0.45933014354066987,0.33783783783783783,0.19047619047619047
named-entity-recognition,5,"+ 1 and g t?1 , together with their corresponding cell values :",baseline,Sentence-State LSTM,0,97,26,9,0,baseline : Sentence-State LSTM,0.46411483253588515,0.35135135135135137,0.21428571428571427
named-entity-recognition,5,"where ? ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ? ti and xi to ct i .",baseline,Sentence-State LSTM,0,98,27,10,0,baseline : Sentence-State LSTM,0.4688995215311005,0.36486486486486486,0.23809523809523808
named-entity-recognition,5,"where ? ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ? ti and xi to ct i .",baseline,Sentence-State LSTM,0,99,28,11,0,baseline : Sentence-State LSTM,0.47368421052631576,0.3783783783783784,0.2619047619047619
named-entity-recognition,5,"where ? ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ? ti and xi to ct i .",baseline,Sentence-State LSTM,0,100,29,12,0,baseline : Sentence-State LSTM,0.4784688995215311,0.3918918918918919,0.2857142857142857
named-entity-recognition,5,"In particular , it i controls information from the input xi ; l ti , rt i , ft i and st i control information from the left context cell c t ?1 i ? 1 , the right context cell c t ?1 i + 1 , c t?1 i and the sentence context cell c t ? 1 g , respectively .",baseline,Sentence-State LSTM,0,101,30,13,0,baseline : Sentence-State LSTM,0.48325358851674644,0.40540540540540543,0.30952380952380953
named-entity-recognition,5,"The values of it i , l ti , rt i , ft i and st i are normalised such that they sum to 1 .",baseline,Sentence-State LSTM,0,102,31,14,0,baseline : Sentence-State LSTM,0.4880382775119617,0.4189189189189189,0.3333333333333333
named-entity-recognition,5,ti is an output gate from the cell state ct i to the hidden state,baseline,Sentence-State LSTM,0,103,32,15,0,baseline : Sentence-State LSTM,0.49282296650717705,0.43243243243243246,0.35714285714285715
named-entity-recognition,5,The value of gt is computed based on the values,baseline,Sentence-State LSTM,0,104,33,16,0,baseline : Sentence-State LSTM,0.49760765550239233,0.44594594594594594,0.38095238095238093
named-entity-recognition,5,". , ft n+1 and ft g are gates controlling information from c t ? 1 0 , . . . , c t?1 n+1 and c t ? 1 g , respectively , which are normalised .",baseline,Sentence-State LSTM,0,105,34,17,0,baseline : Sentence-State LSTM,0.5023923444976076,0.4594594594594595,0.40476190476190477
named-entity-recognition,5,t is an output gate from the recurrent cell ct g tog t .,baseline,Sentence-State LSTM,0,106,35,18,0,baseline : Sentence-State LSTM,0.507177033492823,0.47297297297297297,0.42857142857142855
named-entity-recognition,5,"x , U x and bx ( x ? {g , f , o} ) are model parameters .",baseline,Sentence-State LSTM,0,107,36,19,0,baseline : Sentence-State LSTM,0.5119617224880383,0.4864864864864865,0.4523809523809524
named-entity-recognition,5,Contrast with BiLSTM,baseline,Sentence-State LSTM,0,108,37,20,0,baseline : Sentence-State LSTM,0.5167464114832536,0.5,0.47619047619047616
named-entity-recognition,5,The difference between S - LSTM and BiLSTM can be understood with respect to their recurrent states .,baseline,Sentence-State LSTM,0,109,38,21,0,baseline : Sentence-State LSTM,0.5215311004784688,0.5135135135135135,0.5
named-entity-recognition,5,"While BiL - STM uses only one state in each direction to represent the subsequence from the beginning to a certain word , S - LSTM uses a structural state to represent the full sentence , which consists of a sentence - level sub state and n + 2 word - level sub states , simultaneously .",baseline,Sentence-State LSTM,0,110,39,22,0,baseline : Sentence-State LSTM,0.5263157894736842,0.527027027027027,0.5238095238095238
named-entity-recognition,5,"Different from BiLSTMs , for which ht at different time steps are used to represent w 0 , . . . , w n + 1 , respectively , the word - level states ht i and sentence - level state gt of S - LSTMs directly correspond to the goal outputs hi and g , as introduced in the beginning of this section .",baseline,Sentence-State LSTM,0,111,40,23,0,baseline : Sentence-State LSTM,0.5311004784688995,0.5405405405405406,0.5476190476190477
named-entity-recognition,5,"As t increases from 0 , ht i and gt are enriched with increasingly deeper context information .",baseline,Sentence-State LSTM,0,112,41,24,0,baseline : Sentence-State LSTM,0.5358851674641149,0.5540540540540541,0.5714285714285714
named-entity-recognition,5,"From the perspective of information flow , BiL - STM passes information from one end of the sentence to the other .",baseline,Sentence-State LSTM,0,113,42,25,0,baseline : Sentence-State LSTM,0.5406698564593302,0.5675675675675675,0.5952380952380952
named-entity-recognition,5,"As a result , the number of time steps scales with the size of the input .",baseline,Sentence-State LSTM,0,114,43,26,0,baseline : Sentence-State LSTM,0.5454545454545454,0.581081081081081,0.6190476190476191
named-entity-recognition,5,"In contrast , S - LSTM allows bi-directional information flow at each word simultaneously , and additionally between the sentence - level state and every wordlevel state .",baseline,Sentence-State LSTM,0,115,44,27,0,baseline : Sentence-State LSTM,0.5502392344497608,0.5945945945945946,0.6428571428571429
named-entity-recognition,5,"At each step , each hi captures an increasing larger ngram context , while additionally communicating globally to all other h j via g.",baseline,Sentence-State LSTM,0,116,45,28,0,baseline : Sentence-State LSTM,0.5550239234449761,0.6081081081081081,0.6666666666666666
named-entity-recognition,5,"The optimal number of recurrent steps is decided by the end - task performance , and does not necessarily scale with the sentence size .",baseline,Sentence-State LSTM,0,117,46,29,0,baseline : Sentence-State LSTM,0.5598086124401914,0.6216216216216216,0.6904761904761905
named-entity-recognition,5,"As a result , S - LSTM can potentially be both more efficient and more accurate compared with BiLSTMs .",baseline,Sentence-State LSTM,0,118,47,30,0,baseline : Sentence-State LSTM,0.5645933014354066,0.6351351351351351,0.7142857142857143
named-entity-recognition,5,Increasing window size .,baseline,Sentence-State LSTM,0,119,48,31,0,baseline : Sentence-State LSTM,0.569377990430622,0.6486486486486487,0.7380952380952381
named-entity-recognition,5,"By default S - LSTM exchanges information only between neighbouring words , which can be seen as adopting a 1 word window on each side .",baseline,Sentence-State LSTM,0,120,49,32,0,baseline : Sentence-State LSTM,0.5741626794258373,0.6621621621621622,0.7619047619047619
named-entity-recognition,5,"The window size can be extended to 2 , 3 or more words in order to allow more communication in a state transition , expediting information exchange .",baseline,Sentence-State LSTM,0,121,50,33,0,baseline : Sentence-State LSTM,0.5789473684210527,0.6756756756756757,0.7857142857142857
named-entity-recognition,5,"To this end , we modify Eq 2 , integrating additional context words to ? ti , with extended gates and cells .",baseline,Sentence-State LSTM,0,122,51,34,0,baseline : Sentence-State LSTM,0.583732057416268,0.6891891891891891,0.8095238095238095
named-entity-recognition,5,"For example , with a window size of 2 , We study the effectiveness of window size in our experiments .",baseline,Sentence-State LSTM,0,123,52,35,0,baseline : Sentence-State LSTM,0.5885167464114832,0.7027027027027027,0.8333333333333334
named-entity-recognition,5,"For example , with a window size of 2 , We study the effectiveness of window size in our experiments .",baseline,Sentence-State LSTM,0,124,53,36,0,baseline : Sentence-State LSTM,0.5933014354066986,0.7162162162162162,0.8571428571428571
named-entity-recognition,5,Additional sentence - level nodes .,baseline,Sentence-State LSTM,0,125,54,37,0,baseline : Sentence-State LSTM,0.5980861244019139,0.7297297297297297,0.8809523809523809
named-entity-recognition,5,By default S - LSTM uses one sentence - level node .,baseline,Sentence-State LSTM,0,126,55,38,0,baseline : Sentence-State LSTM,0.6028708133971292,0.7432432432432432,0.9047619047619048
named-entity-recognition,5,"One way of enriching the parameter space is to add more sentence - level nodes , each communicating with word - level nodes in the same way as described by Eq 3 .",baseline,Sentence-State LSTM,0,127,56,39,0,baseline : Sentence-State LSTM,0.6076555023923444,0.7567567567567568,0.9285714285714286
named-entity-recognition,5,"In addition , different sentence - level nodes can communicate with each other during state transition .",baseline,Sentence-State LSTM,0,128,57,40,0,baseline : Sentence-State LSTM,0.6124401913875598,0.7702702702702703,0.9523809523809523
named-entity-recognition,5,"When one sentence - level node is used for classification outputs , the other sentencelevel node can serve as hidden memory units , or latent features .",baseline,Sentence-State LSTM,0,129,58,41,0,baseline : Sentence-State LSTM,0.6172248803827751,0.7837837837837838,0.9761904761904762
named-entity-recognition,5,We study the effectiveness of multiple sentence - level nodes empirically .,baseline,Sentence-State LSTM,0,130,59,42,0,baseline : Sentence-State LSTM,0.6220095693779905,0.7972972972972973,1.0
named-entity-recognition,5,Task settings,baseline,Task settings,0,131,60,1,0,baseline : Task settings,0.6267942583732058,0.8108108108108109,0.06666666666666667
named-entity-recognition,5,"We consider two task settings , namely classification and sequence labelling .",baseline,Task settings,0,132,61,2,0,baseline : Task settings,0.631578947368421,0.8243243243243243,0.13333333333333333
named-entity-recognition,5,"For classification , g is fed to a softmax classification layer :",baseline,Task settings,0,133,62,3,0,baseline : Task settings,0.6363636363636364,0.8378378378378378,0.2
named-entity-recognition,5,where y is the probability distribution of output class labels and W c and b care model parameters .,baseline,Task settings,0,134,63,4,0,baseline : Task settings,0.6411483253588517,0.8513513513513513,0.26666666666666666
named-entity-recognition,5,"For sequence labelling , each hi can be used as feature representation for a corresponding word w i .",baseline,Task settings,0,135,64,5,0,baseline : Task settings,0.645933014354067,0.8648648648648649,0.3333333333333333
named-entity-recognition,5,External attention,baseline,Task settings,0,136,65,6,0,baseline : Task settings,0.6507177033492823,0.8783783783783784,0.4
named-entity-recognition,5,It has been shown that summation of hidden states using attention give better accuracies compared to using the end states of BiLSTMs .,baseline,Task settings,0,137,66,7,0,baseline : Task settings,0.6555023923444976,0.8918918918918919,0.4666666666666667
named-entity-recognition,5,We study the influence of attention on both S - LSTM and BiLSTM for classification .,baseline,Task settings,0,138,67,8,0,baseline : Task settings,0.6602870813397129,0.9054054054054054,0.5333333333333333
named-entity-recognition,5,"In particular , additive attention ( Bahdanau",baseline,Task settings,0,139,68,9,0,baseline : Task settings,0.6650717703349283,0.918918918918919,0.6
named-entity-recognition,5,"Here W ? , u and b ? are model parameters .",baseline,Task settings,0,140,69,10,0,baseline : Task settings,0.6698564593301436,0.9324324324324325,0.6666666666666666
named-entity-recognition,5,"Here W ? , u and b ? are model parameters .",baseline,Task settings,0,141,70,11,0,baseline : Task settings,0.6746411483253588,0.9459459459459459,0.7333333333333333
named-entity-recognition,5,External CRF,baseline,Task settings,0,142,71,12,0,baseline : Task settings,0.6794258373205742,0.9594594594594594,0.8
named-entity-recognition,5,"For sequential labelling , we use a CRF layer on top of the hidden vectors h 1 , h 2 , . . . , h n for calculating the conditional probabilities of label sequences :",baseline,Task settings,0,143,72,13,0,baseline : Task settings,0.6842105263157895,0.972972972972973,0.8666666666666667
named-entity-recognition,5,are parameters specific to two consecutive labels y i ?1 and y i .,baseline,Task settings,0,144,73,14,0,baseline : Task settings,0.6889952153110048,0.9864864864864865,0.9333333333333333
named-entity-recognition,5,"For training , standard log - likelihood loss is used with L 2 regularization given a set of gold - standard instances .",baseline,Task settings,0,145,74,15,0,baseline : Task settings,0.69377990430622,1.0,1.0
named-entity-recognition,5,Experiments,experiment,Experiments,0,146,1,1,0,experiment : Experiments,0.6985645933014354,0.05263157894736842,0.3333333333333333
named-entity-recognition,5,We empirically compare S - LSTMs and BiLSTMs on different classification and sequence labelling tasks .,experiment,Experiments,0,147,2,2,0,experiment : Experiments,0.7033492822966507,0.10526315789473684,0.6666666666666666
named-entity-recognition,5,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,experiment,Experiments,1,148,3,3,0,experiment : Experiments,0.7081339712918661,0.15789473684210525,1.0
named-entity-recognition,5,Development Experiments,experiment,Development Experiments,0,149,4,1,0,experiment : Development Experiments,0.7129186602870813,0.21052631578947367,0.0625
named-entity-recognition,5,We use the movie review development data to investigate different configurations of S - LSTMs and BiLSTMs .,experiment,Development Experiments,0,150,5,2,0,experiment : Development Experiments,0.7177033492822966,0.2631578947368421,0.125
named-entity-recognition,5,"For S - LSTMs , the default configuration uses sand /s words for augmenting words Hyperparameters : shows the development results of various S - LSTM settings , where Time refers to training time per epoch .",experiment,Development Experiments,0,151,6,3,0,experiment : Development Experiments,0.722488038277512,0.3157894736842105,0.1875
named-entity-recognition,5,"Without the sentence - level node , the accuracy of S - LSTM drops to 81.76 % , demonstrating the necessity of global information exchange .",experiment,Development Experiments,0,152,7,4,0,experiment : Development Experiments,0.7272727272727273,0.3684210526315789,0.25
named-entity-recognition,5,"Adding one additional sentence - level node as described in Section 3.2 does not lead to accuracy improvements , although the number of parameters and decoding time increase accordingly .",experiment,Development Experiments,0,153,8,5,0,experiment : Development Experiments,0.7320574162679426,0.42105263157894735,0.3125
named-entity-recognition,5,"As a result , we use only 1 sentence - level node for the remaining experiments .",experiment,Development Experiments,0,154,9,6,0,experiment : Development Experiments,0.7368421052631579,0.47368421052631576,0.375
named-entity-recognition,5,"The accuracies of S - LSTM increases as the hidden layer size for each node increases from 100 to 300 , but does not further increase when the size increases beyond 300 .",experiment,Development Experiments,0,155,10,7,0,experiment : Development Experiments,0.7416267942583732,0.5263157894736842,0.4375
named-entity-recognition,5,We fix the hidden size to 300 accordingly .,experiment,Development Experiments,0,156,11,8,0,experiment : Development Experiments,0.7464114832535885,0.5789473684210527,0.5
named-entity-recognition,5,"Without using sand /s , the performance of S - LSTM drops from 82. 64 % to 82.36 % , showing the effectiveness of having these additional nodes .",experiment,Development Experiments,0,157,12,9,0,experiment : Development Experiments,0.7511961722488039,0.631578947368421,0.5625
named-entity-recognition,5,"Hyperparameters for BiLSTM models are also set according to the development data , which we omit here .",experiment,Development Experiments,0,158,13,10,0,experiment : Development Experiments,0.7559808612440191,0.6842105263157895,0.625
named-entity-recognition,5,State transition .,experiment,Development Experiments,0,159,14,11,0,experiment : Development Experiments,0.7607655502392344,0.7368421052631579,0.6875
named-entity-recognition,5,"In , the number of recurrent state transition steps of S - LSTM is decided according to the best development performance .",experiment,Development Experiments,0,160,15,12,0,experiment : Development Experiments,0.7655502392344498,0.7894736842105263,0.75
named-entity-recognition,5,draws the development accuracies of S - LSTMs with various window sizes against the number of recurrent steps .,experiment,Development Experiments,0,161,16,13,0,experiment : Development Experiments,0.7703349282296651,0.8421052631578947,0.8125
named-entity-recognition,5,"As can be seen from the figure , when the number of time steps increases from 1 to 11 , the accuracies generally increase , before reaching a maximum value .",experiment,Development Experiments,0,162,17,14,0,experiment : Development Experiments,0.7751196172248804,0.8947368421052632,0.875
named-entity-recognition,5,This shows the effectiveness of recurrent information exchange in S - LSTM state transition .,experiment,Development Experiments,0,163,18,15,0,experiment : Development Experiments,0.7799043062200957,0.9473684210526315,0.9375
named-entity-recognition,5,"On the other hand , no significant differences are observed on the peak accuracies given by different window sizes , although a larger window size ( e.g.",experiment,Development Experiments,0,164,19,16,0,experiment : Development Experiments,0.784688995215311,1.0,1.0
named-entity-recognition,5,Final Results for Classification,result,Final Results for Classification,1,165,1,1,0,result : Final Results for Classification,0.7894736842105263,0.034482758620689655,0.07692307692307693
named-entity-recognition,5,"The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5 , respectively .",result,Final Results for Classification,0,166,2,2,0,result : Final Results for Classification,0.7942583732057417,0.06896551724137931,0.15384615384615385
named-entity-recognition,5,"In addition to training time per epoch , test times are additionally reported .",result,Final Results for Classification,0,167,3,3,0,result : Final Results for Classification,0.7990430622009569,0.10344827586206896,0.23076923076923078
named-entity-recognition,5,We use the best settings on the movie review development dataset for both S - LSTMs and BiLSTMs .,result,Final Results for Classification,0,168,4,4,0,result : Final Results for Classification,0.8038277511961722,0.13793103448275862,0.3076923076923077
named-entity-recognition,5,The step number for S - LSTMs is set to 9 .,result,Final Results for Classification,0,169,5,5,0,result : Final Results for Classification,0.8086124401913876,0.1724137931034483,0.38461538461538464
named-entity-recognition,5,"As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .",result,Final Results for Classification,1,170,6,6,0,result : Final Results for Classification,0.8133971291866029,0.20689655172413793,0.46153846153846156
named-entity-recognition,5,Observations on CNN and hierarchical attention are consistent with the development results .,result,Final Results for Classification,0,171,7,7,0,result : Final Results for Classification,0.8181818181818182,0.2413793103448276,0.5384615384615384
named-entity-recognition,5,- LSTM also gives highly competitive results when compared with existing methods in the literature .,result,Final Results for Classification,0,172,8,8,0,result : Final Results for Classification,0.8229665071770335,0.27586206896551724,0.6153846153846154
named-entity-recognition,5,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .",result,Final Results for Classification,1,173,9,9,0,result : Final Results for Classification,0.8277511961722488,0.3103448275862069,0.6923076923076923
named-entity-recognition,5,"The average accuracy of S - LSTM is 85.6 % , significantly higher compared with 84.9 % by 2 - layer stacked BiLSTM .",result,Final Results for Classification,1,174,10,10,0,result : Final Results for Classification,0.8325358851674641,0.3448275862068966,0.7692307692307693
named-entity-recognition,5,"- layer stacked BiL - STM gives an average accuracy of 84. 57 % , which is lower compared to a 2 - layer stacked BiLSTM , with a training time per epoch of 423.6 seconds .",result,Final Results for Classification,0,175,11,11,0,result : Final Results for Classification,0.8373205741626795,0.3793103448275862,0.8461538461538461
named-entity-recognition,5,The relative speed advantage of S - LSTM over BiLSTM is larger on the 16 datasets as compared to the movie review test test .,result,Final Results for Classification,0,176,12,12,0,result : Final Results for Classification,0.8421052631578947,0.41379310344827586,0.9230769230769231
named-entity-recognition,5,This is because the average length of inputs is larger on the 16 datasets ( see Section 4.5 ) .,result,Final Results for Classification,0,177,13,13,0,result : Final Results for Classification,0.84688995215311,0.4482758620689655,1.0
named-entity-recognition,5,Final Results for Sequence Labelling,result,Final Results for Sequence Labelling,1,178,14,1,0,result : Final Results for Sequence Labelling,0.8516746411483254,0.4827586206896552,0.0625
named-entity-recognition,5,Bi-directional,result,Final Results for Sequence Labelling,0,179,15,2,0,result : Final Results for Sequence Labelling,0.8564593301435407,0.5172413793103449,0.125
named-entity-recognition,5,"RNN - CRF structures , and in particular BiLSTM - CRFs , have achieved the state of the art in the literature for sequence labelling tasks , including POS - tagging and NER .",result,Final Results for Sequence Labelling,0,180,16,3,0,result : Final Results for Sequence Labelling,0.861244019138756,0.5517241379310345,0.1875
named-entity-recognition,5,"We compare S - LSTM - CRF with BiLSTM - CRF for sequence labelling , using the same settings as decided on the movie review development experiments for both BiLSTMs and S - LSTMs .",result,Final Results for Sequence Labelling,0,181,17,4,0,result : Final Results for Sequence Labelling,0.8660287081339713,0.5862068965517241,0.25
named-entity-recognition,5,"For the latter , we decide the number of recurrent stepson the respective development sets for sequence labelling .",result,Final Results for Sequence Labelling,0,182,18,5,0,result : Final Results for Sequence Labelling,0.8708133971291866,0.6206896551724138,0.3125
named-entity-recognition,5,"The POS accuracies and NER F1 - scores against the number of recurrent steps are shown in ( a ) and ( b ) , respectively .",result,Final Results for Sequence Labelling,0,183,19,6,0,result : Final Results for Sequence Labelling,0.8755980861244019,0.6551724137931034,0.375
named-entity-recognition,5,"For POS tagging , the best step number is set to 7 , with a development accuracy of 97.58 % .",result,Final Results for Sequence Labelling,0,184,20,7,0,result : Final Results for Sequence Labelling,0.8803827751196173,0.6896551724137931,0.4375
named-entity-recognition,5,"For NER , the step number is set to 9 , with a development F1 - score of 94.98 % .",result,Final Results for Sequence Labelling,0,185,21,8,0,result : Final Results for Sequence Labelling,0.8851674641148325,0.7241379310344828,0.5
named-entity-recognition,5,As can be seen in with three layers of stacked LSTMs .,result,Final Results for Sequence Labelling,0,186,22,9,0,result : Final Results for Sequence Labelling,0.8899521531100478,0.7586206896551724,0.5625
named-entity-recognition,5,"For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",result,Final Results for Sequence Labelling,1,187,23,10,0,result : Final Results for Sequence Labelling,0.8947368421052632,0.7931034482758621,0.625
named-entity-recognition,5,Stacking more layers of BiLSTMs leads to slightly better F1 - scores compared with a single - layer BiL - STM .,result,Final Results for Sequence Labelling,0,188,24,11,0,result : Final Results for Sequence Labelling,0.8995215311004785,0.8275862068965517,0.6875
named-entity-recognition,5,"Our BiLSTM results are comparable to the results reported by and , who also use bidirectional RNN - CRF structures .",result,Final Results for Sequence Labelling,0,189,25,12,0,result : Final Results for Sequence Labelling,0.9043062200956937,0.8620689655172413,0.75
named-entity-recognition,5,"In contrast , S - LSTM gives the best reported results under the same settings .",result,Final Results for Sequence Labelling,0,190,26,13,0,result : Final Results for Sequence Labelling,0.9090909090909091,0.896551724137931,0.8125
named-entity-recognition,5,"In the second section of learning using additional language model objectives , obtaining an F-score of 86 . 26 % ; leverage character - level language models , obtaining an F- score of 91. 93 % , which is the current best result on the dataset .",result,Final Results for Sequence Labelling,0,191,27,14,0,result : Final Results for Sequence Labelling,0.9138755980861244,0.9310344827586207,0.875
named-entity-recognition,5,All the three models are based on BiLSTM - CRF .,result,Final Results for Sequence Labelling,0,192,28,15,0,result : Final Results for Sequence Labelling,0.9186602870813397,0.9655172413793104,0.9375
named-entity-recognition,5,"On the other hand , these semi-supervised learning techniques are orthogonal to our work , and can potentially be used for S - LSTM also .",result,Final Results for Sequence Labelling,0,193,29,16,0,result : Final Results for Sequence Labelling,0.9234449760765551,1.0,1.0
named-entity-recognition,5,Analysis,analysis,Analysis,0,194,1,1,0,analysis : Analysis,0.9282296650717703,0.1,0.1
named-entity-recognition,5,"Figure 4 ( a ) and ( b ) show the accuracies against the sentence length on the movie review and CoNLL datasets , respectively , where test samples are binned in batches of 80 .",analysis,Analysis,0,195,2,2,0,analysis : Analysis,0.9330143540669856,0.2,0.2
named-entity-recognition,5,We find that the performances of both S - LSTM and BiLSTM decrease as the sentence length increases .,analysis,Analysis,0,196,3,3,0,analysis : Analysis,0.937799043062201,0.3,0.3
named-entity-recognition,5,"On the other hand , S - LSTM demonstrates relatively better robustness compared to BiLSTMs .",analysis,Analysis,0,197,4,4,0,analysis : Analysis,0.9425837320574163,0.4,0.4
named-entity-recognition,5,This confirms our intuition that a sentence - level node can facilitate better non-local communication .,analysis,Analysis,0,198,5,5,0,analysis : Analysis,0.9473684210526315,0.5,0.5
named-entity-recognition,5,"these comparisons , we mix all training instances , order them by the size , and put them into 10 equal groups , the medium sentence lengths of which are shown .",analysis,Analysis,0,199,6,6,0,analysis : Analysis,0.9521531100478469,0.6,0.6
named-entity-recognition,5,"As can be seen from the figure , the speed advantage of S - LSTM is larger when the size of the input text increases , thanks to a fixed number of recurrent steps .",analysis,Analysis,0,200,7,7,0,analysis : Analysis,0.9569377990430622,0.7,0.7
named-entity-recognition,5,"Similar to hierarchical attention , there is a relative dis advantage of S - LSTM in comparison with BiLSTM , which is that the memory consumption is relatively larger .",analysis,Analysis,0,201,8,8,0,analysis : Analysis,0.9617224880382775,0.8,0.8
named-entity-recognition,5,"For example , over the movie review development set , the actual GPU memory consumption by S - LSTM , Bi LSTM , 2 - layer stacked BiLSTM and 4 - layer stacked BiLSTM are 252M , 89M , 146 M and 253M , respectively .",analysis,Analysis,0,202,9,9,0,analysis : Analysis,0.9665071770334929,0.9,0.9
named-entity-recognition,5,This is due to the fact that computation is performed in parallel by S - LSTM and hierarchical attention .,analysis,Analysis,0,203,10,10,0,analysis : Analysis,0.9712918660287081,1.0,1.0
named-entity-recognition,5,Conclusion,conclusion,Conclusion,0,204,1,1,0,conclusion : Conclusion,0.9760765550239234,0.16666666666666666,0.16666666666666666
named-entity-recognition,5,"We have investigated S - LSTM , a recurrent neural network for encoding sentences , which offers richer contextual information exchange with more parallelism compared to BiLSTMs .",conclusion,Conclusion,0,205,2,2,0,conclusion : Conclusion,0.9808612440191388,0.3333333333333333,0.3333333333333333
named-entity-recognition,5,"Results on a range of classification and sequence labelling tasks show that S - LSTM outperforms BiLSTMs using the same number of parameters , demonstrating that S - LSTM can be a useful addition to the neural toolbox for encoding sentences .",conclusion,Conclusion,0,206,3,3,0,conclusion : Conclusion,0.9856459330143541,0.5,0.5
named-entity-recognition,5,"The structural nature in S - LSTM states allows straightforward extension to tree structures , resulting in highly parallelis able tree LSTMs .",conclusion,Conclusion,0,207,4,4,0,conclusion : Conclusion,0.9904306220095693,0.6666666666666666,0.6666666666666666
named-entity-recognition,5,We leave such investigation to future work .,conclusion,Conclusion,0,208,5,5,0,conclusion : Conclusion,0.9952153110047847,0.8333333333333334,0.8333333333333334
named-entity-recognition,5,"Next directions also include the investigation of S - LSTM to more NLP tasks , such as machine translation .",conclusion,Conclusion,0,209,6,6,0,conclusion : Conclusion,1.0,1.0,1.0
named-entity-recognition,6,Robust Lexical Features for Improved Neural Network Named - Entity Recognition,title,title,1,2,1,1,0,title : title,0.009433962264150943,1.0,1.0
named-entity-recognition,6,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.014150943396226415,0.1,0.1
named-entity-recognition,6,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.018867924528301886,0.2,0.2
named-entity-recognition,6,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02358490566037736,0.3,0.3
named-entity-recognition,6,"In this work , we show that this is unfair : lexical features are actually quite useful .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02830188679245283,0.4,0.4
named-entity-recognition,6,We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.0330188679245283,0.5,0.5
named-entity-recognition,6,"From this , we compute - offline - a feature vector representing each word .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03773584905660377,0.6,0.6
named-entity-recognition,6,"When used with a vanilla recurrent neural network model , this representation yields substantial improvements .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.04245283018867924,0.7,0.7
named-entity-recognition,6,"We establish a new state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.04716981132075472,0.8,0.8
named-entity-recognition,6,This work is licensed under a Creative Commons Attribution 4.0 International License .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.05188679245283019,0.9,0.9
named-entity-recognition,6,License details :,abstract,abstract,0,12,10,10,0,abstract : abstract,0.05660377358490566,1.0,1.0
named-entity-recognition,6,Introduction,introduction,introduction,0,13,1,1,0,introduction : introduction,0.06132075471698113,0.05263157894736842,0.05263157894736842
named-entity-recognition,6,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,introduction,introduction,1,14,2,2,0,introduction : introduction,0.0660377358490566,0.10526315789473684,0.10526315789473684
named-entity-recognition,6,"Various approaches have been proposed to tackle the task , from hand - crafted feature - based machine learning models like conditional random fields and perceptron , to deep neural models .",introduction,introduction,0,15,3,3,0,introduction : introduction,0.07075471698113207,0.15789473684210525,0.15789473684210525
named-entity-recognition,6,"Word representations , also known as word embeddings , are a key element for multiple NLP tasks including NER .",introduction,introduction,0,16,4,4,0,introduction : introduction,0.07547169811320754,0.21052631578947367,0.21052631578947367
named-entity-recognition,6,"Due to the small amount of named - entity annotated data , embeddings are used to extend , rather than replace , hand - crafted features in order to obtain state - of - the - art performance .",introduction,introduction,0,17,5,5,0,introduction : introduction,0.08018867924528301,0.2631578947368421,0.2631578947368421
named-entity-recognition,6,Recent studies have explored methods for supplying deep sequential taggers with complementary features to standard embeddings .,introduction,introduction,0,18,6,6,0,introduction : introduction,0.08490566037735849,0.3157894736842105,0.3157894736842105
named-entity-recognition,6,and tested special embeddings extracted from a neural language model ( LM ) trained on a large corpus .,introduction,introduction,0,19,7,7,0,introduction : introduction,0.08962264150943396,0.3684210526315789,0.3684210526315789
named-entity-recognition,6,LM embeddings capture context - dependent aspects of word meaning using future ( forward LM ) and previous ( backward LM ) context words .,introduction,introduction,0,20,8,8,0,introduction : introduction,0.09433962264150944,0.42105263157894735,0.42105263157894735
named-entity-recognition,6,"When this information is added to standard features , it leads to significant improvements in NER .",introduction,introduction,0,21,9,9,0,introduction : introduction,0.09905660377358491,0.47368421052631576,0.47368421052631576
named-entity-recognition,6,"Also , showed that external knowledge resources ( namely gazetteers ) are crucial to NER performance .",introduction,introduction,0,22,10,10,0,introduction : introduction,0.10377358490566038,0.5263157894736842,0.5263157894736842
named-entity-recognition,6,Gazetteer features encode the presence of word n-grams in predefined lists of NEs .,introduction,introduction,0,23,11,11,0,introduction : introduction,0.10849056603773585,0.5789473684210527,0.5789473684210527
named-entity-recognition,6,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .",introduction,introduction,1,24,12,12,0,introduction : introduction,0.11320754716981132,0.631578947368421,0.631578947368421
named-entity-recognition,6,"In a nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .",introduction,introduction,1,25,13,13,0,introduction : introduction,0.1179245283018868,0.6842105263157895,0.6842105263157895
named-entity-recognition,6,"From this vector space , we compute for each word a 120 - dimensional vector , where each dimension encodes the similarity of the word with an entity type .",introduction,introduction,0,26,14,14,0,introduction : introduction,0.12264150943396226,0.7368421052631579,0.7368421052631579
named-entity-recognition,6,"We call this vector an LS representation , for Lexical Similarity .",introduction,introduction,0,27,15,15,0,introduction : introduction,0.12735849056603774,0.7894736842105263,0.7894736842105263
named-entity-recognition,6,"When included in a vanilla LSTM - CRF NER model , LS representations lead to significant gains .",introduction,introduction,0,28,16,16,0,introduction : introduction,0.1320754716981132,0.8421052631578947,0.8421052631578947
named-entity-recognition,6,"We establish a new state - of - the - art F 1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance on the over - studied In the rest of this paper , we motivate our work in Section 2 .",introduction,introduction,0,29,17,17,0,introduction : introduction,0.13679245283018868,0.8947368421052632,0.8947368421052632
named-entity-recognition,6,We describe how we compute LS vectors in Section 3 .,introduction,introduction,0,30,18,18,0,introduction : introduction,0.14150943396226415,0.9473684210526315,0.9473684210526315
named-entity-recognition,6,"We present our system in Section 4 and report results in Section 5 . In Section 6 , we discuss related works before concluding in Section 7 .",introduction,introduction,0,31,19,19,0,introduction : introduction,0.14622641509433962,1.0,1.0
named-entity-recognition,6,Motivation,system description,Motivation,0,32,1,1,0,system description : Motivation,0.1509433962264151,0.05,0.05
named-entity-recognition,6,Gazetteers are lists of entities thatare associated with specific NE categories .,system description,Motivation,0,33,2,2,0,system description : Motivation,0.15566037735849056,0.1,0.1
named-entity-recognition,6,"They are widely used as a feature source in NER , and have been successfully included in feature - based models .",system description,Motivation,0,34,3,3,0,system description : Motivation,0.16037735849056603,0.15,0.15
named-entity-recognition,6,"Typically , lists of entities are compiled from structured data sources such as DBpedia or Freebase .",system description,Motivation,0,35,4,4,0,system description : Motivation,0.1650943396226415,0.2,0.2
named-entity-recognition,6,"The surface form of the title of a Wikipedia article , as well as aliases and redirects are mapped to an entity type using the object type attribute of the related DBpedia ( or Freebase ) page .",system description,Motivation,0,36,5,5,0,system description : Motivation,0.16981132075471697,0.25,0.25
named-entity-recognition,6,"use this methodology to compile 30 lists of fine - grained entity types extracted from Wikipedia , while Chiu and Nichols ( 2016 ) create 4 gazetteers that map to CoNLL categories ( PER , LOC , ORG and MISC ) .",system description,Motivation,0,37,6,6,0,system description : Motivation,0.17452830188679244,0.3,0.3
named-entity-recognition,6,"Despite their importance , gazetteer - based features suffer from a number of limitations .",system description,Motivation,0,38,7,7,0,system description : Motivation,0.1792452830188679,0.35,0.35
named-entity-recognition,6,Binary representation .,system description,Motivation,0,39,8,8,0,system description : Motivation,0.18396226415094338,0.4,0.4
named-entity-recognition,6,Gazetteer features encode only the presence of an n-gram in each list and omit its relative frequency .,system description,Motivation,0,40,9,9,0,system description : Motivation,0.18867924528301888,0.45,0.45
named-entity-recognition,6,"For example , the word "" France "" can be used as a person , an organization , or a location , while it likely refers to the country most of the time .",system description,Motivation,0,41,10,10,0,system description : Motivation,0.19339622641509435,0.5,0.5
named-entity-recognition,6,Binary features can not capture this preference .,system description,Motivation,0,42,11,11,0,system description : Motivation,0.19811320754716982,0.55,0.55
named-entity-recognition,6,Generation .,system description,Motivation,0,43,12,12,0,system description : Motivation,0.2028301886792453,0.6,0.6
named-entity-recognition,6,"At test time , we need to match every n-gram ( up to the length of the longest lexicon entry ) in a sentence against entries in the lexicons , which is time consuming .",system description,Motivation,0,44,13,13,0,system description : Motivation,0.20754716981132076,0.65,0.65
named-entity-recognition,6,"In their work , Chiu and Nichols ( 2016 ) use 4 lists that count over 2.3 M entries .",system description,Motivation,0,45,14,14,0,system description : Motivation,0.21226415094339623,0.7,0.7
named-entity-recognition,6,Non-entity words .,system description,Motivation,0,46,15,15,0,system description : Motivation,0.2169811320754717,0.75,0.75
named-entity-recognition,6,"Gazetteer features do not capture signal from non-entity words , while earlier feature - based models strived to encode that some words ( or n-grams ) trigger specific entity types .",system description,Motivation,0,47,16,16,0,system description : Motivation,0.22169811320754718,0.8,0.8
named-entity-recognition,6,"For instance , words such as "" eat "" , "" directed "" or "" born "" are words that typically appear after a mention of type PER .",system description,Motivation,0,48,17,17,0,system description : Motivation,0.22641509433962265,0.85,0.85
named-entity-recognition,6,"To overcome those limitations , we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words .",system description,Motivation,0,49,18,18,0,system description : Motivation,0.23113207547169812,0.9,0.9
named-entity-recognition,6,This vector compactly and efficiently encodes both gazetteer and lexical information .,system description,Motivation,0,50,19,19,0,system description : Motivation,0.2358490566037736,0.95,0.95
named-entity-recognition,6,"Note that at test time , we only have to feed our model with this feature vector , which is efficient .",system description,Motivation,0,51,20,20,0,system description : Motivation,0.24056603773584906,1.0,1.0
named-entity-recognition,6,Our Method,method,Our Method,0,52,1,1,0,method : Our Method,0.24528301886792453,0.02,1.0
named-entity-recognition,6,Embedding Words and Entity Types,method,Embedding Words and Entity Types,0,53,2,1,0,method : Embedding Words and Entity Types,0.25,0.04,0.03333333333333333
named-entity-recognition,6,Turning Wikipedia into a corpus of named - entities annotated with types is a task that received continuous attention over the years .,method,Embedding Words and Entity Types,0,54,3,2,0,method : Embedding Words and Entity Types,0.25471698113207547,0.06,0.06666666666666667
named-entity-recognition,6,It consists mainly in exploiting the hyperlink structure of Wikipedia in order to detect entity mentions .,method,Embedding Words and Entity Types,0,55,4,3,0,method : Embedding Words and Entity Types,0.25943396226415094,0.08,0.1
named-entity-recognition,6,"Then , structured data from a knowledge base ( for instance Freebase ) are used to map hyperlinks to entity types .",method,Embedding Words and Entity Types,0,56,5,4,0,method : Embedding Words and Entity Types,0.2641509433962264,0.1,0.13333333333333333
named-entity-recognition,6,"Because the number of anchored strings in Wikipedia is no more than 3 % of the text tokens , proposed to augment Wikipedia articles with mentions unmarked in Wikipedia , thanks to a mix of heuristics that benefit the Wikipedia structure , as well as a coreference resolution system adapted specifically to Wikipedia .",method,Embedding Words and Entity Types,0,57,6,5,0,method : Embedding Words and Entity Types,0.2688679245283019,0.12,0.16666666666666666
named-entity-recognition,6,"The authors applied their approach on English Wikipedia and produce coarse ( 4 classes ) and finegrained ( 120 labels ) named- entity annotations , leading to WiNER and WiFiNE .",method,Embedding Words and Entity Types,0,58,7,6,0,method : Embedding Words and Entity Types,0.27358490566037735,0.14,0.2
named-entity-recognition,6,"In this work , we adopt WiFiNE which is publicly available at http://rali.iro.umontreal.ca/rali/en/wifiner-wikipedia-for-et as our source of annotations .",method,Embedding Words and Entity Types,0,59,8,7,0,method : Embedding Words and Entity Types,0.2783018867924528,0.16,0.23333333333333334
named-entity-recognition,6,Each entity mention is mapped ( via it s Freebase object type attribute ) to a pre-defined set of 120 entity types .,method,Embedding Words and Entity Types,0,60,9,8,0,method : Embedding Words and Entity Types,0.2830188679245283,0.18,0.26666666666666666
named-entity-recognition,6,Types are stored in a 2 - level hierarchical structure ( e.g. / person and / person / musician ) .,method,Embedding Words and Entity Types,0,61,10,9,0,method : Embedding Words and Entity Types,0.28773584905660377,0.2,0.3
named-entity-recognition,6,"The corpus consist of 3.2 M Wikipedia articles , comprising 1.3G tokens that we annotated with 157.4 M named - entity mentions and their types .",method,Embedding Words and Entity Types,0,62,11,10,0,method : Embedding Words and Entity Types,0.29245283018867924,0.22,0.3333333333333333
named-entity-recognition,6,We used this very large quantity of automatically annotated data for jointly embedding words and entity types into the same low - dimensional space .,method,Embedding Words and Entity Types,0,63,12,11,0,method : Embedding Words and Entity Types,0.2971698113207547,0.24,0.36666666666666664
named-entity-recognition,6,The key idea consists in learning an embedding for each entity type using its surrounding words .,method,Embedding Words and Entity Types,0,64,13,12,0,method : Embedding Words and Entity Types,0.3018867924528302,0.26,0.4
named-entity-recognition,6,"For instance , the embedding for / product / software will be trained using context words that surround all entities that were ( automatically ) labelled as / product / software in Wikipedia .",method,Embedding Words and Entity Types,0,65,14,13,0,method : Embedding Words and Entity Types,0.30660377358490565,0.28,0.43333333333333335
named-entity-recognition,6,"In practice , we found that simply concatenating a sentence ( v1 ) with its annotated version ( v 2 ) , as illustrated in , offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them .",method,Embedding Words and Entity Types,0,66,15,14,0,method : Embedding Words and Entity Types,0.3113207547169811,0.3,0.4666666666666667
named-entity-recognition,6,We use the FastText toolkit to learn the uncased embeddings for both words and entity types .,method,Embedding Words and Entity Types,0,67,16,15,0,method : Embedding Words and Entity Types,0.3160377358490566,0.32,0.5
named-entity-recognition,6,"We train a skipgram model to learn 100 - dimensional vectors with a minimum word frequency cutoff of 5 , and a window size of 5 .",method,Embedding Words and Entity Types,0,68,17,16,0,method : Embedding Words and Entity Types,0.32075471698113206,0.34,0.5333333333333333
named-entity-recognition,6,This configuration ( recommended by the authors ) performs the best in the experiments described in Section 5 .,method,Embedding Words and Entity Types,0,69,18,17,0,method : Embedding Words and Entity Types,0.32547169811320753,0.36,0.5666666666666667
named-entity-recognition,6,"Since FastText learns representations of character ngrams , it has the ability to produce vectors for unknown words .",method,Embedding Words and Entity Types,0,70,19,18,0,method : Embedding Words and Entity Types,0.330188679245283,0.38,0.6
named-entity-recognition,6,"For visualization proposes , we only plot single - word mentions that were annotated in WiFiNE with one of those 6 types .",method,Embedding Words and Entity Types,0,71,20,19,0,method : Embedding Words and Entity Types,0.33490566037735847,0.4,0.6333333333333333
named-entity-recognition,6,Words were randomly and proportionally sampled according to the frequency of each entity type .,method,Embedding Words and Entity Types,0,72,21,20,0,method : Embedding Words and Entity Types,0.33962264150943394,0.42,0.6666666666666666
named-entity-recognition,6,"In addition , words have the color associated with the most frequent type they were annotated within WiFiNE .",method,Embedding Words and Entity Types,0,73,22,21,0,method : Embedding Words and Entity Types,0.3443396226415094,0.44,0.7
named-entity-recognition,6,We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type .,method,Embedding Words and Entity Types,0,74,23,22,0,method : Embedding Words and Entity Types,0.3490566037735849,0.46,0.7333333333333333
named-entity-recognition,6,"For instance , "" firefox "" is close to the type / product / software , while "" enzyme "" is close to the / biology entity type .",method,Embedding Words and Entity Types,0,75,24,23,0,method : Embedding Words and Entity Types,0.35377358490566035,0.48,0.7666666666666667
named-entity-recognition,6,We also notice that words thatare labelled with different types tend to appear between types they were annotated with .,method,Embedding Words and Entity Types,0,76,25,24,0,method : Embedding Words and Entity Types,0.3584905660377358,0.5,0.8
named-entity-recognition,6,"For instance , "" gpx2 "" , which is used both as a software and as a gene , has it s embedding in between / product / software and / biology .",method,Embedding Words and Entity Types,0,77,26,25,0,method : Embedding Words and Entity Types,0.3632075471698113,0.52,0.8333333333333334
named-entity-recognition,6,"We inspected some of the words plotted in , and found that "" jrun "" and "" xp "" are incorrectly labelled as / product / weapon in WiFiNE .",method,Embedding Words and Entity Types,0,78,27,26,0,method : Embedding Words and Entity Types,0.36792452830188677,0.54,0.8666666666666667
named-entity-recognition,6,"But since these words are seen in a software context , their embeddings are closer to the / product / software embedding than the / product / weapon one .",method,Embedding Words and Entity Types,0,79,28,27,0,method : Embedding Words and Entity Types,0.37264150943396224,0.56,0.9
named-entity-recognition,6,"We feel this tolerance to noise is a desirable feature , one that hopefully allows a more efficient use of distant supervision .",method,Embedding Words and Entity Types,0,80,29,28,0,method : Embedding Words and Entity Types,0.37735849056603776,0.58,0.9333333333333333
named-entity-recognition,6,"Last , we also observe the tendency of rare words to cluster around their entity type .",method,Embedding Words and Entity Types,0,81,30,29,0,method : Embedding Words and Entity Types,0.38207547169811323,0.6,0.9666666666666667
named-entity-recognition,6,"For instance , "" iota "" and "" x.org "" are embedded near their respective types , despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE .",method,Embedding Words and Entity Types,0,82,31,30,0,method : Embedding Words and Entity Types,0.3867924528301887,0.62,1.0
named-entity-recognition,6,LS Representation,method,LS Representation,0,83,32,1,0,method : LS Representation,0.3915094339622642,0.64,0.5
named-entity-recognition,6,"This joint vector space only serves the purpose of associating to each word a LS representation , that is , a 120 - dimensional vector where the ith coefficient is a value in the [ ? 1 , + 1 ] interval , equal to the cosine similarity 1 between the word embedding and the embedding of the ith entity type ( we have 120 types ) .",method,LS Representation,0,84,33,2,0,method : LS Representation,0.39622641509433965,0.66,1.0
named-entity-recognition,6,Word,method,Word,0,85,34,1,0,method : Word,0.4009433962264151,0.68,0.09090909090909091
named-entity-recognition,6,Entity .,method,Word,0,86,35,2,0,method : Word,0.4056603773584906,0.7,0.18181818181818182
named-entity-recognition,6,shows the topmost similar entity types for proper names ( left column ) and common words ( right column ) .,method,Word,0,87,36,3,0,method : Word,0.41037735849056606,0.72,0.2727272727272727
named-entity-recognition,6,We observe that ambiguous mentions ( those annotated with several types ) are adequately handled .,method,Word,0,88,37,4,0,method : Word,0.41509433962264153,0.74,0.36363636363636365
named-entity-recognition,6,"For instance , the LS representation of the word "" hilton "" encodes that it more often refers to a hotel or a restaurant than to an actress .",method,Word,0,89,38,5,0,method : Word,0.419811320754717,0.76,0.45454545454545453
named-entity-recognition,6,"Also , we observe that entity words thatare either not or rarely annotated in WiFiNE are still adequately associated with their right type .",method,Word,0,90,39,6,0,method : Word,0.42452830188679247,0.78,0.5454545454545454
named-entity-recognition,6,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .",method,Word,0,91,40,7,0,method : Word,0.42924528301886794,0.8,0.6363636363636364
named-entity-recognition,6,"Interestingly , this mention does not have its page in English Wikipedia .",method,Word,0,92,41,8,0,method : Word,0.4339622641509434,0.82,0.7272727272727273
named-entity-recognition,6,"Furthermore , we observe that non-entity context words have a strong similarity to types they precede or succeed .",method,Word,0,93,42,9,0,method : Word,0.4386792452830189,0.84,0.8181818181818182
named-entity-recognition,6,"For instance the verb "" directed "" is very close to / person / director , an entity type that usually precedes it , and to / art / film , that usually follows it .",method,Word,0,94,43,10,0,method : Word,0.44339622641509435,0.86,0.9090909090909091
named-entity-recognition,6,"Likewise , the preposition "" in "" is near / date and / location / city , which frequently follow "" in "" .",method,Word,0,95,44,11,0,method : Word,0.4481132075471698,0.88,1.0
named-entity-recognition,6,Strength of the LS Representation,method,Strength of the LS Representation,0,96,45,1,0,method : Strength of the LS Representation,0.4528301886792453,0.9,0.16666666666666666
named-entity-recognition,6,"To summarize , we propose a compact lexical representation which is computed offline , therefore incurring no computation burden at test time",method,Strength of the LS Representation,0,97,46,2,0,method : Strength of the LS Representation,0.45754716981132076,0.92,0.3333333333333333
named-entity-recognition,6,"This representation encodes the preference of an entity - mention word for a given type , an information out of reach of binary gazetteer features .",method,Strength of the LS Representation,0,98,47,3,0,method : Strength of the LS Representation,0.46226415094339623,0.94,0.5
named-entity-recognition,6,It also lends itself nicely to the inclusion of lexical features that have been successfully used in earlier feature - based systems .,method,Strength of the LS Representation,0,99,48,4,0,method : Strength of the LS Representation,0.4669811320754717,0.96,0.6666666666666666
named-entity-recognition,6,"Also , because entity types are well represented in WiFiNE , their embeddings are robust :",method,Strength of the LS Representation,0,100,49,5,0,method : Strength of the LS Representation,0.4716981132075472,0.98,0.8333333333333334
named-entity-recognition,6,Our representation does accommodate unfrequent words and seems tolerant to the inherent noise of distant supervision .,method,Strength of the LS Representation,0,101,50,6,0,method : Strength of the LS Representation,0.47641509433962265,1.0,1.0
named-entity-recognition,6,Our NER System,system,Our NER System,0,102,1,1,0,system : Our NER System,0.4811320754716981,0.3333333333333333,0.5
named-entity-recognition,6,"In order to test the efficiency of our lexical feature representation , we implemented a state - of - the - art NER system we now describe .",system,Our NER System,0,103,2,2,0,system : Our NER System,0.4858490566037736,0.6666666666666666,1.0
named-entity-recognition,6,Bi-LSTM- CRF,system,Bi-LSTM-CRF Model,0,104,3,1,0,system : Bi-LSTM-CRF Model,0.49056603773584906,1.0,1.0
named-entity-recognition,6,Model,model,model,0,105,1,1,0,model : model,0.49528301886792453,0.05,0.5
named-entity-recognition,6,"We adopt the popular Bi - LSTM - CRF architecture , a de facto baseline in many sequential tagging tasks .",model,model,0,106,2,2,0,model : model,0.5,0.1,1.0
named-entity-recognition,6,Features,model,Features,0,107,3,1,0,model : Features,0.5047169811320755,0.15,0.3333333333333333
named-entity-recognition,6,"In addition to the LS vector , we incorporate publicly available pre-trained embeddings , as well as character - level , and capitalization features .",model,Features,0,108,4,2,0,model : Features,0.5094339622641509,0.2,0.6666666666666666
named-entity-recognition,6,Those features have been shown to be crucial for stateof - the - art performance .,model,Features,0,109,5,3,0,model : Features,0.5141509433962265,0.25,1.0
named-entity-recognition,6,Word Embeddings,model,Word Embeddings,0,110,6,1,0,model : Word Embeddings,0.5188679245283019,0.3,0.16666666666666666
named-entity-recognition,6,"We experimented with several publicly available word embeddings , such as Senna , Word2 Vec , GloVe , and SSKIP .",model,Word Embeddings,0,111,7,2,0,model : Word Embeddings,0.5235849056603774,0.35,0.3333333333333333
named-entity-recognition,6,We find that the latter performs the best in our experiments .,model,Word Embeddings,0,112,8,3,0,model : Word Embeddings,0.5283018867924528,0.4,0.5
named-entity-recognition,6,SSKIP embeddings are 100 - dimensional case sensitive vectors that where trained using a n-skip - gram model on 42B tokens .,model,Word Embeddings,0,113,9,4,0,model : Word Embeddings,0.5330188679245284,0.45,0.6666666666666666
named-entity-recognition,6,"These embeddings were previously used by , who report good performance on CONLL , and state - of - the - art results on ONTONOTES respectively .",model,Word Embeddings,0,114,10,5,0,model : Word Embeddings,0.5377358490566038,0.5,0.8333333333333334
named-entity-recognition,6,Note that these pre-trained embeddings are adjusted during training .,model,Word Embeddings,0,115,11,6,0,model : Word Embeddings,0.5424528301886793,0.55,1.0
named-entity-recognition,6,Character Embeddings,model,Character Embeddings,0,116,12,1,0,model : Character Embeddings,0.5471698113207547,0.6,0.3333333333333333
named-entity-recognition,6,"Following , we use a forward and a backward LSTM to derive a representation of each word from its characters ( right part of .",model,Character Embeddings,0,117,13,2,0,model : Character Embeddings,0.5518867924528302,0.65,0.6666666666666666
named-entity-recognition,6,"character lookup table is randomly initialized , then trained at the same time as the Bi - LSTM model sketched in Section 4.1 .",model,Character Embeddings,0,118,14,3,0,model : Character Embeddings,0.5566037735849056,0.7,1.0
named-entity-recognition,6,Capitalization Features,model,Capitalization Features,0,119,15,1,0,model : Capitalization Features,0.5613207547169812,0.75,0.3333333333333333
named-entity-recognition,6,"Similarly to previous works , we use capitalization features for characterizing certain categories of capitalization patterns : all Upper , allLower , upperFirst , upperNotFirst , numeric or noAlphaNum .",model,Capitalization Features,0,120,16,2,0,model : Capitalization Features,0.5660377358490566,0.8,0.6666666666666666
named-entity-recognition,6,"We define a random lookup table for these features , and learn its parameters during training .",model,Capitalization Features,0,121,17,3,0,model : Capitalization Features,0.5707547169811321,0.85,1.0
named-entity-recognition,6,LS Vectors,model,LS Vectors,0,122,18,1,0,model : LS Vectors,0.5754716981132075,0.9,0.3333333333333333
named-entity-recognition,6,"Contrarily to previous features , lexical vectors are computed offline and are not adjusted during training .",model,LS Vectors,0,123,19,2,0,model : LS Vectors,0.5801886792452831,0.95,0.6666666666666666
named-entity-recognition,6,"We found useful in practice to apply a MinMax scaler in the range [ ? 1 , + 1 ] to each LS vector we computed ; thus , [.. , 0.095 , .. , 0.20 , .. , 0.76 , .. ] becomes [.. , ? 1 , .. , ? 0.67 , .. , 1 , ..].",model,LS Vectors,0,124,20,3,0,model : LS Vectors,0.5849056603773585,1.0,1.0
named-entity-recognition,6,Experiments,experiment,Experiments,0,125,1,1,0,experiment : Experiments,0.589622641509434,1.0,1.0
named-entity-recognition,6,Data and Evaluation,evaluation,Data and Evaluation,0,126,1,1,0,evaluation : Data and Evaluation,0.5943396226415094,0.08333333333333333,0.08333333333333333
named-entity-recognition,6,We consider two well - established NER benchmarks :,evaluation,Data and Evaluation,0,127,2,2,0,evaluation : Data and Evaluation,0.5990566037735849,0.16666666666666666,0.16666666666666666
named-entity-recognition,6,CONLL-2003 and ONTONOTES 5.0 . provides an overview of the two datasets .,evaluation,Data and Evaluation,0,128,3,3,0,evaluation : Data and Evaluation,0.6037735849056604,0.25,0.25
named-entity-recognition,6,"As we can see , ONTONOTES is much larger .",evaluation,Data and Evaluation,0,129,4,4,0,evaluation : Data and Evaluation,0.6084905660377359,0.3333333333333333,0.3333333333333333
named-entity-recognition,6,"For both datasets , we convert the IOB encoding to BILOU , since found the latter to perform better .",evaluation,Data and Evaluation,0,130,5,5,0,evaluation : Data and Evaluation,0.6132075471698113,0.4166666666666667,0.4166666666666667
named-entity-recognition,6,"In keeping with others , we report mention - level F 1 score using the conlleval script 2 .",evaluation,Data and Evaluation,0,131,6,6,0,evaluation : Data and Evaluation,0.6179245283018868,0.5,0.5
named-entity-recognition,6,The ) is a well known collection of Reuters newswire articles that contains a large portion of sports news .,evaluation,Data and Evaluation,0,132,7,7,0,evaluation : Data and Evaluation,0.6226415094339622,0.5833333333333334,0.5833333333333334
named-entity-recognition,6,"It is annotated with four entity types : Person ( PER ) , Location ( LOC ) , Organization ( ORG ) and Miscellaneous ( MISC ) .",evaluation,Data and Evaluation,0,133,8,8,0,evaluation : Data and Evaluation,0.6273584905660378,0.6666666666666666,0.6666666666666666
named-entity-recognition,6,"The four entity types are fairly evenly distributed , and the train / dev / test datasets present a similar type distribution. , magazine ( 120 k ) , newswire ( 625 k ) , and web data ( 300 k ) .",evaluation,Data and Evaluation,0,134,9,9,0,evaluation : Data and Evaluation,0.6320754716981132,0.75,0.75
named-entity-recognition,6,"This dataset is annotated with 18 entity types , and is much larger than CONLL .",evaluation,Data and Evaluation,0,135,10,10,0,evaluation : Data and Evaluation,0.6367924528301887,0.8333333333333334,0.8333333333333334
named-entity-recognition,6,"Following previous researches , we use the official train / dev / test split of the CoNLL - 2012 shared task .",evaluation,Data and Evaluation,0,136,11,11,0,evaluation : Data and Evaluation,0.6415094339622641,0.9166666666666666,0.9166666666666666
named-entity-recognition,6,"Also , we exclude ( both during training and testing ) the New Testaments portion as it does not contain gold NE annotations .",evaluation,Data and Evaluation,0,137,12,12,0,evaluation : Data and Evaluation,0.6462264150943396,1.0,1.0
named-entity-recognition,6,Training and Implementation,training,Training and Implementation,0,138,1,1,0,training : Training and Implementation,0.6509433962264151,0.030303030303030304,0.03225806451612903
named-entity-recognition,6,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,training,Training and Implementation,1,139,2,2,0,training : Training and Implementation,0.6556603773584906,0.06060606060606061,0.06451612903225806
named-entity-recognition,6,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .",training,Training and Implementation,1,140,3,3,0,training : Training and Implementation,0.660377358490566,0.09090909090909091,0.0967741935483871
named-entity-recognition,6,"More sophisticated optimization algorithms such as AdaDelta or Adam ( Kingma and Ba , 2014 ) converge faster , but none outperformed SGD with exponential learning rate decay in our experiments .",training,Training and Implementation,0,141,4,4,0,training : Training and Implementation,0.6650943396226415,0.12121212121212122,0.12903225806451613
named-entity-recognition,6,Our system uses a single Bi - LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively .,training,Training and Implementation,0,142,5,5,0,training : Training and Implementation,0.6698113207547169,0.15151515151515152,0.16129032258064516
named-entity-recognition,6,"For both models , the character embedding size was set to 25 , and the hidden dimension of the forward and backward character LSTMs are set to 50 .",training,Training and Implementation,0,143,6,6,0,training : Training and Implementation,0.6745283018867925,0.18181818181818182,0.1935483870967742
named-entity-recognition,6,"To mitigate overfitting , we apply a dropout mask with a probability of 0.5 on the input and output vectors of the Bi - LSTM layer .",training,Training and Implementation,0,144,7,7,0,training : Training and Implementation,0.6792452830188679,0.21212121212121213,0.22580645161290322
named-entity-recognition,6,"For both datasets , we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs .",training,Training and Implementation,0,145,8,8,0,training : Training and Implementation,0.6839622641509434,0.24242424242424243,0.25806451612903225
named-entity-recognition,6,"We tuned the hyper - parameters by grid search , and used early stopping based on the performance on the development set .",training,Training and Implementation,0,146,9,9,0,training : Training and Implementation,0.6886792452830188,0.2727272727272727,0.2903225806451613
named-entity-recognition,6,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .",training,Training and Implementation,1,147,10,10,0,training : Training and Implementation,0.6933962264150944,0.30303030303030304,0.3225806451612903
named-entity-recognition,6,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .",training,Training and Implementation,1,148,11,11,0,training : Training and Implementation,0.6981132075471698,0.3333333333333333,0.3548387096774194
named-entity-recognition,6,Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES .,training,Training and Implementation,0,149,12,12,0,training : Training and Implementation,0.7028301886792453,0.36363636363636365,0.3870967741935484
named-entity-recognition,6,shows the development set performance of our final models on each dataset compared to the work of .,training,Training and Implementation,0,150,13,13,0,training : Training and Implementation,0.7075471698113207,0.3939393939393939,0.41935483870967744
named-entity-recognition,6,"The authors use an architecture similar to ours , but use a binary gazetteer feature set , while we use our LS representation .",training,Training and Implementation,0,151,14,14,0,training : Training and Implementation,0.7122641509433962,0.42424242424242425,0.45161290322580644
named-entity-recognition,6,"Since our systems involve random initialization , we report the mean as well as the standard deviation over five runs .",training,Training and Implementation,0,152,15,15,0,training : Training and Implementation,0.7169811320754716,0.45454545454545453,0.4838709677419355
named-entity-recognition,6,"The improvements yielded by our model on the CONLL dataset are significant although modest , while those observed on ONTONOTES are more substantial .",training,Training and Implementation,0,153,16,16,0,training : Training and Implementation,0.7216981132075472,0.48484848484848486,0.5161290322580645
named-entity-recognition,6,We also observe a lower variance of our system over the 5 runs .,training,Training and Implementation,0,154,17,17,0,training : Training and Implementation,0.7264150943396226,0.5151515151515151,0.5483870967741935
named-entity-recognition,6,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .",training,Training and Implementation,1,155,18,18,0,training : Training and Implementation,0.7311320754716981,0.5454545454545454,0.5806451612903226
named-entity-recognition,6,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .",training,Training and Implementation,1,156,19,19,0,training : Training and Implementation,0.7358490566037735,0.5757575757575758,0.6129032258064516
named-entity-recognition,6,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .",training,Training and Implementation,1,157,20,20,0,training : Training and Implementation,0.7405660377358491,0.6060606060606061,0.6451612903225806
named-entity-recognition,6,use three layers of stacked residual RNN ( Bi - LSTM ) with bias decoding .,training,Training and Implementation,0,158,21,21,0,training : Training and Implementation,0.7452830188679245,0.6363636363636364,0.6774193548387096
named-entity-recognition,6,Our model is much simpler and faster .,training,Training and Implementation,0,159,22,22,0,training : Training and Implementation,0.75,0.6666666666666666,0.7096774193548387
named-entity-recognition,6,They report a performance of 90.43 when using an architecture similar to ours .,training,Training and Implementation,0,160,23,23,0,training : Training and Implementation,0.7547169811320755,0.696969696969697,0.7419354838709677
named-entity-recognition,6,The two systems that have slightly higher F 1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark .,training,Training and Implementation,0,161,24,24,0,training : Training and Implementation,0.7594339622641509,0.7272727272727273,0.7741935483870968
named-entity-recognition,6,"They report gains between 0.8 and 1.2 points by using such LM embeddings , which suggests that LS vectors are indeed efficient .",training,Training and Implementation,0,162,25,25,0,training : Training and Implementation,0.7641509433962265,0.7575757575757576,0.8064516129032258
named-entity-recognition,6,"Unfortunately , due to time and resource constraints , 4 we were notable to measure whether both features complement each other .",training,Training and Implementation,0,163,26,26,0,training : Training and Implementation,0.7688679245283019,0.7878787878787878,0.8387096774193549
named-entity-recognition,6,This is left for future investigations .,training,Training and Implementation,0,164,27,27,0,training : Training and Implementation,0.7735849056603774,0.8181818181818182,0.8709677419354839
named-entity-recognition,6,reports the F 1 score of our system compared to the performance reported by others on the ONTONOTES test set .,training,Training and Implementation,0,165,28,28,0,training : Training and Implementation,0.7783018867924528,0.8484848484848485,0.9032258064516129
named-entity-recognition,6,"To the best of our knowledge , we surpass previously reported F 1 scores on this dataset .",training,Training and Implementation,0,166,29,29,0,training : Training and Implementation,0.7830188679245284,0.8787878787878788,0.9354838709677419
named-entity-recognition,6,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .",training,Training and Implementation,1,167,30,30,0,training : Training and Implementation,0.7877358490566038,0.9090909090909091,0.967741935483871
named-entity-recognition,6,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .",training,Training and Implementation,1,168,31,31,0,training : Training and Implementation,0.7924528301886793,0.9393939393939394,1.0
named-entity-recognition,6,Results on the Development Set,training,Results on the Development Set,0,169,32,1,0,training : Results on the Development Set,0.7971698113207547,0.9696969696969697,1.0
named-entity-recognition,6,CONLL,training,CONLL,0,170,33,1,0,training : CONLL,0.8018867924528302,1.0,1.0
named-entity-recognition,6,Results on CONLL,result,Results on CONLL,1,171,1,1,0,result : Results on CONLL,0.8066037735849056,0.5,1.0
named-entity-recognition,6,Results on ONTONOTES,result,Results on ONTONOTES,1,172,2,1,0,result : Results on ONTONOTES,0.8113207547169812,1.0,1.0
named-entity-recognition,6,Model,model,Model,0,173,1,1,0,model : Model,0.8160377358490566,0.2,0.2
named-entity-recognition,6,We also observe that models that use both feature sets significantly outperform other configurations .,model,Model,1,174,2,2,0,model : Model,0.8207547169811321,0.4,0.4
named-entity-recognition,6,"To confirm that the gains came from our feature vector and not from increasing the number of hidden units , we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors .",model,Model,0,175,3,3,0,model : Model,0.8254716981132075,0.6,0.6
named-entity-recognition,6,"We observed a degradation of performance on both datasets , mostly due to overfitting on the training set .",model,Model,0,176,4,4,0,model : Model,0.8301886792452831,0.8,0.8
named-entity-recognition,6,"From those results , we conclude that our lexical representation and the SSKIP one are complementary .",model,Model,0,177,5,5,0,model : Model,0.8349056603773585,1.0,1.0
named-entity-recognition,6,Ablation Results,ablation,Ablation Results,0,178,1,1,0,ablation : Ablation Results,0.839622641509434,0.1111111111111111,0.1111111111111111
named-entity-recognition,6,"In this experiment , we directly compare the LS representation with the SSKIP word - embedding feature set .",ablation,Ablation Results,0,179,2,2,0,ablation : Ablation Results,0.8443396226415094,0.2222222222222222,0.2222222222222222
named-entity-recognition,6,"In order to maintain a high level of performance , both character and capitalization features are used in all configurations .",ablation,Ablation Results,0,180,3,3,0,ablation : Ablation Results,0.8490566037735849,0.3333333333333333,0.3333333333333333
named-entity-recognition,6,"We want to point out that LS vectors are not adapted during training , contrarily to the SSKIP embeddings .",ablation,Ablation Results,0,181,4,4,0,ablation : Ablation Results,0.8537735849056604,0.4444444444444444,0.4444444444444444
named-entity-recognition,6,"Similarly to Section 5.3 , we report in , for each feature configuration , the average F 1 score as well as the standard deviation over five runs .",ablation,Ablation Results,0,182,5,5,0,ablation : Ablation Results,0.8584905660377359,0.5555555555555556,0.5555555555555556
named-entity-recognition,6,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .",ablation,Ablation Results,1,183,6,6,0,ablation : Ablation Results,0.8632075471698113,0.6666666666666666,0.6666666666666666
named-entity-recognition,6,"The difference is not has high as we first expected , especially since the SSKIP model is adjusted during training , while our representation is not .",ablation,Ablation Results,0,184,7,7,0,ablation : Ablation Results,0.8679245283018868,0.7777777777777778,0.7777777777777778
named-entity-recognition,6,"Still , LS vectors seem to encode a large portion of the information needed to model the NER task .",ablation,Ablation Results,0,185,8,8,0,ablation : Ablation Results,0.8726415094339622,0.8888888888888888,0.8888888888888888
named-entity-recognition,6,"Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",ablation,Ablation Results,0,186,9,9,0,ablation : Ablation Results,0.8773584905660378,1.0,1.0
named-entity-recognition,6,Related Works,related work,Related Works,0,187,1,1,0,related work : Related Works,0.8820754716981132,0.05555555555555555,0.05555555555555555
named-entity-recognition,6,"Traditional approaches to NER , like CRF - based and Perceptron - based systems ( Ratinov and Roth , 2009 ) have dominated the field for over a decade .",related work,Related Works,0,188,2,2,0,related work : Related Works,0.8867924528301887,0.1111111111111111,0.1111111111111111
named-entity-recognition,6,They rely heavily on hand - engineered features and external resources such as gazetteers .,related work,Related Works,0,189,3,3,0,related work : Related Works,0.8915094339622641,0.16666666666666666,0.16666666666666666
named-entity-recognition,6,One major drawback of such an approach is its weak generalization power .,related work,Related Works,0,190,4,4,0,related work : Related Works,0.8962264150943396,0.2222222222222222,0.2222222222222222
named-entity-recognition,6,"Current state - of - the art systems use a combination of Convolutional Neural Networks ( CNNs ) , Bi - LSTMs , along with a CRF decoder .",related work,Related Works,0,191,5,5,0,related work : Related Works,0.9009433962264151,0.2777777777777778,0.2777777777777778
named-entity-recognition,6,"CNNs are used to encode character - level features ( prefix and suffix ) , while LSTM is used to encode word - level features .",related work,Related Works,0,192,6,6,0,related work : Related Works,0.9056603773584906,0.3333333333333333,0.3333333333333333
named-entity-recognition,6,"Finally , a CRF is placed on top of those models in order to decode the best tag sequence .",related work,Related Works,0,193,7,7,0,related work : Related Works,0.910377358490566,0.3888888888888889,0.3888888888888889
named-entity-recognition,6,Pre-trained embeddings obtained by unsupervised learning are core features of those models .,related work,Related Works,0,194,8,8,0,related work : Related Works,0.9150943396226415,0.4444444444444444,0.4444444444444444
named-entity-recognition,6,"In this work , we show that deep NN architectures can also benefit from lexical features , at least when encoded in the compact form we propose .",related work,Related Works,0,195,9,9,0,related work : Related Works,0.9198113207547169,0.5,0.5
named-entity-recognition,6,and propose an alternative approach different from ours .,related work,Related Works,0,196,10,10,0,related work : Related Works,0.9245283018867925,0.5555555555555556,0.5555555555555556
named-entity-recognition,6,They incorporate LM embeddings that were pre-trained on a large unlabelled corpus as features for NER .,related work,Related Works,0,197,11,11,0,related work : Related Works,0.9292452830188679,0.6111111111111112,0.6111111111111112
named-entity-recognition,6,These embeddings allow to generate a representation for a word depending on its context .,related work,Related Works,0,198,12,12,0,related work : Related Works,0.9339622641509434,0.6666666666666666,0.6666666666666666
named-entity-recognition,6,"For instance , the LM embeddings of the word France in "" France is a developed country "" is different than that in "" Anatole France began his literary career "" .",related work,Related Works,0,199,13,13,0,related work : Related Works,0.9386792452830188,0.7222222222222222,0.7222222222222222
named-entity-recognition,6,Such embeddings are trained on very large amount of texts .,related work,Related Works,0,200,14,14,0,related work : Related Works,0.9433962264150944,0.7777777777777778,0.7777777777777778
named-entity-recognition,6,"Our feature set is crafted from distant supervision applied to Wikipedia , a much less time - consuming process which we showed to be nevertheless adapted to rare words .",related work,Related Works,0,201,15,15,0,related work : Related Works,0.9481132075471698,0.8333333333333334,0.8333333333333334
named-entity-recognition,6,Chiu and Nichols ( 2016 ) used gazetteer features in order to establish state - of - the - art performance on both CONLL and ONTONOTES .,related work,Related Works,0,202,16,16,0,related work : Related Works,0.9528301886792453,0.8888888888888888,0.8888888888888888
named-entity-recognition,6,They mined DBPedia in order to compile 4 lists of named - entities that contain over 2.3 M entries .,related work,Related Works,0,203,17,17,0,related work : Related Works,0.9575471698113207,0.9444444444444444,0.9444444444444444
named-entity-recognition,6,We show that LS representations outperform their gazetteer features .,related work,Related Works,0,204,18,18,0,related work : Related Works,0.9622641509433962,1.0,1.0
named-entity-recognition,6,Conclusion and Future Work,conclusion,Conclusion and Future Work,0,205,1,1,0,conclusion : Conclusion and Future Work,0.9669811320754716,0.125,0.125
named-entity-recognition,6,We have explored the idea of generating lexical features for NER out of Wikipedia data automatically annotated with fine - grained entity types .,conclusion,Conclusion and Future Work,0,206,2,2,0,conclusion : Conclusion and Future Work,0.9716981132075472,0.25,0.25
named-entity-recognition,6,"We used WiFiNE , a Wikipedia dump annotated with fine entity type mentions , for training a vector space that jointly embeds words and named - entities .",conclusion,Conclusion and Future Work,0,207,3,3,0,conclusion : Conclusion and Future Work,0.9764150943396226,0.375,0.375
named-entity-recognition,6,"This vector space is used to compute a 120 dimensional vector per word , which encodes the similarity of the word to each of the entity types .",conclusion,Conclusion and Future Work,0,208,4,4,0,conclusion : Conclusion and Future Work,0.9811320754716981,0.5,0.5
named-entity-recognition,6,"Our results show that our proposed lexical representation , even though it is not adjusted at training time , matches state - of - the - art results compared to more complex approaches on the well - studied CONLL dataset , and delivers a new state - of - the - art F 1 score of 87.95 on the more diversified ONTONOTES dataset .",conclusion,Conclusion and Future Work,0,209,5,5,0,conclusion : Conclusion and Future Work,0.9858490566037735,0.625,0.625
named-entity-recognition,6,We further observe larger gains on collections with more unfrequent words .,conclusion,Conclusion and Future Work,0,210,6,6,0,conclusion : Conclusion and Future Work,0.9905660377358491,0.75,0.75
named-entity-recognition,6,"The source code and the data we used in this work are publicly available at http://rali.iro. umontreal.ca/rali/en/wikipedia-lex-sim , with the hope that other researchers will report gains , when using our lexical representation .",conclusion,Conclusion and Future Work,0,211,7,7,0,conclusion : Conclusion and Future Work,0.9952830188679245,0.875,0.875
named-entity-recognition,6,"As a future work , we want to investigate the usefulness of our LS feature representation on other NER tasks , including NER in tweets where out - of - vocabulary and low - frequency words represent a challenge ; as well as finer - grained NER which suffers from the lack of manually annotated training data .",conclusion,Conclusion and Future Work,0,212,8,8,0,conclusion : Conclusion and Future Work,1.0,1.0,1.0
named-entity-recognition,7,A Neural Transition - based Model for Nested Mention Recognition,title,title,1,2,1,1,0,title : title,0.012658227848101266,1.0,1.0
named-entity-recognition,7,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.0189873417721519,0.125,0.125
named-entity-recognition,7,It is common that entity mentions can contain other mentions recursively .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.02531645569620253,0.25,0.25
named-entity-recognition,7,This paper introduces a scalable transition - based method to model the nested structure of mentions .,abstract,abstract,1,5,3,3,0,abstract : abstract,0.03164556962025317,0.375,0.375
named-entity-recognition,7,We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.0379746835443038,0.5,0.5
named-entity-recognition,7,Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.04430379746835443,0.625,0.625
named-entity-recognition,7,"Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.05063291139240506,0.75,0.75
named-entity-recognition,7,"Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.056962025316455694,0.875,0.875
named-entity-recognition,7,1,abstract,abstract,0,10,8,8,0,abstract : abstract,0.06329113924050633,1.0,1.0
named-entity-recognition,7,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.06962025316455696,0.05555555555555555,0.05555555555555555
named-entity-recognition,7,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.0759493670886076,0.1111111111111111,0.1111111111111111
named-entity-recognition,7,"Practically , the mentions with nested structures frequently exist in news and biomedical documents .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.08227848101265822,0.16666666666666666,0.16666666666666666
named-entity-recognition,7,"For example in Traditional sequence labeling models such as conditional random fields ( CRF ) do not allow hierarchical structures between segments , making them incapable to handle such problems .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.08860759493670886,0.2222222222222222,0.2222222222222222
named-entity-recognition,7,presented a chart - based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree .,introduction,introduction,0,15,5,5,0,introduction : introduction,0.0949367088607595,0.2777777777777778,0.2777777777777778
named-entity-recognition,7,The issue of using a chart - based parser is its cubic time complexity in the number of words in the sentence .,introduction,introduction,0,16,6,6,0,introduction : introduction,0.10126582278481013,0.3333333333333333,0.3333333333333333
named-entity-recognition,7,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .",introduction,introduction,1,17,7,7,0,introduction : introduction,0.10759493670886076,0.3888888888888889,0.3888888888888889
named-entity-recognition,7,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",introduction,introduction,1,18,8,8,0,introduction : introduction,0.11392405063291139,0.4444444444444444,0.4444444444444444
named-entity-recognition,7,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,introduction,introduction,1,19,9,9,0,introduction : introduction,0.12025316455696203,0.5,0.5
named-entity-recognition,7,shows an example of such a forest .,introduction,introduction,0,20,10,10,0,introduction : introduction,0.12658227848101267,0.5555555555555556,0.5555555555555556
named-entity-recognition,7,"In contrast , the tree structure by further uses a root node to connect all tree elements .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.13291139240506328,0.6111111111111112,0.6111111111111112
named-entity-recognition,7,Our forest representation eliminates the root node so that the number of actions required to construct it can be reduced significantly .,introduction,introduction,0,22,12,12,0,introduction : introduction,0.13924050632911392,0.6666666666666666,0.6666666666666666
named-entity-recognition,7,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .",introduction,introduction,1,23,13,13,0,introduction : introduction,0.14556962025316456,0.7222222222222222,0.7222222222222222
named-entity-recognition,7,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,introduction,introduction,1,24,14,14,0,introduction : introduction,0.1518987341772152,0.7777777777777778,0.7777777777777778
named-entity-recognition,7,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .",introduction,introduction,1,25,15,15,0,introduction : introduction,0.15822784810126583,0.8333333333333334,0.8333333333333334
named-entity-recognition,7,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .",introduction,introduction,1,26,16,16,0,introduction : introduction,0.16455696202531644,0.8888888888888888,0.8888888888888888
named-entity-recognition,7,We conduct experiments in three standard datasets .,introduction,introduction,0,27,17,17,0,introduction : introduction,0.17088607594936708,0.9444444444444444,0.9444444444444444
named-entity-recognition,7,Our system achieves the state - of - the - art performance on ACE datasets and comparable performance in GENIA dataset .,introduction,introduction,0,28,18,18,0,introduction : introduction,0.17721518987341772,1.0,1.0
named-entity-recognition,7,Related Work,related work,Related Work,0,29,1,1,0,related work : Related Work,0.18354430379746836,0.06666666666666667,0.06666666666666667
named-entity-recognition,7,Entity mention recognition with nested structures has been explored first with rule - based approaches where the authors first detected the innermost mentions and then relied on rule - based postprocessing methods to identify outer mentions .,related work,Related Work,0,30,2,2,0,related work : Related Work,0.189873417721519,0.13333333333333333,0.13333333333333333
named-entity-recognition,7,proposed a structured multi-label model to represent overlapping segments in a sentence .,related work,Related Work,0,31,3,3,0,related work : Related Work,0.1962025316455696,0.2,0.2
named-entity-recognition,7,but it came with a cubic time complexity in the number of words .,related work,Related Work,0,32,4,4,0,related work : Related Work,0.20253164556962025,0.26666666666666666,0.26666666666666666
named-entity-recognition,7,proposed several ways to combine multiple conditional random fields ( CRF ) for such tasks .,related work,Related Work,0,33,5,5,0,related work : Related Work,0.2088607594936709,0.3333333333333333,0.3333333333333333
named-entity-recognition,7,Their best results were obtained by cascading several CRF models in a specific order while each model is responsible for detecting mentions of a particular type .,related work,Related Work,0,34,6,6,0,related work : Related Work,0.21518987341772153,0.4,0.4
named-entity-recognition,7,"However , such an approach can not model nested mentions of the same type , which frequently appear .",related work,Related Work,0,35,7,7,0,related work : Related Work,0.22151898734177214,0.4666666666666667,0.4666666666666667
named-entity-recognition,7,and proposed new representations of mention hypergraph and mention separator to model overlapping mentions .,related work,Related Work,0,36,8,8,0,related work : Related Work,0.22784810126582278,0.5333333333333333,0.5333333333333333
named-entity-recognition,7,"However , the nested structure is not guaranteed in such approaches since overlapping structures additionally include the crossing structures 3 , which rarely exist in practice .",related work,Related Work,0,37,9,9,0,related work : Related Work,0.23417721518987342,0.6,0.6
named-entity-recognition,7,"Also , their representations did not model the dependencies between nested mentions explicitly , which may limit their performance .",related work,Related Work,0,38,10,10,0,related work : Related Work,0.24050632911392406,0.6666666666666666,0.6666666666666666
named-entity-recognition,7,"In contrast , the chart - based parsing method can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities .",related work,Related Work,0,39,11,11,0,related work : Related Work,0.2468354430379747,0.7333333333333333,0.7333333333333333
named-entity-recognition,7,"However , their cubic time complexity makes them not scalable to large datasets .",related work,Related Work,0,40,12,12,0,related work : Related Work,0.25316455696202533,0.8,0.8
named-entity-recognition,7,"As neural network based approaches are proven effective in entity or mention recognition , recent efforts focus on incorporating neural components for recognizing nested mentions .",related work,Related Work,0,41,13,13,0,related work : Related Work,0.25949367088607594,0.8666666666666667,0.8666666666666667
named-entity-recognition,7,"dynamically stacked multiple LSTM - CRF layers , detecting mentions in an inside - out manner until no outer entities are extracted .",related work,Related Work,0,42,14,14,0,related work : Related Work,0.26582278481012656,0.9333333333333333,0.9333333333333333
named-entity-recognition,7,used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme .,related work,Related Work,0,43,15,15,0,related work : Related Work,0.2721518987341772,1.0,1.0
named-entity-recognition,7,Model,model,Model,0,44,1,1,0,model : Model,0.27848101265822783,0.014705882352941176,0.2
named-entity-recognition,7,"Specifically , given a sequence of words {x 0 , x 1 , . . . , x n } , the goal of our system is to output a set of mentions where nested structures are allowed .",model,Model,0,45,2,2,0,model : Model,0.2848101265822785,0.029411764705882353,0.4
named-entity-recognition,7,"We use the forest structure to model the nested mentions scattered in a sentence , as shown in .",model,Model,0,46,3,3,0,model : Model,0.2911392405063291,0.04411764705882353,0.6
named-entity-recognition,7,The mapping is straightforward : each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree .,model,Model,0,47,4,4,0,model : Model,0.2974683544303797,0.058823529411764705,0.8
named-entity-recognition,7,4,model,Model,0,48,5,5,0,model : Model,0.3037974683544304,0.07352941176470588,1.0
named-entity-recognition,7,Shift - Reduce System,model,Shift-Reduce System,0,49,6,1,0,model : Shift-Reduce System,0.310126582278481,0.08823529411764706,0.04
named-entity-recognition,7,"Our transition - based model is based on the shiftreduce parser for constituency parsing ( Watan - abe and Sumita , 2015 ) , which adopts .",model,Shift-Reduce System,0,50,7,2,0,model : Shift-Reduce System,0.31645569620253167,0.10294117647058823,0.08
named-entity-recognition,7,"Generally , our system employs a stack to store ( partially ) processed nested elements .",model,Shift-Reduce System,0,51,8,3,0,model : Shift-Reduce System,0.3227848101265823,0.11764705882352941,0.12
named-entity-recognition,7,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",model,Shift-Reduce System,0,52,9,4,0,model : Shift-Reduce System,0.3291139240506329,0.1323529411764706,0.16
named-entity-recognition,7,In each step .,model,Shift-Reduce System,0,53,10,5,0,model : Shift-Reduce System,0.33544303797468356,0.14705882352941177,0.2
named-entity-recognition,7,an action is applied to change the system 's state .,model,Shift-Reduce System,0,54,11,6,0,model : Shift-Reduce System,0.34177215189873417,0.16176470588235295,0.24
named-entity-recognition,7,"Our system consists of three types of transition actions , which are also summarized in :",model,Shift-Reduce System,0,55,12,7,0,model : Shift-Reduce System,0.34810126582278483,0.17647058823529413,0.28
named-entity-recognition,7,SHIFT pushes the next word from buffer to the stack .,model,Shift-Reduce System,0,56,13,8,0,model : Shift-Reduce System,0.35443037974683544,0.19117647058823528,0.32
named-entity-recognition,7,REDUCE - X pops the top two items t 0 and t 1 from the tack and combines them as a new tree element { X ? t 0 t 1 } which is then pushed onto the stack .,model,Shift-Reduce System,0,57,14,9,0,model : Shift-Reduce System,0.36075949367088606,0.20588235294117646,0.36
named-entity-recognition,7,UNARY - X pops the top item t 0 from the stack and constructs a new tree element { X ? t 0 } which is pushed back to the stack .,model,Shift-Reduce System,0,58,15,10,0,model : Shift-Reduce System,0.3670886075949367,0.22058823529411764,0.4
named-entity-recognition,7,"Since the shift - reduce system assumes unary and binary branching , we binarize the trees in each forest in a left - branching manner .",model,Shift-Reduce System,0,59,16,11,0,model : Shift-Reduce System,0.37341772151898733,0.23529411764705882,0.44
named-entity-recognition,7,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ? { P erson * ? A , B} , C} where P erson * is a temporary label for P erson .",model,Shift-Reduce System,0,60,17,12,0,model : Shift-Reduce System,0.379746835443038,0.25,0.48
named-entity-recognition,7,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ? { P erson * ? A , B} , C} where P erson * is a temporary label for P erson .",model,Shift-Reduce System,0,61,18,13,0,model : Shift-Reduce System,0.3860759493670886,0.2647058823529412,0.52
named-entity-recognition,7,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ? { P erson * ? A , B} , C} where P erson * is a temporary label for P erson .",model,Shift-Reduce System,0,62,19,14,0,model : Shift-Reduce System,0.3924050632911392,0.27941176470588236,0.56
named-entity-recognition,7,"Hence , the X in reduce - actions will also include such temporary labels .",model,Shift-Reduce System,0,63,20,15,0,model : Shift-Reduce System,0.3987341772151899,0.29411764705882354,0.6
named-entity-recognition,7,"Note that since most words are not contained in any mention , they are only shifted to the stack and not involved in any reduce - or unary - actions .",model,Shift-Reduce System,0,64,21,16,0,model : Shift-Reduce System,0.4050632911392405,0.3088235294117647,0.64
named-entity-recognition,7,An example sequence of transitions can be found in .,model,Shift-Reduce System,0,65,22,17,0,model : Shift-Reduce System,0.41139240506329117,0.3235294117647059,0.68
named-entity-recognition,7,Our shift - reduce system is different from previous parsers in terms of the terminal state .,model,Shift-Reduce System,0,66,23,18,0,model : Shift-Reduce System,0.4177215189873418,0.3382352941176471,0.72
named-entity-recognition,7,) It does not require the terminal stack to be a rooted tree .,model,Shift-Reduce System,0,67,24,19,0,model : Shift-Reduce System,0.4240506329113924,0.35294117647058826,0.76
named-entity-recognition,7,"Instead , the final stack should be a forest consisting of multiple nested elements with tree structures .",model,Shift-Reduce System,0,68,25,20,0,model : Shift-Reduce System,0.43037974683544306,0.36764705882352944,0.8
named-entity-recognition,7,") To conveniently determine the ending of our transition process , we add an auxiliary symbol $ to each sentence .",model,Shift-Reduce System,0,69,26,21,0,model : Shift-Reduce System,0.43670886075949367,0.38235294117647056,0.84
named-entity-recognition,7,"Once it is pushed to the stack , it implies that all deductions of actual words are finished .",model,Shift-Reduce System,0,70,27,22,0,model : Shift-Reduce System,0.4430379746835443,0.39705882352941174,0.88
named-entity-recognition,7,"Since we do not allow unary rules between labels like X1 ? X2 , the length of maximal action sequence is 3 n .",model,Shift-Reduce System,0,71,28,23,0,model : Shift-Reduce System,0.44936708860759494,0.4117647058823529,0.92
named-entity-recognition,7,"Since we do not allow unary rules between labels like X1 ? X2 , the length of maximal action sequence is 3 n .",model,Shift-Reduce System,0,72,29,24,0,model : Shift-Reduce System,0.45569620253164556,0.4264705882352941,0.96
named-entity-recognition,7,5,model,Shift-Reduce System,0,73,30,25,0,model : Shift-Reduce System,0.4620253164556962,0.4411764705882353,1.0
named-entity-recognition,7,Action Constraints,model,Action Constraints,0,74,31,1,0,model : Action Constraints,0.46835443037974683,0.45588235294117646,0.09090909090909091
named-entity-recognition,7,"To make sure that each action sequence is valid , we need to make some hard constraints on the ac - 5",model,Action Constraints,0,75,32,2,0,model : Action Constraints,0.47468354430379744,0.47058823529411764,0.18181818181818182
named-entity-recognition,7,"In this case , each word is shifted ( n ) and involved in a unary action ( n ) .",model,Action Constraints,0,76,33,3,0,model : Action Constraints,0.4810126582278481,0.4852941176470588,0.2727272727272727
named-entity-recognition,7,Then all elements are reduced to a single node ( n ? 1 ) .,model,Action Constraints,0,77,34,4,0,model : Action Constraints,0.4873417721518987,0.5,0.36363636363636365
named-entity-recognition,7,The last action is to shift the symbol $. tion to take .,model,Action Constraints,0,78,35,5,0,model : Action Constraints,0.4936708860759494,0.5147058823529411,0.45454545454545453
named-entity-recognition,7,"For example , reduce - action can only be conducted when there are at least two elements in the stack .",model,Action Constraints,0,79,36,6,0,model : Action Constraints,0.5,0.5294117647058824,0.5454545454545454
named-entity-recognition,7,Please see the Appendix for the full list of restrictions .,model,Action Constraints,0,80,37,7,0,model : Action Constraints,0.5063291139240507,0.5441176470588235,0.6363636363636364
named-entity-recognition,7,"Formally , we use V(S , i , A ) to denote the valid actions given the parser state .",model,Action Constraints,0,81,38,8,0,model : Action Constraints,0.5126582278481012,0.5588235294117647,0.7272727272727273
named-entity-recognition,7,Let us denote the feature vector for the parser state at time step k asp k .,model,Action Constraints,0,82,39,9,0,model : Action Constraints,0.5189873417721519,0.5735294117647058,0.8181818181818182
named-entity-recognition,7,The distribution of actions is computed as follows :,model,Action Constraints,0,83,40,10,0,model : Action Constraints,0.5253164556962026,0.5882352941176471,0.9090909090909091
named-entity-recognition,7,"1 ) where w z is a column weight vector for action z , and b z is a bias term .",model,Action Constraints,0,84,41,11,0,model : Action Constraints,0.5316455696202531,0.6029411764705882,1.0
named-entity-recognition,7,Neural Transition - based Model,model,Neural Transition-based Model,0,85,42,1,0,model : Neural Transition-based Model,0.5379746835443038,0.6176470588235294,0.5
named-entity-recognition,7,"We use neural networks to learn the representation of the parser state , which is pk in ( 1 ) .",model,Neural Transition-based Model,0,86,43,2,0,model : Neural Transition-based Model,0.5443037974683544,0.6323529411764706,1.0
named-entity-recognition,7,Representation of Words,model,Representation of Words,0,87,44,1,0,model : Representation of Words,0.5506329113924051,0.6470588235294118,0.2
named-entity-recognition,7,Words are represented by concatenating three vectors :,model,Representation of Words,0,88,45,2,0,model : Representation of Words,0.5569620253164557,0.6617647058823529,0.4
named-entity-recognition,7,where e w i and e pi denote the embeddings for i - th word and it s POS tag respectively .,model,Representation of Words,0,89,46,3,0,model : Representation of Words,0.5632911392405063,0.6764705882352942,0.6
named-entity-recognition,7,cw i denotes the representation learned by a character - level model using a bidirectional LSTM .,model,Representation of Words,0,90,47,4,0,model : Representation of Words,0.569620253164557,0.6911764705882353,0.8
named-entity-recognition,7,"Specifically , for character sequence s 0 , s 1 , . . . , s n in the i - th word , we use the last hidden states of forward and backward LSTM as the character - based representation of this word , as shown below :",model,Representation of Words,0,91,48,5,0,model : Representation of Words,0.5759493670886076,0.7058823529411765,1.0
named-entity-recognition,7,Representation of Parser States,model,Representation of Parser States,0,92,49,1,0,model : Representation of Parser States,0.5822784810126582,0.7205882352941176,0.05
named-entity-recognition,7,"Generally , the buffer and action history are encoded using two vanilla LSTMs .",model,Representation of Parser States,0,93,50,2,0,model : Representation of Parser States,0.5886075949367089,0.7352941176470589,0.1
named-entity-recognition,7,"For the stack that involves popping out top elements , we use the Stack - LSTM to efficiently encode it .",model,Representation of Parser States,0,94,51,3,0,model : Representation of Parser States,0.5949367088607594,0.75,0.15
named-entity-recognition,7,"Formally , if the unprocessed word sequence in the buffer is x i , x i +1 , . . . , x n and action history sequence is a 0 , a 1 , . . . , a k?1 , then we can compute buffer representation bk and action history representation a k at time step k as follows :",model,Representation of Parser States,0,95,52,4,0,model : Representation of Parser States,0.6012658227848101,0.7647058823529411,0.2
named-entity-recognition,7,where each action is also mapped to a distributed representation ea .,model,Representation of Parser States,0,96,53,5,0,model : Representation of Parser States,0.6075949367088608,0.7794117647058824,0.25
named-entity-recognition,7,"For the state of the stack , we also use an LSTM to encode a sequence of tree elements .",model,Representation of Parser States,0,97,54,6,0,model : Representation of Parser States,0.6139240506329114,0.7941176470588235,0.3
named-entity-recognition,7,"However , the top elements of the stack are updated frequently .",model,Representation of Parser States,0,98,55,7,0,model : Representation of Parser States,0.620253164556962,0.8088235294117647,0.35
named-entity-recognition,7,Stack - LSTM provides an efficient implementation that incorporates a stackpointer .,model,Representation of Parser States,0,99,56,8,0,model : Representation of Parser States,0.6265822784810127,0.8235294117647058,0.4
named-entity-recognition,7,7,model,Representation of Parser States,0,100,57,9,0,model : Representation of Parser States,0.6329113924050633,0.8382352941176471,0.45
named-entity-recognition,7,"Formally , the state of the stack bk at time step k is computed as :",model,Representation of Parser States,0,101,58,10,0,model : Representation of Parser States,0.6392405063291139,0.8529411764705882,0.5
named-entity-recognition,7,"where ht i denotes the representation of the i - th tree element from the top , which can be computed recursively similar to Recursive Neural Network as follows :",model,Representation of Parser States,0,102,59,11,0,model : Representation of Parser States,0.6455696202531646,0.8676470588235294,0.55
named-entity-recognition,7,"where W u , l and W b , l denote the weight matrices for unary ( u ) and binary ( b ) composition with parent node being label ( l ) .",model,Representation of Parser States,0,103,60,12,0,model : Representation of Parser States,0.6518987341772152,0.8823529411764706,0.6
named-entity-recognition,7,Note that the composition function is distinct for each label l .,model,Representation of Parser States,0,104,61,13,0,model : Representation of Parser States,0.6582278481012658,0.8970588235294118,0.65
named-entity-recognition,7,Recall that the leaf nodes of each tree element are raw words .,model,Representation of Parser States,0,105,62,14,0,model : Representation of Parser States,0.6645569620253164,0.9117647058823529,0.7
named-entity-recognition,7,"Instead of representing them with their original embeddings introduced in Section 3.3 , we found that 6 Note that LSTM b runs in a right - to - left order such that the output can represent the contextual information of x i.",model,Representation of Parser States,0,106,63,15,0,model : Representation of Parser States,0.6708860759493671,0.9264705882352942,0.75
named-entity-recognition,7,7,model,Representation of Parser States,0,107,64,16,0,model : Representation of Parser States,0.6772151898734177,0.9411764705882353,0.8
named-entity-recognition,7,Please refer to for details .,model,Representation of Parser States,0,108,65,17,0,model : Representation of Parser States,0.6835443037974683,0.9558823529411765,0.85
named-entity-recognition,7,concatenating the buffer state in ( 5 ) are beneficial during our initial experiments .,model,Representation of Parser States,0,109,66,18,0,model : Representation of Parser States,0.689873417721519,0.9705882352941176,0.9
named-entity-recognition,7,"Formally , when a word xi is shifted to the stack at time step k , it s representation is computed as :",model,Representation of Parser States,0,110,67,19,0,model : Representation of Parser States,0.6962025316455697,0.9852941176470589,0.95
named-entity-recognition,7,"Finally , the state of the system pk is the concatenation of the states of buffer , stack and action history :",model,Representation of Parser States,0,111,68,20,0,model : Representation of Parser States,0.7025316455696202,1.0,1.0
named-entity-recognition,7,Training,training,training,0,112,1,1,0,training : training,0.7088607594936709,0.2,0.2
named-entity-recognition,7,We employ the greedy strategy to maximize the log -likelihood of the local action classifier in ( 1 ) .,training,training,0,113,2,2,0,training : training,0.7151898734177216,0.4,0.4
named-entity-recognition,7,"Specifically , let z ik denote the k - th action for the i - th sentence , the loss function with 2 norm is :",training,training,0,114,3,3,0,training : training,0.7215189873417721,0.6,0.6
named-entity-recognition,7,where ? is the 2 coefficient .,training,training,0,115,4,4,0,training : training,0.7278481012658228,0.8,0.8
named-entity-recognition,7,where ? is the 2 coefficient .,training,training,0,116,5,5,0,training : training,0.7341772151898734,1.0,1.0
named-entity-recognition,7,Experiments,experiment,Experiments,0,117,1,1,0,experiment : Experiments,0.740506329113924,0.07142857142857142,0.2
named-entity-recognition,7,"We mainly evaluate our models on the standard ACE - 04 , , and GENIA datasets with the same splits used by previous research efforts .",experiment,Experiments,0,118,2,2,0,experiment : Experiments,0.7468354430379747,0.14285714285714285,0.4
named-entity-recognition,7,"In ACE datasets , more than 40 % of the mentions form nested structures with some other mention .",experiment,Experiments,0,119,3,3,0,experiment : Experiments,0.7531645569620253,0.21428571428571427,0.6
named-entity-recognition,7,"In GENIA , this number is 18 % .",experiment,Experiments,0,120,4,4,0,experiment : Experiments,0.759493670886076,0.2857142857142857,0.8
named-entity-recognition,7,Please see for the full statistics .,experiment,Experiments,0,121,5,5,0,experiment : Experiments,0.7658227848101266,0.35714285714285715,1.0
named-entity-recognition,7,Setup,experiment,Setup,0,122,6,1,0,experiment : Setup,0.7721518987341772,0.42857142857142855,0.1111111111111111
named-entity-recognition,7,Pre-trained embeddings,experiment,Setup,1,123,7,2,0,experiment : Setup,0.7784810126582279,0.5,0.2222222222222222
named-entity-recognition,7,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,experiment,Setup,1,124,8,3,0,experiment : Setup,0.7848101265822784,0.5714285714285714,0.3333333333333333
named-entity-recognition,7,The embeddings of POS tags are initialized randomly with dimension 32 .,experiment,Setup,1,125,9,4,0,experiment : Setup,0.7911392405063291,0.6428571428571429,0.4444444444444444
named-entity-recognition,7,The model is trained using Adam and a gradient clipping of 3.0 .,experiment,Setup,1,126,10,5,0,experiment : Setup,0.7974683544303798,0.7142857142857143,0.5555555555555556
named-entity-recognition,7,Early stopping is used based on the performance of development sets .,experiment,Setup,0,127,11,6,0,experiment : Setup,0.8037974683544303,0.7857142857142857,0.6666666666666666
named-entity-recognition,7,Dropout is used after the input layer .,experiment,Setup,0,128,12,7,0,experiment : Setup,0.810126582278481,0.8571428571428571,0.7777777777777778
named-entity-recognition,7,The 2 coefficient ? is also tuned during development process .,experiment,Setup,0,129,13,8,0,experiment : Setup,0.8164556962025317,0.9285714285714286,0.8888888888888888
named-entity-recognition,7,The 2 coefficient ? is also tuned during development process .,experiment,Setup,0,130,14,9,0,experiment : Setup,0.8227848101265823,1.0,1.0
named-entity-recognition,7,Results,result,Results,0,131,1,1,0,result : Results,0.8291139240506329,0.0625,0.2
named-entity-recognition,7,The main results are reported in .,result,Results,0,132,2,2,0,result : Results,0.8354430379746836,0.125,0.4
named-entity-recognition,7,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,result,Results,1,133,3,3,0,result : Results,0.8417721518987342,0.1875,0.6
named-entity-recognition,7,We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets .,result,Results,0,134,4,4,0,result : Results,0.8481012658227848,0.25,0.8
named-entity-recognition,7,"To verify this , we design an experiment to evaluate how well a system can recognize nested mentions .",result,Results,0,135,5,5,0,result : Results,0.8544303797468354,0.3125,1.0
named-entity-recognition,7,Handling Nested Mentions,result,Handling Nested Mentions,0,136,6,1,0,result : Handling Nested Mentions,0.8607594936708861,0.375,0.16666666666666666
named-entity-recognition,7,The idea is that we split the test data into two portions : sentences with and without nested mentions .,result,Handling Nested Mentions,0,137,7,2,0,result : Handling Nested Mentions,0.8670886075949367,0.4375,0.3333333333333333
named-entity-recognition,7,The results of GENIA are listed in .,result,Handling Nested Mentions,0,138,8,3,0,result : Handling Nested Mentions,0.8734177215189873,0.5,0.5
named-entity-recognition,7,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .",result,Handling Nested Mentions,1,139,9,4,0,result : Handling Nested Mentions,0.879746835443038,0.5625,0.6666666666666666
named-entity-recognition,7,This observation helps explain why our model achieves greater improvement in ACE than in GENIA in since the former has much more nested structures than the latter .,result,Handling Nested Mentions,0,140,10,5,0,result : Handling Nested Mentions,0.8860759493670886,0.625,0.8333333333333334
named-entity-recognition,7,"Moreover , performs better when it comes to non-nested mentions possibly due to the CRF they used , which globally normalizes each stacked layer .",result,Handling Nested Mentions,0,141,11,6,0,result : Handling Nested Mentions,0.8924050632911392,0.6875,1.0
named-entity-recognition,7,Decoding Speed,result,Decoding Speed,0,142,12,1,0,result : Decoding Speed,0.8987341772151899,0.75,0.2
named-entity-recognition,7,"Note that and also feature linear - time complexity , but with a greater constant factor .",result,Decoding Speed,0,143,13,2,0,result : Decoding Speed,0.9050632911392406,0.8125,0.4
named-entity-recognition,7,"To compare the decoding speed , we re-implemented their model with the same platform ( PyTorch ) and run them on the same machine ( CPU : Intel i5 2.7 GHz ) .",result,Decoding Speed,0,144,14,3,0,result : Decoding Speed,0.9113924050632911,0.875,0.6
named-entity-recognition,7,"Our model turns out to be around 3 - 5 times faster than theirs , showing its scalability .",result,Decoding Speed,0,145,15,4,0,result : Decoding Speed,0.9177215189873418,0.9375,0.8
named-entity-recognition,7,We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable .,result,Decoding Speed,0,146,16,5,0,result : Decoding Speed,0.9240506329113924,1.0,1.0
named-entity-recognition,7,Ablation Study,ablation,Ablation Study,0,147,1,1,0,ablation : Ablation Study,0.930379746835443,0.25,0.25
named-entity-recognition,7,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .",ablation,Ablation Study,1,148,2,2,0,ablation : Ablation Study,0.9367088607594937,0.5,0.5
named-entity-recognition,7,The results are listed in .,ablation,Ablation Study,0,149,3,3,0,ablation : Ablation Study,0.9430379746835443,0.75,0.75
named-entity-recognition,7,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .",ablation,Ablation Study,1,150,4,4,0,ablation : Ablation Study,0.9493670886075949,1.0,1.0
named-entity-recognition,7,Conclusion and Future Work,conclusion,Conclusion and Future Work,0,151,1,1,0,conclusion : Conclusion and Future Work,0.9556962025316456,0.125,0.125
named-entity-recognition,7,"In this paper , we present a transition - based model for nested mention recognition using a forest representation .",conclusion,Conclusion and Future Work,0,152,2,2,0,conclusion : Conclusion and Future Work,0.9620253164556962,0.25,0.25
named-entity-recognition,7,"Coupled with Stack - LSTM for representing the system 's state , our neural model can capture dependencies between nested mentions efficiently .",conclusion,Conclusion and Future Work,0,153,3,3,0,conclusion : Conclusion and Future Work,0.9683544303797469,0.375,0.375
named-entity-recognition,7,"Moreover , the character - based component helps capture letter - level patterns in words .",conclusion,Conclusion and Future Work,0,154,4,4,0,conclusion : Conclusion and Future Work,0.9746835443037974,0.5,0.5
named-entity-recognition,7,The system achieves the state - of - the - art performance in ACE datasets .,conclusion,Conclusion and Future Work,0,155,5,5,0,conclusion : Conclusion and Future Work,0.9810126582278481,0.625,0.625
named-entity-recognition,7,One potential drawback of the system is the greedy training and decoding .,conclusion,Conclusion and Future Work,0,156,6,6,0,conclusion : Conclusion and Future Work,0.9873417721518988,0.75,0.75
named-entity-recognition,7,We believe that alternatives like beam search and training with exploration could further boost the performance .,conclusion,Conclusion and Future Work,0,157,7,7,0,conclusion : Conclusion and Future Work,0.9936708860759493,0.875,0.875
named-entity-recognition,7,Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans ( Muis and which frequently exist in the biomedical domain .,conclusion,Conclusion and Future Work,0,158,8,8,0,conclusion : Conclusion and Future Work,1.0,1.0,1.0
named-entity-recognition,8,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,title,title,1,2,1,1,0,title : title,0.00516795865633075,1.0,1.0
named-entity-recognition,8,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.007751937984496124,0.1,0.1
named-entity-recognition,8,"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.0103359173126615,0.2,0.2
named-entity-recognition,8,"Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.012919896640826873,0.3,0.3
named-entity-recognition,8,"As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.015503875968992248,0.4,0.4
named-entity-recognition,8,BERT is conceptually simple and empirically powerful .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.01808785529715762,0.5,0.5
named-entity-recognition,8,"It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.020671834625323,0.6,0.6
named-entity-recognition,8,Jeremy Howard and Sebastian Ruder . 2018 .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.023255813953488372,0.7,0.7
named-entity-recognition,8,Universal language model fine - tuning for text classification .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.025839793281653745,0.8,0.8
named-entity-recognition,8,In ACL .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.028423772609819122,0.9,0.9
named-entity-recognition,8,Association for Computational Linguistics .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.031007751937984496,1.0,1.0
named-entity-recognition,8,Introduction,introduction,introduction,0,13,1,1,0,introduction : introduction,0.03359173126614987,0.04,0.04
named-entity-recognition,8,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,introduction,introduction,1,14,2,2,0,introduction : introduction,0.03617571059431524,0.08,0.08
named-entity-recognition,8,"These include sentence - level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level .",introduction,introduction,0,15,3,3,0,introduction : introduction,0.03875968992248062,0.12,0.12
named-entity-recognition,8,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,introduction,introduction,0,16,4,4,0,introduction : introduction,0.041343669250646,0.16,0.16
named-entity-recognition,8,"The feature - based approach , such as ELMo , uses task - specific architectures that include the pre-trained representations as additional features .",introduction,introduction,0,17,5,5,0,introduction : introduction,0.04392764857881137,0.2,0.2
named-entity-recognition,8,"The fine - tuning approach , such as the Generative Pre-trained Transformer ( OpenAI GPT ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",introduction,introduction,0,18,6,6,0,introduction : introduction,0.046511627906976744,0.24,0.24
named-entity-recognition,8,"The two approaches share the same objective function during pre-training , where they use unidirectional language models to learn general language representations .",introduction,introduction,0,19,7,7,0,introduction : introduction,0.04909560723514212,0.28,0.28
named-entity-recognition,8,"We argue that current techniques restrict the power of the pre-trained representations , especially for the fine - tuning approaches .",introduction,introduction,0,20,8,8,0,introduction : introduction,0.05167958656330749,0.32,0.32
named-entity-recognition,8,"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre-training .",introduction,introduction,0,21,9,9,0,introduction : introduction,0.05426356589147287,0.36,0.36
named-entity-recognition,8,"For example , in Open AI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer .",introduction,introduction,0,22,10,10,0,introduction : introduction,0.056847545219638244,0.4,0.4
named-entity-recognition,8,"Such restrictions are sub-optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",introduction,introduction,0,23,11,11,0,introduction : introduction,0.059431524547803614,0.44,0.44
named-entity-recognition,8,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",introduction,introduction,1,24,12,12,0,introduction : introduction,0.06201550387596899,0.48,0.48
named-entity-recognition,8,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .",introduction,introduction,1,25,13,13,0,introduction : introduction,0.06459948320413436,0.52,0.52
named-entity-recognition,8,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .",introduction,introduction,1,26,14,14,0,introduction : introduction,0.06718346253229975,0.56,0.56
named-entity-recognition,8,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",introduction,introduction,1,27,15,15,0,introduction : introduction,0.06976744186046512,0.6,0.6
named-entity-recognition,8,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .",introduction,introduction,1,28,16,16,0,introduction : introduction,0.07235142118863049,0.64,0.64
named-entity-recognition,8,The contributions of our paper are as follows :,introduction,introduction,0,29,17,17,0,introduction : introduction,0.07493540051679587,0.68,0.68
named-entity-recognition,8,We demonstrate the importance of bidirectional pre-training for language representations .,introduction,introduction,0,30,18,18,0,introduction : introduction,0.07751937984496124,0.72,0.72
named-entity-recognition,8,"Unlike , which uses unidirectional language models for pre-training , BERT uses masked language models to enable pretrained deep bidirectional representations .",introduction,introduction,0,31,19,19,0,introduction : introduction,0.08010335917312661,0.76,0.76
named-entity-recognition,8,"This is also in contrast to , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs .",introduction,introduction,0,32,20,20,0,introduction : introduction,0.082687338501292,0.8,0.8
named-entity-recognition,8,We show that pre-trained representations reduce the need for many heavily - engineered taskspecific architectures .,introduction,introduction,0,33,21,21,0,introduction : introduction,0.08527131782945736,0.84,0.84
named-entity-recognition,8,"BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .",introduction,introduction,0,34,22,22,0,introduction : introduction,0.08785529715762273,0.88,0.88
named-entity-recognition,8,BERT advances the state of the art for eleven NLP tasks .,introduction,introduction,0,35,23,23,0,introduction : introduction,0.09043927648578812,0.92,0.92
named-entity-recognition,8,The code and pre-trained models are available at https://github.com/,introduction,introduction,0,36,24,24,0,introduction : introduction,0.09302325581395349,0.96,0.96
named-entity-recognition,8,google-research/bert .,introduction,introduction,0,37,25,25,0,introduction : introduction,0.09560723514211886,1.0,1.0
named-entity-recognition,8,Related Work,related work,Related Work,0,38,1,1,0,related work : Related Work,0.09819121447028424,0.5,0.5
named-entity-recognition,8,"There is along history of pre-training general language representations , and we briefly review the most widely - used approaches in this section .",related work,Related Work,0,39,2,2,0,related work : Related Work,0.10077519379844961,1.0,1.0
named-entity-recognition,8,Unsupervised Feature - based Approaches,system description,Unsupervised Feature-based Approaches,0,40,1,1,0,system description : Unsupervised Feature-based Approaches,0.10335917312661498,0.02564102564102564,0.07692307692307693
named-entity-recognition,8,"Learning widely applicable representations of words has been an active are a of research for decades , including non-neural and neural methods .",system description,Unsupervised Feature-based Approaches,0,41,2,2,0,system description : Unsupervised Feature-based Approaches,0.10594315245478036,0.05128205128205128,0.15384615384615385
named-entity-recognition,8,"Pre-trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch .",system description,Unsupervised Feature-based Approaches,0,42,3,3,0,system description : Unsupervised Feature-based Approaches,0.10852713178294573,0.07692307692307693,0.23076923076923078
named-entity-recognition,8,"To pretrain word embedding vectors , left - to - right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context .",system description,Unsupervised Feature-based Approaches,0,43,4,4,0,system description : Unsupervised Feature-based Approaches,0.1111111111111111,0.10256410256410256,0.3076923076923077
named-entity-recognition,8,"These approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .",system description,Unsupervised Feature-based Approaches,0,44,5,5,0,system description : Unsupervised Feature-based Approaches,0.11369509043927649,0.1282051282051282,0.38461538461538464
named-entity-recognition,8,"To train sentence representations , prior work has used objectives to rank candidate next sentences , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .",system description,Unsupervised Feature-based Approaches,0,45,6,6,0,system description : Unsupervised Feature-based Approaches,0.11627906976744186,0.15384615384615385,0.46153846153846156
named-entity-recognition,8,ELMo and its predecessor generalize traditional word embedding research along a different dimension .,system description,Unsupervised Feature-based Approaches,0,46,7,7,0,system description : Unsupervised Feature-based Approaches,0.11886304909560723,0.1794871794871795,0.5384615384615384
named-entity-recognition,8,They extract context - sensitive features from a left - to - right and a right - to - left language model .,system description,Unsupervised Feature-based Approaches,0,47,8,8,0,system description : Unsupervised Feature-based Approaches,0.12144702842377261,0.20512820512820512,0.6153846153846154
named-entity-recognition,8,The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations .,system description,Unsupervised Feature-based Approaches,0,48,9,9,0,system description : Unsupervised Feature-based Approaches,0.12403100775193798,0.23076923076923078,0.6923076923076923
named-entity-recognition,8,"When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks including question answering , sentiment analysis , and named entity recognition .",system description,Unsupervised Feature-based Approaches,0,49,10,10,0,system description : Unsupervised Feature-based Approaches,0.12661498708010335,0.2564102564102564,0.7692307692307693
named-entity-recognition,8,proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs .,system description,Unsupervised Feature-based Approaches,0,50,11,11,0,system description : Unsupervised Feature-based Approaches,0.12919896640826872,0.28205128205128205,0.8461538461538461
named-entity-recognition,8,"Similar to ELMo , their model is feature - based and not deeply bidirectional .",system description,Unsupervised Feature-based Approaches,0,51,12,12,0,system description : Unsupervised Feature-based Approaches,0.13178294573643412,0.3076923076923077,0.9230769230769231
named-entity-recognition,8,shows that the cloze task can be used to improve the robustness of text generation models .,system description,Unsupervised Feature-based Approaches,0,52,13,13,0,system description : Unsupervised Feature-based Approaches,0.1343669250645995,0.3333333333333333,1.0
named-entity-recognition,8,Unsupervised Fine- tuning Approaches,system description,Unsupervised Fine-tuning Approaches,0,53,14,1,0,system description : Unsupervised Fine-tuning Approaches,0.13695090439276486,0.358974358974359,0.07142857142857142
named-entity-recognition,8,"As with the feature - based approaches , the first works in this direction only pre-trained word embedding parameters from unlabeled text .",system description,Unsupervised Fine-tuning Approaches,0,54,15,2,0,system description : Unsupervised Fine-tuning Approaches,0.13953488372093023,0.38461538461538464,0.14285714285714285
named-entity-recognition,8,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned for a supervised downstream task .",system description,Unsupervised Fine-tuning Approaches,0,55,16,3,0,system description : Unsupervised Fine-tuning Approaches,0.1421188630490956,0.41025641025641024,0.21428571428571427
named-entity-recognition,8,The advantage of these approaches is that few parameters need to be learned from scratch .,system description,Unsupervised Fine-tuning Approaches,0,56,17,4,0,system description : Unsupervised Fine-tuning Approaches,0.14470284237726097,0.4358974358974359,0.2857142857142857
named-entity-recognition,8,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",system description,Unsupervised Fine-tuning Approaches,0,57,18,5,0,system description : Unsupervised Fine-tuning Approaches,0.14728682170542637,0.46153846153846156,0.35714285714285715
named-entity-recognition,8,Left - to - right language model - BERT BERT E E 1 E ...,system description,Unsupervised Fine-tuning Approaches,0,58,19,6,0,system description : Unsupervised Fine-tuning Approaches,0.14987080103359174,0.48717948717948717,0.42857142857142855
named-entity-recognition,8,CT 1 T ... E 1 E ...,system description,Unsupervised Fine-tuning Approaches,0,59,20,7,0,system description : Unsupervised Fine-tuning Approaches,0.1524547803617571,0.5128205128205128,0.5
named-entity-recognition,8,CT 1 T ... :,system description,Unsupervised Fine-tuning Approaches,0,60,21,8,0,system description : Unsupervised Fine-tuning Approaches,0.15503875968992248,0.5384615384615384,0.5714285714285714
named-entity-recognition,8,Overall pre-training and fine - tuning procedures for BERT .,system description,Unsupervised Fine-tuning Approaches,0,61,22,9,0,system description : Unsupervised Fine-tuning Approaches,0.15762273901808785,0.5641025641025641,0.6428571428571429
named-entity-recognition,8,"Apart from output layers , the same architectures are used in both pre-training and fine - tuning .",system description,Unsupervised Fine-tuning Approaches,0,62,23,10,0,system description : Unsupervised Fine-tuning Approaches,0.16020671834625322,0.5897435897435898,0.7142857142857143
named-entity-recognition,8,The same pre-trained model parameters are used to initialize models for different down - stream tasks .,system description,Unsupervised Fine-tuning Approaches,0,63,24,11,0,system description : Unsupervised Fine-tuning Approaches,0.16279069767441862,0.6153846153846154,0.7857142857142857
named-entity-recognition,8,"During fine - tuning , all parameters are fine - tuned .",system description,Unsupervised Fine-tuning Approaches,0,64,25,12,0,system description : Unsupervised Fine-tuning Approaches,0.165374677002584,0.6410256410256411,0.8571428571428571
named-entity-recognition,8,"CLS ] is a special symbol added in front of every input example , and [ SEP ] is a special separator token ( e.g. separating questions / answers ) .",system description,Unsupervised Fine-tuning Approaches,0,65,26,13,0,system description : Unsupervised Fine-tuning Approaches,0.16795865633074936,0.6666666666666666,0.9285714285714286
named-entity-recognition,8,ing and auto - encoder objectives have been used for pre-training such models .,system description,Unsupervised Fine-tuning Approaches,0,66,27,14,0,system description : Unsupervised Fine-tuning Approaches,0.17054263565891473,0.6923076923076923,1.0
named-entity-recognition,8,Transfer Learning from Supervised Data,system description,Transfer Learning from Supervised Data,0,67,28,1,0,system description : Transfer Learning from Supervised Data,0.1731266149870801,0.717948717948718,0.3333333333333333
named-entity-recognition,8,"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation .",system description,Transfer Learning from Supervised Data,0,68,29,2,0,system description : Transfer Learning from Supervised Data,0.17571059431524547,0.7435897435897436,0.6666666666666666
named-entity-recognition,8,"Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models , where an effective recipe is to fine - tune models pre-trained with I ma - geNet .",system description,Transfer Learning from Supervised Data,0,69,30,3,0,system description : Transfer Learning from Supervised Data,0.17829457364341086,0.7692307692307693,1.0
named-entity-recognition,8,BERT,system description,BERT,0,70,31,1,0,system description : BERT,0.18087855297157623,0.7948717948717948,0.1111111111111111
named-entity-recognition,8,We introduce BERT and its detailed implementation in this section .,system description,BERT,0,71,32,2,0,system description : BERT,0.1834625322997416,0.8205128205128205,0.2222222222222222
named-entity-recognition,8,There are two steps in our framework : pre-training and fine - tuning .,system description,BERT,0,72,33,3,0,system description : BERT,0.18604651162790697,0.8461538461538461,0.3333333333333333
named-entity-recognition,8,"During pre-training , the model is trained on unlabeled data over different pre-training tasks .",system description,BERT,0,73,34,4,0,system description : BERT,0.18863049095607234,0.8717948717948718,0.4444444444444444
named-entity-recognition,8,"For finetuning , the BERT model is first initialized with the pre-trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",system description,BERT,0,74,35,5,0,system description : BERT,0.19121447028423771,0.8974358974358975,0.5555555555555556
named-entity-recognition,8,"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre-trained parameters .",system description,BERT,0,75,36,6,0,system description : BERT,0.1937984496124031,0.9230769230769231,0.6666666666666666
named-entity-recognition,8,The question - answering example in will serve as a running example for this section .,system description,BERT,0,76,37,7,0,system description : BERT,0.19638242894056848,0.9487179487179487,0.7777777777777778
named-entity-recognition,8,distinctive feature of BERT is its unified architecture across different tasks .,system description,BERT,0,77,38,8,0,system description : BERT,0.19896640826873385,0.9743589743589743,0.8888888888888888
named-entity-recognition,8,There is mini-mal difference between the pre-trained architecture and the final downstream architecture .,system description,BERT,0,78,39,9,0,system description : BERT,0.20155038759689922,1.0,1.0
named-entity-recognition,8,Model Architecture,model,model,0,79,1,1,0,model : model,0.2041343669250646,0.013513513513513514,0.016129032258064516
named-entity-recognition,8,BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation and released in the tensor2 tensor library .,model,model,0,80,2,2,0,model : model,0.20671834625322996,0.02702702702702703,0.03225806451612903
named-entity-recognition,8,1,model,model,0,81,3,3,0,model : model,0.20930232558139536,0.04054054054054054,0.04838709677419355
named-entity-recognition,8,"Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to as well as excellent guides such as "" The Annotated Transformer . """,model,model,0,82,4,4,0,model : model,0.21188630490956073,0.05405405405405406,0.06451612903225806
named-entity-recognition,8,2,model,model,0,83,5,5,0,model : model,0.2144702842377261,0.06756756756756757,0.08064516129032258
named-entity-recognition,8,"In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A .",model,model,0,84,6,6,0,model : model,0.21705426356589147,0.08108108108108109,0.0967741935483871
named-entity-recognition,8,"We primarily report results on two model sizes : BERT BASE ( L=12 , H = 768 , A = 12 , Total Param-eters=110M ) and BERT LARGE ( L=24 , H = 1024 , A = 16 , Total Parameters=340M ) .",model,model,0,85,7,7,0,model : model,0.21963824289405684,0.0945945945945946,0.11290322580645161
named-entity-recognition,8,BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .,model,model,0,86,8,8,0,model : model,0.2222222222222222,0.10810810810810811,0.12903225806451613
named-entity-recognition,8,"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",model,model,0,87,9,9,0,model : model,0.2248062015503876,0.12162162162162163,0.14516129032258066
named-entity-recognition,8,4,model,model,0,88,10,10,0,model : model,0.22739018087855298,0.13513513513513514,0.16129032258064516
named-entity-recognition,8,Input / Output Representations,model,model,0,89,11,11,0,model : model,0.22997416020671835,0.14864864864864866,0.1774193548387097
named-entity-recognition,8,"To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .",model,model,0,90,12,12,0,model : model,0.23255813953488372,0.16216216216216217,0.1935483870967742
named-entity-recognition,8,"Throughout this work , a "" sentence "" can bean arbitrary span of contiguous text , rather than an actual linguistic sentence .",model,model,0,91,13,13,0,model : model,0.2351421188630491,0.17567567567567569,0.20967741935483872
named-entity-recognition,8,""" sequence "" refers to the input token sequence to BERT , which maybe a single sentence or two sentences packed together .",model,model,0,92,14,14,0,model : model,0.23772609819121446,0.1891891891891892,0.22580645161290322
named-entity-recognition,8,We use WordPiece embeddings,model,model,0,93,15,15,0,model : model,0.24031007751937986,0.20270270270270271,0.24193548387096775
named-entity-recognition,8,"Wu et al. , 2016 ) with a 30,000 token vocabulary .",model,model,0,94,16,16,0,model : model,0.24289405684754523,0.21621621621621623,0.25806451612903225
named-entity-recognition,8,The first token of every sequence is always a special classification token ( [ CLS ] ) .,model,model,0,95,17,17,0,model : model,0.2454780361757106,0.22972972972972974,0.27419354838709675
named-entity-recognition,8,The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks .,model,model,0,96,18,18,0,model : model,0.24806201550387597,0.24324324324324326,0.2903225806451613
named-entity-recognition,8,Sentence pairs are packed together into a single sequence .,model,model,0,97,19,19,0,model : model,0.25064599483204136,0.25675675675675674,0.3064516129032258
named-entity-recognition,8,We differentiate the sentences in two ways .,model,model,0,98,20,20,0,model : model,0.2532299741602067,0.2702702702702703,0.3225806451612903
named-entity-recognition,8,"First , we separate them with a special token ( [ SEP ] ) .",model,model,0,99,21,21,0,model : model,0.2558139534883721,0.28378378378378377,0.3387096774193548
named-entity-recognition,8,"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B .",model,model,0,100,22,22,0,model : model,0.25839793281653745,0.2972972972972973,0.3548387096774194
named-entity-recognition,8,"As shown in , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ? R H , and the final hidden vector for the i th input token as",model,model,0,101,23,23,0,model : model,0.26098191214470284,0.3108108108108108,0.3709677419354839
named-entity-recognition,8,"For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",model,model,0,102,24,24,0,model : model,0.26356589147286824,0.32432432432432434,0.3870967741935484
named-entity-recognition,8,"visualization of this construction can be seen in . ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre-train BERT .",model,model,0,103,25,25,0,model : model,0.2661498708010336,0.33783783783783783,0.4032258064516129
named-entity-recognition,8,"Instead , we pre-train BERT using two unsupervised tasks , described in this section .",model,model,0,104,26,26,0,model : model,0.268733850129199,0.35135135135135137,0.41935483870967744
named-entity-recognition,8,This step is presented in the left part of .,model,model,0,105,27,27,0,model : model,0.2713178294573643,0.36486486486486486,0.43548387096774194
named-entity-recognition,8,Task # 1 : Masked LM,model,model,0,106,28,28,0,model : model,0.2739018087855297,0.3783783783783784,0.45161290322580644
named-entity-recognition,8,"Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",model,model,0,107,29,29,0,model : model,0.27648578811369506,0.3918918918918919,0.46774193548387094
named-entity-recognition,8,"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly "" see itself "" , and the model could trivially predict the target word in a multi - layered context .",model,model,0,108,30,30,0,model : model,0.27906976744186046,0.40540540540540543,0.4838709677419355
named-entity-recognition,8,"former is often referred to as a "" Transformer encoder "" while the left - context - only version is referred to as a "" Transformer decoder "" since it can be used for text generation .",model,model,0,109,31,31,0,model : model,0.28165374677002586,0.4189189189189189,0.5
named-entity-recognition,8,"In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens .",model,model,0,110,32,32,0,model : model,0.2842377260981912,0.43243243243243246,0.5161290322580645
named-entity-recognition,8,"We refer to this procedure as a "" masked LM "" ( MLM ) , although it is often referred to as a Cloze task in the literature .",model,model,0,111,33,33,0,model : model,0.2868217054263566,0.44594594594594594,0.532258064516129
named-entity-recognition,8,"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",model,model,0,112,34,34,0,model : model,0.28940568475452194,0.4594594594594595,0.5483870967741935
named-entity-recognition,8,"In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .",model,model,0,113,35,35,0,model : model,0.29198966408268734,0.47297297297297297,0.5645161290322581
named-entity-recognition,8,"In contrast to denoising auto - encoders , we only predict the masked words rather than reconstructing the entire input .",model,model,0,114,36,36,0,model : model,0.29457364341085274,0.4864864864864865,0.5806451612903226
named-entity-recognition,8,"Although this allows us to obtain a bidirectional pre-trained model , a downside is that we are creating a mismatch between pre-training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",model,model,0,115,37,37,0,model : model,0.2971576227390181,0.5,0.5967741935483871
named-entity-recognition,8,"To mitigate this , we do not always replace "" masked "" words with the actual [ MASK ] token .",model,model,0,116,38,38,0,model : model,0.2997416020671835,0.5135135135135135,0.6129032258064516
named-entity-recognition,8,The training data generator chooses 15 % of the token positions at random for prediction .,model,model,0,117,39,39,0,model : model,0.3023255813953488,0.527027027027027,0.6290322580645161
named-entity-recognition,8,"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",model,model,0,118,40,40,0,model : model,0.3049095607235142,0.5405405405405406,0.6451612903225806
named-entity-recognition,8,"Then , Ti will be used to predict the original token with cross entropy loss .",model,model,0,119,41,41,0,model : model,0.30749354005167956,0.5540540540540541,0.6612903225806451
named-entity-recognition,8,"Then , Ti will be used to predict the original token with cross entropy loss .",model,model,0,120,42,42,0,model : model,0.31007751937984496,0.5675675675675675,0.6774193548387096
named-entity-recognition,8,We compare variations of this procedure in Appendix C.2 .,model,model,0,121,43,43,0,model : model,0.31266149870801035,0.581081081081081,0.6935483870967742
named-entity-recognition,8,"Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .",model,model,0,122,44,44,0,model : model,0.3152454780361757,0.5945945945945946,0.7096774193548387
named-entity-recognition,8,"In order to train a model that understands sentence relationships , we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus .",model,model,0,123,45,45,0,model : model,0.3178294573643411,0.6081081081081081,0.7258064516129032
named-entity-recognition,8,"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",model,model,0,124,46,46,0,model : model,0.32041343669250644,0.6216216216216216,0.7419354838709677
named-entity-recognition,8,"As we show in , C is used for next sentence prediction ( NSP ) .",model,model,0,125,47,47,0,model : model,0.32299741602067183,0.6351351351351351,0.7580645161290323
named-entity-recognition,8,"Despite its simplicity , we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI .",model,model,0,126,48,48,0,model : model,0.32558139534883723,0.6486486486486487,0.7741935483870968
named-entity-recognition,8,6,model,model,0,127,49,49,0,model : model,0.3281653746770026,0.6621621621621622,0.7903225806451613
named-entity-recognition,8,The final model achieves 97 % - 98 % accuracy on NSP .,model,model,0,128,50,50,0,model : model,0.330749354005168,0.6756756756756757,0.8064516129032258
named-entity-recognition,8,"The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",model,model,0,129,51,51,0,model : model,0.3333333333333333,0.6891891891891891,0.8225806451612904
named-entity-recognition,8,he likes play ##ing my dog is cute Input,model,model,0,130,52,52,0,model : model,0.3359173126614987,0.7027027027027027,0.8387096774193549
named-entity-recognition,8,Position,model,model,0,131,53,53,0,model : model,0.3385012919896641,0.7162162162162162,0.8548387096774194
named-entity-recognition,8,Embeddings : BERT input representation .,model,model,0,132,54,54,0,model : model,0.34108527131782945,0.7297297297297297,0.8709677419354839
named-entity-recognition,8,"The input embeddings are the sum of the token embeddings , the segmentation embeddings and the position embeddings .",model,model,0,133,55,55,0,model : model,0.34366925064599485,0.7432432432432432,0.8870967741935484
named-entity-recognition,8,The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee ( 2018 ) .,model,model,0,134,56,56,0,model : model,0.3462532299741602,0.7567567567567568,0.9032258064516129
named-entity-recognition,8,"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",model,model,0,135,57,57,0,model : model,0.3488372093023256,0.7702702702702703,0.9193548387096774
named-entity-recognition,8,Pre-training data,model,model,0,136,58,58,0,model : model,0.35142118863049093,0.7837837837837838,0.9354838709677419
named-entity-recognition,8,The pre-training procedure largely follows the existing literature on language model pre-training .,model,model,0,137,59,59,0,model : model,0.35400516795865633,0.7972972972972973,0.9516129032258065
named-entity-recognition,8,"For the pre-training corpus we use the Books Corpus ( 800M words ) and English Wikipedia ( 2,500 M words ) .",model,model,0,138,60,60,0,model : model,0.35658914728682173,0.8108108108108109,0.967741935483871
named-entity-recognition,8,"For Wikipedia we extract only the text passages and ignore lists , tables , and headers .",model,model,0,139,61,61,0,model : model,0.35917312661498707,0.8243243243243243,0.9838709677419355
named-entity-recognition,8,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,model,model,0,140,62,62,0,model : model,0.36175710594315247,0.8378378378378378,1.0
named-entity-recognition,8,Fine- tuning BERT,model,Fine-tuning BERT,0,141,63,1,0,model : Fine-tuning BERT,0.3643410852713178,0.8513513513513513,0.08333333333333333
named-entity-recognition,8,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,model,Fine-tuning BERT,0,142,64,2,0,model : Fine-tuning BERT,0.3669250645994832,0.8648648648648649,0.16666666666666666
named-entity-recognition,8,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",model,Fine-tuning BERT,0,143,65,3,0,model : Fine-tuning BERT,0.3695090439276486,0.8783783783783784,0.25
named-entity-recognition,8,"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",model,Fine-tuning BERT,0,144,66,4,0,model : Fine-tuning BERT,0.37209302325581395,0.8918918918918919,0.3333333333333333
named-entity-recognition,8,"For each task , we simply plugin the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .",model,Fine-tuning BERT,0,145,67,5,0,model : Fine-tuning BERT,0.37467700258397935,0.9054054054054054,0.4166666666666667
named-entity-recognition,8,"At the input , sentence A and sentence B from pre-training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text -? pair in text classification or sequence tagging .",model,Fine-tuning BERT,0,146,68,6,0,model : Fine-tuning BERT,0.3772609819121447,0.918918918918919,0.5
named-entity-recognition,8,"At the input , sentence A and sentence B from pre-training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text -? pair in text classification or sequence tagging .",model,Fine-tuning BERT,0,147,69,7,0,model : Fine-tuning BERT,0.3798449612403101,0.9324324324324325,0.5833333333333334
named-entity-recognition,8,"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .",model,Fine-tuning BERT,0,148,70,8,0,model : Fine-tuning BERT,0.38242894056847543,0.9459459459459459,0.6666666666666666
named-entity-recognition,8,"Compared to pre-training , fine - tuning is relatively inexpensive .",model,Fine-tuning BERT,0,149,71,9,0,model : Fine-tuning BERT,0.3850129198966408,0.9594594594594594,0.75
named-entity-recognition,8,"All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre-trained model .",model,Fine-tuning BERT,0,150,72,10,0,model : Fine-tuning BERT,0.3875968992248062,0.972972972972973,0.8333333333333334
named-entity-recognition,8,We describe the task - specific details in the corresponding subsections of Section 4 .,model,Fine-tuning BERT,0,151,73,11,0,model : Fine-tuning BERT,0.39018087855297157,0.9864864864864865,0.9166666666666666
named-entity-recognition,8,More details can be found in Appendix A.5 .,model,Fine-tuning BERT,0,152,74,12,0,model : Fine-tuning BERT,0.39276485788113696,1.0,1.0
named-entity-recognition,8,Experiments,experiment,Experiments,0,153,1,1,0,experiment : Experiments,0.3953488372093023,0.014084507042253521,0.5
named-entity-recognition,8,"In this section , we present BERT fine - tuning results on 11 NLP tasks .",experiment,Experiments,0,154,2,2,0,experiment : Experiments,0.3979328165374677,0.028169014084507043,1.0
named-entity-recognition,8,GLUE,experiment,GLUE,1,155,3,1,0,experiment : GLUE,0.4005167958656331,0.04225352112676056,0.014492753623188406
named-entity-recognition,8,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",experiment,GLUE,1,156,4,2,0,experiment : GLUE,0.40310077519379844,0.056338028169014086,0.028985507246376812
named-entity-recognition,8,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,experiment,GLUE,0,157,5,3,0,experiment : GLUE,0.40568475452196384,0.07042253521126761,0.043478260869565216
named-entity-recognition,8,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ? R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .",experiment,GLUE,0,158,6,4,0,experiment : GLUE,0.4082687338501292,0.08450704225352113,0.057971014492753624
named-entity-recognition,8,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ? R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .",experiment,GLUE,0,159,7,5,0,experiment : GLUE,0.4108527131782946,0.09859154929577464,0.07246376811594203
named-entity-recognition,8,"The only new parameters introduced during fine - tuning are classification layer weights W ? R KH , where K is the number of labels .",experiment,GLUE,0,160,8,6,0,experiment : GLUE,0.4134366925064599,0.11267605633802817,0.08695652173913043
named-entity-recognition,8,"The only new parameters introduced during fine - tuning are classification layer weights W ? R KH , where K is the number of labels .",experiment,GLUE,0,161,9,7,0,experiment : GLUE,0.4160206718346253,0.1267605633802817,0.10144927536231885
named-entity-recognition,8,"We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) . :",experiment,GLUE,0,162,10,8,0,experiment : GLUE,0.4186046511627907,0.14084507042253522,0.11594202898550725
named-entity-recognition,8,GLUE,experiment,GLUE,0,163,11,9,0,experiment : GLUE,0.42118863049095606,0.15492957746478872,0.13043478260869565
named-entity-recognition,8,"Test results , scored by the evaluation server ( https://gluebenchmark.com/leaderboard ) .",experiment,GLUE,0,164,12,10,0,experiment : GLUE,0.42377260981912146,0.16901408450704225,0.14492753623188406
named-entity-recognition,8,The number below each task denotes the number of training examples .,experiment,GLUE,0,165,13,11,0,experiment : GLUE,0.4263565891472868,0.18309859154929578,0.15942028985507245
named-entity-recognition,8,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .",experiment,GLUE,0,166,14,12,0,experiment : GLUE,0.4289405684754522,0.19718309859154928,0.17391304347826086
named-entity-recognition,8,"BERT and OpenAI GPT are singlemodel , single task .",experiment,GLUE,0,167,15,13,0,experiment : GLUE,0.4315245478036176,0.2112676056338028,0.18840579710144928
named-entity-recognition,8,"F1 scores are reported for QQP and MRPC , Spearman correlations are reported for STS - B , and accuracy scores are reported for the other tasks .",experiment,GLUE,0,168,16,14,0,experiment : GLUE,0.43410852713178294,0.22535211267605634,0.2028985507246377
named-entity-recognition,8,We exclude entries that use BERT as one of their components .,experiment,GLUE,0,169,17,15,0,experiment : GLUE,0.43669250645994834,0.23943661971830985,0.21739130434782608
named-entity-recognition,8,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,experiment,GLUE,1,170,18,16,0,experiment : GLUE,0.4392764857881137,0.2535211267605634,0.2318840579710145
named-entity-recognition,8,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",experiment,GLUE,1,171,19,17,0,experiment : GLUE,0.4418604651162791,0.2676056338028169,0.2463768115942029
named-entity-recognition,8,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",experiment,GLUE,1,172,20,18,0,experiment : GLUE,0.4444444444444444,0.28169014084507044,0.2608695652173913
named-entity-recognition,8,"With random restarts , we use the same pre-trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization .",experiment,GLUE,0,173,21,19,0,experiment : GLUE,0.4470284237726098,0.29577464788732394,0.2753623188405797
named-entity-recognition,8,Results are presented in .,experiment,GLUE,0,174,22,20,0,experiment : GLUE,0.4496124031007752,0.30985915492957744,0.2898550724637681
named-entity-recognition,8,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",experiment,GLUE,1,175,23,21,0,experiment : GLUE,0.45219638242894056,0.323943661971831,0.30434782608695654
named-entity-recognition,8,Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .,experiment,GLUE,0,176,24,22,0,experiment : GLUE,0.45478036175710596,0.3380281690140845,0.3188405797101449
named-entity-recognition,8,"For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .",experiment,GLUE,0,177,25,23,0,experiment : GLUE,0.4573643410852713,0.352112676056338,0.3333333333333333
named-entity-recognition,8,"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",experiment,GLUE,0,178,26,24,0,experiment : GLUE,0.4599483204134367,0.36619718309859156,0.34782608695652173
named-entity-recognition,8,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",experiment,GLUE,1,179,27,25,0,experiment : GLUE,0.4625322997416021,0.38028169014084506,0.36231884057971014
named-entity-recognition,8,The effect of model size is explored more thoroughly in Section 5.2 .,experiment,GLUE,0,180,28,26,0,experiment : GLUE,0.46511627906976744,0.39436619718309857,0.37681159420289856
named-entity-recognition,8,SQuAD v 1.1,experiment,GLUE,1,181,29,27,0,experiment : GLUE,0.46770025839793283,0.4084507042253521,0.391304347826087
named-entity-recognition,8,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,experiment,GLUE,1,182,30,28,0,experiment : GLUE,0.4702842377260982,0.4225352112676056,0.4057971014492754
named-entity-recognition,8,"Given a question and a passage from The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",experiment,GLUE,0,183,31,29,0,experiment : GLUE,0.4728682170542636,0.43661971830985913,0.42028985507246375
named-entity-recognition,8,10 https://gluebenchmark.com/leaderboard,experiment,GLUE,0,184,32,30,0,experiment : GLUE,0.4754521963824289,0.4507042253521127,0.43478260869565216
named-entity-recognition,8,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",experiment,GLUE,0,185,33,31,0,experiment : GLUE,0.4780361757105943,0.4647887323943662,0.4492753623188406
named-entity-recognition,8,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",experiment,GLUE,0,186,34,32,0,experiment : GLUE,0.4806201550387597,0.4788732394366197,0.463768115942029
named-entity-recognition,8,We only introduce a start vector S ? R H and an end vector E ? R H during fine - tuning .,experiment,GLUE,0,187,35,33,0,experiment : GLUE,0.48320413436692505,0.49295774647887325,0.4782608695652174
named-entity-recognition,8,We only introduce a start vector S ? R H and an end vector E ? R H during fine - tuning .,experiment,GLUE,0,188,36,34,0,experiment : GLUE,0.48578811369509045,0.5070422535211268,0.4927536231884058
named-entity-recognition,8,We only introduce a start vector S ? R H and an end vector E ? R H during fine - tuning .,experiment,GLUE,0,189,37,35,0,experiment : GLUE,0.4883720930232558,0.5211267605633803,0.5072463768115942
named-entity-recognition,8,The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph : P i = e ST i j e ST j .,experiment,GLUE,0,190,38,36,0,experiment : GLUE,0.4909560723514212,0.5352112676056338,0.5217391304347826
named-entity-recognition,8,The analogous formula is used for the end of the answer span .,experiment,GLUE,0,191,39,37,0,experiment : GLUE,0.4935400516795866,0.5492957746478874,0.5362318840579711
named-entity-recognition,8,"The score of a candidate span from position i to position j is defined as ST i + ET j , and the maximum scoring span where j ? i is used as a prediction .",experiment,GLUE,0,192,40,38,0,experiment : GLUE,0.49612403100775193,0.5633802816901409,0.5507246376811594
named-entity-recognition,8,"The score of a candidate span from position i to position j is defined as ST i + ET j , and the maximum scoring span where j ? i is used as a prediction .",experiment,GLUE,0,193,41,39,0,experiment : GLUE,0.49870801033591733,0.5774647887323944,0.5652173913043478
named-entity-recognition,8,The training objective is the sum of the log-likelihoods of the correct start and end positions .,experiment,GLUE,0,194,42,40,0,experiment : GLUE,0.5012919896640827,0.5915492957746479,0.5797101449275363
named-entity-recognition,8,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,experiment,GLUE,1,195,43,41,0,experiment : GLUE,0.5038759689922481,0.6056338028169014,0.5942028985507246
named-entity-recognition,8,shows top leaderboard entries as well as results from top published systems .,experiment,GLUE,0,196,44,42,0,experiment : GLUE,0.5064599483204134,0.6197183098591549,0.6086956521739131
named-entity-recognition,8,"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",experiment,GLUE,0,197,45,43,0,experiment : GLUE,0.5090439276485789,0.6338028169014085,0.6231884057971014
named-entity-recognition,8,"We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al. , 2017 ) befor fine - tuning on SQuAD .",experiment,GLUE,0,198,46,44,0,experiment : GLUE,0.5116279069767442,0.647887323943662,0.6376811594202898
named-entity-recognition,8,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,experiment,GLUE,1,199,47,45,0,experiment : GLUE,0.5142118863049095,0.6619718309859155,0.6521739130434783
named-entity-recognition,8,"In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .",experiment,GLUE,0,200,48,46,0,experiment : GLUE,0.5167958656330749,0.676056338028169,0.6666666666666666
named-entity-recognition,8,"Without Trivia QA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .",experiment,GLUE,0,201,49,47,0,experiment : GLUE,0.5193798449612403,0.6901408450704225,0.6811594202898551
named-entity-recognition,8,12,experiment,GLUE,0,202,50,48,0,experiment : GLUE,0.5219638242894057,0.704225352112676,0.6956521739130435
named-entity-recognition,8,SQuAD v 2.0,experiment,GLUE,1,203,51,49,0,experiment : GLUE,0.524547803617571,0.7183098591549296,0.7101449275362319
named-entity-recognition,8,"The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .",experiment,GLUE,0,204,52,50,0,experiment : GLUE,0.5271317829457365,0.7323943661971831,0.7246376811594203
named-entity-recognition,8,We use a simple approach to extend the SQuAD v1.1 BERT model for this task .,experiment,GLUE,0,205,53,51,0,experiment : GLUE,0.5297157622739018,0.7464788732394366,0.7391304347826086
named-entity-recognition,8,We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .,experiment,GLUE,0,206,54,52,0,experiment : GLUE,0.5322997416020672,0.7605633802816901,0.7536231884057971
named-entity-recognition,8,The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token .,experiment,GLUE,0,207,55,53,0,experiment : GLUE,0.5348837209302325,0.7746478873239436,0.7681159420289855
named-entity-recognition,8,"For prediction , we compare the score of the no -answer span : s null = SC + EC to the score of the best non - null span The Trivia QA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",experiment,GLUE,0,208,56,54,0,experiment : GLUE,0.537467700258398,0.7887323943661971,0.782608695652174
named-entity-recognition,8,"We predict a non-null answer when ? i , j > s null + ? , where the threshold ? is selected on the dev set to maximize F 1 .",experiment,GLUE,0,209,57,55,0,experiment : GLUE,0.5400516795865633,0.8028169014084507,0.7971014492753623
named-entity-recognition,8,"We predict a non-null answer when ? i , j > s null + ? , where the threshold ? is selected on the dev set to maximize F 1 .",experiment,GLUE,0,210,58,56,0,experiment : GLUE,0.5426356589147286,0.8169014084507042,0.8115942028985508
named-entity-recognition,8,"We predict a non-null answer when ? i , j > s null + ? , where the threshold ? is selected on the dev set to maximize F 1 .",experiment,GLUE,0,211,59,57,0,experiment : GLUE,0.5452196382428941,0.8309859154929577,0.8260869565217391
named-entity-recognition,8,We did not use Trivia QA data for this model .,experiment,GLUE,0,212,60,58,0,experiment : GLUE,0.5478036175710594,0.8450704225352113,0.8405797101449275
named-entity-recognition,8,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,experiment,GLUE,1,213,61,59,0,experiment : GLUE,0.5503875968992248,0.8591549295774648,0.855072463768116
named-entity-recognition,8,"The results compared to prior leaderboard entries and top published work are shown in , excluding systems that use BERT as one of their components .",experiment,GLUE,0,214,62,60,0,experiment : GLUE,0.5529715762273901,0.8732394366197183,0.8695652173913043
named-entity-recognition,8,We observe a + 5.1 F1 improvement over the previous best system .,experiment,GLUE,1,215,63,61,0,experiment : GLUE,0.5555555555555556,0.8873239436619719,0.8840579710144928
named-entity-recognition,8,SWAG,experiment,GLUE,1,216,64,62,0,experiment : GLUE,0.5581395348837209,0.9014084507042254,0.8985507246376812
named-entity-recognition,8,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,experiment,GLUE,1,217,65,63,0,experiment : GLUE,0.5607235142118863,0.9154929577464789,0.9130434782608695
named-entity-recognition,8,"Given a sentence , the task is to choose the most plausible continuation among four choices .",experiment,GLUE,0,218,66,64,0,experiment : GLUE,0.5633074935400517,0.9295774647887324,0.927536231884058
named-entity-recognition,8,"When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .",experiment,GLUE,0,219,67,65,0,experiment : GLUE,0.5658914728682171,0.9436619718309859,0.9420289855072463
named-entity-recognition,8,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,experiment,GLUE,0,220,68,66,0,experiment : GLUE,0.5684754521963824,0.9577464788732394,0.9565217391304348
named-entity-recognition,8,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,experiment,GLUE,1,221,69,67,0,experiment : GLUE,0.5710594315245479,0.971830985915493,0.9710144927536232
named-entity-recognition,8,Results are presented in .,experiment,GLUE,0,222,70,68,0,experiment : GLUE,0.5736434108527132,0.9859154929577465,0.9855072463768116
named-entity-recognition,8,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,experiment,GLUE,1,223,71,69,0,experiment : GLUE,0.5762273901808785,1.0,1.0
named-entity-recognition,8,Ablation Studies,ablation,ablation,0,224,1,1,0,ablation : ablation,0.5788113695090439,0.038461538461538464,0.038461538461538464
named-entity-recognition,8,"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .",ablation,ablation,0,225,2,2,0,ablation : ablation,0.5813953488372093,0.07692307692307693,0.07692307692307693
named-entity-recognition,8,Additional : Ablation over the pre-training tasks using the BERT BASE architecture .,ablation,ablation,0,226,3,3,0,ablation : ablation,0.5839793281653747,0.11538461538461539,0.11538461538461539
named-entity-recognition,8,"No NSP "" is trained without the next sentence prediction task .",ablation,ablation,0,227,4,4,0,ablation : ablation,0.58656330749354,0.15384615384615385,0.15384615384615385
named-entity-recognition,8,"LTR & No NSP "" is trained as a left - to - right LM without the next sentence prediction , like OpenAI GPT .",ablation,ablation,0,228,5,5,0,ablation : ablation,0.5891472868217055,0.19230769230769232,0.19230769230769232
named-entity-recognition,8,"+ BiLSTM "" adds a randomly initialized BiLSTM on top of the "" LTR + No NSP "" model during fine - tuning .",ablation,ablation,0,229,6,6,0,ablation : ablation,0.5917312661498708,0.23076923076923078,0.23076923076923078
named-entity-recognition,8,ablation studies can be found in Appendix C.,ablation,ablation,0,230,7,7,0,ablation : ablation,0.5943152454780362,0.2692307692307692,0.2692307692307692
named-entity-recognition,8,Effect of Pre-training Tasks,ablation,ablation,0,231,8,8,0,ablation : ablation,0.5968992248062015,0.3076923076923077,0.3076923076923077
named-entity-recognition,8,"We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE :",ablation,ablation,0,232,9,9,0,ablation : ablation,0.599483204134367,0.34615384615384615,0.34615384615384615
named-entity-recognition,8,No NSP :,ablation,ablation,0,233,10,10,0,ablation : ablation,0.6020671834625323,0.38461538461538464,0.38461538461538464
named-entity-recognition,8,"bidirectional model which is trained using the "" masked LM "" ( MLM ) but without the "" next sentence prediction "" ( NSP ) task .",ablation,ablation,0,234,11,11,0,ablation : ablation,0.6046511627906976,0.4230769230769231,0.4230769230769231
named-entity-recognition,8,LTR & No NSP :,ablation,ablation,0,235,12,12,0,ablation : ablation,0.6072351421188631,0.46153846153846156,0.46153846153846156
named-entity-recognition,8,"left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .",ablation,ablation,0,236,13,13,0,ablation : ablation,0.6098191214470284,0.5,0.5
named-entity-recognition,8,"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre-train / fine - tune mismatch that degraded downstream performance .",ablation,ablation,0,237,14,14,0,ablation : ablation,0.6124031007751938,0.5384615384615384,0.5384615384615384
named-entity-recognition,8,"Additionally , this model was pre-trained without the NSP task .",ablation,ablation,0,238,15,15,0,ablation : ablation,0.6149870801033591,0.5769230769230769,0.5769230769230769
named-entity-recognition,8,"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",ablation,ablation,0,239,16,16,0,ablation : ablation,0.6175710594315246,0.6153846153846154,0.6153846153846154
named-entity-recognition,8,We first examine the impact brought by the NSP task .,ablation,ablation,0,240,17,17,0,ablation : ablation,0.6201550387596899,0.6538461538461539,0.6538461538461539
named-entity-recognition,8,"In , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQu AD 1.1 .",ablation,ablation,0,241,18,18,0,ablation : ablation,0.6227390180878553,0.6923076923076923,0.6923076923076923
named-entity-recognition,8,"Next , we evaluate the impact of training bidirectional representations by comparing "" No NSP "" to "" LTR & No NSP "" .",ablation,ablation,0,242,19,19,0,ablation : ablation,0.6253229974160207,0.7307692307692307,0.7307692307692307
named-entity-recognition,8,"The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .",ablation,ablation,0,243,20,20,0,ablation : ablation,0.627906976744186,0.7692307692307693,0.7692307692307693
named-entity-recognition,8,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",ablation,ablation,0,244,21,21,0,ablation : ablation,0.6304909560723514,0.8076923076923077,0.8076923076923077
named-entity-recognition,8,"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",ablation,ablation,0,245,22,22,0,ablation : ablation,0.6330749354005168,0.8461538461538461,0.8461538461538461
named-entity-recognition,8,"This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .",ablation,ablation,0,246,23,23,0,ablation : ablation,0.6356589147286822,0.8846153846153846,0.8846153846153846
named-entity-recognition,8,The BiLSTM hurts performance on the GLUE tasks .,ablation,ablation,0,247,24,24,0,ablation : ablation,0.6382428940568475,0.9230769230769231,0.9230769230769231
named-entity-recognition,8,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",ablation,ablation,0,248,25,25,0,ablation : ablation,0.6408268733850129,0.9615384615384616,0.9615384615384616
named-entity-recognition,8,"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non-intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",ablation,ablation,0,249,26,26,0,ablation : ablation,0.6434108527131783,1.0,1.0
named-entity-recognition,8,Effect of Model Size,model,model,1,250,1,1,0,model : model,0.6459948320413437,0.03571428571428571,0.03571428571428571
named-entity-recognition,8,"In this section , we explore the effect of model size on fine - tuning task accuracy .",model,model,0,251,2,2,0,model : model,0.648578811369509,0.07142857142857142,0.07142857142857142
named-entity-recognition,8,"We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .",model,model,0,252,3,3,0,model : model,0.6511627906976745,0.10714285714285714,0.10714285714285714
named-entity-recognition,8,Results on selected GLUE tasks are shown in .,model,model,0,253,4,4,0,model : model,0.6537467700258398,0.14285714285714285,0.14285714285714285
named-entity-recognition,8,"In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .",model,model,0,254,5,5,0,model : model,0.6563307493540051,0.17857142857142858,0.17857142857142858
named-entity-recognition,8,"We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre-training tasks .",model,model,0,255,6,6,0,model : model,0.6589147286821705,0.21428571428571427,0.21428571428571427
named-entity-recognition,8,It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature .,model,model,0,256,7,7,0,model : model,0.661498708010336,0.25,0.25
named-entity-recognition,8,"For example , the largest Transformer explored in is ( L=6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H = 512 , A=2 ) with 235M parameters .",model,model,0,257,8,8,0,model : model,0.6640826873385013,0.2857142857142857,0.2857142857142857
named-entity-recognition,8,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .",model,model,0,258,9,9,0,model : model,0.6666666666666666,0.32142857142857145,0.32142857142857145
named-entity-recognition,8,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",model,model,0,259,10,10,0,model : model,0.6692506459948321,0.35714285714285715,0.35714285714285715
named-entity-recognition,8,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",model,model,1,260,11,11,0,model : model,0.6718346253229974,0.39285714285714285,0.39285714285714285
named-entity-recognition,8,"presented mixed results on the downstream task impact of increasing the pre-trained bi - LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",model,model,0,261,12,12,0,model : model,0.6744186046511628,0.42857142857142855,0.42857142857142855
named-entity-recognition,8,"Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre-trained representations even when downstream task data is very small .",model,model,0,262,13,13,0,model : model,0.6770025839793282,0.4642857142857143,0.4642857142857143
named-entity-recognition,8,Feature - based Approach with BERT,model,model,1,263,14,14,0,model : model,0.6795865633074936,0.5,0.5
named-entity-recognition,8,"All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre-trained model , and all parameters are jointly fine - tuned on a downstream task .",model,model,0,264,15,15,0,model : model,0.6821705426356589,0.5357142857142857,0.5357142857142857
named-entity-recognition,8,"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",model,model,0,265,16,16,0,model : model,0.6847545219638242,0.5714285714285714,0.5714285714285714
named-entity-recognition,8,"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",model,model,0,266,17,17,0,model : model,0.6873385012919897,0.6071428571428571,0.6071428571428571
named-entity-recognition,8,"Second , there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation .",model,model,0,267,18,18,0,model : model,0.689922480620155,0.6428571428571429,0.6428571428571429
named-entity-recognition,8,"In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task .",model,model,0,268,19,19,0,model : model,0.6925064599483204,0.6785714285714286,0.6785714285714286
named-entity-recognition,8,"In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .",model,model,0,269,20,20,0,model : model,0.6950904392764858,0.7142857142857143,0.7142857142857143
named-entity-recognition,8,"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",model,model,0,270,21,21,0,model : model,0.6976744186046512,0.75,0.75
named-entity-recognition,8,We use the representation of the first sub-token as the input to the token - level classifier over the NER label set .,model,model,0,271,22,22,0,model : model,0.7002583979328165,0.7857142857142857,0.7857142857142857
named-entity-recognition,8,"To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .",model,model,0,272,23,23,0,model : model,0.7028423772609819,0.8214285714285714,0.8214285714285714
named-entity-recognition,8,These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .,model,model,0,273,24,24,0,model : model,0.7054263565891473,0.8571428571428571,0.8571428571428571
named-entity-recognition,8,Results are presented in .,model,model,0,274,25,25,0,model : model,0.7080103359173127,0.8928571428571429,0.8928571428571429
named-entity-recognition,8,BERT LARGE performs competitively with state - of - the - art methods .,model,model,1,275,26,26,0,model : model,0.710594315245478,0.9285714285714286,0.9285714285714286
named-entity-recognition,8,"The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .",model,model,0,276,27,27,0,model : model,0.7131782945736435,0.9642857142857143,0.9642857142857143
named-entity-recognition,8,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,model,model,1,277,28,28,0,model : model,0.7157622739018088,1.0,1.0
named-entity-recognition,8,Conclusion,conclusion,conclusion,0,278,1,1,0,conclusion : conclusion,0.7183462532299741,0.014925373134328358,0.014925373134328358
named-entity-recognition,8,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre-training is an integral part of many language understanding systems .",conclusion,conclusion,0,279,2,2,0,conclusion : conclusion,0.7209302325581395,0.029850746268656716,0.029850746268656716
named-entity-recognition,8,"In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures .",conclusion,conclusion,0,280,3,3,0,conclusion : conclusion,0.7235142118863049,0.04477611940298507,0.04477611940298507
named-entity-recognition,8,"Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre-trained model to successfully tackle a broad set of NLP tasks .",conclusion,conclusion,0,281,4,4,0,conclusion : conclusion,0.7260981912144703,0.05970149253731343,0.05970149253731343
named-entity-recognition,8,We organize the appendix into three sections :,conclusion,conclusion,0,282,5,5,0,conclusion : conclusion,0.7286821705426356,0.07462686567164178,0.07462686567164178
named-entity-recognition,8,Additional implementation details for BERT are presented in Appendix A ; Additional details for our experiments are presented in Appendix B ; and,conclusion,conclusion,0,283,6,6,0,conclusion : conclusion,0.7312661498708011,0.08955223880597014,0.08955223880597014
named-entity-recognition,8,Additional implementation details for BERT are presented in Appendix A ; Additional details for our experiments are presented in Appendix B ; and,conclusion,conclusion,0,284,7,7,0,conclusion : conclusion,0.7338501291989664,0.1044776119402985,0.1044776119402985
named-entity-recognition,8,Additional ablation studies are presented in Appendix C.,conclusion,conclusion,0,285,8,8,0,conclusion : conclusion,0.7364341085271318,0.11940298507462686,0.11940298507462686
named-entity-recognition,8,We present additional ablation studies for BERT including :,conclusion,conclusion,0,286,9,9,0,conclusion : conclusion,0.7390180878552972,0.13432835820895522,0.13432835820895522
named-entity-recognition,8,Effect of Number of Training Steps ; and - Ablation for Different Masking Procedures .,conclusion,conclusion,0,287,10,10,0,conclusion : conclusion,0.7416020671834626,0.14925373134328357,0.14925373134328357
named-entity-recognition,8,Additional Details for BERT,conclusion,conclusion,0,288,11,11,0,conclusion : conclusion,0.7441860465116279,0.16417910447761194,0.16417910447761194
named-entity-recognition,8,Illustration of the Pre-training Tasks,conclusion,conclusion,0,289,12,12,0,conclusion : conclusion,0.7467700258397932,0.1791044776119403,0.1791044776119403
named-entity-recognition,8,We provide examples of the pre-training tasks in the following .,conclusion,conclusion,0,290,13,13,0,conclusion : conclusion,0.7493540051679587,0.19402985074626866,0.19402985074626866
named-entity-recognition,8,"Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by The purpose of this is to bias the representation towards the actual observed word .",conclusion,conclusion,0,291,14,14,0,conclusion : conclusion,0.751937984496124,0.208955223880597,0.208955223880597
named-entity-recognition,8,"The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predictor which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .",conclusion,conclusion,0,292,15,15,0,conclusion : conclusion,0.7545219638242894,0.22388059701492538,0.22388059701492538
named-entity-recognition,8,"Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability .",conclusion,conclusion,0,293,16,16,0,conclusion : conclusion,0.7571059431524548,0.23880597014925373,0.23880597014925373
named-entity-recognition,8,"In Section C.2 , we evaluate the impact this procedure .",conclusion,conclusion,0,294,17,17,0,conclusion : conclusion,0.7596899224806202,0.2537313432835821,0.2537313432835821
named-entity-recognition,8,"Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre-training steps maybe required for the model to converge .",conclusion,conclusion,0,295,18,18,0,conclusion : conclusion,0.7622739018087855,0.26865671641791045,0.26865671641791045
named-entity-recognition,8,"In Section C.1 we demonstrate that MLM does converge marginally slower than a leftto - right model ( which predicts every token ) , but the empirical improvements of the MLM model far outweigh the increased training cost .",conclusion,conclusion,0,296,19,19,0,conclusion : conclusion,0.7648578811369509,0.2835820895522388,0.2835820895522388
named-entity-recognition,8,Next Sentence Prediction,conclusion,conclusion,0,297,20,20,0,conclusion : conclusion,0.7674418604651163,0.29850746268656714,0.29850746268656714
named-entity-recognition,8,The next sentence prediction task can be illustrated in the following examples .,conclusion,conclusion,0,298,21,21,0,conclusion : conclusion,0.7700258397932817,0.31343283582089554,0.31343283582089554
named-entity-recognition,8,"To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as "" sentences "" even though they are typically much longer than single sentences ( but can be shorter also ) .",conclusion,conclusion,0,299,22,22,0,conclusion : conclusion,0.772609819121447,0.3283582089552239,0.3283582089552239
named-entity-recognition,8,The first sentence receives the A embedding and the second receives the B embedding .,conclusion,conclusion,0,300,23,23,0,conclusion : conclusion,0.7751937984496124,0.34328358208955223,0.34328358208955223
named-entity-recognition,8,"50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the "" next sentence prediction "" task .",conclusion,conclusion,0,301,24,24,0,conclusion : conclusion,0.7777777777777778,0.3582089552238806,0.3582089552238806
named-entity-recognition,8,They are sampled such that the combined length is ? 512 tokens .,conclusion,conclusion,0,302,25,25,0,conclusion : conclusion,0.7803617571059431,0.373134328358209,0.373134328358209
named-entity-recognition,8,"The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .",conclusion,conclusion,0,303,26,26,0,conclusion : conclusion,0.7829457364341085,0.3880597014925373,0.3880597014925373
named-entity-recognition,8,"We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .",conclusion,conclusion,0,304,27,27,0,conclusion : conclusion,0.7855297157622739,0.40298507462686567,0.40298507462686567
named-entity-recognition,8,"We use Adam with learning rate of 1 e - 4 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",conclusion,conclusion,0,305,28,28,0,conclusion : conclusion,0.7881136950904393,0.417910447761194,0.417910447761194
named-entity-recognition,8,We use a dropout probability of 0.1 on all layers .,conclusion,conclusion,0,306,29,29,0,conclusion : conclusion,0.7906976744186046,0.43283582089552236,0.43283582089552236
named-entity-recognition,8,"We use a gelu activation rather than the standard relu , following OpenAI GPT .",conclusion,conclusion,0,307,30,30,0,conclusion : conclusion,0.7932816537467701,0.44776119402985076,0.44776119402985076
named-entity-recognition,8,The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .,conclusion,conclusion,0,308,31,31,0,conclusion : conclusion,0.7958656330749354,0.4626865671641791,0.4626865671641791
named-entity-recognition,8,Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .,conclusion,conclusion,0,309,32,32,0,conclusion : conclusion,0.7984496124031008,0.47761194029850745,0.47761194029850745
named-entity-recognition,8,13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .,conclusion,conclusion,0,310,33,33,0,conclusion : conclusion,0.8010335917312662,0.4925373134328358,0.4925373134328358
named-entity-recognition,8,Each pretraining took 4 days to complete .,conclusion,conclusion,0,311,34,34,0,conclusion : conclusion,0.8036175710594315,0.5074626865671642,0.5074626865671642
named-entity-recognition,8,Longer sequences are disproportionately expensive because attention is quadratic to the sequence length .,conclusion,conclusion,0,312,35,35,0,conclusion : conclusion,0.8062015503875969,0.5223880597014925,0.5223880597014925
named-entity-recognition,8,"To speedup pretraing in our experiments , we pre-train the model with sequence length of 128 for 90 % of the steps .",conclusion,conclusion,0,313,36,36,0,conclusion : conclusion,0.8087855297157622,0.5373134328358209,0.5373134328358209
named-entity-recognition,8,"Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .",conclusion,conclusion,0,314,37,37,0,conclusion : conclusion,0.8113695090439277,0.5522388059701493,0.5522388059701493
named-entity-recognition,8,Fine- tuning Procedure,conclusion,conclusion,0,315,38,38,0,conclusion : conclusion,0.813953488372093,0.5671641791044776,0.5671641791044776
named-entity-recognition,8,"For fine - tuning , most model hyperparameters are the same as in pre-training , with the exception of the batch size , learning rate , and number of training epochs .",conclusion,conclusion,0,316,39,39,0,conclusion : conclusion,0.8165374677002584,0.582089552238806,0.582089552238806
named-entity-recognition,8,The dropout probability was always kept at 0.1 .,conclusion,conclusion,0,317,40,40,0,conclusion : conclusion,0.8191214470284238,0.5970149253731343,0.5970149253731343
named-entity-recognition,8,"The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks :",conclusion,conclusion,0,318,41,41,0,conclusion : conclusion,0.8217054263565892,0.6119402985074627,0.6119402985074627
named-entity-recognition,8,"Batch size : 16 , 32",conclusion,conclusion,0,319,42,42,0,conclusion : conclusion,0.8242894056847545,0.6268656716417911,0.6268656716417911
named-entity-recognition,8,"Learning rate ( Adam ) : 5 e - 5 , 3 e - 5 , 2 e - 5 Number of epochs : 2 , 3 , 4",conclusion,conclusion,0,320,43,43,0,conclusion : conclusion,0.8268733850129198,0.6417910447761194,0.6417910447761194
named-entity-recognition,8,"We also observed that large data sets ( e.g. , 100 k + labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .",conclusion,conclusion,0,321,44,44,0,conclusion : conclusion,0.8294573643410853,0.6567164179104478,0.6567164179104478
named-entity-recognition,8,"Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set .",conclusion,conclusion,0,322,45,45,0,conclusion : conclusion,0.8320413436692506,0.6716417910447762,0.6716417910447762
named-entity-recognition,8,"Comparison of BERT , ELMo , and",conclusion,conclusion,0,323,46,46,0,conclusion : conclusion,0.834625322997416,0.6865671641791045,0.6865671641791045
named-entity-recognition,8,Open AI GPT,conclusion,conclusion,0,324,47,47,0,conclusion : conclusion,0.8372093023255814,0.7014925373134329,0.7014925373134329
named-entity-recognition,8,"Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .",conclusion,conclusion,0,325,48,48,0,conclusion : conclusion,0.8397932816537468,0.7164179104477612,0.7164179104477612
named-entity-recognition,8,The comparisons between the model architectures are shown visually in .,conclusion,conclusion,0,326,49,49,0,conclusion : conclusion,0.8423772609819121,0.7313432835820896,0.7313432835820896
named-entity-recognition,8,"Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .",conclusion,conclusion,0,327,50,50,0,conclusion : conclusion,0.8449612403100775,0.746268656716418,0.746268656716418
named-entity-recognition,8,"The most comparable existing pre-training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .",conclusion,conclusion,0,328,51,51,0,conclusion : conclusion,0.8475452196382429,0.7611940298507462,0.7611940298507462
named-entity-recognition,8,"In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .",conclusion,conclusion,0,329,52,52,0,conclusion : conclusion,0.8501291989664083,0.7761194029850746,0.7761194029850746
named-entity-recognition,8,"The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained :",conclusion,conclusion,0,330,53,53,0,conclusion : conclusion,0.8527131782945736,0.7910447761194029,0.7910447761194029
named-entity-recognition,8,"GPT is trained on the Books Corpus ( 800M words ) ; BERT is trained on the Books Corpus ( 800M words ) and Wikipedia ( 2,500 M words ) .",conclusion,conclusion,0,331,54,54,0,conclusion : conclusion,0.8552971576227391,0.8059701492537313,0.8059701492537313
named-entity-recognition,8,"GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words .",conclusion,conclusion,0,332,55,55,0,conclusion : conclusion,0.8578811369509044,0.8208955223880597,0.8208955223880597
named-entity-recognition,8,GPT used the same learning rate of 5 e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .,conclusion,conclusion,0,333,56,56,0,conclusion : conclusion,0.8604651162790697,0.835820895522388,0.835820895522388
named-entity-recognition,8,"To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable .",conclusion,conclusion,0,334,57,57,0,conclusion : conclusion,0.8630490956072352,0.8507462686567164,0.8507462686567164
named-entity-recognition,8,Illustrations of Fine - tuning on Different Tasks,conclusion,conclusion,0,335,58,58,0,conclusion : conclusion,0.8656330749354005,0.8656716417910447,0.8656716417910447
named-entity-recognition,8,The illustration of fine - tuning BERT on different tasks can be seen in .,conclusion,conclusion,0,336,59,59,0,conclusion : conclusion,0.8682170542635659,0.8805970149253731,0.8805970149253731
named-entity-recognition,8,"Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .",conclusion,conclusion,0,337,60,60,0,conclusion : conclusion,0.8708010335917312,0.8955223880597015,0.8955223880597015
named-entity-recognition,8,"Among the tasks , ( a ) and MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task .",conclusion,conclusion,0,338,61,61,0,conclusion : conclusion,0.8733850129198967,0.9104477611940298,0.9104477611940298
named-entity-recognition,8,"Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one .",conclusion,conclusion,0,339,62,62,0,conclusion : conclusion,0.875968992248062,0.9253731343283582,0.9253731343283582
named-entity-recognition,8,QQP,conclusion,conclusion,0,340,63,63,0,conclusion : conclusion,0.8785529715762274,0.9402985074626866,0.9402985074626866
named-entity-recognition,8,Quora Question,conclusion,conclusion,0,341,64,64,0,conclusion : conclusion,0.8811369509043928,0.9552238805970149,0.9552238805970149
named-entity-recognition,8,Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent .,conclusion,conclusion,0,342,65,65,0,conclusion : conclusion,0.8837209302325582,0.9701492537313433,0.9701492537313433
named-entity-recognition,8,BERT E [ CLS ],conclusion,conclusion,0,343,66,66,0,conclusion : conclusion,0.8863049095607235,0.9850746268656716,0.9850746268656716
named-entity-recognition,8,1 E ...,conclusion,conclusion,0,344,67,67,0,conclusion : conclusion,0.8888888888888888,1.0,1.0
named-entity-recognition,8,...,SQuAD v1.1,SQuAD v1.1,0,345,1,1,0,SQuAD v1.1 : SQuAD v1.1,0.8914728682170543,0.058823529411764705,0.058823529411764705
named-entity-recognition,8,SST - 2,SQuAD v1.1,SQuAD v1.1,0,346,2,2,0,SQuAD v1.1 : SQuAD v1.1,0.8940568475452196,0.11764705882352941,0.11764705882352941
named-entity-recognition,8,The Stanford Sentiment Treebank is a binary single - sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment .,SQuAD v1.1,SQuAD v1.1,0,347,3,3,0,SQuAD v1.1 : SQuAD v1.1,0.896640826873385,0.17647058823529413,0.17647058823529413
named-entity-recognition,8,CoLA,SQuAD v1.1,SQuAD v1.1,0,348,4,4,0,SQuAD v1.1 : SQuAD v1.1,0.8992248062015504,0.23529411764705882,0.23529411764705882
named-entity-recognition,8,"The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically "" acceptable "" or not .",SQuAD v1.1,SQuAD v1.1,0,349,5,5,0,SQuAD v1.1 : SQuAD v1.1,0.9018087855297158,0.29411764705882354,0.29411764705882354
named-entity-recognition,8,STS - B,SQuAD v1.1,SQuAD v1.1,0,350,6,6,0,SQuAD v1.1 : SQuAD v1.1,0.9043927648578811,0.35294117647058826,0.35294117647058826
named-entity-recognition,8,The Semantic Textual Similarity,SQuAD v1.1,SQuAD v1.1,0,351,7,7,0,SQuAD v1.1 : SQuAD v1.1,0.9069767441860465,0.4117647058823529,0.4117647058823529
named-entity-recognition,8,Benchmark is a collection of sentence pairs drawn from news headlines and other sources .,SQuAD v1.1,SQuAD v1.1,0,352,8,8,0,SQuAD v1.1 : SQuAD v1.1,0.9095607235142119,0.47058823529411764,0.47058823529411764
named-entity-recognition,8,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,SQuAD v1.1,SQuAD v1.1,0,353,9,9,0,SQuAD v1.1 : SQuAD v1.1,0.9121447028423773,0.5294117647058824,0.5294117647058824
named-entity-recognition,8,MRPC,SQuAD v1.1,SQuAD v1.1,0,354,10,10,0,SQuAD v1.1 : SQuAD v1.1,0.9147286821705426,0.5882352941176471,0.5882352941176471
named-entity-recognition,8,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",SQuAD v1.1,SQuAD v1.1,0,355,11,11,0,SQuAD v1.1 : SQuAD v1.1,0.917312661498708,0.6470588235294118,0.6470588235294118
named-entity-recognition,8,RTE,SQuAD v1.1,SQuAD v1.1,0,356,12,12,0,SQuAD v1.1 : SQuAD v1.1,0.9198966408268734,0.7058823529411765,0.7058823529411765
named-entity-recognition,8,"Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ) .",SQuAD v1.1,SQuAD v1.1,0,357,13,13,0,SQuAD v1.1 : SQuAD v1.1,0.9224806201550387,0.7647058823529411,0.7647058823529411
named-entity-recognition,8,14 WNLI Winograd NLI is a small natural language inference dataset .,SQuAD v1.1,SQuAD v1.1,0,358,14,14,0,SQuAD v1.1 : SQuAD v1.1,0.9250645994832042,0.8235294117647058,0.8235294117647058
named-entity-recognition,8,"The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .",SQuAD v1.1,SQuAD v1.1,0,359,15,15,0,SQuAD v1.1 : SQuAD v1.1,0.9276485788113695,0.8823529411764706,0.8823529411764706
named-entity-recognition,8,We therefore exclude this set to be fair to OpenAI GPT .,SQuAD v1.1,SQuAD v1.1,0,360,16,16,0,SQuAD v1.1 : SQuAD v1.1,0.9302325581395349,0.9411764705882353,0.9411764705882353
named-entity-recognition,8,"For our GLUE submission , we always predicted the ma-jority class .",SQuAD v1.1,SQuAD v1.1,0,361,17,17,0,SQuAD v1.1 : SQuAD v1.1,0.9328165374677002,1.0,1.0
named-entity-recognition,8,Additional Ablation Studies,ablation,ablation,0,362,1,1,0,ablation : ablation,0.9354005167958657,0.038461538461538464,0.038461538461538464
named-entity-recognition,8,Effect of Number of Training Steps presents MNLI,ablation,ablation,0,363,2,2,0,ablation : ablation,0.937984496124031,0.07692307692307693,0.07692307692307693
named-entity-recognition,8,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,ablation,ablation,0,364,3,3,0,ablation : ablation,0.9405684754521964,0.11538461538461539,0.11538461538461539
named-entity-recognition,8,This allows us to answer the following questions :,ablation,ablation,0,365,4,4,0,ablation : ablation,0.9431524547803618,0.15384615384615385,0.15384615384615385
named-entity-recognition,8,1 .,ablation,ablation,0,366,5,5,0,ablation : ablation,0.9457364341085271,0.19230769230769232,0.19230769230769232
named-entity-recognition,8,Question :,ablation,ablation,0,367,6,6,0,ablation : ablation,0.9483204134366925,0.23076923076923078,0.23076923076923078
named-entity-recognition,8,"Does BERT really need such a large amount of pre-training ( 128,000 words / batch * 1,000,000 steps ) to achieve high fine - tuning accuracy ? Answer : Yes , BERT BASE achieves almost 1.0 % additional accuracy on MNLI when trained on 1 M steps compared to 500 k steps .",ablation,ablation,0,368,7,7,0,ablation : ablation,0.9509043927648578,0.2692307692307692,0.2692307692307692
named-entity-recognition,8,"Does BERT really need such a large amount of pre-training ( 128,000 words / batch * 1,000,000 steps ) to achieve high fine - tuning accuracy ? Answer : Yes , BERT BASE achieves almost 1.0 % additional accuracy on MNLI when trained on 1 M steps compared to 500 k steps .",ablation,ablation,0,369,8,8,0,ablation : ablation,0.9534883720930233,0.3076923076923077,0.3076923076923077
named-entity-recognition,8,. Question :,ablation,ablation,0,370,9,9,0,ablation : ablation,0.9560723514211886,0.34615384615384615,0.34615384615384615
named-entity-recognition,8,"Does MLM pre-training converge slower than LTR pre-training , since only 15 % of words are predicted in each batch rather than every word ? Answer : The MLM model does converge slightly slower than the LTR model .",ablation,ablation,0,371,10,10,0,ablation : ablation,0.958656330749354,0.38461538461538464,0.38461538461538464
named-entity-recognition,8,"Does MLM pre-training converge slower than LTR pre-training , since only 15 % of words are predicted in each batch rather than every word ? Answer : The MLM model does converge slightly slower than the LTR model .",ablation,ablation,0,372,11,11,0,ablation : ablation,0.9612403100775194,0.4230769230769231,0.4230769230769231
named-entity-recognition,8,"However , in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately .",ablation,ablation,0,373,12,12,0,ablation : ablation,0.9638242894056848,0.46153846153846156,0.46153846153846156
named-entity-recognition,8,Ablation for Different Masking Procedures,ablation,ablation,0,374,13,13,0,ablation : ablation,0.9664082687338501,0.5,0.5
named-entity-recognition,8,"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model ( MLM ) objective .",ablation,ablation,0,375,14,14,0,ablation : ablation,0.9689922480620154,0.5384615384615384,0.5384615384615384
named-entity-recognition,8,The following is an ablation study to evaluate the effect of different masking strategies .,ablation,ablation,0,376,15,15,0,ablation : ablation,0.9715762273901809,0.5769230769230769,0.5769230769230769
named-entity-recognition,8,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",ablation,ablation,0,377,16,16,0,ablation : ablation,0.9741602067183462,0.6153846153846154,0.6153846153846154
named-entity-recognition,8,We report the Dev results for both MNLI and NER .,ablation,ablation,0,378,17,17,0,ablation : ablation,0.9767441860465116,0.6538461538461539,0.6538461538461539
named-entity-recognition,8,"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",ablation,ablation,0,379,18,18,0,ablation : ablation,0.979328165374677,0.6923076923076923,0.6923076923076923
named-entity-recognition,8,The results are presented in .,ablation,ablation,0,380,19,19,0,ablation : ablation,0.9819121447028424,0.7307692307692307,0.7307692307692307
named-entity-recognition,8,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",ablation,ablation,0,381,20,20,0,ablation : ablation,0.9844961240310077,0.7692307692307693,0.7692307692307693
named-entity-recognition,8,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",ablation,ablation,0,382,21,21,0,ablation : ablation,0.9870801033591732,0.8076923076923077,0.8076923076923077
named-entity-recognition,8,The right part of the paper represents the Dev set results .,ablation,ablation,0,383,22,22,0,ablation : ablation,0.9896640826873385,0.8461538461538461,0.8461538461538461
named-entity-recognition,8,"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .",ablation,ablation,0,384,23,23,0,ablation : ablation,0.9922480620155039,0.8846153846153846,0.8846153846153846
named-entity-recognition,8,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,ablation,ablation,0,385,24,24,0,ablation : ablation,0.9948320413436692,0.9230769230769231,0.9230769230769231
named-entity-recognition,8,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",ablation,ablation,0,386,25,25,0,ablation : ablation,0.9974160206718347,0.9615384615384616,0.9615384615384616
named-entity-recognition,8,"Interestingly , using only the RND strategy performs much worse than our strategy as well .",ablation,ablation,0,387,26,26,0,ablation : ablation,1.0,1.0,1.0
named-entity-recognition,9,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,title,1,2,1,1,0,title : title,0.010050251256281407,1.0,1.0
named-entity-recognition,9,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01507537688442211,0.07692307692307693,0.07692307692307693
named-entity-recognition,9,Motivation :,abstract,abstract,0,4,2,2,0,abstract : abstract,0.020100502512562814,0.15384615384615385,0.15384615384615385
named-entity-recognition,9,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.02512562814070352,0.23076923076923078,0.23076923076923078
named-entity-recognition,9,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.03015075376884422,0.3076923076923077,0.3076923076923077
named-entity-recognition,9,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.035175879396984924,0.38461538461538464,0.38461538461538464
named-entity-recognition,9,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,abstract,1,8,6,6,0,abstract : abstract,0.04020100502512563,0.46153846153846156,0.46153846153846156
named-entity-recognition,9,Results :,abstract,abstract,0,9,7,7,0,abstract : abstract,0.04522613065326633,0.5384615384615384,0.5384615384615384
named-entity-recognition,9,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.05025125628140704,0.6153846153846154,0.6153846153846154
named-entity-recognition,9,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.05527638190954774,0.6923076923076923,0.6923076923076923
named-entity-recognition,9,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.06030150753768844,0.7692307692307693,0.7692307692307693
named-entity-recognition,9,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.06532663316582915,0.8461538461538461,0.8461538461538461
named-entity-recognition,9,Availability and implementation :,abstract,abstract,0,14,12,12,0,abstract : abstract,0.07035175879396985,0.9230769230769231,0.9230769230769231
named-entity-recognition,9,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract,abstract,1,15,13,13,0,abstract : abstract,0.07537688442211055,1.0,1.0
named-entity-recognition,9,Introduction,introduction,introduction,0,16,1,1,0,introduction : introduction,0.08040201005025126,0.058823529411764705,0.058823529411764705
named-entity-recognition,9,The volume of biomedical literature continues to rapidly increase .,introduction,introduction,0,17,2,2,0,introduction : introduction,0.08542713567839195,0.11764705882352941,0.11764705882352941
named-entity-recognition,9,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",introduction,introduction,0,18,3,3,0,introduction : introduction,0.09045226130653267,0.17647058823529413,0.17647058823529413
named-entity-recognition,9,PubMed alone has a total of 29M articles as of January 2019 .,introduction,introduction,0,19,4,4,0,introduction : introduction,0.09547738693467336,0.23529411764705882,0.23529411764705882
named-entity-recognition,9,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,introduction,introduction,0,20,5,5,0,introduction : introduction,0.10050251256281408,0.29411764705882354,0.29411764705882354
named-entity-recognition,9,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",introduction,introduction,0,21,6,6,0,introduction : introduction,0.10552763819095477,0.35294117647058826,0.35294117647058826
named-entity-recognition,9,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,introduction,introduction,0,22,7,7,0,introduction : introduction,0.11055276381909548,0.4117647058823529,0.4117647058823529
named-entity-recognition,9,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",introduction,introduction,0,23,8,8,0,introduction : introduction,0.11557788944723618,0.47058823529411764,0.47058823529411764
named-entity-recognition,9,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,introduction,introduction,0,24,9,9,0,introduction : introduction,0.12060301507537688,0.5294117647058824,0.5294117647058824
named-entity-recognition,9,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",introduction,introduction,0,25,10,10,0,introduction : introduction,0.12562814070351758,0.5882352941176471,0.5882352941176471
named-entity-recognition,9,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",introduction,introduction,0,26,11,11,0,introduction : introduction,0.1306532663316583,0.6470588235294118,0.6470588235294118
named-entity-recognition,9,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",introduction,introduction,1,27,12,12,0,introduction : introduction,0.135678391959799,0.7058823529411765,0.7058823529411765
named-entity-recognition,9,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",introduction,introduction,0,28,13,13,0,introduction : introduction,0.1407035175879397,0.7647058823529411,0.7647058823529411
named-entity-recognition,9,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",introduction,introduction,0,29,14,14,0,introduction : introduction,0.1457286432160804,0.8235294117647058,0.8235294117647058
named-entity-recognition,9,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions thatare usually not included in a general domain corpus .",introduction,introduction,0,30,15,15,0,introduction : introduction,0.1507537688442211,0.8823529411764706,0.8823529411764706
named-entity-recognition,9,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",introduction,introduction,0,31,16,16,0,introduction : introduction,0.15577889447236182,0.9411764705882353,0.9411764705882353
named-entity-recognition,9,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",introduction,introduction,0,32,17,17,0,introduction : introduction,0.16080402010050251,1.0,1.0
named-entity-recognition,9,Approach,approach,Approach,0,33,1,1,0,approach : Approach,0.1658291457286432,0.07142857142857142,0.07142857142857142
named-entity-recognition,9,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",approach,Approach,1,34,2,2,0,approach : Approach,0.1708542713567839,0.14285714285714285,0.14285714285714285
named-entity-recognition,9,The over all process of pre-training and fine - tuning BioBERT is illustrated in .,approach,Approach,0,35,3,3,0,approach : Approach,0.17587939698492464,0.21428571428571427,0.21428571428571427
named-entity-recognition,9,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",approach,Approach,1,36,4,4,0,approach : Approach,0.18090452261306533,0.2857142857142857,0.2857142857142857
named-entity-recognition,9,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",approach,Approach,1,37,5,5,0,approach : Approach,0.18592964824120603,0.35714285714285715,0.35714285714285715
named-entity-recognition,9,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",approach,Approach,1,38,6,6,0,approach : Approach,0.19095477386934673,0.42857142857142855,0.42857142857142855
named-entity-recognition,9,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",approach,Approach,0,39,7,7,0,approach : Approach,0.19597989949748743,0.5,0.5
named-entity-recognition,9,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,approach,Approach,0,40,8,8,0,approach : Approach,0.20100502512562815,0.5714285714285714,0.5714285714285714
named-entity-recognition,9,The contributions of our paper are as follows :,approach,Approach,0,41,9,9,0,approach : Approach,0.20603015075376885,0.6428571428571429,0.6428571428571429
named-entity-recognition,9,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,approach,Approach,0,42,10,10,0,approach : Approach,0.21105527638190955,0.7142857142857143,0.7142857142857143
named-entity-recognition,9,We show that pre-training BERT on biomedical corpora largely improves its performance .,approach,Approach,0,43,11,11,0,approach : Approach,0.21608040201005024,0.7857142857142857,0.7857142857142857
named-entity-recognition,9,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",approach,Approach,0,44,12,12,0,approach : Approach,0.22110552763819097,0.8571428571428571,0.8571428571428571
named-entity-recognition,9,"Compared with most previous biomedical text mining models thatare mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",approach,Approach,0,45,13,13,0,approach : Approach,0.22613065326633167,0.9285714285714286,0.9285714285714286
named-entity-recognition,9,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",approach,Approach,0,46,14,14,0,approach : Approach,0.23115577889447236,1.0,1.0
named-entity-recognition,9,Materials and methods,method,Materials and methods,0,47,1,1,0,method : Materials and methods,0.23618090452261306,0.02,0.3333333333333333
named-entity-recognition,9,BioBERT basically has the same structure as BERT .,method,Materials and methods,0,48,2,2,0,method : Materials and methods,0.24120603015075376,0.04,0.6666666666666666
named-entity-recognition,9,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",method,Materials and methods,0,49,3,3,0,method : Materials and methods,0.24623115577889448,0.06,1.0
named-entity-recognition,9,BERT : bidirectional encoder representations from transformers,method,BERT: bidirectional encoder representations from transformers,0,50,4,1,0,method : BERT: bidirectional encoder representations from transformers,0.25125628140703515,0.08,0.09090909090909091
named-entity-recognition,9,Learning word representations from a large amount of unannotated text is a long - established method .,method,BERT: bidirectional encoder representations from transformers,0,51,5,2,0,method : BERT: bidirectional encoder representations from transformers,0.2562814070351759,0.1,0.18181818181818182
named-entity-recognition,9,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",method,BERT: bidirectional encoder representations from transformers,0,52,6,3,0,method : BERT: bidirectional encoder representations from transformers,0.2613065326633166,0.12,0.2727272727272727
named-entity-recognition,9,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",method,BERT: bidirectional encoder representations from transformers,0,53,7,4,0,method : BERT: bidirectional encoder representations from transformers,0.2663316582914573,0.14,0.36363636363636365
named-entity-recognition,9,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,method,BERT: bidirectional encoder representations from transformers,0,54,8,5,0,method : BERT: bidirectional encoder representations from transformers,0.271356783919598,0.16,0.45454545454545453
named-entity-recognition,9,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",method,BERT: bidirectional encoder representations from transformers,0,55,9,6,0,method : BERT: bidirectional encoder representations from transformers,0.27638190954773867,0.18,0.5454545454545454
named-entity-recognition,9,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",method,BERT: bidirectional encoder representations from transformers,0,56,10,7,0,method : BERT: bidirectional encoder representations from transformers,0.2814070351758794,0.2,0.6363636363636364
named-entity-recognition,9,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",method,BERT: bidirectional encoder representations from transformers,0,57,11,8,0,method : BERT: bidirectional encoder representations from transformers,0.2864321608040201,0.22,0.7272727272727273
named-entity-recognition,9,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",method,BERT: bidirectional encoder representations from transformers,0,58,12,9,0,method : BERT: bidirectional encoder representations from transformers,0.2914572864321608,0.24,0.8181818181818182
named-entity-recognition,9,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,method,BERT: bidirectional encoder representations from transformers,0,59,13,10,0,method : BERT: bidirectional encoder representations from transformers,0.2964824120603015,0.26,0.9090909090909091
named-entity-recognition,9,"Due to the space limitations , we refer readers to for a more detailed description of BERT .",method,BERT: bidirectional encoder representations from transformers,0,60,14,11,0,method : BERT: bidirectional encoder representations from transformers,0.3015075376884422,0.28,1.0
named-entity-recognition,9,Pre-training BioBERT,method,Pre-training BioBERT,0,61,15,1,0,method : Pre-training BioBERT,0.3065326633165829,0.3,0.07142857142857142
named-entity-recognition,9,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",method,Pre-training BioBERT,0,62,16,2,0,method : Pre-training BioBERT,0.31155778894472363,0.32,0.14285714285714285
named-entity-recognition,9,"However , biomedical domain texts contain a considerable number of domain - specific .",method,Pre-training BioBERT,0,63,17,3,0,method : Pre-training BioBERT,0.3165829145728643,0.34,0.21428571428571427
named-entity-recognition,9,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",method,Pre-training BioBERT,0,64,18,4,0,method : Pre-training BioBERT,0.32160804020100503,0.36,0.2857142857142857
named-entity-recognition,9,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",method,Pre-training BioBERT,0,65,19,5,0,method : Pre-training BioBERT,0.32663316582914576,0.38,0.35714285714285715
named-entity-recognition,9,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",method,Pre-training BioBERT,0,66,20,6,0,method : Pre-training BioBERT,0.3316582914572864,0.4,0.42857142857142855
named-entity-recognition,9,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",method,Pre-training BioBERT,0,67,21,7,0,method : Pre-training BioBERT,0.33668341708542715,0.42,0.5
named-entity-recognition,9,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",method,Pre-training BioBERT,0,68,22,8,0,method : Pre-training BioBERT,0.3417085427135678,0.44,0.5714285714285714
named-entity-recognition,9,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,method,Pre-training BioBERT,0,69,23,9,0,method : Pre-training BioBERT,0.34673366834170855,0.46,0.6428571428571429
named-entity-recognition,9,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",method,Pre-training BioBERT,0,70,24,10,0,method : Pre-training BioBERT,0.35175879396984927,0.48,0.7142857142857143
named-entity-recognition,9,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",method,Pre-training BioBERT,0,71,25,11,0,method : Pre-training BioBERT,0.35678391959798994,0.5,0.7857142857142857
named-entity-recognition,9,##mm ##uno ##g ##lo # #bul # #in ) .,method,Pre-training BioBERT,0,72,26,12,0,method : Pre-training BioBERT,0.36180904522613067,0.52,0.8571428571428571
named-entity-recognition,9,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,method,Pre-training BioBERT,0,73,27,13,0,method : Pre-training BioBERT,0.36683417085427134,0.54,0.9285714285714286
named-entity-recognition,9,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",method,Pre-training BioBERT,0,74,28,14,0,method : Pre-training BioBERT,0.37185929648241206,0.56,1.0
named-entity-recognition,9,Fine-tuning BioBERT,method,Fine-tuning BioBERT,0,75,29,1,0,method : Fine-tuning BioBERT,0.3768844221105528,0.58,0.045454545454545456
named-entity-recognition,9,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",method,Fine-tuning BioBERT,0,76,30,2,0,method : Fine-tuning BioBERT,0.38190954773869346,0.6,0.09090909090909091
named-entity-recognition,9,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",method,Fine-tuning BioBERT,0,77,31,3,0,method : Fine-tuning BioBERT,0.3869346733668342,0.62,0.13636363636363635
named-entity-recognition,9,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",method,Fine-tuning BioBERT,0,78,32,4,0,method : Fine-tuning BioBERT,0.39195979899497485,0.64,0.18181818181818182
named-entity-recognition,9,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",method,Fine-tuning BioBERT,0,79,33,5,0,method : Fine-tuning BioBERT,0.3969849246231156,0.66,0.22727272727272727
named-entity-recognition,9,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,method,Fine-tuning BioBERT,0,80,34,6,0,method : Fine-tuning BioBERT,0.4020100502512563,0.68,0.2727272727272727
named-entity-recognition,9,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",method,Fine-tuning BioBERT,0,81,35,7,0,method : Fine-tuning BioBERT,0.40703517587939697,0.7,0.3181818181818182
named-entity-recognition,9,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",method,Fine-tuning BioBERT,0,82,36,8,0,method : Fine-tuning BioBERT,0.4120603015075377,0.72,0.36363636363636365
named-entity-recognition,9,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,method,Fine-tuning BioBERT,0,83,37,9,0,method : Fine-tuning BioBERT,0.41708542713567837,0.74,0.4090909090909091
named-entity-recognition,9,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",method,Fine-tuning BioBERT,0,84,38,10,0,method : Fine-tuning BioBERT,0.4221105527638191,0.76,0.45454545454545453
named-entity-recognition,9,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,method,Fine-tuning BioBERT,0,85,39,11,0,method : Fine-tuning BioBERT,0.4271356783919598,0.78,0.5
named-entity-recognition,9,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,method,Fine-tuning BioBERT,0,86,40,12,0,method : Fine-tuning BioBERT,0.4321608040201005,0.8,0.5454545454545454
named-entity-recognition,9,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,method,Fine-tuning BioBERT,0,87,41,13,0,method : Fine-tuning BioBERT,0.4371859296482412,0.82,0.5909090909090909
named-entity-recognition,9,"The precision , recall and F 1 scores on the RE task are reported .",method,Fine-tuning BioBERT,0,88,42,14,0,method : Fine-tuning BioBERT,0.44221105527638194,0.84,0.6363636363636364
named-entity-recognition,9,Question answering is a task of answering questions posed in natural language given related passages .,method,Fine-tuning BioBERT,0,89,43,15,0,method : Fine-tuning BioBERT,0.4472361809045226,0.86,0.6818181818181818
named-entity-recognition,9,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",method,Fine-tuning BioBERT,0,90,44,16,0,method : Fine-tuning BioBERT,0.45226130653266333,0.88,0.7272727272727273
named-entity-recognition,9,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,method,Fine-tuning BioBERT,0,91,45,17,0,method : Fine-tuning BioBERT,0.457286432160804,0.9,0.7727272727272727
named-entity-recognition,9,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,method,Fine-tuning BioBERT,0,92,46,18,0,method : Fine-tuning BioBERT,0.4623115577889447,0.92,0.8181818181818182
named-entity-recognition,9,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",method,Fine-tuning BioBERT,0,93,47,19,0,method : Fine-tuning BioBERT,0.46733668341708545,0.94,0.8636363636363636
named-entity-recognition,9,"Like , we excluded the samples with unanswerable questions from the training sets .",method,Fine-tuning BioBERT,0,94,48,20,0,method : Fine-tuning BioBERT,0.4723618090452261,0.96,0.9090909090909091
named-entity-recognition,9,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",method,Fine-tuning BioBERT,0,95,49,21,0,method : Fine-tuning BioBERT,0.47738693467336685,0.98,0.9545454545454546
named-entity-recognition,9,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",method,Fine-tuning BioBERT,0,96,50,22,0,method : Fine-tuning BioBERT,0.4824120603015075,1.0,1.0
named-entity-recognition,9,Results,result,Results,0,97,1,1,0,result : Results,0.48743718592964824,1.0,1.0
named-entity-recognition,9,Datasets,dataset,Datasets,0,98,1,1,0,dataset : Datasets,0.49246231155778897,0.05,0.05
named-entity-recognition,9,The statistics of biomedical NER datasets are listed in .,dataset,Datasets,0,99,2,2,0,dataset : Datasets,0.49748743718592964,0.1,0.1
named-entity-recognition,9,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",dataset,Datasets,0,100,3,3,0,dataset : Datasets,0.5025125628140703,0.15,0.15
named-entity-recognition,9,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,dataset,Datasets,0,101,4,4,0,dataset : Datasets,0.507537688442211,0.2,0.2
named-entity-recognition,9,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,dataset,Datasets,0,102,5,5,0,dataset : Datasets,0.5125628140703518,0.25,0.25
named-entity-recognition,9,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,dataset,Datasets,0,103,6,6,0,dataset : Datasets,0.5175879396984925,0.3,0.3
named-entity-recognition,9,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",dataset,Datasets,0,104,7,7,0,dataset : Datasets,0.5226130653266332,0.35,0.35
named-entity-recognition,9,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets thatare frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",dataset,Datasets,0,105,8,8,0,dataset : Datasets,0.5276381909547738,0.4,0.4
named-entity-recognition,9,The RE datasets contain gene - disease relations and protein - chemical relations ) .,dataset,Datasets,0,106,9,9,0,dataset : Datasets,0.5326633165829145,0.45,0.45
named-entity-recognition,9,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,dataset,Datasets,0,107,10,10,0,dataset : Datasets,0.5376884422110553,0.5,0.5
named-entity-recognition,9,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",dataset,Datasets,0,108,11,11,0,dataset : Datasets,0.542713567839196,0.55,0.55
named-entity-recognition,9,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",dataset,Datasets,0,109,12,12,0,dataset : Datasets,0.5477386934673367,0.6,0.6
named-entity-recognition,9,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,dataset,Datasets,0,110,13,13,0,dataset : Datasets,0.5527638190954773,0.65,0.65
named-entity-recognition,9,We have made the pre-processed BioASQ datasets publicly available .,dataset,Datasets,0,111,14,14,0,dataset : Datasets,0.5577889447236181,0.7,0.7
named-entity-recognition,9,"For all the datasets , we used the same dataset splits used in previous works ) for a fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",dataset,Datasets,0,112,15,15,0,dataset : Datasets,0.5628140703517588,0.75,0.75
named-entity-recognition,9,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",dataset,Datasets,0,113,16,16,0,dataset : Datasets,0.5678391959798995,0.8,0.8
named-entity-recognition,9,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,dataset,Datasets,0,114,17,17,0,dataset : Datasets,0.5728643216080402,0.85,0.85
named-entity-recognition,9,Note that the state - of - the - art models each have a different architecture and training procedure .,dataset,Datasets,0,115,18,18,0,dataset : Datasets,0.5778894472361809,0.9,0.9
named-entity-recognition,9,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",dataset,Datasets,0,116,19,19,0,dataset : Datasets,0.5829145728643216,0.95,0.95
named-entity-recognition,9,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",dataset,Datasets,0,117,20,20,0,dataset : Datasets,0.5879396984924623,1.0,1.0
named-entity-recognition,9,Experimental setups,experiment,Experimental setups,0,118,1,1,0,experiment : Experimental setups,0.592964824120603,0.03125,0.058823529411764705
named-entity-recognition,9,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,experiment,Experimental setups,1,119,2,2,0,experiment : Experimental setups,0.5979899497487438,0.0625,0.11764705882352941
named-entity-recognition,9,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,experiment,Experimental setups,1,120,3,3,0,experiment : Experimental setups,0.6030150753768844,0.09375,0.17647058823529413
named-entity-recognition,9,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",experiment,Experimental setups,0,121,4,4,0,experiment : Experimental setups,0.6080402010050251,0.125,0.23529411764705882
named-entity-recognition,9,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",experiment,Experimental setups,0,122,5,5,0,experiment : Experimental setups,0.6130653266331658,0.15625,0.29411764705882354
named-entity-recognition,9,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",experiment,Experimental setups,0,123,6,6,0,experiment : Experimental setups,0.6180904522613065,0.1875,0.35294117647058826
named-entity-recognition,9,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,experiment,Experimental setups,0,124,7,7,0,experiment : Experimental setups,0.6231155778894473,0.21875,0.4117647058823529
named-entity-recognition,9,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",experiment,Experimental setups,0,125,8,8,0,experiment : Experimental setups,0.628140703517588,0.25,0.47058823529411764
named-entity-recognition,9,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,experiment,Experimental setups,1,126,9,9,0,experiment : Experimental setups,0.6331658291457286,0.28125,0.5294117647058824
named-entity-recognition,9,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",experiment,Experimental setups,1,127,10,10,0,experiment : Experimental setups,0.6381909547738693,0.3125,0.5882352941176471
named-entity-recognition,9,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,experiment,Experimental setups,0,128,11,11,0,experiment : Experimental setups,0.6432160804020101,0.34375,0.6470588235294118
named-entity-recognition,9,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",experiment,Experimental setups,0,129,12,12,0,experiment : Experimental setups,0.6482412060301508,0.375,0.7058823529411765
named-entity-recognition,9,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,experiment,Experimental setups,1,130,13,13,0,experiment : Experimental setups,0.6532663316582915,0.40625,0.7647058823529411
named-entity-recognition,9,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,experiment,Experimental setups,0,131,14,14,0,experiment : Experimental setups,0.6582914572864321,0.4375,0.8235294117647058
named-entity-recognition,9,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",experiment,Experimental setups,0,132,15,15,0,experiment : Experimental setups,0.6633165829145728,0.46875,0.8823529411764706
named-entity-recognition,9,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,experiment,Experimental setups,0,133,16,16,0,experiment : Experimental setups,0.6683417085427136,0.5,0.9411764705882353
named-entity-recognition,9,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",experiment,Experimental setups,0,134,17,17,0,experiment : Experimental setups,0.6733668341708543,0.53125,1.0
named-entity-recognition,9,Experimental results,experiment,Experimental results,0,135,18,1,0,experiment : Experimental results,0.678391959798995,0.5625,0.06666666666666667
named-entity-recognition,9,The results of NER are shown in .,experiment,Experimental results,1,136,19,2,0,experiment : Experimental results,0.6834170854271356,0.59375,0.13333333333333333
named-entity-recognition,9,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",experiment,Experimental results,0,137,20,3,0,experiment : Experimental results,0.6884422110552764,0.625,0.2
named-entity-recognition,9,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",experiment,Experimental results,1,138,21,4,0,experiment : Experimental results,0.6934673366834171,0.65625,0.26666666666666666
named-entity-recognition,9,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",experiment,Experimental results,1,139,22,5,0,experiment : Experimental results,0.6984924623115578,0.6875,0.3333333333333333
named-entity-recognition,9,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",experiment,Experimental results,0,140,23,6,0,experiment : Experimental results,0.7035175879396985,0.71875,0.4
named-entity-recognition,9,The RE results of each model are shown in .,experiment,Experimental results,1,141,24,7,0,experiment : Experimental results,0.7085427135678392,0.75,0.4666666666666667
named-entity-recognition,9,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",experiment,Experimental results,0,142,25,8,0,experiment : Experimental results,0.7135678391959799,0.78125,0.5333333333333333
named-entity-recognition,9,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",experiment,Experimental results,1,143,26,9,0,experiment : Experimental results,0.7185929648241206,0.8125,0.6
named-entity-recognition,9,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",experiment,Experimental results,1,144,27,10,0,experiment : Experimental results,0.7236180904522613,0.84375,0.6666666666666666
named-entity-recognition,9,The QA results are shown in .,experiment,Experimental results,1,145,28,11,0,experiment : Experimental results,0.7286432160804021,0.875,0.7333333333333333
named-entity-recognition,9,We micro averaged the best scores of the state - of - the - art models from each batch .,experiment,Experimental results,0,146,29,12,0,experiment : Experimental results,0.7336683417085427,0.90625,0.8
named-entity-recognition,9,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,experiment,Experimental results,0,147,30,13,0,experiment : Experimental results,0.7386934673366834,0.9375,0.8666666666666667
named-entity-recognition,9,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",experiment,Experimental results,1,148,31,14,0,experiment : Experimental results,0.7437185929648241,0.96875,0.9333333333333333
named-entity-recognition,9,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",experiment,Experimental results,1,149,32,15,0,experiment : Experimental results,0.7487437185929648,1.0,1.0
named-entity-recognition,9,Discussion,discussion,Discussion,0,150,1,1,0,discussion : Discussion,0.7537688442211056,0.058823529411764705,0.058823529411764705
named-entity-recognition,9,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,discussion,Discussion,0,151,2,2,0,discussion : Discussion,0.7587939698492462,0.11764705882352941,0.11764705882352941
named-entity-recognition,9,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",discussion,Discussion,0,152,3,3,0,discussion : Discussion,0.7638190954773869,0.17647058823529413,0.17647058823529413
named-entity-recognition,9,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",discussion,Discussion,0,153,4,4,0,discussion : Discussion,0.7688442211055276,0.23529411764705882,0.23529411764705882
named-entity-recognition,9,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",discussion,Discussion,0,154,5,5,0,discussion : Discussion,0.7738693467336684,0.29411764705882354,0.29411764705882354
named-entity-recognition,9,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,discussion,Discussion,0,155,6,6,0,discussion : Discussion,0.7788944723618091,0.35294117647058826,0.35294117647058826
named-entity-recognition,9,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,discussion,Discussion,0,156,7,7,0,discussion : Discussion,0.7839195979899497,0.4117647058823529,0.4117647058823529
named-entity-recognition,9,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,discussion,Discussion,0,157,8,8,0,discussion : Discussion,0.7889447236180904,0.47058823529411764,0.47058823529411764
named-entity-recognition,9,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",discussion,Discussion,0,158,9,9,0,discussion : Discussion,0.7939698492462312,0.5294117647058824,0.5294117647058824
named-entity-recognition,9,"F1 scores were used for NER / RE , and MRR scores were used for QA .",discussion,Discussion,0,159,10,10,0,discussion : Discussion,0.7989949748743719,0.5882352941176471,0.5882352941176471
named-entity-recognition,9,BioBERT significantly improves performance on most of the datasets .,discussion,Discussion,0,160,11,11,0,discussion : Discussion,0.8040201005025126,0.6470588235294118,0.6470588235294118
named-entity-recognition,9,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to see the effect of pre-training on downstream tasks .",discussion,Discussion,0,161,12,12,0,discussion : Discussion,0.8090452261306532,0.7058823529411765,0.7058823529411765
named-entity-recognition,9,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,discussion,Discussion,0,162,13,13,0,discussion : Discussion,0.8140703517587939,0.7647058823529411,0.7647058823529411
named-entity-recognition,9,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",discussion,Discussion,0,163,14,14,0,discussion : Discussion,0.8190954773869347,0.8235294117647058,0.8235294117647058
named-entity-recognition,9,entities .,discussion,Discussion,0,164,15,15,0,discussion : Discussion,0.8241206030150754,0.8823529411764706,0.8823529411764706
named-entity-recognition,9,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",discussion,Discussion,0,165,16,16,0,discussion : Discussion,0.8291457286432161,0.9411764705882353,0.9411764705882353
named-entity-recognition,9,"Also , BioBERT can provide longer named entities as answers .",discussion,Discussion,0,166,17,17,0,discussion : Discussion,0.8341708542713567,1.0,1.0
named-entity-recognition,9,Conclusion,conclusion,Conclusion,0,167,1,1,0,conclusion : Conclusion,0.8391959798994975,0.06666666666666667,0.06666666666666667
named-entity-recognition,9,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",conclusion,Conclusion,0,168,2,2,0,conclusion : Conclusion,0.8442211055276382,0.13333333333333333,0.13333333333333333
named-entity-recognition,9,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,conclusion,Conclusion,0,169,3,3,0,conclusion : Conclusion,0.8492462311557789,0.2,0.2
named-entity-recognition,9,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",conclusion,Conclusion,0,170,4,4,0,conclusion : Conclusion,0.8542713567839196,0.26666666666666666,0.26666666666666666
named-entity-recognition,9,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",conclusion,Conclusion,0,171,5,5,0,conclusion : Conclusion,0.8592964824120602,0.3333333333333333,0.3333333333333333
named-entity-recognition,9,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,conclusion,Conclusion,0,172,6,6,0,conclusion : Conclusion,0.864321608040201,0.4,0.4
named-entity-recognition,9,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",conclusion,Conclusion,0,173,7,7,0,conclusion : Conclusion,0.8693467336683417,0.4666666666666667,0.4666666666666667
named-entity-recognition,9,"The best scores are in bold , and the second best scores are underlined .",conclusion,Conclusion,0,174,8,8,0,conclusion : Conclusion,0.8743718592964824,0.5333333333333333,0.5333333333333333
named-entity-recognition,9,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",conclusion,Conclusion,0,175,9,9,0,conclusion : Conclusion,0.8793969849246231,0.6,0.6
named-entity-recognition,9,"The best scores are in bold , and the second best scores are underlined .",conclusion,Conclusion,0,176,10,10,0,conclusion : Conclusion,0.8844221105527639,0.6666666666666666,0.6666666666666666
named-entity-recognition,9,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",conclusion,Conclusion,0,177,11,11,0,conclusion : Conclusion,0.8894472361809045,0.7333333333333333,0.7333333333333333
named-entity-recognition,9,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",conclusion,Conclusion,0,178,12,12,0,conclusion : Conclusion,0.8944723618090452,0.8,0.8
named-entity-recognition,9,"The best scores are in bold , and the second best scores are underlined .",conclusion,Conclusion,0,179,13,13,0,conclusion : Conclusion,0.8994974874371859,0.8666666666666667,0.8666666666666667
named-entity-recognition,9,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-are a.bioasq.org ) .,conclusion,Conclusion,0,180,14,14,0,conclusion : Conclusion,0.9045226130653267,0.9333333333333333,0.9333333333333333
named-entity-recognition,9,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",conclusion,Conclusion,0,181,15,15,0,conclusion : Conclusion,0.9095477386934674,1.0,1.0
named-entity-recognition,9,BioBERT,BioBERT,BioBERT,0,182,1,1,0,BioBERT : BioBERT,0.914572864321608,0.05555555555555555,0.5
named-entity-recognition,9,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,BioBERT,BioBERT,0,183,2,2,0,BioBERT : BioBERT,0.9195979899497487,0.1111111111111111,1.0
named-entity-recognition,9,BC2GM,BioBERT,BC2GM,0,184,3,1,0,BioBERT : BC2GM,0.9246231155778895,0.16666666666666666,0.3333333333333333
named-entity-recognition,9,BERT,BioBERT,BC2GM,0,185,4,2,0,BioBERT : BC2GM,0.9296482412060302,0.2222222222222222,0.6666666666666666
named-entity-recognition,9,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",BioBERT,BC2GM,0,186,5,3,0,BioBERT : BC2GM,0.9346733668341709,0.2777777777777778,1.0
named-entity-recognition,9,QA,BioBERT,QA,0,187,6,1,0,BioBERT : QA,0.9396984924623115,0.3333333333333333,0.14285714285714285
named-entity-recognition,9,BioASQ 6 b - factoid,BioBERT,QA,0,188,7,2,0,BioBERT : QA,0.9447236180904522,0.3888888888888889,0.2857142857142857
named-entity-recognition,9,: Which type of urinary incontinence is diagnosed with the Q tip test ? BERT,BioBERT,QA,0,189,8,3,0,BioBERT : QA,0.949748743718593,0.4444444444444444,0.42857142857142855
named-entity-recognition,9,: Which type of urinary incontinence is diagnosed with the Q tip test ? BERT,BioBERT,QA,0,190,9,4,0,BioBERT : QA,0.9547738693467337,0.5,0.5714285714285714
named-entity-recognition,9,total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,BioBERT,QA,0,191,10,5,0,BioBERT : QA,0.9597989949748744,0.5555555555555556,0.7142857142857143
named-entity-recognition,9,After undergoing ( . . .),BioBERT,QA,0,192,11,6,0,BioBERT : QA,0.964824120603015,0.6111111111111112,0.8571428571428571
named-entity-recognition,9,"Q-tip test , . . .",BioBERT,QA,0,193,12,7,0,BioBERT : QA,0.9698492462311558,0.6666666666666666,1.0
named-entity-recognition,9,: Which bacteria causes erythrasma ? BERT,BioBERT,Q: Which bacteria causes erythrasma? BERT,0,194,13,1,0,BioBERT : Q: Which bacteria causes erythrasma? BERT,0.9748743718592965,0.7222222222222222,0.3333333333333333
named-entity-recognition,9,: Which bacteria causes erythrasma ? BERT,BioBERT,Q: Which bacteria causes erythrasma? BERT,0,195,14,2,0,BioBERT : Q: Which bacteria causes erythrasma? BERT,0.9798994974874372,0.7777777777777778,0.6666666666666666
named-entity-recognition,9,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,BioBERT,Q: Which bacteria causes erythrasma? BERT,0,196,15,3,0,BioBERT : Q: Which bacteria causes erythrasma? BERT,0.9849246231155779,0.8333333333333334,1.0
named-entity-recognition,9,Note :,BioBERT,Note:,0,197,16,1,0,BioBERT : Note:,0.9899497487437185,0.8888888888888888,0.5
named-entity-recognition,9,Predicted named entities for NER and predicted answers for QA are in bold .,BioBERT,Note:,0,198,17,2,0,BioBERT : Note:,0.9949748743718593,0.9444444444444444,1.0
named-entity-recognition,9,Funding,BioBERT,Funding,0,199,18,1,0,BioBERT : Funding,1.0,1.0,1.0
question-answering,0,Open Question Answering with Weakly Supervised Embedding Models,title,title,1,2,1,1,0,title : title,0.007751937984496124,1.0,1.0
question-answering,0,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011627906976744186,0.125,0.125
question-answering,0,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.015503875968992248,0.25,0.25
question-answering,0,Promising progress has recently been achieved by methods that learn to map questions to logical forms or data base queries .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.01937984496124031,0.375,0.375
question-answering,0,Such approaches can be effective but at the cost of either large amounts of human - labeled data or by defining lexicons and grammars tailored by practitioners .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.023255813953488372,0.5,0.5
question-answering,0,"In this paper , we instead take the radical approach of learning to map questions to vectorial feature representations .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.027131782945736434,0.625,0.625
question-answering,0,"By mapping answers into the same space one can query any knowledge base independent of its schema , without requiring any grammar or lexicon .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.031007751937984496,0.75,0.75
question-answering,0,Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine - tuning step using the weak supervision provided by blending automatically and collaboratively generated resources .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03488372093023256,0.875,0.875
question-answering,0,"We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex , the only existing method able to be trained on similar weakly labeled data .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.03875968992248062,1.0,1.0
question-answering,0,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.04263565891472868,0.03225806451612903,0.03225806451612903
question-answering,0,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.046511627906976744,0.06451612903225806,0.06451612903225806
question-answering,0,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.050387596899224806,0.0967741935483871,0.0967741935483871
question-answering,0,"An important development in this are a has been the creation of large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia which store huge amounts of general - purpose information .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05426356589147287,0.12903225806451613,0.12903225806451613
question-answering,0,"They are organized as data bases of triples connecting pairs of entities by various relationships and of the form ( left entity , relationship , right entity ) .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.05813953488372093,0.16129032258064516,0.16129032258064516
question-answering,0,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,introduction,introduction,0,16,6,6,0,introduction : introduction,0.06201550387596899,0.1935483870967742,0.1935483870967742
question-answering,0,The use of KBs simplifies the problem by separating the issue of collecting and organizing information ( i.e. information extraction ) from the one of searching through it ( i.e. question answering or natural language interfacing ) .,introduction,introduction,0,17,7,7,0,introduction : introduction,0.06589147286821706,0.22580645161290322,0.22580645161290322
question-answering,0,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.06976744186046512,0.25806451612903225,0.25806451612903225
question-answering,0,Recent progress has been made by tackling this problem with semantic parsers .,introduction,introduction,0,19,9,9,0,introduction : introduction,0.07364341085271318,0.2903225806451613,0.2903225806451613
question-answering,0,These methods convert questions into logical forms or data base queries ( e.g. in SPARQL ) which are then subsequently used to query KBs for answers .,introduction,introduction,0,20,10,10,0,introduction : introduction,0.07751937984496124,0.3225806451612903,0.3225806451612903
question-answering,0,"Even if such systems have shown the ability to handle large - scale KBs , they require practitioners to hand - craft lexicons , grammars , and KB schema for the parsing to be effective .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.08139534883720931,0.3548387096774194,0.3548387096774194
question-answering,0,"This nonnegligible human intervention might not be generic enough to conveniently scale up to new data bases with other schema , broader vocabularies or other languages than English .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.08527131782945736,0.3870967741935484,0.3870967741935484
question-answering,0,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",introduction,introduction,1,23,13,13,0,introduction : introduction,0.08914728682170543,0.41935483870967744,0.41935483870967744
question-answering,0,"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.09302325581395349,0.45161290322580644,0.45161290322580644
question-answering,0,"For example , ( parrotfish.e , live - in.r , southern - water .e ) stands for",introduction,introduction,0,25,15,15,0,introduction : introduction,0.09689922480620156,0.4838709677419355,0.4838709677419355
question-answering,0,"What is parrotfish 's habitat ? and southern - water.e and ( cantonese.e , be-major - language - in.r , hong - kong.e ) for",introduction,introduction,0,26,16,16,0,introduction : introduction,0.10077519379844961,0.5161290322580645,0.5161290322580645
question-answering,0,"What is parrotfish 's habitat ? and southern - water.e and ( cantonese.e , be-major - language - in.r , hong - kong.e ) for",introduction,introduction,0,27,17,17,0,introduction : introduction,0.10465116279069768,0.5483870967741935,0.5483870967741935
question-answering,0,What is the main language of Hong - Kong ? and cantonese.e.,introduction,introduction,0,28,18,18,0,introduction : introduction,0.10852713178294573,0.5806451612903226,0.5806451612903226
question-answering,0,What is the main language of Hong - Kong ? and cantonese.e.,introduction,introduction,0,29,19,19,0,introduction : introduction,0.1124031007751938,0.6129032258064516,0.6129032258064516
question-answering,0,"In this task , the main difficulties come from lexical variability rather than from complex syntax , having multiple answers per question , and the absence of a supervised training signal .",introduction,introduction,0,30,20,20,0,introduction : introduction,0.11627906976744186,0.6451612903225806,0.6451612903225806
question-answering,0,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,introduction,introduction,1,31,21,21,0,introduction : introduction,0.12015503875968993,0.6774193548387096,0.6774193548387096
question-answering,0,"Unfortunately , we do not have access to any human labeled ( query , answer ) supervision for this task .",introduction,introduction,0,32,22,22,0,introduction : introduction,0.12403100775193798,0.7096774193548387,0.7096774193548387
question-answering,0,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .",introduction,introduction,1,33,23,23,0,introduction : introduction,0.12790697674418605,0.7419354838709677,0.7419354838709677
question-answering,0,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,introduction,introduction,0,34,24,24,0,introduction : introduction,0.13178294573643412,0.7741935483870968,0.7741935483870968
question-answering,0,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,introduction,introduction,1,35,25,25,0,introduction : introduction,0.13565891472868216,0.8064516129032258,0.8064516129032258
question-answering,0,Our method strongly outperforms previous results on the WikiAnswers + ReVerb evaluation data set introduced by .,introduction,introduction,0,36,26,26,0,introduction : introduction,0.13953488372093023,0.8387096774193549,0.8387096774193549
question-answering,0,"Even if the embeddings obtained after training are of good quality , the scale of the optimization problem makes it hard to control and to lead to convergence .",introduction,introduction,0,37,27,27,0,introduction : introduction,0.1434108527131783,0.8709677419354839,0.8709677419354839
question-answering,0,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .",introduction,introduction,1,38,28,28,0,introduction : introduction,0.14728682170542637,0.9032258064516129,0.9032258064516129
question-answering,0,The rest of the paper is organized as follows .,introduction,introduction,0,39,29,29,0,introduction : introduction,0.1511627906976744,0.9354838709677419,0.9354838709677419
question-answering,0,Section 2 discusses some previous work and Section 3 introduces the problem of open question answering .,introduction,introduction,0,40,30,30,0,introduction : introduction,0.15503875968992248,0.967741935483871,0.967741935483871
question-answering,0,"Then , Section 4 presents our model and Section 5 our experimental results .",introduction,introduction,0,41,31,31,0,introduction : introduction,0.15891472868217055,1.0,1.0
question-answering,0,Related Work,related work,Related Work,0,42,1,1,0,related work : Related Work,0.16279069767441862,0.05263157894736842,0.05263157894736842
question-answering,0,"Large - scale question answering has along history , mostly initiated via the TREC tracks .",related work,Related Work,0,43,2,2,0,related work : Related Work,0.16666666666666666,0.10526315789473684,0.10526315789473684
question-answering,0,"The first successful systems transformed the questions into queries which were fed to web search engines , the answer being subsequently extracted from top returned pages or snippets .",related work,Related Work,0,44,3,3,0,related work : Related Work,0.17054263565891473,0.15789473684210525,0.15789473684210525
question-answering,0,Such approaches require significant engineering to hand - craft queries and then parse and search over results .,related work,Related Work,0,45,4,4,0,related work : Related Work,0.1744186046511628,0.21052631578947367,0.21052631578947367
question-answering,0,"The emergence of large - scale KBs , such as Freebase or DBpedia , changed the setting by transforming open question answering into a problem of querying a KB using natural language .",related work,Related Work,0,46,5,5,0,related work : Related Work,0.17829457364341086,0.2631578947368421,0.2631578947368421
question-answering,0,"This is a challenging problem , which would require huge amount of labeled data to be tackled properly by purely supervised machine learning methods because of the great variability of language and of the large scale of KBs .",related work,Related Work,0,47,6,6,0,related work : Related Work,0.1821705426356589,0.3157894736842105,0.3157894736842105
question-answering,0,"The earliest methods for open question - answering with KBs , based on hand - written templates , were not robust enough to such variability over possibly evolving KBs ( addition / deletion of triples and entities ) .",related work,Related Work,0,48,7,7,0,related work : Related Work,0.18604651162790697,0.3684210526315789,0.3684210526315789
question-answering,0,The solution to gain more expressiveness via machine learning comes from distant or indirect supervision to circumvent the issue of labeled data .,related work,Related Work,0,49,8,8,0,related work : Related Work,0.18992248062015504,0.42105263157894735,0.42105263157894735
question-answering,0,Initial works attempting to learn to connect KBs and natural language with less supervision have actually been tackling the information extraction problem .,related work,Related Work,0,50,9,9,0,related work : Related Work,0.1937984496124031,0.47368421052631576,0.47368421052631576
question-answering,0,"Recently , new systems for learning question answering systems with few labeled data have been introduced based on semantic parsers .",related work,Related Work,0,51,10,10,0,related work : Related Work,0.19767441860465115,0.5263157894736842,0.5263157894736842
question-answering,0,"Such works tend to require realistic amounts of manual intervention via labeled examples , but still need vast efforts to carefully design lexicons , grammars and the KB .",related work,Related Work,0,52,11,11,0,related work : Related Work,0.20155038759689922,0.5789473684210527,0.5789473684210527
question-answering,0,"In contrast , proposed a framework for open question answering requiring little human annotation .",related work,Related Work,0,53,12,12,0,related work : Related Work,0.2054263565891473,0.631578947368421,0.631578947368421
question-answering,0,"Their system , Paralex , answers questions with more limited semantics than those introduced in , but does so at a very large scale in an open - domain manner .",related work,Related Work,0,54,13,13,0,related work : Related Work,0.20930232558139536,0.6842105263157895,0.6842105263157895
question-answering,0,It is trained using automatically and collaboratively generated data and using the KB ReVerb .,related work,Related Work,0,55,14,14,0,related work : Related Work,0.2131782945736434,0.7368421052631579,0.7368421052631579
question-answering,0,"In this work , we follow this trend by proposing an embedding - based model for question answering that is also trained under weak and cheap supervision .",related work,Related Work,0,56,15,15,0,related work : Related Work,0.21705426356589147,0.7894736842105263,0.7894736842105263
question-answering,0,Embedding - based models are getting more and more popular in natural language processing .,related work,Related Work,0,57,16,16,0,related work : Related Work,0.22093023255813954,0.8421052631578947,0.8421052631578947
question-answering,0,"Starting from the neural network language model of , these methods have now reached near state - of - the - art performance on many standard tasks while usually requiring less hand - crafted features .",related work,Related Work,0,58,17,17,0,related work : Related Work,0.2248062015503876,0.8947368421052632,0.8947368421052632
question-answering,0,"Recently , some embedding models have been proposed to perform a connection between natural language and KBs for word - sense dis ambiguation and for information extraction .",related work,Related Work,0,59,18,18,0,related work : Related Work,0.22868217054263565,0.9473684210526315,0.9473684210526315
question-answering,0,"Our work builds on these approaches to instead learn to perform open question answering under weak supervision , which to our knowledge has not been attempted before .",related work,Related Work,0,60,19,19,0,related work : Related Work,0.23255813953488372,1.0,1.0
question-answering,0,Open-domain Question Answering,system description,Open-domain Question Answering,0,61,1,1,0,system description : Open-domain Question Answering,0.2364341085271318,0.07142857142857142,0.3333333333333333
question-answering,0,"In this paper , we follow the question answering framework of and use the same data .",system description,Open-domain Question Answering,0,62,2,2,0,system description : Open-domain Question Answering,0.24031007751937986,0.14285714285714285,0.6666666666666666
question-answering,0,"Hence , relatively little labeling or feature engineering has been used .",system description,Open-domain Question Answering,0,63,3,3,0,system description : Open-domain Question Answering,0.2441860465116279,0.21428571428571427,1.0
question-answering,0,Task Definition,system description,Task Definition,0,64,4,1,0,system description : Task Definition,0.24806201550387597,0.2857142857142857,0.09090909090909091
question-answering,0,"Our work considers the task of question answering as in : given a question q , the corresponding answer is given by a triplet from a KB .",system description,Task Definition,0,65,5,2,0,system description : Task Definition,0.25193798449612403,0.35714285714285715,0.18181818181818182
question-answering,0,"This means that we consider questions for which a set of triples t provide an interpretation of the question and it s answer , such as : Here , we only give a singlet per question , but many can exist .",system description,Task Definition,0,66,6,3,0,system description : Task Definition,0.2558139534883721,0.42857142857142855,0.2727272727272727
question-answering,0,"In the remainder , the KB is denoted K and its set of entities and relationships is E .",system description,Task Definition,0,67,7,4,0,system description : Task Definition,0.2596899224806202,0.5,0.36363636363636365
question-answering,0,The word vocabulary for questions is termed V. n v and n e are the sizes of V and E respectively .,system description,Task Definition,0,68,8,5,0,system description : Task Definition,0.26356589147286824,0.5714285714285714,0.45454545454545453
question-answering,0,"Our model consists in learning a function S ( ) , which can score questionanswer triple pairs ( q , t ) .",system description,Task Definition,0,69,9,6,0,system description : Task Definition,0.26744186046511625,0.6428571428571429,0.5454545454545454
question-answering,0,"Hence , finding the top - ranked answert ( q ) to a question q is directly carried out by : t",system description,Task Definition,0,70,10,7,0,system description : Task Definition,0.2713178294573643,0.7142857142857143,0.6363636363636364
question-answering,0,"To handle multiple answer , we instead present the results as a ranked list , rather than taking the top prediction , and evaluate that instead .",system description,Task Definition,0,71,11,8,0,system description : Task Definition,0.2751937984496124,0.7857142857142857,0.7272727272727273
question-answering,0,Using the scoring function S ( ) allows to directly query the KB without needing to define an intermediate structured logical representation for questions as in semantic parsing systems .,system description,Task Definition,0,72,12,9,0,system description : Task Definition,0.27906976744186046,0.8571428571428571,0.8181818181818182
question-answering,0,"We aim at learning S ( ) , with no human - labeled supervised data in the form ( question , answer ) pairs , but only by indirect supervision , generated either automatically or collaboratively .",system description,Task Definition,0,73,13,10,0,system description : Task Definition,0.28294573643410853,0.9285714285714286,0.9090909090909091
question-answering,0,We detail in the rest of this section our process for creating training data .,system description,Task Definition,0,74,14,11,0,system description : Task Definition,0.2868217054263566,1.0,1.0
question-answering,0,Training Data,training,Training Data,0,75,1,1,0,training : Training Data,0.29069767441860467,0.022222222222222223,0.09090909090909091
question-answering,0,"Our training data consists of two sources : an automatically created KB , Re - Verb , from which we generate questions and a set of pairs of questions collaboratively labeled as paraphrases from the website WikiAnswers .",training,Training Data,0,76,2,2,0,training : Training Data,0.29457364341085274,0.044444444444444446,0.18181818181818182
question-answering,0,Knowledge Base,training,Training Data,0,77,3,3,0,training : Training Data,0.29844961240310075,0.06666666666666667,0.2727272727272727
question-answering,0,The set of potential answers K is given by the KB ReVerb .,training,Training Data,0,78,4,4,0,training : Training Data,0.3023255813953488,0.08888888888888889,0.36363636363636365
question-answering,0,"ReVerb is an open - source data base composed of more than 14M triples , made of more than 2 M entities and 600 k relationships , which have been automatically extracted from the ClueWeb09 corpus .",training,Training Data,0,79,5,5,0,training : Training Data,0.3062015503875969,0.1111111111111111,0.45454545454545453
question-answering,0,"In the following , entities are denoted with a .e suffix and relationships with a .r suffix .",training,Training Data,0,80,6,6,0,training : Training Data,0.31007751937984496,0.13333333333333333,0.5454545454545454
question-answering,0,"ReVerb contains broad and general knowledge harvested with very little human intervention , which suits the realistically supervised setting .",training,Training Data,0,81,7,7,0,training : Training Data,0.313953488372093,0.15555555555555556,0.6363636363636364
question-answering,0,"But , as a result , ReVerb is ambiguous and noisy with many useless triples and entities as well as numerous duplicates .",training,Training Data,0,82,8,8,0,training : Training Data,0.3178294573643411,0.17777777777777778,0.7272727272727273
question-answering,0,"For instance , winston - churchill.e , churchill.e and even roosevelt - and - churchill.e are all distinct entities .. 2 presents some examples of triples : some make sense , some others are completely unclear or useless .",training,Training Data,0,83,9,9,0,training : Training Data,0.32170542635658916,0.2,0.8181818181818182
question-answering,0,"In contrast to highly curated data bases such Freebase , ReVerb has more noise but also many more relation types ( Freebase has around 20 k ) .",training,Training Data,0,84,10,10,0,training : Training Data,0.32558139534883723,0.2222222222222222,0.9090909090909091
question-answering,0,"So for some types of triple it has much better coverage , despite the larger size of Freebase ; for example Freebase does not cover verbs like afraid - of or suffer - from .",training,Training Data,0,85,11,11,0,training : Training Data,0.32945736434108525,0.24444444444444444,1.0
question-answering,0,Questions Generation,training,Questions Generation,0,86,12,1,0,training : Questions Generation,0.3333333333333333,0.26666666666666666,0.03225806451612903
question-answering,0,"We have no available data of questions q labeled with their answers , i.e. with the corresponding triples t ? K .",training,Questions Generation,0,87,13,2,0,training : Questions Generation,0.3372093023255814,0.28888888888888886,0.06451612903225806
question-answering,0,"We have no available data of questions q labeled with their answers , i.e. with the corresponding triples t ? K .",training,Questions Generation,0,88,14,3,0,training : Questions Generation,0.34108527131782945,0.3111111111111111,0.0967741935483871
question-answering,0,"Following , we hence decided to create such question - triple pairs automatically .",training,Questions Generation,0,89,15,4,0,training : Questions Generation,0.3449612403100775,0.3333333333333333,0.12903225806451613
question-answering,0,These pairs are generated using the 16 seed questions displayed in .,training,Questions Generation,0,90,16,5,0,training : Questions Generation,0.3488372093023256,0.35555555555555557,0.16129032258064516
question-answering,0,"At each round , we pick a triple at random and then generate randomly one of the seed questions .",training,Questions Generation,0,91,17,6,0,training : Questions Generation,0.35271317829457366,0.37777777777777777,0.1935483870967742
question-answering,0,"Note only triples with a *-in.r relation ( denoted r- in in ) can generate from the pattern where did er ? , for example , and similar for other constraints .",training,Questions Generation,0,92,18,7,0,training : Questions Generation,0.35658914728682173,0.4,0.22580645161290322
question-answering,0,"Otherwise , the pattern is chosen randomly .",training,Questions Generation,0,93,19,8,0,training : Questions Generation,0.36046511627906974,0.4222222222222222,0.25806451612903225
question-answering,0,"Except for these exceptions , we used all 16 seed questions for all triples hence generating approximately 16 14M questions stored in a training set we denote D.",training,Questions Generation,0,94,20,9,0,training : Questions Generation,0.3643410852713178,0.4444444444444444,0.2903225806451613
question-answering,0,The generated questions are imperfect and noisy and create a weak training signal .,training,Questions Generation,0,95,21,10,0,training : Questions Generation,0.3682170542635659,0.4666666666666667,0.3225806451612903
question-answering,0,"Firstly , their syntactic structure is rather simplistic , and real questions as posed by humans ( such as in our actual test ) can look quite different to them .",training,Questions Generation,0,96,22,11,0,training : Questions Generation,0.37209302325581395,0.4888888888888889,0.3548387096774194
question-answering,0,"Secondly , many generated questions do not correspond to semantically valid English sentences .",training,Questions Generation,0,97,23,12,0,training : Questions Generation,0.375968992248062,0.5111111111111111,0.3870967741935484
question-answering,0,"For instance , since the type of entities in ReVerb is unknown , a pattern like who does er ? can be chosen for a triple where the type of ? in ( ? , r , e ) is not a person , and similar for other types ( e.g. when ) .",training,Questions Generation,0,98,24,13,0,training : Questions Generation,0.3798449612403101,0.5333333333333333,0.41935483870967744
question-answering,0,"For instance , since the type of entities in ReVerb is unknown , a pattern like who does er ? can be chosen for a triple where the type of ? in ( ? , r , e ) is not a person , and similar for other types ( e.g. when ) .",training,Questions Generation,0,99,25,14,0,training : Questions Generation,0.38372093023255816,0.5555555555555556,0.45161290322580644
question-answering,0,"Besides , for the strings representing entities and relationships in the questions , we simply used their names in ReVerb , replacingby spaces and stripping off .",training,Questions Generation,0,100,26,15,0,training : Questions Generation,0.3875968992248062,0.5777777777777777,0.4838709677419355
question-answering,0,Patterns for generating questions from ReVerb triples following .,training,Questions Generation,0,101,27,16,0,training : Questions Generation,0.39147286821705424,0.6,0.5161290322580645
question-answering,0,"their suffixes , i.e. the string representing winston - churchill.e is simply winston churchill .",training,Questions Generation,0,102,28,17,0,training : Questions Generation,0.3953488372093023,0.6222222222222222,0.5483870967741935
question-answering,0,"While this is often fine , this is also very limited and caused many incoherences in the data .",training,Questions Generation,0,103,29,18,0,training : Questions Generation,0.3992248062015504,0.6444444444444445,0.5806451612903226
question-answering,0,"Generating questions with a richer KB than ReVerb , such as Freebase or DBpedia , would lead to better quality because typing and better lexicons could be used .",training,Questions Generation,0,104,30,19,0,training : Questions Generation,0.40310077519379844,0.6666666666666666,0.6129032258064516
question-answering,0,"However , this would contradict one of our motivations which is to train a system with as little human intervention as possible ( and hence choosing ReVerb over hand - curated KBs ) .",training,Questions Generation,0,105,31,20,0,training : Questions Generation,0.4069767441860465,0.6888888888888889,0.6451612903225806
question-answering,0,Paraphrases,training,Questions Generation,0,106,32,21,0,training : Questions Generation,0.4108527131782946,0.7111111111111111,0.6774193548387096
question-answering,0,The automatically generated examples are useful to connect KB triples and natural language .,training,Questions Generation,0,107,33,22,0,training : Questions Generation,0.41472868217054265,0.7333333333333333,0.7096774193548387
question-answering,0,"However , they do not allow for a satisfactory modeling of English language because of their poor wording .",training,Questions Generation,0,108,34,23,0,training : Questions Generation,0.4186046511627907,0.7555555555555555,0.7419354838709677
question-answering,0,"To overcome this issue , we again follow and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website .",training,Questions Generation,0,109,35,24,0,training : Questions Generation,0.42248062015503873,0.7777777777777778,0.7741935483870968
question-answering,0,"On WikiAnswers , users can tag pairs of questions as rephrasing of each other .",training,Questions Generation,0,110,36,25,0,training : Questions Generation,0.4263565891472868,0.8,0.8064516129032258
question-answering,0,"harvested a set of 18 M of these question - paraphrase pairs , with 2.4 M distinct questions in the corpus .",training,Questions Generation,0,111,37,26,0,training : Questions Generation,0.43023255813953487,0.8222222222222222,0.8387096774193549
question-answering,0,These pairs have been labeled collaboratively .,training,Questions Generation,0,112,38,27,0,training : Questions Generation,0.43410852713178294,0.8444444444444444,0.8709677419354839
question-answering,0,This is cheap but also causes the data to be noisy .,training,Questions Generation,0,113,39,28,0,training : Questions Generation,0.437984496124031,0.8666666666666667,0.9032258064516129
question-answering,0,"Hence , estimated that only 55 % of the pairs were actual paraphrases .",training,Questions Generation,0,114,40,29,0,training : Questions Generation,0.4418604651162791,0.8888888888888888,0.9354838709677419
question-answering,0,The set of paraphrases is denoted P in the following .,training,Questions Generation,0,115,41,30,0,training : Questions Generation,0.44573643410852715,0.9111111111111111,0.967741935483871
question-answering,0,"By considering all words and tokens appearing in P and D , we end up with a size for the vocabulary V of more than 800k .",training,Questions Generation,0,116,42,31,0,training : Questions Generation,0.4496124031007752,0.9333333333333333,1.0
question-answering,0,Embedding - based Model,training,Embedding-based Model,0,117,43,1,0,training : Embedding-based Model,0.45348837209302323,0.9555555555555556,0.5
question-answering,0,"Our model ends up learning vector embeddings of symbols , either for entities or relationships from ReVerb , or for each word of the vocabulary .",training,Embedding-based Model,0,118,44,2,0,training : Embedding-based Model,0.4573643410852713,0.9777777777777777,1.0
question-answering,0,Question - KB Triple Scoring,training,Question-KB Triple Scoring,0,119,45,1,0,training : Question-KB Triple Scoring,0.46124031007751937,1.0,1.0
question-answering,0,Architecture,architecture,architecture,0,120,1,1,0,architecture : architecture,0.46511627906976744,0.016129032258064516,0.029411764705882353
question-answering,0,"Our framework concerns the learning of a function S ( q , t ) , based on embeddings , that is designed to score the similarity of a question q and a triplet from K.",architecture,architecture,0,121,2,2,0,architecture : architecture,0.4689922480620155,0.03225806451612903,0.058823529411764705
question-answering,0,"Our scoring approach is inspired by previous work for labeling images with words , which we adapted , replacing images and labels by questions and triples .",architecture,architecture,0,122,3,3,0,architecture : architecture,0.4728682170542636,0.04838709677419355,0.08823529411764706
question-answering,0,"Intuitively , it consists of projecting questions , treated as a bag of words ( and possibly n-grams as well ) , on the one hand , and triples on the other hand , into a shared embedding space and then computing a similarity measure ( the dot product in this paper ) between both projections .",architecture,architecture,0,123,4,4,0,architecture : architecture,0.47674418604651164,0.06451612903225806,0.11764705882352941
question-answering,0,The scoring function is then :,architecture,architecture,0,124,5,5,0,architecture : architecture,0.4806201550387597,0.08064516129032258,0.14705882352941177
question-answering,0,"with f ( ) a function mapping words from questions into Contrary to previous work modeling KBs with embeddings ( e.g. ) , in our model , an entity does not have the same embedding when appearing in the lefthand or in the right - hand side of a triple .",architecture,architecture,0,125,6,6,0,architecture : architecture,0.4844961240310077,0.0967741935483871,0.17647058823529413
question-answering,0,"Since , g ( ) sums embeddings of all constituents of a triple , we need to use 2 embeddings per entity to encode for the fact that relationships in the KB are not symmetric and so that appearing as a left - hand or right - hand entity is different .",architecture,architecture,0,126,7,7,0,architecture : architecture,0.4883720930232558,0.11290322580645161,0.20588235294117646
question-answering,0,"This approach can be easily applied at test time to score any ( question , triple ) pairs .",architecture,architecture,0,127,8,8,0,architecture : architecture,0.49224806201550386,0.12903225806451613,0.23529411764705882
question-answering,0,"Given a question q , one can predict the corresponding answer ( a triple ) t ( q ) wit h:t ( q ) = arg max",architecture,architecture,0,128,9,9,0,architecture : architecture,0.49612403100775193,0.14516129032258066,0.2647058823529412
question-answering,0,Training by Ranking Previous work has shown that this kind of model can be conveniently trained using a ranking loss .,architecture,architecture,0,129,10,10,0,architecture : architecture,0.5,0.16129032258064516,0.29411764705882354
question-answering,0,"Hence , given our data set D = { ( q i , ti ) , i = 1 , . . . , | D |} consisting of ( question , answer triple ) training pairs , one could learn the embeddings using constraints of the form :",architecture,architecture,0,130,11,11,0,architecture : architecture,0.5038759689922481,0.1774193548387097,0.3235294117647059
question-answering,0,where 0.1 is the margin .,architecture,architecture,0,131,12,12,0,architecture : architecture,0.5077519379844961,0.1935483870967742,0.35294117647058826
question-answering,0,"That is , we want the triple that labels a given question to be scored higher than other triples in K by a margin of 0.1 .",architecture,architecture,0,132,13,13,0,architecture : architecture,0.5116279069767442,0.20967741935483872,0.38235294117647056
question-answering,0,"We also enforce a constraint on the norms of the columns of V and W , i.e. ? i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ? 1 .",architecture,architecture,0,133,14,14,0,architecture : architecture,0.5155038759689923,0.22580645161290322,0.4117647058823529
question-answering,0,"We also enforce a constraint on the norms of the columns of V and W , i.e. ? i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ? 1 .",architecture,architecture,0,134,15,15,0,architecture : architecture,0.5193798449612403,0.24193548387096775,0.4411764705882353
question-answering,0,"We also enforce a constraint on the norms of the columns of V and W , i.e. ? i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ? 1 .",architecture,architecture,0,135,16,16,0,architecture : architecture,0.5232558139534884,0.25806451612903225,0.47058823529411764
question-answering,0,"To train our model , we need positive and negative examples of ( q , t ) pairs .",architecture,architecture,0,136,17,17,0,architecture : architecture,0.5271317829457365,0.27419354838709675,0.5
question-answering,0,"However , D only contains positive samples , for which the triple actually corresponds to the question .",architecture,architecture,0,137,18,18,0,architecture : architecture,0.5310077519379846,0.2903225806451613,0.5294117647058824
question-answering,0,"Hence , during training , we use a procedure to corrupt triples .",architecture,architecture,0,138,19,19,0,architecture : architecture,0.5348837209302325,0.3064516129032258,0.5588235294117647
question-answering,0,"Given ( q , t ) ? D , we create a corrupted triplet with the following method : pick another random triplet tmp from K , and then , replace with 66 % chance each member oft ( left entity , relationship and right entity ) by the corresponding element int tmp .",architecture,architecture,0,139,20,20,0,architecture : architecture,0.5387596899224806,0.3225806451612903,0.5882352941176471
question-answering,0,"Given ( q , t ) ? D , we create a corrupted triplet with the following method : pick another random triplet tmp from K , and then , replace with 66 % chance each member oft ( left entity , relationship and right entity ) by the corresponding element int tmp .",architecture,architecture,0,140,21,21,0,architecture : architecture,0.5426356589147286,0.3387096774193548,0.6176470588235294
question-answering,0,"This heuristic creates negative triples t somewhat similar to their positive counterpart t , and is similar to schemes of previous work ( e.g. in ) .",architecture,architecture,0,141,22,22,0,architecture : architecture,0.5465116279069767,0.3548387096774194,0.6470588235294118
question-answering,0,"Training the embedding model is carried out by stochastic gradient descent ( SGD ) , updating W and V at each step .",architecture,architecture,0,142,23,23,0,architecture : architecture,0.5503875968992248,0.3709677419354839,0.6764705882352942
question-answering,0,At the start of training the parameters off ( ) and g ( ) ( the n v k word embeddings in V and then e k entities and rel .,architecture,architecture,0,143,24,24,0,architecture : architecture,0.5542635658914729,0.3870967741935484,0.7058823529411765
question-answering,0,"embeddings in W ) are initialized to random weights ( mean 0 , standard deviation 1 k ) .",architecture,architecture,0,144,25,25,0,architecture : architecture,0.5581395348837209,0.4032258064516129,0.7352941176470589
question-answering,0,"Then , we iterate the following steps to train them :",architecture,architecture,0,145,26,26,0,architecture : architecture,0.562015503875969,0.41935483870967744,0.7647058823529411
question-answering,0,". Sample a positive training pair ( q i , ti ) from D .",architecture,architecture,0,146,27,27,0,architecture : architecture,0.5658914728682171,0.43548387096774194,0.7941176470588235
question-answering,0,2 .,architecture,architecture,0,147,28,28,0,architecture : architecture,0.5697674418604651,0.45161290322580644,0.8235294117647058
question-answering,0,Create a corrupted triplet i ensuring that ti = ti .,architecture,architecture,0,148,29,29,0,architecture : architecture,0.5736434108527132,0.46774193548387094,0.8529411764705882
question-answering,0,3 .,architecture,architecture,0,149,30,30,0,architecture : architecture,0.5775193798449613,0.4838709677419355,0.8823529411764706
question-answering,0,Make a stochastic gradient step to minimize 0.1 ? f ( q i ) g (t i ) +f ( q i ) g (t i ) + .,architecture,architecture,0,150,31,31,0,architecture : architecture,0.5813953488372093,0.5,0.9117647058823529
question-answering,0,. Enforce the constraint that each embedding vector is normalized .,architecture,architecture,0,151,32,32,0,architecture : architecture,0.5852713178294574,0.5161290322580645,0.9411764705882353
question-answering,0,The learning rate of SGD is updated during the course of learning using adagrad .,architecture,architecture,0,152,33,33,0,architecture : architecture,0.5891472868217055,0.532258064516129,0.9705882352941176
question-answering,0,+ is the positive part of x .,architecture,architecture,0,153,34,34,0,architecture : architecture,0.5930232558139535,0.5483870967741935,1.0
question-answering,0,Multitask Training with Paraphrases Pairs,architecture,Multitask Training with Paraphrases Pairs,0,154,35,1,0,architecture : Multitask Training with Paraphrases Pairs,0.5968992248062015,0.5645161290322581,0.125
question-answering,0,"We multitask the training of our model by training on pairs of paraphrases of questions ( q 1 , q 2 ) from P as well as training on the pseudolabeled data constructed in D .",architecture,Multitask Training with Paraphrases Pairs,0,155,36,2,0,architecture : Multitask Training with Paraphrases Pairs,0.6007751937984496,0.5806451612903226,0.25
question-answering,0,We use the same architecture simply replacing g ( ) by a copy off ( ) .,architecture,Multitask Training with Paraphrases Pairs,0,156,37,3,0,architecture : Multitask Training with Paraphrases Pairs,0.6046511627906976,0.5967741935483871,0.375
question-answering,0,This leads to the following function that scores the similarity between two questions :,architecture,Multitask Training with Paraphrases Pairs,0,157,38,4,0,architecture : Multitask Training with Paraphrases Pairs,0.6085271317829457,0.6129032258064516,0.5
question-answering,0,"The matrix W containing embeddings of words is shared between Sand S prp , allowing it to encode information from examples from both D and P. Training of S prp is also conducted with SGD ( and adagrad ) as for S , but , in this case , negative examples are created by replacing one of the questions from the pair by another question chosen at random in P.",architecture,Multitask Training with Paraphrases Pairs,0,158,39,5,0,architecture : Multitask Training with Paraphrases Pairs,0.6124031007751938,0.6290322580645161,0.625
question-answering,0,"During our experiments , W and V were learned by alternating training steps using Sand S prp , switching from one to another at each step .",architecture,Multitask Training with Paraphrases Pairs,0,159,40,6,0,architecture : Multitask Training with Paraphrases Pairs,0.6162790697674418,0.6451612903225806,0.75
question-answering,0,The initial learning rate was set to 0.1 and the dimension k of the embedding space to 64 .,architecture,Multitask Training with Paraphrases Pairs,0,160,41,7,0,architecture : Multitask Training with Paraphrases Pairs,0.6201550387596899,0.6612903225806451,0.875
question-answering,0,Training ran for 1 day on a 16 core machine using hogwild .,architecture,Multitask Training with Paraphrases Pairs,0,161,42,8,0,architecture : Multitask Training with Paraphrases Pairs,0.624031007751938,0.6774193548387096,1.0
question-answering,0,Fine - tuning the Similarity between Embeddings,architecture,Fine-tuning the Similarity between Embeddings,0,162,43,1,0,architecture : Fine-tuning the Similarity between Embeddings,0.627906976744186,0.6935483870967742,0.05
question-answering,0,"The scale of the problem forced us to keep our architecture simple : with n e ? 3.5M ( with 2 embeddings for each entity ) and n v ? 800 k , we have to learn around 4.3 M embeddings .",architecture,Fine-tuning the Similarity between Embeddings,0,163,44,2,0,architecture : Fine-tuning the Similarity between Embeddings,0.6317829457364341,0.7096774193548387,0.1
question-answering,0,"With an embedding space of dimension k = 64 , this leads to around 275M parameters to learn .",architecture,Fine-tuning the Similarity between Embeddings,0,164,45,3,0,architecture : Fine-tuning the Similarity between Embeddings,0.6356589147286822,0.7258064516129032,0.15
question-answering,0,The training algorithm must also stay simple to scale on a training set of around 250 M of examples ( D and P combined ) ; SGD appears as the only viable option .,architecture,Fine-tuning the Similarity between Embeddings,0,165,46,4,0,architecture : Fine-tuning the Similarity between Embeddings,0.6395348837209303,0.7419354838709677,0.2
question-answering,0,"SGD , combined with adagrad for adapting the learning rate on the course of training , is a powerful algorithm .",architecture,Fine-tuning the Similarity between Embeddings,0,166,47,5,0,architecture : Fine-tuning the Similarity between Embeddings,0.6434108527131783,0.7580645161290323,0.25
question-answering,0,"However , the scale of the optimization problem makes it very hard to control and conduct properly until convergence .",architecture,Fine-tuning the Similarity between Embeddings,0,167,48,6,0,architecture : Fine-tuning the Similarity between Embeddings,0.6472868217054264,0.7741935483870968,0.3
question-answering,0,"When SGD stops after a pre-defined number of epochs , we are almost certain that the problem is not fully solved and that some room for improvement remains : we observed that embeddings were able to often rank correct answers near the top of the candidates list , but not always in the first place .",architecture,Fine-tuning the Similarity between Embeddings,0,168,49,7,0,architecture : Fine-tuning the Similarity between Embeddings,0.6511627906976745,0.7903225806451613,0.35
question-answering,0,"In this paper , we introduce away to fine - tune our embedding - based model so that correct answers might end up more often at the top of the list .",architecture,Fine-tuning the Similarity between Embeddings,0,169,50,8,0,architecture : Fine-tuning the Similarity between Embeddings,0.6550387596899225,0.8064516129032258,0.4
question-answering,0,"Updating the embeddings involves working on too many parameters , but ultimately , these embeddings are meant to be used in a dot -product that computes the similarity between q and t.",architecture,Fine-tuning the Similarity between Embeddings,0,170,51,9,0,architecture : Fine-tuning the Similarity between Embeddings,0.6589147286821705,0.8225806451612904,0.45
question-answering,0,We propose to learn a matrix M ? R kk parameterizing the similarity between words and triples embeddings .,architecture,Fine-tuning the Similarity between Embeddings,0,171,52,10,0,architecture : Fine-tuning the Similarity between Embeddings,0.6627906976744186,0.8387096774193549,0.5
question-answering,0,We propose to learn a matrix M ? R kk parameterizing the similarity between words and triples embeddings .,architecture,Fine-tuning the Similarity between Embeddings,0,172,53,11,0,architecture : Fine-tuning the Similarity between Embeddings,0.6666666666666666,0.8548387096774194,0.55
question-answering,0,The scoring function becomes :,architecture,Fine-tuning the Similarity between Embeddings,0,173,54,12,0,architecture : Fine-tuning the Similarity between Embeddings,0.6705426356589147,0.8709677419354839,0.6
question-answering,0,has only k 2 parameters and can be efficiently determined by solving the following convex problem ( fixing the embedding matrices W and V ) :,architecture,Fine-tuning the Similarity between Embeddings,0,174,55,13,0,architecture : Fine-tuning the Similarity between Embeddings,0.6744186046511628,0.8870967741935484,0.65
question-answering,0,where X F is the Frobenius norm of X .,architecture,Fine-tuning the Similarity between Embeddings,0,175,56,14,0,architecture : Fine-tuning the Similarity between Embeddings,0.6782945736434108,0.9032258064516129,0.7
question-answering,0,We solve this problem in a few minutes using L - BFGS on a subset of m = 10 M examples from D .,architecture,Fine-tuning the Similarity between Embeddings,0,176,57,15,0,architecture : Fine-tuning the Similarity between Embeddings,0.6821705426356589,0.9193548387096774,0.75
question-answering,0,We first use 4 M examples to train and 6M as validation set to determine the value of the regularization parameter ?.,architecture,Fine-tuning the Similarity between Embeddings,0,177,58,16,0,architecture : Fine-tuning the Similarity between Embeddings,0.686046511627907,0.9354838709677419,0.8
question-answering,0,"We then retrain the model on the whole 10M examples using the selected value , which happened to be ? = 1.7 10 ?5 .",architecture,Fine-tuning the Similarity between Embeddings,0,178,59,17,0,architecture : Fine-tuning the Similarity between Embeddings,0.689922480620155,0.9516129032258065,0.85
question-answering,0,"This fine - tuning is related to learning a new metric in the embedding space , but since the resulting M is not symmetric , it does not define a dot-product .",architecture,Fine-tuning the Similarity between Embeddings,0,179,60,18,0,architecture : Fine-tuning the Similarity between Embeddings,0.6937984496124031,0.967741935483871,0.9
question-answering,0,"Still , M is close to a constant factor times identity ( as in the original score S ( ) ) .",architecture,Fine-tuning the Similarity between Embeddings,0,180,61,19,0,architecture : Fine-tuning the Similarity between Embeddings,0.6976744186046512,0.9838709677419355,0.95
question-answering,0,"The fine - tuning does not deeply alter the ranking , but , as expected , allows for a slight change in the triples ranking , which ends in consistent improvement in performance , as we show in the experiments .",architecture,Fine-tuning the Similarity between Embeddings,0,181,62,20,0,architecture : Fine-tuning the Similarity between Embeddings,0.7015503875968992,1.0,1.0
question-answering,0,Experiments,experiment,Experiments,0,182,1,1,0,experiment : Experiments,0.7054263565891473,1.0,1.0
question-answering,0,Evaluation Protocols,evaluation,Evaluation Protocols,0,183,1,1,0,evaluation : Evaluation Protocols,0.7093023255813954,0.07142857142857142,0.5
question-answering,0,We first detail the data and metrics which were chosen to assess the quality of our embedding model .,evaluation,Evaluation Protocols,0,184,2,2,0,evaluation : Evaluation Protocols,0.7131782945736435,0.14285714285714285,1.0
question-answering,0,Test Set,evaluation,Test Set,0,185,3,1,0,evaluation : Test Set,0.7170542635658915,0.21428571428571427,0.14285714285714285
question-answering,0,The data set WikiAnswers + ReVerb contains no labeled examples but some are needed for evaluating models .,evaluation,Test Set,0,186,4,2,0,evaluation : Test Set,0.7209302325581395,0.2857142857142857,0.2857142857142857
question-answering,0,"We used the test set which has been created by in the following way : ( 1 ) they identified 37 questions from a heldout portion of WikiAnswers which were likely to have at least one answer in ReVerb , ( 2 ) they added all valid paraphrases of these questions to obtain a set of 691 questions , ( 3 ) they ran various versions of their paralex system on them to gather candidate triples ( for a total of 48 k ) , which they finally hand - labeled .",evaluation,Test Set,0,187,5,3,0,evaluation : Test Set,0.7248062015503876,0.35714285714285715,0.42857142857142855
question-answering,0,Reranking,evaluation,Test Set,1,188,6,4,0,evaluation : Test Set,0.7286821705426356,0.42857142857142855,0.5714285714285714
question-answering,0,We first evaluated different versions of our model against the paralex system in a reranking setting .,evaluation,Test Set,0,189,7,5,0,evaluation : Test Set,0.7325581395348837,0.5,0.7142857142857143
question-answering,0,"For each question q from the WikiAn - swers + ReVerb test set , we take the provided candidate triples t and rerank them by sorting by the score S ( q , t ) or S ft ( q , t ) of our model , depending whether we use fine - tuning or not .",evaluation,Test Set,0,190,8,6,0,evaluation : Test Set,0.7364341085271318,0.5714285714285714,0.8571428571428571
question-answering,0,"As in , we then compute the precision , recall and F1 -score of the highest ranked answer as well as the mean average precision ( MAP ) of the whole output , which measures the average precision over all levels of recall .",evaluation,Test Set,0,191,9,7,0,evaluation : Test Set,0.7403100775193798,0.6428571428571429,1.0
question-answering,0,Full Ranking,evaluation,Full Ranking,0,192,10,1,0,evaluation : Full Ranking,0.7441860465116279,0.7142857142857143,0.2
question-answering,0,"We hence decided to filter out some candidates before ranking by using a simple string matching strategy : after pos-tagging the question , we construct a set of candidate strings containing ( i ) all noun phrases that appear less than 1,000 .",evaluation,Full Ranking,0,193,11,2,0,evaluation : Full Ranking,0.748062015503876,0.7857142857142857,0.4
question-answering,0,Examples of nearest neighboring entities and relationships from REVERB for some words from our vocabulary .,evaluation,Full Ranking,0,194,12,3,0,evaluation : Full Ranking,0.751937984496124,0.8571428571428571,0.6
question-answering,0,"The prefix L : , resp.",evaluation,Full Ranking,0,195,13,4,0,evaluation : Full Ranking,0.7558139534883721,0.9285714285714286,0.8
question-answering,0,": , indicates the embedding of an entity when appearing in left - hand side , resp. right - hand side , of triples .",evaluation,Full Ranking,0,196,14,5,0,evaluation : Full Ranking,0.7596899224806202,1.0,1.0
question-answering,0,Results,result,Results,0,197,1,1,0,result : Results,0.7635658914728682,0.024390243902439025,0.047619047619047616
question-answering,0,This section now discusses our empirical performance .,result,Results,0,198,2,2,0,result : Results,0.7674418604651163,0.04878048780487805,0.09523809523809523
question-answering,0,Reranking and present the results of the reranking experiments .,result,Results,0,199,3,3,0,result : Results,0.7713178294573644,0.07317073170731707,0.14285714285714285
question-answering,0,"We compare various versions of our model against two versions of paralex , whose results were given in .",result,Results,0,200,4,4,0,result : Results,0.7751937984496124,0.0975609756097561,0.19047619047619047
question-answering,0,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .",result,Results,1,201,5,5,0,result : Results,0.7790697674418605,0.12195121951219512,0.23809523809523808
question-answering,0,"Paraphrases allow for the embeddings to encode a richer connection between KB constituents and words , as well as between words themselves .",result,Results,0,202,6,6,0,result : Results,0.7829457364341085,0.14634146341463414,0.2857142857142857
question-answering,0,"Note that the WikiAnswers data provides word alignment between paraphrases , which we did not use , unlike paralex .",result,Results,0,203,7,7,0,result : Results,0.7868217054263565,0.17073170731707318,0.3333333333333333
question-answering,0,"We also tried to use n-grams ( 2.5 M most frequent ) as well as the words to represent the question , but this did not bring any improvement , which might at first seem counter - intuitive .",result,Results,0,204,8,8,0,result : Results,0.7906976744186046,0.1951219512195122,0.38095238095238093
question-answering,0,"We believe this is due to two factors : it is hard to learn good embeddings for n-grams since their frequency is usually very low and ( 2 ) our automatically generated questions have a poor syntax and hence , many n-grams in this data set do not make sense .",result,Results,0,205,9,9,0,result : Results,0.7945736434108527,0.21951219512195122,0.42857142857142855
question-answering,0,"We actually conducted experiments with several variants of our model , which tried to take the word ordering into account ( e.g. with convolutions ) , and they all failed to outperform our best performance without word order , once again perhaps because the supervision is not clean enough to allow for such elaborated language modeling .",result,Results,0,206,10,10,0,result : Results,0.7984496124031008,0.24390243902439024,0.47619047619047616
question-answering,0,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,result,Results,1,207,11,11,0,result : Results,0.8023255813953488,0.2682926829268293,0.5238095238095238
question-answering,0,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .",result,Results,1,208,12,12,0,result : Results,0.8062015503875969,0.2926829268292683,0.5714285714285714
question-answering,0,paralex works by starting with an initial lexicon mapping from the KB to language and then gradually increasing its coverage by iterating on the WikiAnswers + ReVerb data .,result,Results,0,209,13,13,0,result : Results,0.810077519379845,0.3170731707317073,0.6190476190476191
question-answering,0,Most of its predictions come from automatically acquired templates and rules : this allows for a good precision but it is not flexible enough across language variations to grant a satisfying recall .,result,Results,0,210,14,14,0,result : Results,0.813953488372093,0.34146341463414637,0.6666666666666666
question-answering,0,Most of our improvement comes from a much better recall .,result,Results,0,211,15,15,0,result : Results,0.8178294573643411,0.36585365853658536,0.7142857142857143
question-answering,0,"However , as we said earlier , this reranking setting is detrimental for paralex because paralex was evaluated on the task of reranking some of its own predictions .",result,Results,0,212,16,16,0,result : Results,0.8217054263565892,0.3902439024390244,0.7619047619047619
question-answering,0,"The results provided for paralex , while not corresponding to those of a full ranking among all triples from ReVerb ( it is still reranking among a subset of candidates ) , concerns an evaluation setting more complicated than for our model .",result,Results,0,213,17,17,0,result : Results,0.8255813953488372,0.4146341463414634,0.8095238095238095
question-answering,0,"Hence , we also display the results of a full ranking by our system in the following .",result,Results,0,214,18,18,0,result : Results,0.8294573643410853,0.43902439024390244,0.8571428571428571
question-answering,0,and display the results of our model to rank all 14 M triples from ReVerb .,result,Results,0,215,19,19,0,result : Results,0.8333333333333334,0.4634146341463415,0.9047619047619048
question-answering,0,The performance of the plain models is not good ( F1 = 0.22 only for S ft ) because the ranking is degraded by too many candidates .,result,Results,0,216,20,20,0,result : Results,0.8372093023255814,0.4878048780487805,0.9523809523809523
question-answering,0,But most of these can be discarded beforehand .,result,Results,0,217,21,21,0,result : Results,0.8410852713178295,0.5121951219512195,1.0
question-answering,0,Word,result,Word,0,218,22,1,0,result : Word,0.8449612403100775,0.5365853658536586,0.05
question-answering,0,Closest entities or relationships from ReVerb in the embedding space get rid of get -rid - of.r be -get - rid - of.r rid-of.r can -get - rid - of.r will - get - rid - of.r should - get - rid - of.r have - to - get - rid - of.r want - to - get- rid- of.r will - not - get - rid-of.r,result,Word,0,219,23,2,0,result : Word,0.8488372093023255,0.5609756097560976,0.1
question-answering,0,help- get-rid-of.r useful be-useful - for.r be-useful - in.r,result,Word,0,220,24,3,0,result : Word,0.8527131782945736,0.5853658536585366,0.15
question-answering,0,R:wide-range-of-application.e can-be-useful-for.r,result,Word,0,221,25,4,0,result : Word,0.8565891472868217,0.6097560975609756,0.2
question-answering,0,be-use-extensively-for. r be-not-very-useful-for.r,result,Word,0,222,26,5,0,result : Word,0.8604651162790697,0.6341463414634146,0.25
question-answering,0,R:plex-or-technical-algorithm.e,result,Word,0,223,27,6,0,result : Word,0.8643410852713178,0.6585365853658537,0.3
question-answering,0,R:internal-and-external-use.e,result,Word,0,224,28,7,0,result : Word,0.8682170542635659,0.6829268292682927,0.35
question-answering,0,R:authoring.e,result,Word,0,225,29,8,0,result : Word,0.872093023255814,0.7073170731707317,0.4
question-answering,0,R:good- or- bad- purpose.e radiation R:radiation.e,result,Word,0,226,30,9,0,result : Word,0.875968992248062,0.7317073170731707,0.45
question-answering,0,L:radiation.e,result,Word,0,227,31,10,0,result : Word,0.8798449612403101,0.7560975609756098,0.5
question-answering,0,R:gamma-radiation.e,result,Word,0,228,32,11,0,result : Word,0.8837209302325582,0.7804878048780488,0.55
question-answering,0,L:gamma- radiation.e,result,Word,0,229,33,12,0,result : Word,0.8875968992248062,0.8048780487804879,0.6
question-answering,0,L:x - ray.e L:gamma - ray .,result,Word,0,230,34,13,0,result : Word,0.8914728682170543,0.8292682926829268,0.65
question-answering,0,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .",result,Word,0,231,35,14,0,result : Word,0.8953488372093024,0.8536585365853658,0.7
question-answering,0,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .",result,Word,0,232,36,15,0,result : Word,0.8992248062015504,0.8780487804878049,0.75
question-answering,0,Embeddings displays some examples of nearest neighboring entities from ReVerb for some words from our vocabulary .,result,Word,0,233,37,16,0,result : Word,0.9031007751937985,0.9024390243902439,0.8
question-answering,0,"As expected , we can see that verbs or adverbs tend to correspond to relationships while nouns refer to entities .",result,Word,0,234,38,17,0,result : Word,0.9069767441860465,0.926829268292683,0.85
question-answering,0,"Interestingly , the model learns some synonymy and hyper / hyponymy .",result,Word,0,235,39,18,0,result : Word,0.9108527131782945,0.9512195121951219,0.9
question-answering,0,"For instance , radiation is close to x - ray.e and iphone to smartphone .e.",result,Word,0,236,40,19,0,result : Word,0.9147286821705426,0.975609756097561,0.95
question-answering,0,"This happens thanks to the multitasking with paraphrase data , since in our automatically generated ( q , t ) pairs , the words radiation and iphone are only used for entities with the strings radiation and iphone respectively in their names .",result,Word,0,237,41,20,0,result : Word,0.9186046511627907,1.0,1.0
question-answering,0,Evaluation on WebQuestions,evaluation,Evaluation on WebQuestions,0,238,1,1,0,evaluation : Evaluation on WebQuestions,0.9224806201550387,0.07142857142857142,0.07142857142857142
question-answering,0,Our initial objective was to be able to perform open - domain question answering .,evaluation,Evaluation on WebQuestions,0,239,2,2,0,evaluation : Evaluation on WebQuestions,0.9263565891472868,0.14285714285714285,0.14285714285714285
question-answering,0,"In this last experimental section , we tend to evaluate how generic our learned system is .",evaluation,Evaluation on WebQuestions,0,240,3,3,0,evaluation : Evaluation on WebQuestions,0.9302325581395349,0.21428571428571427,0.21428571428571427
question-answering,0,"To this end , we propose to ask our model to answer questions coming from another dataset from the literature , but without retraining it with labeled data , just by directly using the parameters learned on WikiAnswers + ReVerb .",evaluation,Evaluation on WebQuestions,0,241,4,4,0,evaluation : Evaluation on WebQuestions,0.9341085271317829,0.2857142857142857,0.2857142857142857
question-answering,0,"We chose the data set WebQuestions , which consists of natural language questions matched with answers corresponding to entities of Freebase : in this case , no triple has to be returned , only a single entity .",evaluation,Evaluation on WebQuestions,0,242,5,5,0,evaluation : Evaluation on WebQuestions,0.937984496124031,0.35714285714285715,0.35714285714285715
question-answering,0,"We used exact string matching to find the ReVerb entities corresponding to the Freebase answers from the test set of WebQuestions and obtained 1,538 questions labeled with ReVerb out of the original 2,034 .",evaluation,Evaluation on WebQuestions,0,243,6,6,0,evaluation : Evaluation on WebQuestions,0.9418604651162791,0.42857142857142855,0.42857142857142855
question-answering,0,Results of different versions of our model are displayed in .,evaluation,Evaluation on WebQuestions,0,244,7,7,0,evaluation : Evaluation on WebQuestions,0.9457364341085271,0.5,0.5
question-answering,0,"For each test question , we record the rank of the first ReVerb triple containing the answer entity .",evaluation,Evaluation on WebQuestions,0,245,8,8,0,evaluation : Evaluation on WebQuestions,0.9496124031007752,0.5714285714285714,0.5714285714285714
question-answering,0,"Top - 1 and Top - 10 are computed on questions for which the system returned at least one answer ( around 1,000 questions using string matching ) , while F1 is computed for all questions .",evaluation,Evaluation on WebQuestions,0,246,9,9,0,evaluation : Evaluation on WebQuestions,0.9534883720930233,0.6428571428571429,0.6428571428571429
question-answering,0,"Of course , performance is not great and can not be directly compared with that of the best system reported in ( more than 0.30 of F1 ) .",evaluation,Evaluation on WebQuestions,0,247,10,10,0,evaluation : Evaluation on WebQuestions,0.9573643410852714,0.7142857142857143,0.7142857142857143
question-answering,0,"One of the main reasons is that most questions of WebQuestions , such as Who was vice - president after Kennedy died ? , should be represented by multiple triples , a setting for which our system has not been designed .",evaluation,Evaluation on WebQuestions,0,248,11,11,0,evaluation : Evaluation on WebQuestions,0.9612403100775194,0.7857142857142857,0.7857142857142857
question-answering,0,"Still , for a system trained with almost no manual annotation nor prior information on another dataset , with another - very noisy - KB , the results can be seen as particularly promising .",evaluation,Evaluation on WebQuestions,0,249,12,12,0,evaluation : Evaluation on WebQuestions,0.9651162790697675,0.8571428571428571,0.8571428571428571
question-answering,0,"Besides , evaluation is broad since , in ReVerb , most entities actually appear many times under different names as explained in Section 3 .",evaluation,Evaluation on WebQuestions,0,250,13,13,0,evaluation : Evaluation on WebQuestions,0.9689922480620154,0.9285714285714286,0.9285714285714286
question-answering,0,"Hence , there might be higher ranked answers but they are missed by our evaluation script .",evaluation,Evaluation on WebQuestions,0,251,14,14,0,evaluation : Evaluation on WebQuestions,0.9728682170542635,1.0,1.0
question-answering,0,Conclusion,conclusion,Conclusion,0,252,1,1,0,conclusion : Conclusion,0.9767441860465116,0.14285714285714285,0.14285714285714285
question-answering,0,This paper introduces a new framework for learning to perform open question answering with very little supervision .,conclusion,Conclusion,0,253,2,2,0,conclusion : Conclusion,0.9806201550387597,0.2857142857142857,0.2857142857142857
question-answering,0,"Using embeddings as its core , our approach can be successfully trained on imperfect labeled data and indirect supervision and significantly outperforms previous work for answering simple factual questions .",conclusion,Conclusion,0,254,3,3,0,conclusion : Conclusion,0.9844961240310077,0.42857142857142855,0.42857142857142855
question-answering,0,"Besides , we introduce a new way to fine - tune embedding models for cases where their optimization problem can not be completely solved .",conclusion,Conclusion,0,255,4,4,0,conclusion : Conclusion,0.9883720930232558,0.5714285714285714,0.5714285714285714
question-answering,0,"In spite of these promising results , some exciting challenges remain , especially in order to scale up this model to questions with more complex semantics .",conclusion,Conclusion,0,256,5,5,0,conclusion : Conclusion,0.9922480620155039,0.7142857142857143,0.7142857142857143
question-answering,0,"Due to the very low supervision signal , our work can only answer satisfactorily simple factual questions , and does not even take into account the word ordering when modeling them .",conclusion,Conclusion,0,257,6,6,0,conclusion : Conclusion,0.9961240310077519,0.8571428571428571,0.8571428571428571
question-answering,0,"Further , much more work has to be carried out to encode the semantics of more complex questions into the embedding space .",conclusion,Conclusion,0,258,7,7,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,1,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,title,1,2,1,1,0,title : title,0.010362694300518135,1.0,1.0
question-answering,1,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.015544041450777202,0.14285714285714285,0.14285714285714285
question-answering,1,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.02072538860103627,0.2857142857142857,0.2857142857142857
question-answering,1,successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.025906735751295335,0.42857142857142855,0.42857142857142855
question-answering,1,"As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.031088082901554404,0.5714285714285714,0.5714285714285714
question-answering,1,"The proposed models not only nicely represent the hierarchical structures of sentences with their layerby - layer composition and pooling , but also capture the rich matching patterns at different levels .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03626943005181347,0.7142857142857143,0.7142857142857143
question-answering,1,"Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.04145077720207254,0.8571428571428571,0.8571428571428571
question-answering,1,The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.046632124352331605,1.0,1.0
question-answering,1,Introduction,introduction,introduction,0,10,1,1,0,introduction : introduction,0.05181347150259067,0.058823529411764705,0.058823529411764705
question-answering,1,Matching two potentially heterogenous language objects is central to many natural language applications .,introduction,introduction,1,11,2,2,0,introduction : introduction,0.05699481865284974,0.11764705882352941,0.11764705882352941
question-answering,1,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.06217616580310881,0.17647058823529413,0.17647058823529413
question-answering,1,"Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.06735751295336788,0.23529411764705882,0.23529411764705882
question-answering,1,"Natural language sentences have complicated structures , both sequential and hierarchical , thatare essential for understanding them .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.07253886010362694,0.29411764705882354,0.29411764705882354
question-answering,1,successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,introduction,introduction,1,15,6,6,0,introduction : introduction,0.07772020725388601,0.35294117647058826,0.35294117647058826
question-answering,1,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",introduction,introduction,1,16,7,7,0,introduction : introduction,0.08290155440414508,0.4117647058823529,0.4117647058823529
question-answering,1,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",introduction,introduction,1,17,8,8,0,introduction : introduction,0.08808290155440414,0.47058823529411764,0.47058823529411764
question-answering,1,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.09326424870466321,0.5294117647058824,0.5294117647058824
question-answering,1,This is part of our continuing effort 1 in understanding natural language objects and the matching between them .,introduction,introduction,0,19,10,10,0,introduction : introduction,0.09844559585492228,0.5882352941176471,0.5882352941176471
question-answering,1,Our main contributions can be summarized as follows .,introduction,introduction,0,20,11,11,0,introduction : introduction,0.10362694300518134,0.6470588235294118,0.6470588235294118
question-answering,1,"First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .",introduction,introduction,0,21,12,12,0,introduction : introduction,0.10880829015544041,0.7058823529411765,0.7058823529411765
question-answering,1,Roadmap,introduction,introduction,0,22,13,13,0,introduction : introduction,0.11398963730569948,0.7647058823529411,0.7647058823529411
question-answering,1,"We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .",introduction,introduction,0,23,14,14,0,introduction : introduction,0.11917098445595854,0.8235294117647058,0.8235294117647058
question-answering,1,"Based on that , in Section 3 , we propose two architectures for sentence matching , with a detailed discussion of their relation .",introduction,introduction,0,24,15,15,0,introduction : introduction,0.12435233160621761,0.8823529411764706,0.8823529411764706
question-answering,1,"In Section 4 , we briefly discuss the learning of the proposed architectures .",introduction,introduction,0,25,16,16,0,introduction : introduction,0.12953367875647667,0.9411764705882353,0.9411764705882353
question-answering,1,"Then in Section 5 , we report our empirical study , followed by a brief discussion of related work in Section 6 .",introduction,introduction,0,26,17,17,0,introduction : introduction,0.13471502590673576,1.0,1.0
question-answering,1,Convolutional Sentence Model,model,Convolutional Sentence Model,0,27,1,1,0,model : Convolutional Sentence Model,0.13989637305699482,0.030303030303030304,0.06666666666666667
question-answering,1,We start with proposing a new convolutional architecture for modeling sentences .,model,Convolutional Sentence Model,0,28,2,2,0,model : Convolutional Sentence Model,0.14507772020725387,0.06060606060606061,0.13333333333333333
question-answering,1,"As illustrated in , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .",model,Convolutional Sentence Model,0,29,3,3,0,model : Convolutional Sentence Model,0.15025906735751296,0.09090909090909091,0.2
question-answering,1,"As in most convolutional models , we use convolution units with a local "" receptive field "" and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .",model,Convolutional Sentence Model,0,30,4,4,0,model : Convolutional Sentence Model,0.15544041450777202,0.12121212121212122,0.26666666666666666
question-answering,1,Convolution,model,Convolutional Sentence Model,0,31,5,5,0,model : Convolutional Sentence Model,0.16062176165803108,0.15151515151515152,0.3333333333333333
question-answering,1,"As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .",model,Convolutional Sentence Model,0,32,6,6,0,model : Convolutional Sentence Model,0.16580310880829016,0.18181818181818182,0.4
question-answering,1,"Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is",model,Convolutional Sentence Model,0,33,7,7,0,model : Convolutional Sentence Model,0.17098445595854922,0.21212121212121213,0.4666666666666667
question-answering,1,and it s matrix form is z,model,Convolutional Sentence Model,0,34,8,8,0,model : Convolutional Sentence Model,0.17616580310880828,0.24242424242424243,0.5333333333333333
question-answering,1,"gives the output of feature map of type - f for location i in Layer - ; w ( , f ) is the parameters for f on Layer - , with matrix form",model,Convolutional Sentence Model,0,35,9,9,0,model : Convolutional Sentence Model,0.18134715025906736,0.2727272727272727,0.6
question-answering,1,"gives the output of feature map of type - f for location i in Layer - ; w ( , f ) is the parameters for f on Layer - , with matrix form",model,Convolutional Sentence Model,0,36,10,10,0,model : Convolutional Sentence Model,0.18652849740932642,0.30303030303030304,0.6666666666666666
question-answering,1,"( ) is the activation function ( e.g. , Sigmoid or Relu )",model,Convolutional Sentence Model,0,37,11,11,0,model : Convolutional Sentence Model,0.19170984455958548,0.3333333333333333,0.7333333333333333
question-answering,1,"( ?1 ) i denotes the segment of Layer - ? 1 for the convolution at location i , whil",model,Convolutional Sentence Model,0,38,12,12,0,model : Convolutional Sentence Model,0.19689119170984457,0.36363636363636365,0.8
question-answering,1,"( ?1 ) i denotes the segment of Layer - ? 1 for the convolution at location i , whil",model,Convolutional Sentence Model,0,39,13,13,0,model : Convolutional Sentence Model,0.20207253886010362,0.3939393939393939,0.8666666666666667
question-answering,1,"( ?1 ) i denotes the segment of Layer - ? 1 for the convolution at location i , whil",model,Convolutional Sentence Model,0,40,14,14,0,model : Convolutional Sentence Model,0.20725388601036268,0.42424242424242425,0.9333333333333333
question-answering,1,concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .,model,Convolutional Sentence Model,0,41,15,15,0,model : Convolutional Sentence Model,0.21243523316062177,0.45454545454545453,1.0
question-answering,1,Max - Pooling,model,Max-Pooling,0,42,16,1,0,model : Max-Pooling,0.21761658031088082,0.48484848484848486,0.25
question-answering,1,"We take a max - pooling in every two - unit window for every f , after each convolution",model,Max-Pooling,0,43,17,2,0,model : Max-Pooling,0.22279792746113988,0.5151515151515151,0.5
question-answering,1,The effects of pooling are two - fold :,model,Max-Pooling,0,44,18,3,0,model : Max-Pooling,0.22797927461139897,0.5454545454545454,0.75
question-answering,1,") it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .",model,Max-Pooling,0,45,19,4,0,model : Max-Pooling,0.23316062176165803,0.5757575757575758,1.0
question-answering,1,Length Variability,model,Length Variability,0,46,20,1,0,model : Length Variability,0.23834196891191708,0.6060606060606061,0.07692307692307693
question-answering,1,The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .,model,Length Variability,0,47,21,2,0,model : Length Variability,0.24352331606217617,0.6363636363636364,0.15384615384615385
question-answering,1,"More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .",model,Length Variability,0,48,22,3,0,model : Length Variability,0.24870466321243523,0.6666666666666666,0.23076923076923078
question-answering,1,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .",model,Length Variability,0,49,23,4,0,model : Length Variability,0.2538860103626943,0.696969696969697,0.3076923076923077
question-answering,1,"For any given sentence input x , the output of type - f filter for location i in the th layer is given by",model,Length Variability,0,50,24,5,0,model : Length Variability,0.25906735751295334,0.7272727272727273,0.38461538461538464
question-answering,1,"2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .",model,Length Variability,0,51,25,6,0,model : Length Variability,0.26424870466321243,0.7575757575757576,0.46153846153846156
question-answering,1,"Actually it creates a natural hierarchy of all - zero padding ( as illustrated in ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .",model,Length Variability,0,52,26,7,0,model : Length Variability,0.2694300518134715,0.7878787878787878,0.5384615384615384
question-answering,1,"The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .",model,Length Variability,0,53,27,8,0,model : Length Variability,0.27461139896373055,0.8181818181818182,0.6153846153846154
question-answering,1,"gives an example on what could happen on the first two layers with input sentence "" The cat sat on the mat "" .",model,Length Variability,0,54,28,9,0,model : Length Variability,0.27979274611398963,0.8484848484848485,0.6923076923076923
question-answering,1,"Just for illustration purpose , we present a dramatic choice of parameters ( by turning off some elements in W ) to make the convolution units focus on different segments within a 3 - word window .",model,Length Variability,0,55,29,10,0,model : Length Variability,0.2849740932642487,0.8787878787878788,0.7692307692307693
question-answering,1,"For example , some feature maps ( group 2 ) give compositions for "" the cat "" and "" cat sat "" , each being a vector .",model,Length Variability,0,56,30,11,0,model : Length Variability,0.29015544041450775,0.9090909090909091,0.8461538461538461
question-answering,1,"Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .",model,Length Variability,0,57,31,12,0,model : Length Variability,0.29533678756476683,0.9393939393939394,0.9230769230769231
question-answering,1,"The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between "" on the "" and "" the mat "" for feature maps group 2 from the rightmost two sliding windows .",model,Length Variability,0,58,32,13,0,model : Length Variability,0.3005181347150259,0.9696969696969697,1.0
question-answering,1,Some Analysis on the Convolutional Architecture,model,Some Analysis on the Convolutional Architecture,0,59,33,1,0,model : Some Analysis on the Convolutional Architecture,0.30569948186528495,1.0,1.0
question-answering,1,Relation to Recursive Models,model,model,0,60,1,1,0,model : model,0.31088082901554404,0.021739130434782608,0.125
question-answering,1,"Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .",model,model,0,61,2,2,0,model : model,0.3160621761658031,0.043478260869565216,0.25
question-answering,1,"First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .",model,model,0,62,3,3,0,model : model,0.32124352331606215,0.06521739130434782,0.375
question-answering,1,"Instead , it takes multiple choices of composition via a large feature map ( encoded in w ( , f ) for different f ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .",model,model,0,63,4,4,0,model : model,0.32642487046632124,0.08695652173913043,0.5
question-answering,1,"With any window width k ? 3 , the type of composition would be much richer than that of RAE .",model,model,0,64,5,5,0,model : model,0.3316062176165803,0.10869565217391304,0.625
question-answering,1,"Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .",model,model,0,65,6,6,0,model : model,0.33678756476683935,0.13043478260869565,0.75
question-answering,1,"However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .",model,model,0,66,7,7,0,model : model,0.34196891191709844,0.15217391304347827,0.875
question-answering,1,"For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a "" global "" synthesis on the learned sentence representation .",model,model,0,67,8,8,0,model : model,0.3471502590673575,0.17391304347826086,1.0
question-answering,1,"Relation to "" Shallow "" Convolutional Models",model,"Relation to ""Shallow"" Convolutional Models",0,68,9,1,0,"model : Relation to ""Shallow"" Convolutional Models",0.35233160621761656,0.1956521739130435,0.2
question-answering,1,"The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .",model,"Relation to ""Shallow"" Convolutional Models",0,69,10,2,0,"model : Relation to ""Shallow"" Convolutional Models",0.35751295336787564,0.21739130434782608,0.4
question-answering,1,"This type of models , with local convolutions and a global pooling , essentially do a "" soft "" local template matching and is able to detect local features useful for a certain task .",model,"Relation to ""Shallow"" Convolutional Models",0,70,11,3,0,"model : Relation to ""Shallow"" Convolutional Models",0.3626943005181347,0.2391304347826087,0.6
question-answering,1,"Since the sentencelevel sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .",model,"Relation to ""Shallow"" Convolutional Models",0,71,12,4,0,"model : Relation to ""Shallow"" Convolutional Models",0.36787564766839376,0.2608695652173913,0.8
question-answering,1,It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .,model,"Relation to ""Shallow"" Convolutional Models",0,72,13,5,0,"model : Relation to ""Shallow"" Convolutional Models",0.37305699481865284,0.2826086956521739,1.0
question-answering,1,Convolutional Matching Models,model,Convolutional Matching Models,0,73,14,1,0,model : Convolutional Matching Models,0.37823834196891193,0.30434782608695654,0.5
question-answering,1,"Based on the discussion in Section 2 , we propose two related convolutional architectures , namely ARC - I and ARC - II ) , for matching two sentences .",model,Convolutional Matching Models,0,74,15,2,0,model : Convolutional Matching Models,0.38341968911917096,0.32608695652173914,1.0
question-answering,1,Architecture - I ( ARC - I ),model,Architecture-I (ARC-I),0,75,16,1,0,model : Architecture-I (ARC-I),0.38860103626943004,0.34782608695652173,0.14285714285714285
question-answering,1,"Architecture - I ( ARC - I ) , as illustrated in , takes a conventional approach :",model,Architecture-I (ARC-I),0,76,17,2,0,model : Architecture-I (ARC-I),0.39378238341968913,0.3695652173913043,0.2857142857142857
question-answering,1,"It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .",model,Architecture-I (ARC-I),0,77,18,3,0,model : Architecture-I (ARC-I),0.39896373056994816,0.391304347826087,0.42857142857142855
question-answering,1,"It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .",model,Architecture-I (ARC-I),0,78,19,4,0,model : Architecture-I (ARC-I),0.40414507772020725,0.41304347826086957,0.5714285714285714
question-answering,1,"Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .",model,Architecture-I (ARC-I),0,79,20,5,0,model : Architecture-I (ARC-I),0.40932642487046633,0.43478260869565216,0.7142857142857143
question-answering,1,"In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .",model,Architecture-I (ARC-I),0,80,21,6,0,model : Architecture-I (ARC-I),0.41450777202072536,0.45652173913043476,0.8571428571428571
question-answering,1,"This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .",model,Architecture-I (ARC-I),0,81,22,7,0,model : Architecture-I (ARC-I),0.41968911917098445,0.4782608695652174,1.0
question-answering,1,Architecture - II ( ARC - II ),model,Architecture-II (ARC-II),0,82,23,1,0,model : Architecture-II (ARC-II),0.42487046632124353,0.5,0.14285714285714285
question-answering,1,"In view of the drawback of Architecture - I , we propose Architecture - II ( ARC - II ) that is built directly on the interaction space between two sentences .",model,Architecture-II (ARC-II),0,83,24,2,0,model : Architecture-II (ARC-II),0.43005181347150256,0.5217391304347826,0.2857142857142857
question-answering,1,"It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .",model,Architecture-II (ARC-II),0,84,25,3,0,model : Architecture-II (ARC-II),0.43523316062176165,0.5434782608695652,0.42857142857142855
question-answering,1,"Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through "" one-dimensional "" ( 1D ) convolutions .",model,Architecture-II (ARC-II),0,85,26,4,0,model : Architecture-II (ARC-II),0.44041450777202074,0.5652173913043478,0.5714285714285714
question-answering,1,"For segment ion S X and segment j on S Y , we have the feature map",model,Architecture-II (ARC-II),0,86,27,5,0,model : Architecture-II (ARC-II),0.44559585492227977,0.5869565217391305,0.7142857142857143
question-answering,1,"where ? i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z",model,Architecture-II (ARC-II),0,87,28,6,0,model : Architecture-II (ARC-II),0.45077720207253885,0.6086956521739131,0.8571428571428571
question-answering,1,"where ? i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z",model,Architecture-II (ARC-II),0,88,29,7,0,model : Architecture-II (ARC-II),0.45595854922279794,0.6304347826086957,1.0
question-answering,1,.,model,Training,0,89,30,1,0,model : Training,0.46113989637305697,0.6521739130434783,0.058823529411764705
question-answering,1,Clearly the 1D convolution preserves the location information about both segments .,model,Training,0,90,31,2,0,model : Training,0.46632124352331605,0.6739130434782609,0.11764705882352941
question-answering,1,"After that in Layer - 2 , it performs a 2D max - pooling in non-overlapping 2 2 windows ( illustrated in )",model,Training,0,91,32,3,0,model : Training,0.47150259067357514,0.6956521739130435,0.17647058823529413
question-answering,1,"4 ) In Layer - 3 , we perform a 2D convolution on k 3 k 3 windows of output from Layer - 2 :",model,Training,0,92,33,4,0,model : Training,0.47668393782383417,0.717391304347826,0.23529411764705882
question-answering,1,5 ),model,Training,0,93,34,5,0,model : Training,0.48186528497409326,0.7391304347826086,0.29411764705882354
question-answering,1,"This could goon for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .",model,Training,0,94,35,6,0,model : Training,0.48704663212435234,0.7608695652173914,0.35294117647058826
question-answering,1,The 2D - Convolution,model,Training,0,95,36,7,0,model : Training,0.49222797927461137,0.782608695652174,0.4117647058823529
question-answering,1,"After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation z ( ) i , j which encodes the information from both sentences .",model,Training,0,96,37,8,0,model : Training,0.49740932642487046,0.8043478260869565,0.47058823529411764
question-answering,1,The general two - dimensional convolution is formulated as z ( ),model,Training,0,97,38,9,0,model : Training,0.5025906735751295,0.8260869565217391,0.5294117647058824
question-answering,1,where ? concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,model,Training,0,98,39,10,0,model : Training,0.5077720207253886,0.8478260869565217,0.5882352941176471
question-answering,1,where ? concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,model,Training,0,99,40,11,0,model : Training,0.5129533678756477,0.8695652173913043,0.6470588235294118
question-answering,1,"This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .",model,Training,0,100,41,12,0,model : Training,0.5181347150259067,0.8913043478260869,0.7058823529411765
question-answering,1,"This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :",model,Training,0,101,42,13,0,model : Training,0.5233160621761658,0.9130434782608695,0.7647058823529411
question-answering,1,) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .,model,Training,0,102,43,14,0,model : Training,0.5284974093264249,0.9347826086956522,0.8235294117647058
question-answering,1,"contains information about the words in S X before those in z ( ) i + 1 , j , although they maybe generated with slightly different segments in S Y , due to the 2D pooling ( illustrated in .",model,Training,0,103,44,15,0,model : Training,0.533678756476684,0.9565217391304348,0.8823529411764706
question-answering,1,"The orders is however retained in a "" conditional "" sense .",model,Training,0,104,45,16,0,model : Training,0.538860103626943,0.9782608695652174,0.9411764705882353
question-answering,1,"Our experiments show that when ARC - II is trained on the ( S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting , which however does not happen with ARC - I.",model,Training,0,105,46,17,0,model : Training,0.5440414507772021,1.0,1.0
question-answering,1,Model Generality,model,model,0,106,1,1,0,model : model,0.5492227979274611,0.08333333333333333,0.08333333333333333
question-answering,1,It is not hard to show that ARC - II actually subsumes ARC - I as a special case .,model,model,0,107,2,2,0,model : model,0.5544041450777202,0.16666666666666666,0.16666666666666666
question-answering,1,"Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .",model,model,0,108,3,3,0,model : model,0.5595854922279793,0.25,0.25
question-answering,1,"More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .",model,model,0,109,4,4,0,model : model,0.5647668393782384,0.3333333333333333,0.3333333333333333
question-answering,1,"As a result , the output for each filter f , denoted z",model,model,0,110,5,5,0,model : model,0.5699481865284974,0.4166666666666667,0.4166666666666667
question-answering,1,"1 , f ) 1:n , 1:n ( n is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in ARC - I .",model,model,0,111,6,6,0,model : model,0.5751295336787565,0.5,0.5
question-answering,1,"Clearly the 2D pooling that follows will reduce to 1 D pooling , with this separateness preserved .",model,model,0,112,7,7,0,model : model,0.5803108808290155,0.5833333333333334,0.5833333333333334
question-answering,1,"If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .",model,model,0,113,8,8,0,model : model,0.5854922279792746,0.6666666666666666,0.6666666666666666
question-answering,1,"As suggested by the order - preserving property and the generality of ARC - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .",model,model,0,114,9,9,0,model : model,0.5906735751295337,0.75,0.75
question-answering,1,"As a result , ARC - II can naturally blend two seemingly diverging processes :",model,model,0,115,10,10,0,model : model,0.5958549222797928,0.8333333333333334,0.8333333333333334
question-answering,1,") the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .",model,model,0,116,11,11,0,model : model,0.6010362694300518,0.9166666666666666,0.9166666666666666
question-answering,1,This intuition is verified by the superior performance of ARC - II in experiments ( Section 5 ) on different matching tasks .,model,model,0,117,12,12,0,model : model,0.6062176165803109,1.0,1.0
question-answering,1,Training,training,training,0,118,1,1,0,training : training,0.6113989637305699,0.05263157894736842,0.05263157894736842
question-answering,1,We employ a discriminative training strategy with a large margin objective .,training,training,0,119,2,2,0,training : training,0.616580310880829,0.10526315789473684,0.10526315789473684
question-answering,1,"Suppose that we are given the following triples ( x , y + , y ? )",training,training,0,120,3,3,0,training : training,0.6217616580310881,0.15789473684210525,0.15789473684210525
question-answering,1,"from the oracle , with x matched with y + better than with y ? .",training,training,0,121,4,4,0,training : training,0.6269430051813472,0.21052631578947367,0.21052631578947367
question-answering,1,We have the following ranking - based loss as objective :,training,training,0,122,5,5,0,training : training,0.6321243523316062,0.2631578947368421,0.2631578947368421
question-answering,1,"where s ( x , y) is predicted matching score for ( x , y ) , and ? includes the parameters for convolution layers and those for the MLP .",training,training,0,123,6,6,0,training : training,0.6373056994818653,0.3157894736842105,0.3157894736842105
question-answering,1,"where s ( x , y) is predicted matching score for ( x , y ) , and ? includes the parameters for convolution layers and those for the MLP .",training,training,0,124,7,7,0,training : training,0.6424870466321243,0.3684210526315789,0.3684210526315789
question-answering,1,The optimization is relatively straightforward for both architectures with the standard back - propagation .,training,training,0,125,8,8,0,training : training,0.6476683937823834,0.42105263157894735,0.42105263157894735
question-answering,1,The gating function ( see Section 2 ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .,training,training,0,126,9,9,0,training : training,0.6528497409326425,0.47368421052631576,0.47368421052631576
question-answering,1,"In other words , We use stochastic gradient descent for the optimization of models .",training,training,1,127,10,10,0,training : training,0.6580310880829016,0.5263157894736842,0.5263157894736842
question-answering,1,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,training,training,1,128,11,11,0,training : training,0.6632124352331606,0.5789473684210527,0.5789473684210527
question-answering,1,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .",training,training,1,129,12,12,0,training : training,0.6683937823834197,0.631578947368421,0.631578947368421
question-answering,1,"For small datasets ( less than 10 k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .",training,training,0,130,13,13,0,training : training,0.6735751295336787,0.6842105263157895,0.6842105263157895
question-answering,1,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .",training,training,1,131,14,14,0,training : training,0.6787564766839378,0.7368421052631579,0.7368421052631579
question-answering,1,"Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .",training,training,0,132,15,15,0,training : training,0.6839378238341969,0.7894736842105263,0.7894736842105263
question-answering,1,We vary the maximum length of words for different tasks to cope with its longest sentence .,training,training,0,133,16,16,0,training : training,0.689119170984456,0.8421052631578947,0.8421052631578947
question-answering,1,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .",training,training,1,134,17,17,0,training : training,0.694300518134715,0.8947368421052632,0.8947368421052632
question-answering,1,"ARC - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while ARC - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .",training,training,0,135,18,18,0,training : training,0.6994818652849741,0.9473684210526315,0.9473684210526315
question-answering,1,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .",training,training,1,136,19,19,0,training : training,0.7046632124352331,1.0,1.0
question-answering,1,Experiments,experiment,Experiments,0,137,1,1,0,experiment : Experiments,0.7098445595854922,0.25,0.25
question-answering,1,"We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .",experiment,Experiments,0,138,2,2,0,experiment : Experiments,0.7150259067357513,0.5,0.5
question-answering,1,"Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .",experiment,Experiments,0,139,3,3,0,experiment : Experiments,0.7202072538860104,0.75,0.75
question-answering,1,"Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .",experiment,Experiments,0,140,4,4,0,experiment : Experiments,0.7253886010362695,1.0,1.0
question-answering,1,Competitor Methods,method,Competitor Methods,0,141,1,1,0,method : Competitor Methods,0.7305699481865285,0.03225806451612903,0.14285714285714285
question-answering,1,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,method,Competitor Methods,1,142,2,2,0,method : Competitor Methods,0.7357512953367875,0.06451612903225806,0.2857142857142857
question-answering,1,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :",method,Competitor Methods,1,143,3,3,0,method : Competitor Methods,0.7409326424870466,0.0967741935483871,0.42857142857142855
question-answering,1,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",method,Competitor Methods,1,144,4,4,0,method : Competitor Methods,0.7461139896373057,0.12903225806451613,0.5714285714285714
question-answering,1,"We use the SENNA - type sentence model for sentence representation ; SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",method,Competitor Methods,1,145,5,5,0,method : Competitor Methods,0.7512953367875648,0.16129032258064516,0.7142857142857143
question-answering,1,"We use the SENNA - type sentence model for sentence representation ; SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",method,Competitor Methods,1,146,6,6,0,method : Competitor Methods,0.7564766839378239,0.1935483870967742,0.8571428571428571
question-answering,1,"All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .",method,Competitor Methods,0,147,7,7,0,method : Competitor Methods,0.7616580310880829,0.22580645161290322,1.0
question-answering,1,Experiment I : Sentence,method,Experiment I: Sentence,0,148,8,1,0,method : Experiment I: Sentence,0.7668393782383419,0.25806451612903225,0.0625
question-answering,1,Completion,method,Experiment I: Sentence,0,149,9,2,0,method : Experiment I: Sentence,0.772020725388601,0.2903225806451613,0.125
question-answering,1,This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .,method,Experiment I: Sentence,0,150,10,3,0,method : Experiment I: Sentence,0.7772020725388601,0.3225806451612903,0.1875
question-answering,1,"Basically , we take a sentence from Reuterswith two "" balanced "" clauses ( with 8 ? 28 words ) divided by one comma , and use the first clause as S X and the second as S Y .",method,Experiment I: Sentence,0,151,11,4,0,method : Experiment I: Sentence,0.7823834196891192,0.3548387096774194,0.25
question-answering,1,The task is then to recover the original second clause for any given first clause .,method,Experiment I: Sentence,0,152,12,5,0,method : Experiment I: Sentence,0.7875647668393783,0.3870967741935484,0.3125
question-answering,1,The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .,method,Experiment I: Sentence,0,153,13,6,0,method : Experiment I: Sentence,0.7927461139896373,0.41935483870967744,0.375
question-answering,1,"We deliberately make the task harder by using negative second clauses similar to the original ones 4 , both in training and testing .",method,Experiment I: Sentence,0,154,14,7,0,method : Experiment I: Sentence,0.7979274611398963,0.45161290322580644,0.4375
question-answering,1,One representative example is given as follows :,method,Experiment I: Sentence,0,155,15,8,0,method : Experiment I: Sentence,0.8031088082901554,0.4838709677419355,0.5
question-answering,1,"All models are trained on 3 million triples ( from 600K positive pairs ) , and tested on 50K positive pairs , each accompanied by four negatives , with results shown in .",method,Experiment I: Sentence,0,156,16,9,0,method : Experiment I: Sentence,0.8082901554404145,0.5161290322580645,0.5625
question-answering,1,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .",method,Experiment I: Sentence,0,157,17,10,0,method : Experiment I: Sentence,0.8134715025906736,0.5483870967741935,0.625
question-answering,1,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .",method,Experiment I: Sentence,0,158,18,11,0,method : Experiment I: Sentence,0.8186528497409327,0.5806451612903226,0.6875
question-answering,1,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",method,Experiment I: Sentence,0,159,19,12,0,method : Experiment I: Sentence,0.8238341968911918,0.6129032258064516,0.75
question-answering,1,"It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..",method,Experiment I: Sentence,0,160,20,13,0,method : Experiment I: Sentence,0.8290155440414507,0.6451612903225806,0.8125
question-answering,1,"This task is slightly easier than Experiment I , with more training instances and purely random negatives .",method,Experiment I: Sentence,0,161,21,14,0,method : Experiment I: Sentence,0.8341968911917098,0.6774193548387096,0.875
question-answering,1,"It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime ? rest ) .",method,Experiment I: Sentence,0,162,22,15,0,method : Experiment I: Sentence,0.8393782383419689,0.7096774193548387,0.9375
question-answering,1,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .",method,Experiment I: Sentence,0,163,23,16,0,method : Experiment I: Sentence,0.844559585492228,0.7419354838709677,1.0
question-answering,1,Experiment II : Matching A Response to A Tweet,method,Experiment II: Matching A Response to A Tweet,0,164,24,1,0,method : Experiment II: Matching A Response to A Tweet,0.8497409326424871,0.7741935483870968,1.0
question-answering,1,Experiment III : Paraphrase Identification,method,Experiment III: Paraphrase Identification,0,165,25,1,0,method : Experiment III: Paraphrase Identification,0.8549222797927462,0.8064516129032258,0.14285714285714285
question-answering,1,"Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .",method,Experiment III: Paraphrase Identification,0,166,26,2,0,method : Experiment III: Paraphrase Identification,0.8601036269430051,0.8387096774193549,0.2857142857142857
question-answering,1,is included to test our methods on matching homogenous objects .,method,Experiment III: Paraphrase Identification,0,167,27,3,0,method : Experiment III: Paraphrase Identification,0.8652849740932642,0.8709677419354839,0.42857142857142855
question-answering,1,"Here we use the benchmark MSRP dataset , which contains 4,076 instances for training and 1,725 for test .",method,Experiment III: Paraphrase Identification,0,168,28,4,0,method : Experiment III: Paraphrase Identification,0.8704663212435233,0.9032258064516129,0.5714285714285714
question-answering,1,We use all the training instances and report the test performance from early stopping .,method,Experiment III: Paraphrase Identification,0,169,29,5,0,method : Experiment III: Paraphrase Identification,0.8756476683937824,0.9354838709677419,0.7142857142857143
question-answering,1,"As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .",method,Experiment III: Paraphrase Identification,0,170,30,6,0,method : Experiment III: Paraphrase Identification,0.8808290155440415,0.967741935483871,0.8571428571428571
question-answering,1,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .",method,Experiment III: Paraphrase Identification,0,171,31,7,0,method : Experiment III: Paraphrase Identification,0.8860103626943006,1.0,1.0
question-answering,1,Discussions,discussion,Discussions,0,172,1,1,0,discussion : Discussions,0.8911917098445595,0.1,0.1
question-answering,1,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,discussion,Discussions,1,173,2,2,0,discussion : Discussions,0.8963730569948186,0.2,0.2
question-answering,1,"It s superiority over ARC - I , however , is less salient when the sentences have deep grammatical structures and the matching relies lesson the local matching patterns , as in Experiment - I .",discussion,Discussions,0,174,3,3,0,discussion : Discussions,0.9015544041450777,0.3,0.3
question-answering,1,"This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .",discussion,Discussions,0,175,4,4,0,discussion : Discussions,0.9067357512953368,0.4,0.4
question-answering,1,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .",discussion,Discussions,1,176,5,5,0,discussion : Discussions,0.9119170984455959,0.5,0.5
question-answering,1,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .",discussion,Discussions,1,177,6,6,0,discussion : Discussions,0.917098445595855,0.6,0.6
question-answering,1,It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .,discussion,Discussions,0,178,7,7,0,discussion : Discussions,0.9222797927461139,0.7,0.7
question-answering,1,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,discussion,Discussions,1,179,8,8,0,discussion : Discussions,0.927461139896373,0.8,0.8
question-answering,1,"We hypothesize that the Word2 Vec embedding is trained in such away that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .",discussion,Discussions,0,180,9,9,0,discussion : Discussions,0.9326424870466321,0.9,0.9
question-answering,1,This is in contrast with other bag - of - words models like DEEPMATCH .,discussion,Discussions,0,181,10,10,0,discussion : Discussions,0.9378238341968912,1.0,1.0
question-answering,1,Related Work,related work,Related Work,0,182,1,1,0,related work : Related Work,0.9430051813471503,0.1111111111111111,0.1111111111111111
question-answering,1,"Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .",related work,Related Work,0,183,2,2,0,related work : Related Work,0.9481865284974094,0.2222222222222222,0.2222222222222222
question-answering,1,"When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .",related work,Related Work,0,184,3,3,0,related work : Related Work,0.9533678756476683,0.3333333333333333,0.3333333333333333
question-answering,1,"Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .",related work,Related Work,0,185,4,4,0,related work : Related Work,0.9585492227979274,0.4444444444444444,0.4444444444444444
question-answering,1,Our models are related to the long thread of work on sentence representation .,related work,Related Work,0,186,5,5,0,related work : Related Work,0.9637305699481865,0.5555555555555556,0.5555555555555556
question-answering,1,"Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .",related work,Related Work,0,187,6,6,0,related work : Related Work,0.9689119170984456,0.6666666666666666,0.6666666666666666
question-answering,1,There is very little work on convolutional modeling of language .,related work,Related Work,0,188,7,7,0,related work : Related Work,0.9740932642487047,0.7777777777777778,0.7777777777777778
question-answering,1,"In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .",related work,Related Work,0,189,8,8,0,related work : Related Work,0.9792746113989638,0.8888888888888888,0.8888888888888888
question-answering,1,"This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .",related work,Related Work,0,190,9,9,0,related work : Related Work,0.9844559585492227,1.0,1.0
question-answering,1,Conclusion,conclusion,Conclusion,0,191,1,1,0,conclusion : Conclusion,0.9896373056994818,0.3333333333333333,0.3333333333333333
question-answering,1,"We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .",conclusion,Conclusion,0,192,2,2,0,conclusion : Conclusion,0.9948186528497409,0.6666666666666666,0.6666666666666666
question-answering,1,Empirical study shows our models can outperform competitors on a variety of matching tasks .,conclusion,Conclusion,0,193,3,3,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,2,Large - scale Simple Question Answering with Memory Networks,title,title,1,2,1,1,0,title : title,0.007326007326007326,1.0,1.0
question-answering,2,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01098901098901099,0.2,0.2
question-answering,2,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.014652014652014652,0.4,0.4
question-answering,2,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",abstract,abstract,1,5,3,3,0,abstract : abstract,0.018315018315018316,0.6,0.6
question-answering,2,"To this end , we introduce a new dataset of 100 k questions that we use in conjunction with existing benchmarks .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02197802197802198,0.8,0.8
question-answering,2,"We conduct our study within the framework of Memory Networks ( Weston et al. , 2015 ) because this perspective allows us to eventually scale up to more complex reasoning , and show that Memory Networks can be successfully trained to achieve excellent performance .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02564102564102564,1.0,1.0
question-answering,2,Introduction,introduction,introduction,0,8,1,1,0,introduction : introduction,0.029304029304029304,0.04,0.04
question-answering,2,"Open-domain Question Answering ( QA ) systems aim at providing the exact answer ( s ) to questions formulated in natural language , without restriction of domain .",introduction,introduction,0,9,2,2,0,introduction : introduction,0.03296703296703297,0.08,0.08
question-answering,2,"While there is along history of QA systems that search for textual documents or on the Web and extract answers from them ( see e.g. ) , recent progress has been made with the release of large Knowledge Bases ( KBs ) such as Freebase , which contain consolidated knowledge stored as atomic facts , and extracted from different sources , such as free text , tables in webpages or collaborative input .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.03663003663003663,0.12,0.12
question-answering,2,Existing approaches for QA from KBs use learnable components to either transform the question into a structured KB query or learn to embed questions and facts in a low dimensional vector space and retrieve the answer by computing similarities in this embedding space .,introduction,introduction,0,11,4,4,0,introduction : introduction,0.040293040293040296,0.16,0.16
question-answering,2,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",introduction,introduction,0,12,5,5,0,introduction : introduction,0.04395604395604396,0.2,0.2
question-answering,2,"Hence , existing benchmarks are small ; they mostly cover the head of the distributions of facts , and are restricted in their question types and their syntactic and lexical variations .",introduction,introduction,0,13,6,6,0,introduction : introduction,0.047619047619047616,0.24,0.24
question-answering,2,"As such , it is still unknown how much the existing systems perform outside the range of the specific question templates of a few , small benchmark datasets , and it is also unknown whether learning on a single dataset transfers well on other ones , and whether such systems can learn from different training sources , which we believe is necessary to capture the whole range of possible questions .",introduction,introduction,0,14,7,7,0,introduction : introduction,0.05128205128205128,0.28,0.28
question-answering,2,"Besides , the actual need for reasoning , i.e. constructing the answer from more than a single fact from the KB , depends on the actual structure of the KB .",introduction,introduction,0,15,8,8,0,introduction : introduction,0.054945054945054944,0.32,0.32
question-answering,2,"As we shall see , for instance , a simple preprocessing of Freebase tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact , including list questions that expect more than a single answer .",introduction,introduction,0,16,9,9,0,introduction : introduction,0.05860805860805861,0.36,0.36
question-answering,2,"In fact , the task of simple QA itself might already cover a wide range of practical usages , if the KB is properly organized .",introduction,introduction,0,17,10,10,0,introduction : introduction,0.06227106227106227,0.4,0.4
question-answering,2,This paper presents two contributions .,introduction,introduction,0,18,11,11,0,introduction : introduction,0.06593406593406594,0.44,0.44
question-answering,2,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",introduction,introduction,1,19,12,12,0,introduction : introduction,0.0695970695970696,0.48,0.48
question-answering,2,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ? Which forest is Fires Creek in ? What is an active ingredient in childrens earache relief ? tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.07326007326007326,0.52,0.52
question-answering,2,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ? Which forest is Fires Creek in ? What is an active ingredient in childrens earache relief ? tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction,introduction,0,21,14,14,0,introduction : introduction,0.07692307692307693,0.56,0.56
question-answering,2,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ? Which forest is Fires Creek in ? What is an active ingredient in childrens earache relief ? tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction,introduction,0,22,15,15,0,introduction : introduction,0.08058608058608059,0.6,0.6
question-answering,2,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ? Which forest is Fires Creek in ? What is an active ingredient in childrens earache relief ? tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction,introduction,0,23,16,16,0,introduction : introduction,0.08424908424908426,0.64,0.64
question-answering,2,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",introduction,introduction,1,24,17,17,0,introduction : introduction,0.08791208791208792,0.68,0.68
question-answering,2,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",introduction,introduction,0,25,18,18,0,introduction : introduction,0.09157509157509157,0.72,0.72
question-answering,2,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,introduction,introduction,1,26,19,19,0,introduction : introduction,0.09523809523809523,0.76,0.76
question-answering,2,"While our model bares similarity with previous embedding models for QA , using the framework of MemNNs opens the perspective to more involved inference schemes in future work , since MemNNs were shown to perform well on complex reasoning toy QA tasks .",introduction,introduction,0,27,20,20,0,introduction : introduction,0.0989010989010989,0.8,0.8
question-answering,2,We discuss related work in Section 5 .,introduction,introduction,0,28,21,21,0,introduction : introduction,0.10256410256410256,0.84,0.84
question-answering,2,"We report experimental results in Section 6 , where we show that our model achieves excellent results on the benchmark WebQuestions .",introduction,introduction,0,29,22,22,0,introduction : introduction,0.10622710622710622,0.88,0.88
question-answering,2,We also show that it can learn from two different QA datasets to improve its performance on both .,introduction,introduction,0,30,23,23,0,introduction : introduction,0.10989010989010989,0.92,0.92
question-answering,2,We also present the first successful application of transfer learning for QA .,introduction,introduction,0,31,24,24,0,introduction : introduction,0.11355311355311355,0.96,0.96
question-answering,2,"Using the Reverb KB and QA datasets , we show that Reverb facts can be added to the memory and used to answer without retraining , and that MemNNs achieve better results than some systems designed on this dataset .",introduction,introduction,0,32,25,25,0,introduction : introduction,0.11721611721611722,1.0,1.0
question-answering,2,Simple Question Answering,system description,Simple Question Answering,0,33,1,1,0,system description : Simple Question Answering,0.12087912087912088,0.04,0.09090909090909091
question-answering,2,Knowledge Bases contain facts expressed as triples where subject and object are entities and relationship describes the type of ( directed ) link between these entities .,system description,Simple Question Answering,0,34,2,2,0,system description : Simple Question Answering,0.12454212454212454,0.08,0.18181818181818182
question-answering,2,"The simple QA prob - lem we address here consist in finding the answer to questions that can be rephrased as queries of the form , asking for all objects linked to subject by relationship .",system description,Simple Question Answering,0,35,3,3,0,system description : Simple Question Answering,0.1282051282051282,0.12,0.2727272727272727
question-answering,2,The question,system description,Simple Question Answering,0,36,4,4,0,system description : Simple Question Answering,0.13186813186813187,0.16,0.36363636363636365
question-answering,2,"What do Jamaican people speak ? , for instance , could be rephrased as the Freebase query ( jamaica , language spoken , ? ) .",system description,Simple Question Answering,0,37,5,5,0,system description : Simple Question Answering,0.13553113553113552,0.2,0.45454545454545453
question-answering,2,"In other words , fetching a single fact from a KB is sufficient to answer correctly .",system description,Simple Question Answering,0,38,6,6,0,system description : Simple Question Answering,0.1391941391941392,0.24,0.5454545454545454
question-answering,2,"The term simple QA refers to the simplicity of the reasoning process needed to answer questions , since it involves a single fact .",system description,Simple Question Answering,0,39,7,7,0,system description : Simple Question Answering,0.14285714285714285,0.28,0.6363636363636364
question-answering,2,"However , this does not mean that the QA problem is easy per se , since retrieving this single supporting fact can be very challenging as it involves to search over millions of alternatives given a query expressed in natural language .",system description,Simple Question Answering,0,40,8,8,0,system description : Simple Question Answering,0.14652014652014653,0.32,0.7272727272727273
question-answering,2,"shows that , with a KB with many types of relationships like",system description,Simple Question Answering,0,41,9,9,0,system description : Simple Question Answering,0.15018315018315018,0.36,0.8181818181818182
question-answering,2,"Freebase , the range of questions that can be answered with a single fact is already very broad .",system description,Simple Question Answering,0,42,10,10,0,system description : Simple Question Answering,0.15384615384615385,0.4,0.9090909090909091
question-answering,2,"Besides , as we shall see , modiying slightly the structure of the KB can make some QA problems simpler by adding direct connections between entities and hence allow to bypass the need for more complex reasoning .",system description,Simple Question Answering,0,43,11,11,0,system description : Simple Question Answering,0.1575091575091575,0.44,1.0
question-answering,2,Knowledge Bases,system description,Knowledge Bases,0,44,12,1,0,system description : Knowledge Bases,0.16117216117216118,0.48,0.07142857142857142
question-answering,2,"We use the KB Freebase 1 as the basis of our QA system , our source of facts and answers .",system description,Knowledge Bases,0,45,13,2,0,system description : Knowledge Bases,0.16483516483516483,0.52,0.14285714285714285
question-answering,2,All Freebase entities and relationships are typed and the lexicon for types and relationships is closed .,system description,Knowledge Bases,0,46,14,3,0,system description : Knowledge Bases,0.1684981684981685,0.56,0.21428571428571427
question-answering,2,"Freebase data is collaboratively collected and curated , to ensure a high reliability of the facts .",system description,Knowledge Bases,0,47,15,4,0,system description : Knowledge Bases,0.17216117216117216,0.6,0.2857142857142857
question-answering,2,"Each entity has an internal identifier and a set of strings thatare usually used to refer to that entity in text , termed aliases .",system description,Knowledge Bases,0,48,16,5,0,system description : Knowledge Bases,0.17582417582417584,0.64,0.35714285714285715
question-answering,2,"We consider two extracts of Freebase , whose statistics are given in .",system description,Knowledge Bases,0,49,17,6,0,system description : Knowledge Bases,0.1794871794871795,0.68,0.42857142857142855
question-answering,2,"FB2M , which was used in , contains about 2 M entities and 5 k relationships .",system description,Knowledge Bases,0,50,18,7,0,system description : Knowledge Bases,0.18315018315018314,0.72,0.5
question-answering,2,"FB5M , is much larger with about 5 M entities and more than 7.5 k relationships .",system description,Knowledge Bases,0,51,19,8,0,system description : Knowledge Bases,0.18681318681318682,0.76,0.5714285714285714
question-answering,2,"We also use the KB Reverb as a secondary source of facts to study how well a model trained to answer questions using Freebase facts could be used to answer using Reverb 's as well , without being trained on Reverb data .",system description,Knowledge Bases,0,52,20,9,0,system description : Knowledge Bases,0.19047619047619047,0.8,0.6428571428571429
question-answering,2,This is a pure setting of transfer learning .,system description,Knowledge Bases,0,53,21,10,0,system description : Knowledge Bases,0.19413919413919414,0.84,0.7142857142857143
question-answering,2,Reverb is interesting for this experiment because it differs a lot from Freebase .,system description,Knowledge Bases,0,54,22,11,0,system description : Knowledge Bases,0.1978021978021978,0.88,0.7857142857142857
question-answering,2,Its data was extracted automatically from text with minimal human intervention and is highly unstructured : entities are unique strings and the lexicon for relationships is open .,system description,Knowledge Bases,0,55,23,12,0,system description : Knowledge Bases,0.20146520146520147,0.92,0.8571428571428571
question-answering,2,"This leads to many more relationships , but entities with multiple references are not deduplicated , ambiguous referents are not resolved , and the reliability of the stored facts is much lower than in Freebase .",system description,Knowledge Bases,0,56,24,13,0,system description : Knowledge Bases,0.20512820512820512,0.96,0.9285714285714286
question-answering,2,"We used the full extraction from , which contains 2 M entities and 600 k relationships .",system description,Knowledge Bases,0,57,25,14,0,system description : Knowledge Bases,0.2087912087912088,1.0,1.0
question-answering,2,The SimpleQuestions dataset,dataset,The SimpleQuestions dataset,0,58,1,1,0,dataset : The SimpleQuestions dataset,0.21245421245421245,0.047619047619047616,0.047619047619047616
question-answering,2,"Existing resources for QA such as WebQuestions are rather small ( few thousands questions ) and hence do not provide a very thorough coverage of the variety of questions that could be answered using a KB like Freebase , even in the context of simple QA .",dataset,The SimpleQuestions dataset,0,59,2,2,0,dataset : The SimpleQuestions dataset,0.21611721611721613,0.09523809523809523,0.09523809523809523
question-answering,2,"Hence , in this paper , we introduce a new dataset of much larger scale for the task of simple QA called SimpleQuestions .",dataset,The SimpleQuestions dataset,0,60,3,3,0,dataset : The SimpleQuestions dataset,0.21978021978021978,0.14285714285714285,0.14285714285714285
question-answering,2,2,dataset,The SimpleQuestions dataset,0,61,4,4,0,dataset : The SimpleQuestions dataset,0.22344322344322345,0.19047619047619047,0.19047619047619047
question-answering,2,"This dataset consists of a total of 108,442 questions written in natural language by human English - speaking annotators each paired with a corresponding fact from FB2M that provides the answer and explains it .",dataset,The SimpleQuestions dataset,0,62,5,5,0,dataset : The SimpleQuestions dataset,0.2271062271062271,0.23809523809523808,0.23809523809523808
question-answering,2,"We randomly shuffle these questions and use 70 % of them ( 75910 ) as training set , 10 % as validation set , and the remaining 20 % as test set .",dataset,The SimpleQuestions dataset,0,63,6,6,0,dataset : The SimpleQuestions dataset,0.23076923076923078,0.2857142857142857,0.2857142857142857
question-answering,2,Examples of questions and facts are given in .,dataset,The SimpleQuestions dataset,0,64,7,7,0,dataset : The SimpleQuestions dataset,0.23443223443223443,0.3333333333333333,0.3333333333333333
question-answering,2,We collected SimpleQuestions in two phases .,dataset,The SimpleQuestions dataset,0,65,8,8,0,dataset : The SimpleQuestions dataset,0.23809523809523808,0.38095238095238093,0.38095238095238093
question-answering,2,The first phase consisted of shortlisting the set of facts from Freebase to be annotated with questions .,dataset,The SimpleQuestions dataset,0,66,9,9,0,dataset : The SimpleQuestions dataset,0.24175824175824176,0.42857142857142855,0.42857142857142855
question-answering,2,We used FB2M as background KB and removed all facts with undefined relationship type i.e. containing the word freebase .,dataset,The SimpleQuestions dataset,0,67,10,10,0,dataset : The SimpleQuestions dataset,0.2454212454212454,0.47619047619047616,0.47619047619047616
question-answering,2,"We also removed all facts for which the ( subject , relationship ) pair had more than a threshold number of objects .",dataset,The SimpleQuestions dataset,0,68,11,11,0,dataset : The SimpleQuestions dataset,0.2490842490842491,0.5238095238095238,0.5238095238095238
question-answering,2,This filtering step is crucial to remove facts,dataset,The SimpleQuestions dataset,0,69,12,12,0,dataset : The SimpleQuestions dataset,0.25274725274725274,0.5714285714285714,0.5714285714285714
question-answering,2,"The dataset is available from http://fb.ai/babi. which would result in trivial uninformative questions , such as , Name a person who is an actor ?.",dataset,The SimpleQuestions dataset,0,70,13,13,0,dataset : The SimpleQuestions dataset,0.2564102564102564,0.6190476190476191,0.6190476190476191
question-answering,2,The threshold was set to 10 .,dataset,The SimpleQuestions dataset,0,71,14,14,0,dataset : The SimpleQuestions dataset,0.2600732600732601,0.6666666666666666,0.6666666666666666
question-answering,2,"In the second phase , these selected facts were sampled and delivered to human annotators to generate questions from them .",dataset,The SimpleQuestions dataset,0,72,15,15,0,dataset : The SimpleQuestions dataset,0.26373626373626374,0.7142857142857143,0.7142857142857143
question-answering,2,"For the sampling , each fact was associated with a probability which defined as a function of its relationship frequency in the KB : to favor variability , facts with relationship appearing more frequently were given lower probabilities .",dataset,The SimpleQuestions dataset,0,73,16,16,0,dataset : The SimpleQuestions dataset,0.2673992673992674,0.7619047619047619,0.7619047619047619
question-answering,2,"For each sampled facts , annotators were shown the facts along with hyperlinks to freebase.com to provide some context while framing the question .",dataset,The SimpleQuestions dataset,0,74,17,17,0,dataset : The SimpleQuestions dataset,0.27106227106227104,0.8095238095238095,0.8095238095238095
question-answering,2,"Given this information , annotators were asked to phrase a question involving the subject and the relationship of the fact , with the answer being the object .",dataset,The SimpleQuestions dataset,0,75,18,18,0,dataset : The SimpleQuestions dataset,0.27472527472527475,0.8571428571428571,0.8571428571428571
question-answering,2,"The annotators were explicitly instructed to phrase the question differently as much as possible , if they encounter multiple facts with similar relationship .",dataset,The SimpleQuestions dataset,0,76,19,19,0,dataset : The SimpleQuestions dataset,0.2783882783882784,0.9047619047619048,0.9047619047619048
question-answering,2,They were also given the option of skipping facts if they wish to do so .,dataset,The SimpleQuestions dataset,0,77,20,20,0,dataset : The SimpleQuestions dataset,0.28205128205128205,0.9523809523809523,0.9523809523809523
question-answering,2,This was very important to avoid the annotators to write a boilerplate questions when they had no background knowledge about some facts .,dataset,The SimpleQuestions dataset,0,78,21,21,0,dataset : The SimpleQuestions dataset,0.2857142857142857,1.0,1.0
question-answering,2,Memory Networks for Simple QA,system description,Memory Networks for Simple QA,0,79,1,1,0,system description : Memory Networks for Simple QA,0.2893772893772894,0.0125,0.2
question-answering,2,Memory Network consists of a memory ( an indexed array of objects ) and a neural network that is trained to query it given some inputs ( usually questions ) .,system description,Memory Networks for Simple QA,0,80,2,2,0,system description : Memory Networks for Simple QA,0.29304029304029305,0.025,0.4
question-answering,2,"It has four components : Input map ( I ) , Generalization ( G ) , Output map ( O ) and Response ( R ) which we detail below .",system description,Memory Networks for Simple QA,0,81,3,3,0,system description : Memory Networks for Simple QA,0.2967032967032967,0.0375,0.6
question-answering,2,"But first , we describe the MemNNs workflow used to setup a model for simple QA .",system description,Memory Networks for Simple QA,0,82,4,4,0,system description : Memory Networks for Simple QA,0.30036630036630035,0.05,0.8
question-answering,2,This proceeds in three steps :,system description,Memory Networks for Simple QA,0,83,5,5,0,system description : Memory Networks for Simple QA,0.304029304029304,0.0625,1.0
question-answering,2,Storing Freebase : this first phase parses,system description,Storing Freebase: this first phase parses,0,84,6,1,0,system description : Storing Freebase: this first phase parses,0.3076923076923077,0.075,0.16666666666666666
question-answering,2,Freebase ( either FB2M or FB5M depending on the setting ) and stores it in memory .,system description,Storing Freebase: this first phase parses,0,85,7,2,0,system description : Storing Freebase: this first phase parses,0.31135531135531136,0.0875,0.3333333333333333
question-answering,2,It uses the Input module to preprocess the data .,system description,Storing Freebase: this first phase parses,0,86,8,3,0,system description : Storing Freebase: this first phase parses,0.315018315018315,0.1,0.5
question-answering,2,2 .,system description,Storing Freebase: this first phase parses,0,87,9,4,0,system description : Storing Freebase: this first phase parses,0.31868131868131866,0.1125,0.6666666666666666
question-answering,2,Training : this second phase trains the Mem NN to answer question .,system description,Storing Freebase: this first phase parses,0,88,10,5,0,system description : Storing Freebase: this first phase parses,0.32234432234432236,0.125,0.8333333333333334
question-answering,2,"This uses Input , Output and Response modules , the training concerns mainly the parameters of the embedding model at the core of the Output module .",system description,Storing Freebase: this first phase parses,0,89,11,6,0,system description : Storing Freebase: this first phase parses,0.326007326007326,0.1375,1.0
question-answering,2,Connecting Reverb : this third phase adds new facts coming from,system description,Connecting Reverb: this third phase adds new facts coming from,0,90,12,1,0,system description : Connecting Reverb: this third phase adds new facts coming from,0.32967032967032966,0.15,0.16666666666666666
question-answering,2,Reverb to the memory .,system description,Connecting Reverb: this third phase adds new facts coming from,0,91,13,2,0,system description : Connecting Reverb: this third phase adds new facts coming from,0.3333333333333333,0.1625,0.3333333333333333
question-answering,2,This is done after training to test the ability of MemNNs to handle new facts without having to be re-trained .,system description,Connecting Reverb: this third phase adds new facts coming from,0,92,14,3,0,system description : Connecting Reverb: this third phase adds new facts coming from,0.336996336996337,0.175,0.5
question-answering,2,It uses the Input module to preprocess Reverb facts and the Generalization module to connect them to the facts already stored .,system description,Connecting Reverb: this third phase adds new facts coming from,0,93,15,4,0,system description : Connecting Reverb: this third phase adds new facts coming from,0.34065934065934067,0.1875,0.6666666666666666
question-answering,2,"After these three stages , the MemNN is ready to answer any question by running the I , O and R modules in turn .",system description,Connecting Reverb: this third phase adds new facts coming from,0,94,16,5,0,system description : Connecting Reverb: this third phase adds new facts coming from,0.3443223443223443,0.2,0.8333333333333334
question-answering,2,We now detail the implementation of the four modules .,system description,Connecting Reverb: this third phase adds new facts coming from,0,95,17,6,0,system description : Connecting Reverb: this third phase adds new facts coming from,0.34798534798534797,0.2125,1.0
question-answering,2,Input module,system description,Input module,0,96,18,1,0,system description : Input module,0.3516483516483517,0.225,0.3333333333333333
question-answering,2,This module preprocesses the three types of data thatare input to the network :,system description,Input module,0,97,19,2,0,system description : Input module,0.3553113553113553,0.2375,0.6666666666666666
question-answering,2,"Freebase facts thatare used to populate the memory , questions that the system need to answer , and Reverb facts that we use , in a second phase , to extend the memory .",system description,Input module,0,98,20,3,0,system description : Input module,0.358974358974359,0.25,1.0
question-answering,2,Preprocessing Freebase,system description,Preprocessing Freebase,0,99,21,1,0,system description : Preprocessing Freebase,0.3626373626373626,0.2625,0.047619047619047616
question-answering,2,"The Freebase data is initially stored as atomic facts involving single entities as subject and object , plus a relationship between them .",system description,Preprocessing Freebase,0,100,22,2,0,system description : Preprocessing Freebase,0.3663003663003663,0.275,0.09523809523809523
question-answering,2,"However , this storage needs to be adapted to the QA task in two aspects .",system description,Preprocessing Freebase,0,101,23,3,0,system description : Preprocessing Freebase,0.36996336996337,0.2875,0.14285714285714285
question-answering,2,"First , in order to answer list questions , which expect more than one answer , we redefine a fact as being a triple containing a subject , a relationship , and the set of all objects linked to the subject by the relationship .",system description,Preprocessing Freebase,0,102,24,4,0,system description : Preprocessing Freebase,0.37362637362637363,0.3,0.19047619047619047
question-answering,2,"This grouping process transforms atomic facts into grouped facts , which we simply refer to as facts in the following .",system description,Preprocessing Freebase,0,103,25,5,0,system description : Preprocessing Freebase,0.3772893772893773,0.3125,0.23809523809523808
question-answering,2,"shows the impact of this grouping : on FB2M , this decreases the number of facts from 14 M to 11 M and , on FB5 M , from 22 M to 12M .",system description,Preprocessing Freebase,0,104,26,6,0,system description : Preprocessing Freebase,0.38095238095238093,0.325,0.2857142857142857
question-answering,2,"Second , the underlying structure of Freebase is a hypergraph , in which more than two entities can be linked .",system description,Preprocessing Freebase,0,105,27,7,0,system description : Preprocessing Freebase,0.38461538461538464,0.3375,0.3333333333333333
question-answering,2,For instance dates can be linked together with two entities to specify the time period over which the link was valid .,system description,Preprocessing Freebase,0,106,28,8,0,system description : Preprocessing Freebase,0.3882783882783883,0.35,0.38095238095238093
question-answering,2,"The underlying triple storage involves mediator nodes for each such fact , effectively making entities linked through paths of length 2 , instead of 1 .",system description,Preprocessing Freebase,0,107,29,9,0,system description : Preprocessing Freebase,0.39194139194139194,0.3625,0.42857142857142855
question-answering,2,"To obtain direct links between entities in such cases , we created a single fact for these facts by removing the intermediate node and using the second relationship as the relationship for the new condensed fact .",system description,Preprocessing Freebase,0,108,30,10,0,system description : Preprocessing Freebase,0.3956043956043956,0.375,0.47619047619047616
question-answering,2,"This step reduces the need for searching the answer outside the immediate neighborhood of the subject referred to in the question , widely increasing the scope of the simple QA task on Freebase .",system description,Preprocessing Freebase,0,109,31,11,0,system description : Preprocessing Freebase,0.3992673992673993,0.3875,0.5238095238095238
question-answering,2,"On WebQuestions , a benchmark not primarily designed for simple QA , removing mediator nodes allows to jump from around 65 % to 86 % of questions that can be answered with a single fact .",system description,Preprocessing Freebase,0,110,32,12,0,system description : Preprocessing Freebase,0.40293040293040294,0.4,0.5714285714285714
question-answering,2,Preprocessing Freebase facts,system description,Preprocessing Freebase,0,111,33,13,0,system description : Preprocessing Freebase,0.4065934065934066,0.4125,0.6190476190476191
question-answering,2,"fact with k objects y = ( s , r , {o 1 , ... , o k } ) is represented by a bag - of - symbol vector f ( y ) in RN S , where NS is the number of entities and relationships .",system description,Preprocessing Freebase,0,112,34,14,0,system description : Preprocessing Freebase,0.41025641025641024,0.425,0.6666666666666666
question-answering,2,Each dimension off ( y ) corresponds to a relationship or an entity ( independent of whether it appears as subject or object ) .,system description,Preprocessing Freebase,0,113,35,15,0,system description : Preprocessing Freebase,0.4139194139194139,0.4375,0.7142857142857143
question-answering,2,"The entries of the subject and of the relationship have value 1 , and the entries of the objects are set to 1 / k .",system description,Preprocessing Freebase,0,114,36,16,0,system description : Preprocessing Freebase,0.4175824175824176,0.45,0.7619047619047619
question-answering,2,All other entries are 0 .,system description,Preprocessing Freebase,0,115,37,17,0,system description : Preprocessing Freebase,0.42124542124542125,0.4625,0.8095238095238095
question-answering,2,Preprocessing questions,system description,Preprocessing Freebase,0,116,38,18,0,system description : Preprocessing Freebase,0.4249084249084249,0.475,0.8571428571428571
question-answering,2,question q is mapped to a bag - of - ngrams representation g ( q ) of dimension RN V where NV is the size of the vocabulary .,system description,Preprocessing Freebase,0,117,39,19,0,system description : Preprocessing Freebase,0.42857142857142855,0.4875,0.9047619047619048
question-answering,2,"The vocabulary contains all individual words that appear in the questions of our datasets , together with the aliases of Freebase entities , each alias being a single n-gram .",system description,Preprocessing Freebase,0,118,40,20,0,system description : Preprocessing Freebase,0.43223443223443225,0.5,0.9523809523809523
question-answering,2,"The entries of g ( q ) that correspond to words and n-grams of q are equal to 1 , all other ones are set to 0 .",system description,Preprocessing Freebase,0,119,41,21,0,system description : Preprocessing Freebase,0.4358974358974359,0.5125,1.0
question-answering,2,Preprocessing Reverb facts,system description,Preprocessing Reverb facts,0,120,42,1,0,system description : Preprocessing Reverb facts,0.43956043956043955,0.525,0.2
question-answering,2,"In our experiments with Reverb , each fact y = ( s , r , o) is represented as a vector h ( y ) ? RN S +N V .",system description,Preprocessing Reverb facts,0,121,43,2,0,system description : Preprocessing Reverb facts,0.4432234432234432,0.5375,0.4
question-answering,2,"In our experiments with Reverb , each fact y = ( s , r , o) is represented as a vector h ( y ) ? RN S +N V .",system description,Preprocessing Reverb facts,0,122,44,3,0,system description : Preprocessing Reverb facts,0.4468864468864469,0.55,0.6
question-answering,2,"This vector is a bagof - symbol for the subject sand the object o , and a bag - of - words for the relationship r.",system description,Preprocessing Reverb facts,0,123,45,4,0,system description : Preprocessing Reverb facts,0.45054945054945056,0.5625,0.8
question-answering,2,"The exact composition of h is provided by the Generalization module , which we describe now .",system description,Preprocessing Reverb facts,0,124,46,5,0,system description : Preprocessing Reverb facts,0.4542124542124542,0.575,1.0
question-answering,2,Generalization module,system description,Generalization module,0,125,47,1,0,system description : Generalization module,0.45787545787545786,0.5875,0.08333333333333333
question-answering,2,This module is responsible for adding new elements to the memory .,system description,Generalization module,0,126,48,2,0,system description : Generalization module,0.46153846153846156,0.6,0.16666666666666666
question-answering,2,"In our case , the memory has a multigraph structure where each node is a Freebase entity and labeled arcs in the multigraph are Freebase relationships : after their preprocessing , all Freebase facts are stored using this structure .",system description,Generalization module,0,127,49,3,0,system description : Generalization module,0.4652014652014652,0.6125,0.25
question-answering,2,"We also consider the case where new facts , with a different structure ( i.e. new kinds of relationship ) , are provided to the MemNNs by using Reverb .",system description,Generalization module,0,128,50,4,0,system description : Generalization module,0.46886446886446886,0.625,0.3333333333333333
question-answering,2,"In this case , the generalization module is then used to connect Reverb facts to the Freebase - based memory structure , in order to make them usable and searchable by the MemNN .",system description,Generalization module,0,129,51,5,0,system description : Generalization module,0.4725274725274725,0.6375,0.4166666666666667
question-answering,2,"To link the subject and the object of a Reverb fact to Freebase entities , we use precomputed entity links .",system description,Generalization module,0,130,52,6,0,system description : Generalization module,0.47619047619047616,0.65,0.5
question-answering,2,"If such links do not give any result for an entity , we search for Freebase entities with at least one alias that matches the Reverb entity string .",system description,Generalization module,0,131,53,7,0,system description : Generalization module,0.47985347985347987,0.6625,0.5833333333333334
question-answering,2,These two processes allowed to match 17 % of Reverb entities to Freebase ones .,system description,Generalization module,0,132,54,8,0,system description : Generalization module,0.4835164835164835,0.675,0.6666666666666666
question-answering,2,"The remainder of entities were encoded using bag - of - words representation of their strings , since we had no other way of matching them to Freebase entities .",system description,Generalization module,0,133,55,9,0,system description : Generalization module,0.48717948717948717,0.6875,0.75
question-answering,2,All Reverb relationships were encoded using bag - of - words of their strings .,system description,Generalization module,0,134,56,10,0,system description : Generalization module,0.4908424908424908,0.7,0.8333333333333334
question-answering,2,"Using this approximate process , we are able to store each Reverb fact as a bag - of - symbols ( words or Freebase entities ) all already seen by the MemNN during its training phase based on Freebase .",system description,Generalization module,0,135,57,11,0,system description : Generalization module,0.4945054945054945,0.7125,0.9166666666666666
question-answering,2,We can then hope that what had been learned there could also be successfully used to query Reverb facts .,system description,Generalization module,0,136,58,12,0,system description : Generalization module,0.4981684981684982,0.725,1.0
question-answering,2,Output module,system description,Output module,0,137,59,1,0,system description : Output module,0.5018315018315018,0.7375,0.2
question-answering,2,The output module performs the memory lookups given the input to return the supporting facts destined to eventually provide the answer given a question .,system description,Output module,0,138,60,2,0,system description : Output module,0.5054945054945055,0.75,0.4
question-answering,2,"In our case of simple QA , this module only returns a single supporting fact .",system description,Output module,0,139,61,3,0,system description : Output module,0.5091575091575091,0.7625,0.6
question-answering,2,"To avoid scoring all the stored facts , we first perform an approximate entity linking step to generate a small set of candidate facts .",system description,Output module,0,140,62,4,0,system description : Output module,0.5128205128205128,0.775,0.8
question-answering,2,The supporting fact is the candidate fact that is most similar to the question according to an embedding model .,system description,Output module,0,141,63,5,0,system description : Output module,0.5164835164835165,0.7875,1.0
question-answering,2,Candidate generation,system description,Candidate generation,0,142,64,1,0,system description : Candidate generation,0.5201465201465202,0.8,0.07142857142857142
question-answering,2,"To generate candidate facts , we match n-grams of words of the question to aliases of Freebase entities and select a few matching entities .",system description,Candidate generation,0,143,65,2,0,system description : Candidate generation,0.5238095238095238,0.8125,0.14285714285714285
question-answering,2,All facts having one of these entities as subject are scored in a second step .,system description,Candidate generation,0,144,66,3,0,system description : Candidate generation,0.5274725274725275,0.825,0.21428571428571427
question-answering,2,"We first generate all possible n-grams from the question , removing those that contain an interrogative pronoun or 1 - grams that belong to a list of stopwords .",system description,Candidate generation,0,145,67,4,0,system description : Candidate generation,0.5311355311355311,0.8375,0.2857142857142857
question-answering,2,"We only keep the n-grams which are an alias of an entity , and then discard all n-grams thatare a subsequence of another n-gram , except if the longer n-gram only differs by in , of , for or the at the beginning .",system description,Candidate generation,0,146,68,5,0,system description : Candidate generation,0.5347985347985348,0.85,0.35714285714285715
question-answering,2,We finally keep the two entities with the most links in Freebase retrieved for each of the five longest matched n-grams .,system description,Candidate generation,0,147,69,6,0,system description : Candidate generation,0.5384615384615384,0.8625,0.42857142857142855
question-answering,2,Scoring Scoring is performed using an embedding model .,system description,Candidate generation,0,148,70,7,0,system description : Candidate generation,0.5421245421245421,0.875,0.5
question-answering,2,"Given two embedding matrices WV ? R dN V and W S ? R dN S , which respectively contain , in columns , the d-dimensional embeddings of the words / n - grams of the vocabulary and the embeddings of the Freebase entities and relationships , the similarity between question q and a Freebase candidate fact y is computed as :",system description,Candidate generation,0,149,71,8,0,system description : Candidate generation,0.5457875457875457,0.8875,0.5714285714285714
question-answering,2,"Given two embedding matrices WV ? R dN V and W S ? R dN S , which respectively contain , in columns , the d-dimensional embeddings of the words / n - grams of the vocabulary and the embeddings of the Freebase entities and relationships , the similarity between question q and a Freebase candidate fact y is computed as :",system description,Candidate generation,0,150,72,9,0,system description : Candidate generation,0.5494505494505495,0.9,0.6428571428571429
question-answering,2,"Given two embedding matrices WV ? R dN V and W S ? R dN S , which respectively contain , in columns , the d-dimensional embeddings of the words / n - grams of the vocabulary and the embeddings of the Freebase entities and relationships , the similarity between question q and a Freebase candidate fact y is computed as :",system description,Candidate generation,0,151,73,10,0,system description : Candidate generation,0.5531135531135531,0.9125,0.7142857142857143
question-answering,2,with cos ( ) the cosine similarity .,system description,Candidate generation,0,152,74,11,0,system description : Candidate generation,0.5567765567765568,0.925,0.7857142857142857
question-answering,2,"When scoring a fact y from Reverb , we use the same embeddings and build the matrix WV S ? R d( N V +N S ) , which contains the concatenation in columns of WV and W S , and also compute the cosine similarity :",system description,Candidate generation,0,153,75,12,0,system description : Candidate generation,0.5604395604395604,0.9375,0.8571428571428571
question-answering,2,"RV B ( q , y ) = cos ( W V g ( q ) , WV S h ( y ) ) .",system description,Candidate generation,0,154,76,13,0,system description : Candidate generation,0.5641025641025641,0.95,0.9285714285714286
question-answering,2,"The dimension dis a hyperparameter , and the embedding matrices WV and W S are the parameters learned with the training algorithm of Section 4 .",system description,Candidate generation,0,155,77,14,0,system description : Candidate generation,0.5677655677655677,0.9625,1.0
question-answering,2,Response module,system description,Response module,0,156,78,1,0,system description : Response module,0.5714285714285714,0.975,0.3333333333333333
question-answering,2,"In Memory Networks , the Response module postprocesses the result of the Output module to compute the intended answer .",system description,Response module,0,157,79,2,0,system description : Response module,0.575091575091575,0.9875,0.6666666666666666
question-answering,2,"In our case , it returns the set of objects of the selected supporting fact .",system description,Response module,0,158,80,3,0,system description : Response module,0.5787545787545788,1.0,1.0
question-answering,2,Training,training,Training,0,159,1,1,0,training : Training,0.5824175824175825,0.021739130434782608,0.1111111111111111
question-answering,2,This section details how we trained the scoring function of the Output module using a multitask training process on four different sources of data .,training,Training,0,160,2,2,0,training : Training,0.5860805860805861,0.043478260869565216,0.2222222222222222
question-answering,2,"First , in addition to the new SimpleQuestions dataset described in Section 2 , we also used We-bQuestions , a benchmark for QA introduced in : questions are labeled with answer strings from aliases of Freebase entities , and many questions expect multiple answers .",training,Training,0,161,3,3,0,training : Training,0.5897435897435898,0.06521739130434782,0.3333333333333333
question-answering,2,Table 3 details the statistics of both datasets .,training,Training,0,162,4,4,0,training : Training,0.5934065934065934,0.08695652173913043,0.4444444444444444
question-answering,2,"We also train on automatic questions generated from the KB , that is FB2M or FB5M depending on the setting , which are essential to learn embeddings for the entities not appearing in either WebQuestions or SimpleQuestions .",training,Training,0,163,5,5,0,training : Training,0.5970695970695971,0.10869565217391304,0.5555555555555556
question-answering,2,Statistics of FB2M or FB5M are given in ; we generated one training question per fact following the same process as that used in .,training,Training,0,164,6,6,0,training : Training,0.6007326007326007,0.13043478260869565,0.6666666666666666
question-answering,2,"Following previous work such as , we also use the indirect supervision signal of pairs of question paraphrases .",training,Training,0,165,7,7,0,training : Training,0.6043956043956044,0.15217391304347827,0.7777777777777778
question-answering,2,We used a subset of the large set of paraphrases extracted from WIKIANSWERS and introduced in .,training,Training,0,166,8,8,0,training : Training,0.608058608058608,0.17391304347826086,0.8888888888888888
question-answering,2,Our Paraphrases dataset is made of 15M clusters containing 2 or more paraphrases each .,training,Training,0,167,9,9,0,training : Training,0.6117216117216118,0.1956521739130435,1.0
question-answering,2,Multitask training,training,Multitask training,0,168,10,1,0,training : Multitask training,0.6153846153846154,0.21739130434782608,0.07142857142857142
question-answering,2,"As in previous work on embedding models and Memory Networks , the embeddings are trained with a ranking criterion .",training,Multitask training,0,169,11,2,0,training : Multitask training,0.6190476190476191,0.2391304347826087,0.14285714285714285
question-answering,2,"For QA datasets the goal is that in the embedding space , a supporting fact is more similar to the question than any other non-supporting fact .",training,Multitask training,0,170,12,3,0,training : Multitask training,0.6227106227106227,0.2608695652173913,0.21428571428571427
question-answering,2,"For the paraphrase dataset , a question should be more similar to one of its paraphrases than to any another question .",training,Multitask training,0,171,13,4,0,training : Multitask training,0.6263736263736264,0.2826086956521739,0.2857142857142857
question-answering,2,The multitask learning of the embedding matrices WV and W S is performed by alternating stochastic gradient descent ( SGD ) steps over the loss function on the different datasets .,training,Multitask training,0,172,14,5,0,training : Multitask training,0.63003663003663,0.30434782608695654,0.35714285714285715
question-answering,2,"For the QA datasets , given a question / supporting fact pair ( q , y) and a non-supporting fact y ? , we perform a step to minimize the loss function ? QA ( q , y , y ? ) = ? ? S QA ( q , y ) + S QA ( q , y ? ) + , where [. ] + is the positive part and ? is a margin hyperparameter .",training,Multitask training,0,173,15,6,0,training : Multitask training,0.6336996336996337,0.32608695652173914,0.42857142857142855
question-answering,2,"For the QA datasets , given a question / supporting fact pair ( q , y) and a non-supporting fact y ? , we perform a step to minimize the loss function ? QA ( q , y , y ? ) = ? ? S QA ( q , y ) + S QA ( q , y ? ) + , where [. ] + is the positive part and ? is a margin hyperparameter .",training,Multitask training,0,174,16,7,0,training : Multitask training,0.6373626373626373,0.34782608695652173,0.5
question-answering,2,"For the paraphrase dataset , the similarity score between two questions q and q ? is also the cosine between their embeddings , i.e. S QQ ( q , q ? ) = cos ( W V g ( q ) , WV g ( q ? ) ) , and given a paraphrase pair ( q , q ? ) and another question q ?? , the loss is :",training,Multitask training,0,175,17,8,0,training : Multitask training,0.6410256410256411,0.3695652173913043,0.5714285714285714
question-answering,2,"For the paraphrase dataset , the similarity score between two questions q and q ? is also the cosine between their embeddings , i.e. S QQ ( q , q ? ) = cos ( W V g ( q ) , WV g ( q ? ) ) , and given a paraphrase pair ( q , q ? ) and another question q ?? , the loss is :",training,Multitask training,0,176,18,9,0,training : Multitask training,0.6446886446886447,0.391304347826087,0.6428571428571429
question-answering,2,"For the paraphrase dataset , the similarity score between two questions q and q ? is also the cosine between their embeddings , i.e. S QQ ( q , q ? ) = cos ( W V g ( q ) , WV g ( q ? ) ) , and given a paraphrase pair ( q , q ? ) and another question q ?? , the loss is :",training,Multitask training,0,177,19,10,0,training : Multitask training,0.6483516483516484,0.41304347826086957,0.7142857142857143
question-answering,2,The embeddings ( i.e. the columns of WV and W S ) are projected onto the L 2 unit ball after each update .,training,Multitask training,0,178,20,11,0,training : Multitask training,0.652014652014652,0.43478260869565216,0.7857142857142857
question-answering,2,"At each time step , a sample from the paraphrase dataset is drawn with probability 0.2 ( this probability is arbitrary ) .",training,Multitask training,0,179,21,12,0,training : Multitask training,0.6556776556776557,0.45652173913043476,0.8571428571428571
question-answering,2,"Otherwise , a sample from one of the three QA datasets , chosen uniformly at random , is taken .",training,Multitask training,0,180,22,13,0,training : Multitask training,0.6593406593406593,0.4782608695652174,0.9285714285714286
question-answering,2,We use the WARP loss,training,Multitask training,0,181,23,14,0,training : Multitask training,0.663003663003663,0.5,1.0
question-answering,2,Distant supervision,training,Distant supervision,0,182,24,1,0,training : Distant supervision,0.6666666666666666,0.5217391304347826,0.09090909090909091
question-answering,2,"Unlike for SimpleQuestions or the synthetic QA data generated from Freebase , for WebQuestions only answer strings are provided for questions : the supporting facts are unknown .",training,Distant supervision,0,183,25,2,0,training : Distant supervision,0.6703296703296703,0.5434782608695652,0.18181818181818182
question-answering,2,"In order to generate the supervision , we use the candidate fact generation algorithm of Section 3.3 .",training,Distant supervision,0,184,26,3,0,training : Distant supervision,0.673992673992674,0.5652173913043478,0.2727272727272727
question-answering,2,"For each candidate fact , the aliases of its objects are compared to the set of provided answer strings .",training,Distant supervision,0,185,27,4,0,training : Distant supervision,0.6776556776556777,0.5869565217391305,0.36363636363636365
question-answering,2,The fact ( s ) which can generate the maximum number of answer strings from their objects ' aliases are then kept .,training,Distant supervision,0,186,28,5,0,training : Distant supervision,0.6813186813186813,0.6086956521739131,0.45454545454545453
question-answering,2,"If multiple facts are obtained for the same question , the ones with the minimal number of objects are considered as supervision facts .",training,Distant supervision,0,187,29,6,0,training : Distant supervision,0.684981684981685,0.6304347826086957,0.5454545454545454
question-answering,2,This last selection avoids favoring irrelevant relationships that would be kept only because they point to many objects but would not be specific enough .,training,Distant supervision,0,188,30,7,0,training : Distant supervision,0.6886446886446886,0.6521739130434783,0.6363636363636364
question-answering,2,"If no answer string could be found from the objects of the initial candidates , the question is discarded from the training set .",training,Distant supervision,0,189,31,8,0,training : Distant supervision,0.6923076923076923,0.6739130434782609,0.7272727272727273
question-answering,2,Future work should investigate the process of weak supervised training of MemNNs recently introduced in that allows to train them without any supervision coming from the supporting facts . :,training,Distant supervision,0,190,32,9,0,training : Distant supervision,0.6959706959706959,0.6956521739130435,0.8181818181818182
question-answering,2,Training and evaluation datasets .,training,Distant supervision,0,191,33,10,0,training : Distant supervision,0.6996336996336996,0.717391304347826,0.9090909090909091
question-answering,2,Questions automatically generated from the KB and paraphrases can also be used in training .,training,Distant supervision,0,192,34,11,0,training : Distant supervision,0.7032967032967034,0.7391304347826086,1.0
question-answering,2,WebQuestions SimpleQuestions,training,WebQuestions SimpleQuestions Reverb,0,193,35,1,0,training : WebQuestions SimpleQuestions Reverb,0.706959706959707,0.7608695652173914,0.5
question-answering,2,Reverb,training,WebQuestions SimpleQuestions Reverb,0,194,36,2,0,training : WebQuestions SimpleQuestions Reverb,0.7106227106227107,0.782608695652174,1.0
question-answering,2,Generating negative examples,training,Generating negative examples,0,195,37,1,0,training : Generating negative examples,0.7142857142857143,0.8043478260869565,0.1
question-answering,2,"As in , learning is performed with gradient descent , so that negative examples ( non - supporting facts or non-paraphrases ) are generated according to a randomized policy during training .",training,Generating negative examples,0,196,38,2,0,training : Generating negative examples,0.717948717948718,0.8260869565217391,0.2
question-answering,2,"For paraphrases , given a pair ( q , q ? ) , a nonparaphrase pair is generated as ( q , q ?? )",training,Generating negative examples,0,197,39,3,0,training : Generating negative examples,0.7216117216117216,0.8478260869565217,0.3
question-answering,2,"where q ?? is a random question of the dataset , not belonging to the cluser of q .",training,Generating negative examples,0,198,40,4,0,training : Generating negative examples,0.7252747252747253,0.8695652173913043,0.4
question-answering,2,"where q ?? is a random question of the dataset , not belonging to the cluser of q .",training,Generating negative examples,0,199,41,5,0,training : Generating negative examples,0.7289377289377289,0.8913043478260869,0.5
question-answering,2,"For question / supporting fact pairs , we use two policies .",training,Generating negative examples,0,200,42,6,0,training : Generating negative examples,0.7326007326007326,0.9130434782608695,0.6
question-answering,2,"The default policy to obtain a non-supporting fact is to corrupt the answer fact by exchanging it s subject , its relationship or its object ( s ) with that of another fact chosen uniformly at random from the KB .",training,Generating negative examples,0,201,43,7,0,training : Generating negative examples,0.7362637362637363,0.9347826086956522,0.7
question-answering,2,"In this policy , the element of the fact to corrupt is chosen randomly , with a small probability ( 0.3 ) of corrupting more than one element of the answer fact .",training,Generating negative examples,0,202,44,8,0,training : Generating negative examples,0.73992673992674,0.9565217391304348,0.8
question-answering,2,"The second policy we propose , called candidates as negatives , is to take as non-supporting fact a randomly chosen fact from the set of candidate facts .",training,Generating negative examples,0,203,45,9,0,training : Generating negative examples,0.7435897435897436,0.9782608695652174,0.9
question-answering,2,"While the first policy is standard in learning embeddings , the second one is more original , and , as we see in the experiments , gives slightly better performance .",training,Generating negative examples,0,204,46,10,0,training : Generating negative examples,0.7472527472527473,1.0,1.0
question-answering,2,Related Work,related work,Related Work,0,205,1,1,0,related work : Related Work,0.7509157509157509,0.1,0.1
question-answering,2,"The first approaches to open - domain QA were search engine - based systems , where keywords extracted from the question are sent to a search engine , and the answer is extracted from the top results .",related work,Related Work,0,206,2,2,0,related work : Related Work,0.7545787545787546,0.2,0.2
question-answering,2,"This method has been adapted to KB - based QA , and obtained competitive results with respect to semantic parsing and embedding - based approaches .",related work,Related Work,0,207,3,3,0,related work : Related Work,0.7582417582417582,0.3,0.3
question-answering,2,Semantic parsing approaches perform a functional parse of the sentence that can be interpreted as a KB query .,related work,Related Work,0,208,4,4,0,related work : Related Work,0.7619047619047619,0.4,0.4
question-answering,2,"Even though these approaches are difficult to train at scale because of the complexity of their inference , their advantage is to provide a deep interpretation of the question .",related work,Related Work,0,209,5,5,0,related work : Related Work,0.7655677655677655,0.5,0.5
question-answering,2,"Some of these approaches require little to no question - answer pairs , relying on simple rules to tranform the semantic interpretation to a KB query .",related work,Related Work,0,210,6,6,0,related work : Related Work,0.7692307692307693,0.6,0.6
question-answering,2,"Like our work , embedding - based methods for QA can be seen as simple MemNNs .",related work,Related Work,0,211,7,7,0,related work : Related Work,0.7728937728937729,0.7,0.7
question-answering,2,"The algorithms of use an approach similar to ours but are based on Reverb rather than Freebase , and relied purely on bag - of - word for both questions and facts .",related work,Related Work,0,212,8,8,0,related work : Related Work,0.7765567765567766,0.8,0.8
question-answering,2,"The approach of ( Yang et al. , 2014 ) uses a different representation of questions , in which recognized entities are replaced by an entity token , and a different training data using entity mentions from WIKIPEDIA .",related work,Related Work,0,213,9,9,0,related work : Related Work,0.7802197802197802,0.9,0.9
question-answering,2,"Our model is closest to the one presented in , which is discussed in more details in the experiments .",related work,Related Work,0,214,10,10,0,related work : Related Work,0.7838827838827839,1.0,1.0
question-answering,2,Experiments,experiment,Experiments,0,215,1,1,0,experiment : Experiments,0.7875457875457875,0.3333333333333333,0.3333333333333333
question-answering,2,This section provides an extensive evaluation of our MemNNs implementation against state - of the - art QA methods as well as an empirical study of the impact of using multiple training sources on the prediction performance .,experiment,Experiments,0,216,2,2,0,experiment : Experiments,0.7912087912087912,0.6666666666666666,0.6666666666666666
question-answering,2,"details the dimensions of the test sets of WebQuestions , SimpleQuestions and Reverb which we used for evaluation .",experiment,Experiments,0,217,3,3,0,experiment : Experiments,0.7948717948717948,1.0,1.0
question-answering,2,Evaluation and baselines,evaluation,Evaluation and baselines,0,218,1,1,0,evaluation : Evaluation and baselines,0.7985347985347986,0.125,0.125
question-answering,2,"On WebQuestions , we evaluate against previous results on this benchmark in terms of F1 - score as defined in , which is the average , over all test questions , of the F1 - score of the sets of predicted answers .",evaluation,Evaluation and baselines,0,219,2,2,0,evaluation : Evaluation and baselines,0.8021978021978022,0.25,0.25
question-answering,2,"Since no previous result was published on SimpleQuestions , we only compare different versions of MemNNs .",evaluation,Evaluation and baselines,0,220,3,3,0,evaluation : Evaluation and baselines,0.8058608058608059,0.375,0.375
question-answering,2,"SimpleQuestions questions are labeled with their entire Freebase fact , so we evaluate in terms of path - level accuracy , in which a prediction is correct if the subject and the relationship were correctly retrieved by the system .",evaluation,Evaluation and baselines,0,221,4,4,0,evaluation : Evaluation and baselines,0.8095238095238095,0.5,0.5
question-answering,2,"The Reverb test set , based on the KB of the same name and introduced in is used for evaluation only .",evaluation,Evaluation and baselines,0,222,5,5,0,evaluation : Evaluation and baselines,0.8131868131868132,0.625,0.625
question-answering,2,It contains 691 questions .,evaluation,Evaluation and baselines,0,223,6,6,0,evaluation : Evaluation and baselines,0.8168498168498168,0.75,0.75
question-answering,2,"We consider the task of re-ranking a small set of candidate answers , which are Reverb facts and are labeled as corrector incorrect .",evaluation,Evaluation and baselines,0,224,7,7,0,evaluation : Evaluation and baselines,0.8205128205128205,0.875,0.875
question-answering,2,"We compare our approach to the original system , to and to the original MemNNs , in terms of accuracy , which is the percentage of questions for which the top - ranked candidate fact is correct .",evaluation,Evaluation and baselines,0,225,8,8,0,evaluation : Evaluation and baselines,0.8241758241758241,1.0,1.0
question-answering,2,Experimental setup,experiment,Experimental setup,0,226,1,1,0,experiment : Experimental setup,0.8278388278388278,0.07692307692307693,0.07692307692307693
question-answering,2,All models were trained with at least the dataset made of synthetic questions created from the KB .,experiment,Experimental setup,0,227,2,2,0,experiment : Experimental setup,0.8315018315018315,0.15384615384615385,0.15384615384615385
question-answering,2,"The hyperparameters were chosen to maximize the F1-score on WebQuestions validation set , independently of the testing dataset .",experiment,Experimental setup,0,228,3,3,0,experiment : Experimental setup,0.8351648351648352,0.23076923076923078,0.23076923076923078
question-answering,2,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ? was set to 0.1 .",experiment,Experimental setup,1,229,4,4,0,experiment : Experimental setup,0.8388278388278388,0.3076923076923077,0.3076923076923077
question-answering,2,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ? was set to 0.1 .",experiment,Experimental setup,1,230,5,5,0,experiment : Experimental setup,0.8424908424908425,0.38461538461538464,0.38461538461538464
question-answering,2,"For each configuration of hyperparameters , the F1score on the validation set was computed regularly during learning to perform early stopping .",experiment,Experimental setup,0,231,6,6,0,experiment : Experimental setup,0.8461538461538461,0.46153846153846156,0.46153846153846156
question-answering,2,We tested additional configurations for our algorithm .,experiment,Experimental setup,0,232,7,7,0,experiment : Experimental setup,0.8498168498168498,0.5384615384615384,0.5384615384615384
question-answering,2,"First , in the Candidates as Negatives setting ( negative facts are sampled from the candidate set , see Section 4 ) , abbreviated CANDS AS NEGS , the experimental protocol is the same as in the default setting but the embeddings are initialized with the best configuration of the default setup .",experiment,Experimental setup,0,233,8,8,0,experiment : Experimental setup,0.8534798534798534,0.6153846153846154,0.6153846153846154
question-answering,2,"Second , our model shares some similarities with an approach studied in , in which the authors noticed important gains using a subgraph representation of answers .",experiment,Experimental setup,0,234,9,9,0,experiment : Experimental setup,0.8571428571428571,0.6923076923076923,0.6923076923076923
question-answering,2,"For completeness , we also added such a subgraph representation of objects .",experiment,Experimental setup,0,235,10,10,0,experiment : Experimental setup,0.8608058608058609,0.7692307692307693,0.7692307692307693
question-answering,2,"In that setting , called Subgraph , each object o of a fact is itself represented as a bag - of - entities that encodes the immediate neighborhood of o .",experiment,Experimental setup,0,236,11,11,0,experiment : Experimental setup,0.8644688644688645,0.8461538461538461,0.8461538461538461
question-answering,2,This Subgraph model is trained similarly as our main approach and only the results of a post -hoc ensemble combination of the two models ( where the scores are added ) are presented .,experiment,Experimental setup,0,237,12,12,0,experiment : Experimental setup,0.8681318681318682,0.9230769230769231,0.9230769230769231
question-answering,2,We also report the results obtained by an ensemble of the 5 best models on validation ( subgraph excepted ) ; this is denoted 5 models .,experiment,Experimental setup,0,238,13,13,0,experiment : Experimental setup,0.8717948717948718,1.0,1.0
question-answering,2,Results,result,Results,0,239,1,1,0,result : Results,0.8754578754578755,0.03333333333333333,1.0
question-answering,2,Comparative results,result,Comparative results,0,240,2,1,0,result : Comparative results,0.8791208791208791,0.06666666666666667,0.25
question-answering,2,The results of the comparative experiments are given in .,result,Comparative results,0,241,3,2,0,result : Comparative results,0.8827838827838828,0.1,0.5
question-answering,2,"On the main benchmark WebQuestions , our best results use all data sources , the bigger extract from Freebase and the CANDS AS NEGS setting .",result,Comparative results,1,242,4,3,0,result : Comparative results,0.8864468864468864,0.13333333333333333,0.75
question-answering,2,"The two ensembles achieve excellent results , with F1 -",result,Comparative results,0,243,5,4,0,result : Comparative results,0.8901098901098901,0.16666666666666666,1.0
question-answering,2,WebQuestions SimpleQuestions,result,WebQuestions SimpleQuestions,0,244,6,1,0,result : WebQuestions SimpleQuestions,0.8937728937728938,0.2,0.08333333333333333
question-answering,2,Reverb F1-SCORE ( % ) ACCURACY ( % ) ACCURACY ( % ) BASELINES Random guess 1.9 4.9 35 31.3 n / a n / a n / a n / a 54 29.7 n / a 73 ) - using path 35.3 n/ a n / a ) - using path + subgraph 39.2 n / a n / a 39.9 n/ a n/ a 41.3 n/ a n / a of SimpleQuestions questions .,result,WebQuestions SimpleQuestions,0,245,7,2,0,result : WebQuestions SimpleQuestions,0.8974358974358975,0.23333333333333334,0.16666666666666666
question-answering,2,"This shows that MemNNs are effective at re-ranking the candidates , but also that simple QA is still not solved .",result,WebQuestions SimpleQuestions,0,246,8,3,0,result : WebQuestions SimpleQuestions,0.9010989010989011,0.26666666666666666,0.25
question-answering,2,Our approach bares similarity to ) - using path .,result,WebQuestions SimpleQuestions,0,247,9,4,0,result : WebQuestions SimpleQuestions,0.9047619047619048,0.3,0.3333333333333333
question-answering,2,"They use FB2M , and so their result ( 35.3 % F1 - score on WebQuestions ) should be compared to our 36.2 % .",result,WebQuestions SimpleQuestions,0,248,10,5,0,result : WebQuestions SimpleQuestions,0.9084249084249084,0.3333333333333333,0.4166666666666667
question-answering,2,"The models are slightly different in that they replace the entity string with the subject entity in the question representation and that we use the cosine similarity instead of the dot product , which gave consistent improvements .",result,WebQuestions SimpleQuestions,0,249,11,6,0,result : WebQuestions SimpleQuestions,0.9120879120879121,0.36666666666666664,0.5
question-answering,2,"Still , the major differences come from how we use Freebase .",result,WebQuestions SimpleQuestions,0,250,12,7,0,result : WebQuestions SimpleQuestions,0.9157509157509157,0.4,0.5833333333333334
question-answering,2,"First , the removal of the mediator nodes allows us to restrict ourselves to single supporting facts , while they search in paths of length 2 with a heuristic to select the paths to follow ( otherwise , inference is too costly ) , which makes our inference simpler and more efficient .",result,WebQuestions SimpleQuestions,0,251,13,8,0,result : WebQuestions SimpleQuestions,0.9194139194139194,0.43333333333333335,0.6666666666666666
question-answering,2,"Second , using grouped facts , we integrate multiple answers during learning ( through the distant supervision ) , while they use a grouping heuristic at test time .",result,WebQuestions SimpleQuestions,0,252,14,9,0,result : WebQuestions SimpleQuestions,0.9230769230769231,0.4666666666666667,0.75
question-answering,2,Grouping facts also allows us to scale much better and to train on FB5M .,result,WebQuestions SimpleQuestions,0,253,15,10,0,result : WebQuestions SimpleQuestions,0.9267399267399268,0.5,0.8333333333333334
question-answering,2,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .",result,WebQuestions SimpleQuestions,0,254,16,11,0,result : WebQuestions SimpleQuestions,0.9304029304029304,0.5333333333333333,0.9166666666666666
question-answering,2,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .",result,WebQuestions SimpleQuestions,0,255,17,12,0,result : WebQuestions SimpleQuestions,0.9340659340659341,0.5666666666666667,1.0
question-answering,2,Transfer learning on Reverb,result,Transfer learning on Reverb,1,256,18,1,0,result : Transfer learning on Reverb,0.9377289377289377,0.6,0.125
question-answering,2,"In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .",result,Transfer learning on Reverb,1,257,19,2,0,result : Transfer learning on Reverb,0.9413919413919414,0.6333333333333333,0.25
question-answering,2,"Thus , ( last column ) presents the result of our model without training on Reverb against methods specifically developed on that dataset .",result,Transfer learning on Reverb,0,258,20,3,0,result : Transfer learning on Reverb,0.945054945054945,0.6666666666666666,0.375
question-answering,2,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .",result,Transfer learning on Reverb,1,259,21,4,0,result : Transfer learning on Reverb,0.9487179487179487,0.7,0.5
question-answering,2,These results show that the Memory Network approach can integrate and use new entities and links .,result,Transfer learning on Reverb,0,260,22,5,0,result : Transfer learning on Reverb,0.9523809523809523,0.7333333333333333,0.625
question-answering,2,presents the results on the three datasets when our model is trained with different data sources .,result,Transfer learning on Reverb,0,261,23,6,0,result : Transfer learning on Reverb,0.9560439560439561,0.7666666666666667,0.75
question-answering,2,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",result,Transfer learning on Reverb,1,262,24,7,0,result : Transfer learning on Reverb,0.9597069597069597,0.8,0.875
question-answering,2,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .",result,Transfer learning on Reverb,1,263,25,8,0,result : Transfer learning on Reverb,0.9633699633699634,0.8333333333333334,1.0
question-answering,2,Importance of data sources,result,Importance of data sources The bottom half of,0,264,26,1,0,result : Importance of data sources The bottom half of,0.967032967032967,0.8666666666666667,0.2
question-answering,2,The bottom half of,result,Importance of data sources The bottom half of,0,265,27,2,0,result : Importance of data sources The bottom half of,0.9706959706959707,0.9,0.4
question-answering,2,"While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .",result,Importance of data sources The bottom half of,0,266,28,3,0,result : Importance of data sources The bottom half of,0.9743589743589743,0.9333333333333333,0.6
question-answering,2,"This is because WebQuestions and SimpleQuestions questions follow simple patterns and are well formed , while Reverb questions have more syntactic and lexical variability .",result,Importance of data sources The bottom half of,0,267,29,4,0,result : Importance of data sources The bottom half of,0.978021978021978,0.9666666666666667,0.8
question-answering,2,"Thus , paraphrases are important to avoid overfitting on specific question patterns of the training sets .",result,Importance of data sources The bottom half of,0,268,30,5,0,result : Importance of data sources The bottom half of,0.9816849816849816,1.0,1.0
question-answering,2,Conclusion,conclusion,Conclusion,0,269,1,1,0,conclusion : Conclusion,0.9853479853479854,0.2,0.2
question-answering,2,This paper presents an implementation of MemNNs for the task of large - scale simple QA .,conclusion,Conclusion,0,270,2,2,0,conclusion : Conclusion,0.989010989010989,0.4,0.4
question-answering,2,"Our results demonstrate that , if properly trained , MemNNs are able to handle natural language and a very large memory ( millions of entries ) , and hence can reach state - of - the - art on the popular benchmark WebQuestions .",conclusion,Conclusion,0,271,3,3,0,conclusion : Conclusion,0.9926739926739927,0.6,0.6
question-answering,2,"We want to emphasize that many of our findings , especially those regarding how to format the KB , do not only concern MemNNs but potentially any QA system .",conclusion,Conclusion,0,272,4,4,0,conclusion : Conclusion,0.9963369963369964,0.8,0.8
question-answering,2,"This paper also introduced the new dataset SimpleQuestions , which , with 100 k examples , is one order of magnitude bigger than WebQuestions : we hope that it will foster interesting new research in QA , simple or not .",conclusion,Conclusion,0,273,5,5,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,3,Sentence Similarity Learning by Lexical Decomposition and Composition,title,title,1,2,1,1,0,title : title,0.007874015748031496,1.0,1.0
question-answering,3,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011811023622047244,0.0196078431372549,0.0196078431372549
question-answering,3,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.015748031496062992,0.0392156862745098,0.0392156862745098
question-answering,3,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01968503937007874,0.058823529411764705,0.058823529411764705
question-answering,3,"The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.023622047244094488,0.0784313725490196,0.0784313725490196
question-answering,3,"Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.027559055118110236,0.09803921568627451,0.09803921568627451
question-answering,3,"After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.031496062992125984,0.11764705882352941,0.11764705882352941
question-answering,3,"Finally , a similarity score is estimated over the composed feature vectors .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.03543307086614173,0.13725490196078433,0.13725490196078433
question-answering,3,"Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.03937007874015748,0.1568627450980392,0.1568627450980392
question-answering,3,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.04330708661417323,0.17647058823529413,0.17647058823529413
question-answering,3,It plays an important role for a variety of tasks in both NLP and IR communities .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.047244094488188976,0.19607843137254902,0.19607843137254902
question-answering,3,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,0,13,11,11,0,abstract : abstract,0.051181102362204724,0.21568627450980393,0.21568627450980393
question-answering,3,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",abstract,abstract,0,14,12,12,0,abstract : abstract,0.05511811023622047,0.23529411764705882,0.23529411764705882
question-answering,3,"However , sentence similarity learning has following challenges :",abstract,abstract,0,15,13,13,0,abstract : abstract,0.05905511811023622,0.2549019607843137,0.2549019607843137
question-answering,3,1 .,abstract,abstract,0,16,14,14,0,abstract : abstract,0.06299212598425197,0.27450980392156865,0.27450980392156865
question-answering,3,There is a lexical gap between semantically equivalent sentences .,abstract,abstract,0,17,15,15,0,abstract : abstract,0.06692913385826772,0.29411764705882354,0.29411764705882354
question-answering,3,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",abstract,abstract,0,18,16,16,0,abstract : abstract,0.07086614173228346,0.3137254901960784,0.3137254901960784
question-answering,3,". Semantic similarity should be measured at different levels of granularity ( word - level , phrase - level and syntax - level ) .",abstract,abstract,0,19,17,17,0,abstract : abstract,0.07480314960629922,0.3333333333333333,0.3333333333333333
question-answering,3,", "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",abstract,abstract,0,20,18,18,0,abstract : abstract,0.07874015748031496,0.35294117647058826,0.35294117647058826
question-answering,3,3 .,abstract,abstract,0,21,19,19,0,abstract : abstract,0.08267716535433071,0.37254901960784315,0.37254901960784315
question-answering,3,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",abstract,abstract,0,22,20,20,0,abstract : abstract,0.08661417322834646,0.39215686274509803,0.39215686274509803
question-answering,3,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",abstract,abstract,0,23,21,21,0,abstract : abstract,0.09055118110236221,0.4117647058823529,0.4117647058823529
question-answering,3,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",abstract,abstract,0,24,22,22,0,abstract : abstract,0.09448818897637795,0.43137254901960786,0.43137254901960786
question-answering,3,How we can extract and utilize those information becomes another challenge .,abstract,abstract,0,25,23,23,0,abstract : abstract,0.0984251968503937,0.45098039215686275,0.45098039215686275
question-answering,3,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms for a longtime .",abstract,abstract,0,26,24,24,0,abstract : abstract,0.10236220472440945,0.47058823529411764,0.47058823529411764
question-answering,3,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",abstract,abstract,0,27,25,25,0,abstract : abstract,0.1062992125984252,0.49019607843137253,0.49019607843137253
question-answering,3,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,0,28,26,26,0,abstract : abstract,0.11023622047244094,0.5098039215686274,0.5098039215686274
question-answering,3,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",abstract,abstract,0,29,27,27,0,abstract : abstract,0.1141732283464567,0.5294117647058824,0.5294117647058824
question-answering,3,The third challenge did not get much,abstract,abstract,0,30,28,28,0,abstract : abstract,0.11811023622047244,0.5490196078431373,0.5490196078431373
question-answering,3,narrative,abstract,abstract,0,31,29,29,0,abstract : abstract,0.1220472440944882,0.5686274509803921,0.5686274509803921
question-answering,3,E1,abstract,abstract,0,32,30,30,0,abstract : abstract,0.12598425196850394,0.5882352941176471,0.5882352941176471
question-answering,3,The research is to sockeye .,abstract,abstract,0,33,31,31,0,abstract : abstract,0.12992125984251968,0.6078431372549019,0.6078431372549019
question-answering,3,E2,abstract,abstract,0,34,32,32,0,abstract : abstract,0.13385826771653545,0.6274509803921569,0.6274509803921569
question-answering,3,The study is [ not related ] to salmon .,abstract,abstract,0,35,33,33,0,abstract : abstract,0.1377952755905512,0.6470588235294118,0.6470588235294118
question-answering,3,E3,abstract,abstract,0,36,34,34,0,abstract : abstract,0.14173228346456693,0.6666666666666666,0.6666666666666666
question-answering,3,The research is relevant to salmon .,abstract,abstract,0,37,35,35,0,abstract : abstract,0.14566929133858267,0.6862745098039216,0.6862745098039216
question-answering,3,E4,abstract,abstract,0,38,36,36,0,abstract : abstract,0.14960629921259844,0.7058823529411765,0.7058823529411765
question-answering,3,"The study is relevant to sockeye , instead of coho .",abstract,abstract,0,39,37,37,0,abstract : abstract,0.15354330708661418,0.7254901960784313,0.7254901960784313
question-answering,3,E5,abstract,abstract,0,40,38,38,0,abstract : abstract,0.15748031496062992,0.7450980392156863,0.7450980392156863
question-answering,3,"The study is relevant to sockeye , rather than flounder .:",abstract,abstract,0,41,39,39,0,abstract : abstract,0.16141732283464566,0.7647058823529411,0.7647058823529411
question-answering,3,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",abstract,abstract,0,42,40,40,0,abstract : abstract,0.16535433070866143,0.7843137254901961,0.7843137254901961
question-answering,3,"coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",abstract,abstract,0,43,41,41,0,abstract : abstract,0.16929133858267717,0.803921568627451,0.803921568627451
question-answering,3,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",abstract,abstract,0,44,42,42,0,abstract : abstract,0.1732283464566929,0.8235294117647058,0.8235294117647058
question-answering,3,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",abstract,abstract,1,45,43,43,0,abstract : abstract,0.17716535433070865,0.8431372549019608,0.8431372549019608
question-answering,3,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",abstract,abstract,1,46,44,44,0,abstract : abstract,0.18110236220472442,0.8627450980392157,0.8627450980392157
question-answering,3,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",abstract,abstract,1,47,45,45,0,abstract : abstract,0.18503937007874016,0.8823529411764706,0.8823529411764706
question-answering,3,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",abstract,abstract,1,48,46,46,0,abstract : abstract,0.1889763779527559,0.9019607843137255,0.9019607843137255
question-answering,3,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",abstract,abstract,1,49,47,47,0,abstract : abstract,0.19291338582677164,0.9215686274509803,0.9215686274509803
question-answering,3,"Finally , the composed feature vector is utilized to predict the sentence similarity .",abstract,abstract,1,50,48,48,0,abstract : abstract,0.1968503937007874,0.9411764705882353,0.9411764705882353
question-answering,3,"Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,abstract,0,51,49,49,0,abstract : abstract,0.20078740157480315,0.9607843137254902,0.9607843137254902
question-answering,3,"In following parts , we start with a brief overview of our model ( Section 2 ) , followed by the details of our end - to - end implementation ( Section 3 ) .",abstract,abstract,0,52,50,50,0,abstract : abstract,0.2047244094488189,0.9803921568627451,0.9803921568627451
question-answering,3,Then we evaluate our model on answer sentence selection and paraphrase identifications tasks ( Section 4 ) .,abstract,abstract,0,53,51,51,0,abstract : abstract,0.20866141732283464,1.0,1.0
question-answering,3,Model Overview,model,Model Overview,0,54,1,1,0,model : Model Overview,0.2125984251968504,0.01,0.1
question-answering,3,"In this section , we propose a sentence similarity learning model to tackle all three challenges ( mentioned in Section 1 ) .",model,Model Overview,0,55,2,2,0,model : Model Overview,0.21653543307086615,0.02,0.2
question-answering,3,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",model,Model Overview,0,56,3,3,0,model : Model Overview,0.2204724409448819,0.03,0.3
question-answering,3,"To tackle the second challenge , we assume that each word can be semantically matched by several words in the other sentence , and we calculate a semantic matching vector for each word vector based on all the word vectors in the other side .",model,Model Overview,0,57,4,4,0,model : Model Overview,0.22440944881889763,0.04,0.4
question-answering,3,"To cope with the third challenge , we assume that each semantic unit ( word ) can be partially matched , and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector .",model,Model Overview,0,58,5,5,0,model : Model Overview,0.2283464566929134,0.05,0.5
question-answering,3,shows an overview of our sentence similarity model .,model,Model Overview,0,59,6,6,0,model : Model Overview,0.23228346456692914,0.06,0.6
question-answering,3,"Given a pair of sentences Sand T , our task is to calculate a similarity score sim ( S , T ) in following steps :",model,Model Overview,0,60,7,7,0,model : Model Overview,0.23622047244094488,0.07,0.7
question-answering,3,Word Representation .,model,Model Overview,0,61,8,8,0,model : Model Overview,0.24015748031496062,0.08,0.8
question-answering,3,"Word embedding of is an effective way to handle the lexical gap challenge in the sentence similarity task , as it represents each word with a distributed vector , and words appearing in similar contexts tend to have similar meanings .",model,Model Overview,0,62,9,9,0,model : Model Overview,0.2440944881889764,0.09,0.9
question-answering,3,"With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .",model,Model Overview,0,63,10,10,0,model : Model Overview,0.24803149606299213,0.1,1.0
question-answering,3,Semantic Matching .,model,Semantic Matching Functions,0,64,11,1,0,model : Semantic Matching Functions,0.25196850393700787,0.11,0.14285714285714285
question-answering,3,"In order to judge the similarity between two sentences , we need to check whether each semantic unit in one sentence is covered by the other sentence , or vice versa .",model,Semantic Matching Functions,0,65,12,2,0,model : Semantic Matching Functions,0.2559055118110236,0.12,0.2857142857142857
question-answering,3,"For example , in , to check whether E 2 is a paraphrase of E 1 , we need to know the single word "" irrelevant "" in E 1 is matched or covered by the phrase "" not related "" in E 2 .",model,Semantic Matching Functions,0,66,13,3,0,model : Semantic Matching Functions,0.25984251968503935,0.13,0.42857142857142855
question-answering,3,"In our model , we treat each word as a primitive semantic unit , and calculate a semantic matching vector ? i for each word s i by composing part or full word vectors in the other sentence T .",model,Semantic Matching Functions,0,67,14,4,0,model : Semantic Matching Functions,0.2637795275590551,0.14,0.5714285714285714
question-answering,3,"In our model , we treat each word as a primitive semantic unit , and calculate a semantic matching vector ? i for each word s i by composing part or full word vectors in the other sentence T .",model,Semantic Matching Functions,0,68,15,5,0,model : Semantic Matching Functions,0.2677165354330709,0.15,0.7142857142857143
question-answering,3,"In this way , we can match a word s i to a word or phrase in T . Similarly , for the reverse direction , we also calculate all semantic matching vectorst j in T .",model,Semantic Matching Functions,0,69,16,6,0,model : Semantic Matching Functions,0.27165354330708663,0.16,0.8571428571428571
question-answering,3,We explore different f match functions later in Section 3 .,model,Semantic Matching Functions,0,70,17,7,0,model : Semantic Matching Functions,0.2755905511811024,0.17,1.0
question-answering,3,Decomposition .,model,Decomposition Functions,0,71,18,1,0,model : Decomposition Functions,0.2795275590551181,0.18,0.014925373134328358
question-answering,3,"After the semantic matching phase , we have the semantic matching vectors of ? i and t j .",model,Decomposition Functions,0,72,19,2,0,model : Decomposition Functions,0.28346456692913385,0.19,0.029850746268656716
question-answering,3,"After the semantic matching phase , we have the semantic matching vectors of ? i and t j .",model,Decomposition Functions,0,73,20,3,0,model : Decomposition Functions,0.2874015748031496,0.2,0.04477611940298507
question-answering,3,We interpret ? i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,model,Decomposition Functions,0,74,21,4,0,model : Decomposition Functions,0.29133858267716534,0.21,0.05970149253731343
question-answering,3,We interpret ? i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,model,Decomposition Functions,0,75,22,5,0,model : Decomposition Functions,0.2952755905511811,0.22,0.07462686567164178
question-answering,3,"However , it is not necessary that all the semantics of s i ( or t j ) are fully covered by ? i ( ort j ) .",model,Decomposition Functions,0,76,23,6,0,model : Decomposition Functions,0.2992125984251969,0.23,0.08955223880597014
question-answering,3,"However , it is not necessary that all the semantics of s i ( or t j ) are fully covered by ? i ( ort j ) .",model,Decomposition Functions,0,77,24,7,0,model : Decomposition Functions,0.3031496062992126,0.24,0.1044776119402985
question-answering,3,"Take the E 1 and E 2 in for example , the word "" sockeye "" in E 1 is only partially matched by the word "" salmon "" ( the similar part ) in E 2 , as the full meaning of "" sockeye "" is "" red salmon "" ( the semantic meaning of "" red "" is the dissimilar part ) .",model,Decomposition Functions,0,78,25,8,0,model : Decomposition Functions,0.30708661417322836,0.25,0.11940298507462686
question-answering,3,"Motivated by this phenomenon , our model further decomposes word s i ( or t j ) , based on its semantic matching vector ? i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",model,Decomposition Functions,0,79,26,9,0,model : Decomposition Functions,0.3110236220472441,0.26,0.13432835820895522
question-answering,3,"Motivated by this phenomenon , our model further decomposes word s i ( or t j ) , based on its semantic matching vector ? i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",model,Decomposition Functions,0,80,27,10,0,model : Decomposition Functions,0.31496062992125984,0.27,0.14925373134328357
question-answering,3,"Formally , we define the decomposition function as :",model,Decomposition Functions,0,81,28,11,0,model : Decomposition Functions,0.3188976377952756,0.28,0.16417910447761194
question-answering,3,our goal in this step is how to utilize those information .,model,Decomposition Functions,0,82,29,12,0,model : Decomposition Functions,0.3228346456692913,0.29,0.1791044776119403
question-answering,3,"Besides the suggestion from that the significance of the dissimilar parts alone between two sentences has a great effect of their similarity , we also think that the dissimilar and similar components have strong connections .",model,Decomposition Functions,0,83,30,13,0,model : Decomposition Functions,0.32677165354330706,0.3,0.19402985074626866
question-answering,3,"For example , in , if we only look at the dissimilar or similar part alone , it is hard to judge which one between E 4 and E 5 is more similar to E 3 .",model,Decomposition Functions,0,84,31,14,0,model : Decomposition Functions,0.33070866141732286,0.31,0.208955223880597
question-answering,3,"We can easily identify that E 5 is more similar to E 3 , when we consider both the similar and dissimilar parts .",model,Decomposition Functions,0,85,32,15,0,model : Decomposition Functions,0.3346456692913386,0.32,0.22388059701492538
question-answering,3,"Therefore , our model composes the similar component matrix and dissimilar component matrix into a feature vector S ( or T ) with the composition function :",model,Decomposition Functions,0,86,33,16,0,model : Decomposition Functions,0.33858267716535434,0.33,0.23880597014925373
question-answering,3,Similarity assessing .,model,Decomposition Functions,0,87,34,17,0,model : Decomposition Functions,0.3425196850393701,0.34,0.2537313432835821
question-answering,3,"In the final stage , we concatenate the two feature vectors ( Sand T ) and predict the final similarity score :",model,Decomposition Functions,0,88,35,18,0,model : Decomposition Functions,0.3464566929133858,0.35,0.26865671641791045
question-answering,3,An End - to - End Implementation Section 2 gives us a glance of our model .,model,Decomposition Functions,0,89,36,19,0,model : Decomposition Functions,0.35039370078740156,0.36,0.2835820895522388
question-answering,3,"In this section , we describe details of each phase .",model,Decomposition Functions,0,90,37,20,0,model : Decomposition Functions,0.3543307086614173,0.37,0.29850746268656714
question-answering,3,Semantic Matching Functions,model,Decomposition Functions,0,91,38,21,0,model : Decomposition Functions,0.35826771653543305,0.38,0.31343283582089554
question-answering,3,This subsection describes our specifications for the semantic matching function f match in Eq.,model,Decomposition Functions,0,92,39,22,0,model : Decomposition Functions,0.36220472440944884,0.39,0.3283582089552239
question-answering,3,1 ) .,model,Decomposition Functions,0,93,40,23,0,model : Decomposition Functions,0.3661417322834646,0.4,0.34328358208955223
question-answering,3,The goal off match is to generate a semantic matching vector ? i for s i by composing the vectors from T .,model,Decomposition Functions,0,94,41,24,0,model : Decomposition Functions,0.3700787401574803,0.41,0.3582089552238806
question-answering,3,The goal off match is to generate a semantic matching vector ? i for s i by composing the vectors from T .,model,Decomposition Functions,0,95,42,25,0,model : Decomposition Functions,0.37401574803149606,0.42,0.373134328358209
question-answering,3,"For a sentence pair Sand T , we first calculate a similarity matrix A mn , where each element a i , j ? A mn computes the cosine similarity between words s i and t j as",model,Decomposition Functions,0,96,43,26,0,model : Decomposition Functions,0.3779527559055118,0.43,0.3880597014925373
question-answering,3,"For a sentence pair Sand T , we first calculate a similarity matrix A mn , where each element a i , j ? A mn computes the cosine similarity between words s i and t j as",model,Decomposition Functions,0,97,44,27,0,model : Decomposition Functions,0.38188976377952755,0.44,0.40298507462686567
question-answering,3,"Then , we define three different semantic matching functions over A mn :",model,Decomposition Functions,0,98,45,28,0,model : Decomposition Functions,0.3858267716535433,0.45,0.417910447761194
question-answering,3,"where k = argmax j a i , j .",model,Decomposition Functions,0,99,46,29,0,model : Decomposition Functions,0.38976377952755903,0.46,0.43283582089552236
question-answering,3,The idea of the global function is to consider all word vectors t j in T .,model,Decomposition Functions,0,100,47,30,0,model : Decomposition Functions,0.3937007874015748,0.47,0.44776119402985076
question-answering,3,"semantic matching vector ? i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",model,Decomposition Functions,0,101,48,31,0,model : Decomposition Functions,0.39763779527559057,0.48,0.4626865671641791
question-answering,3,"semantic matching vector ? i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",model,Decomposition Functions,0,102,49,32,0,model : Decomposition Functions,0.4015748031496063,0.49,0.47761194029850745
question-answering,3,The max function moves to the other extreme .,model,Decomposition Functions,0,103,50,33,0,model : Decomposition Functions,0.40551181102362205,0.5,0.4925373134328358
question-answering,3,It generates the semantic matching vector by selecting the most similar word vector t k from T .,model,Decomposition Functions,0,104,51,34,0,model : Decomposition Functions,0.4094488188976378,0.51,0.5074626865671642
question-answering,3,"The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .",model,Decomposition Functions,0,105,52,35,0,model : Decomposition Functions,0.41338582677165353,0.52,0.5223880597014925
question-answering,3,So the semantic matching vector is a weighted average vector from t k?w tot k+w .,model,Decomposition Functions,0,106,53,36,0,model : Decomposition Functions,0.41732283464566927,0.53,0.5373134328358209
question-answering,3,Decomposition Functions,model,Decomposition Functions,0,107,54,37,0,model : Decomposition Functions,0.421259842519685,0.54,0.5522388059701493
question-answering,3,This subsection describes the implementations for the decomposition function f decomp in Eq.,model,Decomposition Functions,0,108,55,38,0,model : Decomposition Functions,0.4251968503937008,0.55,0.5671641791044776
question-answering,3,2 ) .,model,Decomposition Functions,0,109,56,39,0,model : Decomposition Functions,0.42913385826771655,0.56,0.582089552238806
question-answering,3,"The intention off decomp is to decompose a word vector s j based on its semantic matching vector ? j into a similar component s + i and a dissimilar component s ? i , where s +",model,Decomposition Functions,0,110,57,40,0,model : Decomposition Functions,0.4330708661417323,0.57,0.5970149253731343
question-answering,3,"The intention off decomp is to decompose a word vector s j based on its semantic matching vector ? j into a similar component s + i and a dissimilar component s ? i , where s +",model,Decomposition Functions,0,111,58,41,0,model : Decomposition Functions,0.43700787401574803,0.58,0.6119402985074627
question-answering,3,"The intention off decomp is to decompose a word vector s j based on its semantic matching vector ? j into a similar component s + i and a dissimilar component s ? i , where s +",model,Decomposition Functions,0,112,59,42,0,model : Decomposition Functions,0.4409448818897638,0.59,0.6268656716417911
question-answering,3,indicates the semantics of s i covered by ? i and s ? i indicates the uncovered part .,model,Decomposition Functions,0,113,60,43,0,model : Decomposition Functions,0.4448818897637795,0.6,0.6417910447761194
question-answering,3,indicates the semantics of s i covered by ? i and s ? i indicates the uncovered part .,model,Decomposition Functions,0,114,61,44,0,model : Decomposition Functions,0.44881889763779526,0.61,0.6567164179104478
question-answering,3,indicates the semantics of s i covered by ? i and s ? i indicates the uncovered part .,model,Decomposition Functions,0,115,62,45,0,model : Decomposition Functions,0.452755905511811,0.62,0.6716417910447762
question-answering,3,"We implement three types of decomposition function : rigid , linear and orthogonal .",model,Decomposition Functions,0,116,63,46,0,model : Decomposition Functions,0.4566929133858268,0.63,0.6865671641791045
question-answering,3,The rigid decomposition only adapts to the max version off match .,model,Decomposition Functions,0,117,64,47,0,model : Decomposition Functions,0.46062992125984253,0.64,0.7014925373134329
question-answering,3,"First , it detects whether there is an exactly matched word in the other sentence , or s i equal to ? i .",model,Decomposition Functions,0,118,65,48,0,model : Decomposition Functions,0.4645669291338583,0.65,0.7164179104477612
question-answering,3,"First , it detects whether there is an exactly matched word in the other sentence , or s i equal to ? i .",model,Decomposition Functions,0,119,66,49,0,model : Decomposition Functions,0.468503937007874,0.66,0.7313432835820896
question-answering,3,"If yes , the vector s i is dispatched to the similar component s + i , and the dissimilar component is assigned with a zero vector",model,Decomposition Functions,0,120,67,50,0,model : Decomposition Functions,0.47244094488188976,0.67,0.746268656716418
question-answering,3,0 .,model,Decomposition Functions,0,121,68,51,0,model : Decomposition Functions,0.4763779527559055,0.68,0.7611940298507462
question-answering,3,"Otherwise , the vector s i is assigned to the dissimilar component s ? i .",model,Decomposition Functions,0,122,69,52,0,model : Decomposition Functions,0.48031496062992124,0.69,0.7761194029850746
question-answering,3,"Otherwise , the vector s i is assigned to the dissimilar component s ? i .",model,Decomposition Functions,0,123,70,53,0,model : Decomposition Functions,0.484251968503937,0.7,0.7910447761194029
question-answering,3,Eq. gives the formal definition :,model,Decomposition Functions,0,124,71,54,0,model : Decomposition Functions,0.4881889763779528,0.71,0.8059701492537313
question-answering,3,"The motivation for the linear decomposition is that the more similar between s i and ? i , the higher proportion of s i should be assigned to the similar component .",model,Decomposition Functions,0,125,72,55,0,model : Decomposition Functions,0.4921259842519685,0.72,0.8208955223880597
question-answering,3,"The motivation for the linear decomposition is that the more similar between s i and ? i , the higher proportion of s i should be assigned to the similar component .",model,Decomposition Functions,0,126,73,56,0,model : Decomposition Functions,0.49606299212598426,0.73,0.835820895522388
question-answering,3,"First , we calculate the cosine similarity ? between s i and ? i .",model,Decomposition Functions,0,127,74,57,0,model : Decomposition Functions,0.5,0.74,0.8507462686567164
question-answering,3,"First , we calculate the cosine similarity ? between s i and ? i .",model,Decomposition Functions,0,128,75,58,0,model : Decomposition Functions,0.5039370078740157,0.75,0.8656716417910447
question-answering,3,"First , we calculate the cosine similarity ? between s i and ? i .",model,Decomposition Functions,0,129,76,59,0,model : Decomposition Functions,0.5078740157480315,0.76,0.8805970149253731
question-answering,3,"Then , we decompose s i linearly based on ?.",model,Decomposition Functions,0,130,77,60,0,model : Decomposition Functions,0.5118110236220472,0.77,0.8955223880597015
question-answering,3,Eq. gives the corresponding definition :,model,Decomposition Functions,0,131,78,61,0,model : Decomposition Functions,0.515748031496063,0.78,0.9104477611940298
question-answering,3,The orthogonal decomposition is to decompose a vector in the geometric space .,model,Decomposition Functions,0,132,79,62,0,model : Decomposition Functions,0.5196850393700787,0.79,0.9253731343283582
question-answering,3,"Based on the semantic matching vector ? i , our model decomposes s i into a parallel component and a perpendicular component .",model,Decomposition Functions,0,133,80,63,0,model : Decomposition Functions,0.5236220472440944,0.8,0.9402985074626866
question-answering,3,"Based on the semantic matching vector ? i , our model decomposes s i into a parallel component and a perpendicular component .",model,Decomposition Functions,0,134,81,64,0,model : Decomposition Functions,0.5275590551181102,0.81,0.9552238805970149
question-answering,3,"Then , the parallel component is viewed as the similar component s + i , and perpendicular component is taken as the dissimilar component s ? i .",model,Decomposition Functions,0,135,82,65,0,model : Decomposition Functions,0.531496062992126,0.82,0.9701492537313433
question-answering,3,"Then , the parallel component is viewed as the similar component s + i , and perpendicular component is taken as the dissimilar component s ? i .",model,Decomposition Functions,0,136,83,66,0,model : Decomposition Functions,0.5354330708661418,0.83,0.9850746268656716
question-answering,3,Eq. gives the concrete definitions .,model,Decomposition Functions,0,137,84,67,0,model : Decomposition Functions,0.5393700787401575,0.84,1.0
question-answering,3,Composition Functions,model,Composition Functions,0,138,85,1,0,model : Composition Functions,0.5433070866141733,0.85,0.07692307692307693
question-answering,3,The aim of composition function f comp in Eq. is to extract features from both the similar component matrix and the dissimilar component matrix .,model,Composition Functions,0,139,86,2,0,model : Composition Functions,0.547244094488189,0.86,0.15384615384615385
question-answering,3,We also want to acquire similarities and dissimilarities of various granularity during the composition phase .,model,Composition Functions,0,140,87,3,0,model : Composition Functions,0.5511811023622047,0.87,0.23076923076923078
question-answering,3,"Inspired from Kim , we utilize a two - channel convolutional neural networks ( CNN ) and design filters based on various order of n-grams , e.g. , unigram , bigram and trigram .",model,Composition Functions,0,141,88,4,0,model : Composition Functions,0.5551181102362205,0.88,0.3076923076923077
question-answering,3,The CNN model involves two sequential operations : convolution and max - pooling .,model,Composition Functions,0,142,89,5,0,model : Composition Functions,0.5590551181102362,0.89,0.38461538461538464
question-answering,3,"For the convolution operation , we define a list of filters {w o }.",model,Composition Functions,0,143,90,6,0,model : Composition Functions,0.562992125984252,0.9,0.46153846153846156
question-answering,3,"The shape of each filter is d h , where d is the dimension of word vectors and h is the window size .",model,Composition Functions,0,144,91,7,0,model : Composition Functions,0.5669291338582677,0.91,0.5384615384615384
question-answering,3,"Each filter is applied to two patches ( a window size h of vectors ) from both similar and dissimilar channels , and generates a feature .",model,Composition Functions,0,145,92,8,0,model : Composition Functions,0.5708661417322834,0.92,0.6153846153846154
question-answering,3,Eq. ( 10 ) expresses this process .,model,Composition Functions,0,146,93,9,0,model : Composition Functions,0.5748031496062992,0.93,0.6923076923076923
question-answering,3,"To deal with variable feature size , we perform a max - pooling operation over co by selecting the maximum value co = max co .",model,Composition Functions,0,147,94,10,0,model : Composition Functions,0.5787401574803149,0.94,0.7692307692307693
question-answering,3,"Therefore , after these two operations , each filter generates only one feature .",model,Composition Functions,0,148,95,11,0,model : Composition Functions,0.5826771653543307,0.95,0.8461538461538461
question-answering,3,We define several filters by varying the window size and the initial values .,model,Composition Functions,0,149,96,12,0,model : Composition Functions,0.5866141732283464,0.96,0.9230769230769231
question-answering,3,"Eventually , a vector of features is captured by composing the two component matrixes , and the feature dimension is equal to the number of filters .",model,Composition Functions,0,150,97,13,0,model : Composition Functions,0.5905511811023622,0.97,1.0
question-answering,3,Similarity Assessment Function,model,Similarity Assessment Function,0,151,98,1,0,model : Similarity Assessment Function,0.594488188976378,0.98,0.3333333333333333
question-answering,3,The similarity assessment function f sim in Eq. ( 4 ) predicts a similarity score by taking two feature vectors as input .,model,Similarity Assessment Function,0,152,99,2,0,model : Similarity Assessment Function,0.5984251968503937,0.99,0.6666666666666666
question-answering,3,"We employ a linear function to sum up all the features and apply a sigmoid function to constrain the similarity within the range [ 0 , 1 ] .",model,Similarity Assessment Function,0,153,100,3,0,model : Similarity Assessment Function,0.6023622047244095,1.0,1.0
question-answering,3,Training,training,Training,0,154,1,1,0,training : Training,0.6062992125984252,0.16666666666666666,0.16666666666666666
question-answering,3,We train our sentence similariy model by maximizing the likelihood on a training set .,training,Training,0,155,2,2,0,training : Training,0.610236220472441,0.3333333333333333,0.3333333333333333
question-answering,3,"Each training instance in the training set is represented as a triple ( S i , Ti , Li ) , where Si and Ti are a pair of sentences , and Li ? { 0 , 1 } indicates the similarity between them .",training,Training,0,156,3,3,0,training : Training,0.6141732283464567,0.5,0.5
question-answering,3,"We assign Li = 1 if Ti is a paraphrase of Si for the paraphrase identification task , or Ti is a correct answer for Si for the answer sentence selection task .",training,Training,0,157,4,4,0,training : Training,0.6181102362204725,0.6666666666666666,0.6666666666666666
question-answering,3,"Otherwise , we assign Li = 0 .",training,Training,0,158,5,5,0,training : Training,0.6220472440944882,0.8333333333333334,0.8333333333333334
question-answering,3,We implement the mathematical expressions with Theano and use Adam for optimization .,training,Training,0,159,6,6,0,training : Training,0.6259842519685039,1.0,1.0
question-answering,3,Experiment,experiment,Experiment,0,160,1,1,0,experiment : Experiment,0.6299212598425197,0.09090909090909091,1.0
question-answering,3,Experimental Setting,experiment,Experimental Setting,0,161,2,1,0,experiment : Experimental Setting,0.6338582677165354,0.18181818181818182,0.1
question-answering,3,We evaluate our model on two tasks : answer sentence selection and paraphrase identification .,experiment,Experimental Setting,0,162,3,2,0,experiment : Experimental Setting,0.6377952755905512,0.2727272727272727,0.2
question-answering,3,"The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence , and the performance is measured by mean average precision ( MAP ) and mean reciprocal rank ( MRR ) .",experiment,Experimental Setting,0,163,4,3,0,experiment : Experimental Setting,0.6417322834645669,0.36363636363636365,0.3
question-answering,3,We experiment on two datasets : QASent and Wiki QA .,experiment,Experimental Setting,0,164,5,4,0,experiment : Experimental Setting,0.6456692913385826,0.45454545454545453,0.4
question-answering,3,"The statistics of the two datasets can be found in , where QASent was created from the TREC QA track , and WikiQA ( Yang et al. , 2015 ) is constructed from real queries of Bing and Wikipedia .",experiment,Experimental Setting,0,165,6,5,0,experiment : Experimental Setting,0.6496062992125984,0.5454545454545454,0.5
question-answering,3,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,experiment,Experimental Setting,0,166,7,6,0,experiment : Experimental Setting,0.6535433070866141,0.6363636363636364,0.6
question-answering,3,The metrics include the accuracy and the positive class F 1 score .,experiment,Experimental Setting,0,167,8,7,0,experiment : Experimental Setting,0.65748031496063,0.7272727272727273,0.7
question-answering,3,"We experiment on the Microsoft Research Paraphrase corpus ( MSRP ) , which includes 2753 true and 1323 false instances in the training set , and 1147 true and 578 false instances in the test set .",experiment,Experimental Setting,0,168,9,8,0,experiment : Experimental Setting,0.6614173228346457,0.8181818181818182,0.8
question-answering,3,We build a development set by randomly selecting 100 true and 100 false instances from the training set .,experiment,Experimental Setting,0,169,10,9,0,experiment : Experimental Setting,0.6653543307086615,0.9090909090909091,0.9
question-answering,3,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .",experiment,Experimental Setting,0,170,11,10,0,experiment : Experimental Setting,0.6692913385826772,1.0,1.0
question-answering,3,Model Properties,model,Model Properties,0,171,1,1,0,model : Model Properties,0.6732283464566929,0.014285714285714285,0.034482758620689655
question-answering,3,"There are several alternative options in our model , e.g. , the semantic matching functions , the decomposition operations , and the filter types .",model,Model Properties,0,172,2,2,0,model : Model Properties,0.6771653543307087,0.02857142857142857,0.06896551724137931
question-answering,3,The choice of these options may affect the final performance .,model,Model Properties,0,173,3,3,0,model : Model Properties,0.6811023622047244,0.04285714285714286,0.10344827586206896
question-answering,3,"In this subsection , we present some experiments to demonstrate the properties of our model , and find a good configuration that we use to evaluate our final model .",model,Model Properties,0,174,4,4,0,model : Model Properties,0.6850393700787402,0.05714285714285714,0.13793103448275862
question-answering,3,All the experiments in this subsection were performed on the QASent dataset and evaluated on the development set .,model,Model Properties,0,175,5,5,0,model : Model Properties,0.6889763779527559,0.07142857142857142,0.1724137931034483
question-answering,3,"First , we evaluated the effectiveness of various semantic matching functions .",model,Model Properties,0,176,6,6,0,model : Model Properties,0.6929133858267716,0.08571428571428572,0.20689655172413793
question-answering,3,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",model,Model Properties,1,177,7,7,0,model : Model Properties,0.6968503937007874,0.1,0.2413793103448276
question-answering,3,presents the results .,model,Model Properties,0,178,8,8,0,model : Model Properties,0.7007874015748031,0.11428571428571428,0.27586206896551724
question-answering,3,We found that the max function worked better than the global function on both MAP and MRR .,model,Model Properties,1,179,9,9,0,model : Model Properties,0.7047244094488189,0.12857142857142856,0.3103448275862069
question-answering,3,"By increasing the window size , the local -l function acquired progressive improvements when the window size is smaller than 4 .",model,Model Properties,0,180,10,10,0,model : Model Properties,0.7086614173228346,0.14285714285714285,0.3448275862068966
question-answering,3,"But after we enlarged the window size to 4 , the performance dropped .",model,Model Properties,0,181,11,11,0,model : Model Properties,0.7125984251968503,0.15714285714285714,0.3793103448275862
question-answering,3,"The local - 3 function worked better than the max function in term of the MAP , and also got a comparable MRR .",model,Model Properties,0,182,12,12,0,model : Model Properties,0.7165354330708661,0.17142857142857143,0.41379310344827586
question-answering,3,"Therefore , we use the local - 3 function in the following experiments .",model,Model Properties,0,183,13,13,0,model : Model Properties,0.7204724409448819,0.18571428571428572,0.4482758620689655
question-answering,3,"Second , we studied the effect of various decomposition operations .",model,Model Properties,0,184,14,14,0,model : Model Properties,0.7244094488188977,0.2,0.4827586206896552
question-answering,3,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .",model,Model Properties,1,185,15,15,0,model : Model Properties,0.7283464566929134,0.21428571428571427,0.5172413793103449
question-answering,3,shows the performance .,model,Model Properties,0,186,16,16,0,model : Model Properties,0.7322834645669292,0.22857142857142856,0.5517241379310345
question-answering,3,We found that the rigid operation got the worst result .,model,Model Properties,0,187,17,17,0,model : Model Properties,0.7362204724409449,0.24285714285714285,0.5862068965517241
question-answering,3,"This is reasonable , because the rigid operation decomposes word vectors by exactly matching words .",model,Model Properties,0,188,18,18,0,model : Model Properties,0.7401574803149606,0.2571428571428571,0.6206896551724138
question-answering,3,"The orthogonal operation got a similar MAP as the linear operation , and it worked better in term of MRR .",model,Model Properties,0,189,19,19,0,model : Model Properties,0.7440944881889764,0.2714285714285714,0.6551724137931034
question-answering,3,"Therefore , we choose the orthogonal operation in the following experiments .",model,Model Properties,0,190,20,20,0,model : Model Properties,0.7480314960629921,0.2857142857142857,0.6896551724137931
question-answering,3,"Third , we tested the influence of various filter types .",model,Model Properties,1,191,21,21,0,model : Model Properties,0.7519685039370079,0.3,0.7241379310344828
question-answering,3,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .",model,Model Properties,1,192,22,22,0,model : Model Properties,0.7559055118110236,0.3142857142857143,0.7586206896551724
question-answering,3,We generate 500 filters for each filter type ( with different initial values ) .,model,Model Properties,0,193,23,23,0,model : Model Properties,0.7598425196850394,0.32857142857142857,0.7931034482758621
question-answering,3,Experimental results are shown in .,model,Model Properties,0,194,24,24,0,model : Model Properties,0.7637795275590551,0.34285714285714286,0.8275862068965517
question-answering,3,"At the beginning , adding higher - order ngram filters was helpful for the performance .",model,Model Properties,0,195,25,25,0,model : Model Properties,0.7677165354330708,0.35714285714285715,0.8620689655172413
question-answering,3,"The performance reached to the peak , when we used the win - 3 filters .",model,Model Properties,0,196,26,26,0,model : Model Properties,0.7716535433070866,0.37142857142857144,0.896551724137931
question-answering,3,"After that , adding more complex filters decreased the performance .",model,Model Properties,0,197,27,27,0,model : Model Properties,0.7755905511811023,0.38571428571428573,0.9310344827586207
question-answering,3,"Therefore , the trigram is the best granularity for our model .",model,Model Properties,0,198,28,28,0,model : Model Properties,0.7795275590551181,0.4,0.9655172413793104
question-answering,3,"In the following experiments , we utilize filter types in win - 3 .",model,Model Properties,0,199,29,29,0,model : Model Properties,0.7834645669291339,0.4142857142857143,1.0
question-answering,3,Comparing with State - of - the - art Models,model,Comparing with State-of-the-art Models,0,200,30,1,0,model : Comparing with State-of-the-art Models,0.7874015748031497,0.42857142857142855,0.024390243902439025
question-answering,3,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .",model,Comparing with State-of-the-art Models,0,201,31,2,0,model : Comparing with State-of-the-art Models,0.7913385826771654,0.44285714285714284,0.04878048780487805
question-answering,3,QASent dataset .,model,Comparing with State-of-the-art Models,1,202,32,3,0,model : Comparing with State-of-the-art Models,0.7952755905511811,0.45714285714285713,0.07317073170731707
question-answering,3,"presents the performances of the state - of - the - art systems and our model , where the performances were evaluated with the standard trec eval - 8.1 script",model,Comparing with State-of-the-art Models,0,203,33,4,0,model : Comparing with State-of-the-art Models,0.7992125984251969,0.4714285714285714,0.0975609756097561
question-answering,3,1 .,model,Comparing with State-of-the-art Models,0,204,34,5,0,model : Comparing with State-of-the-art Models,0.8031496062992126,0.4857142857142857,0.12195121951219512
question-answering,3,"Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",model,Comparing with State-of-the-art Models,0,205,35,6,0,model : Comparing with State-of-the-art Models,0.8070866141732284,0.5,0.14634146341463414
question-answering,3,"Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .",model,Comparing with State-of-the-art Models,0,206,36,7,0,model : Comparing with State-of-the-art Models,0.8110236220472441,0.5142857142857142,0.17073170731707318
question-answering,3,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .",model,Comparing with State-of-the-art Models,0,207,37,8,0,model : Comparing with State-of-the-art Models,0.8149606299212598,0.5285714285714286,0.1951219512195122
question-answering,3,"Therefore , the lower - level granularity is an indispensable factor for a good performance .",model,Comparing with State-of-the-art Models,0,208,38,9,0,model : Comparing with State-of-the-art Models,0.8188976377952756,0.5428571428571428,0.21951219512195122
question-answering,3,"conducted word alignment for a sentence pair based on word vectors , and measured the sentence similarity based on a couple of word alignment features .",model,Comparing with State-of-the-art Models,0,209,39,10,0,model : Comparing with State-of-the-art Models,0.8228346456692913,0.5571428571428572,0.24390243902439024
question-answering,3,"They got a slightly better performance ( the fourth row of ) , which indicates that the vector representation for words is helpful to bridging the lexical gap problem .",model,Comparing with State-of-the-art Models,0,210,40,11,0,model : Comparing with State-of-the-art Models,0.8267716535433071,0.5714285714285714,0.2682926829268293
question-answering,3,"dos introduced the attention mechanism into the CNN model , and learnt sentence representation by considering the influence of the other sentence .",model,Comparing with State-of-the-art Models,0,211,41,12,0,model : Comparing with State-of-the-art Models,0.8307086614173228,0.5857142857142857,0.2926829268292683
question-answering,3,They got better performance than all the other previous work .,model,Comparing with State-of-the-art Models,0,212,42,13,0,model : Comparing with State-of-the-art Models,0.8346456692913385,0.6,0.3170731707317073
question-answering,3,Our model makes use of all these useful factors and also considers the dissimilarities of a sentence pair .,model,Comparing with State-of-the-art Models,0,213,43,14,0,model : Comparing with State-of-the-art Models,0.8385826771653543,0.6142857142857143,0.34146341463414637
question-answering,3,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",model,Comparing with State-of-the-art Models,1,214,44,15,0,model : Comparing with State-of-the-art Models,0.84251968503937,0.6285714285714286,0.36585365853658536
question-answering,3,Wiki QA dataset .,model,Comparing with State-of-the-art Models,1,215,45,16,0,model : Comparing with State-of-the-art Models,0.8464566929133859,0.6428571428571429,0.3902439024390244
question-answering,3,presents the results of our model and several state - of - the - art models .,model,Comparing with State-of-the-art Models,0,216,46,17,0,model : Comparing with State-of-the-art Models,0.8503937007874016,0.6571428571428571,0.4146341463414634
question-answering,3,constructed the dataset and reimplemented several baseline models .,model,Comparing with State-of-the-art Models,0,217,47,18,0,model : Comparing with State-of-the-art Models,0.8543307086614174,0.6714285714285714,0.43902439024390244
question-answering,3,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,model,Comparing with State-of-the-art Models,0,218,48,19,0,model : Comparing with State-of-the-art Models,0.8582677165354331,0.6857142857142857,0.4634146341463415
question-answering,3,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,model,Comparing with State-of-the-art Models,0,219,49,20,0,model : Comparing with State-of-the-art Models,0.8622047244094488,0.7,0.4878048780487805
question-answering,3,The corresponding performance is given at the fourth row of .,model,Comparing with State-of-the-art Models,0,220,50,21,0,model : Comparing with State-of-the-art Models,0.8661417322834646,0.7142857142857143,0.5121951219512195
question-answering,3,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",model,Comparing with State-of-the-art Models,0,221,51,22,0,model : Comparing with State-of-the-art Models,0.8700787401574803,0.7285714285714285,0.5365853658536586
question-answering,3,The semantic matching phase in our model is similar to the attention mechanism .,model,Comparing with State-of-the-art Models,0,222,52,23,0,model : Comparing with State-of-the-art Models,0.8740157480314961,0.7428571428571429,0.5609756097560976
question-answering,3,"But different from the previous models , our model utilizes both the similarity and dissimilarity simultaneously .",model,Comparing with State-of-the-art Models,0,223,53,24,0,model : Comparing with State-of-the-art Models,0.8779527559055118,0.7571428571428571,0.5853658536585366
question-answering,3,The last row of shows that our model is more effective than the other models .,model,Comparing with State-of-the-art Models,1,224,54,25,0,model : Comparing with State-of-the-art Models,0.8818897637795275,0.7714285714285715,0.6097560975609756
question-answering,3,MSRP dataset .,model,Comparing with State-of-the-art Models,1,225,55,26,0,model : Comparing with State-of-the-art Models,0.8858267716535433,0.7857142857142857,0.6341463414634146
question-answering,3,granularity and modeled interaction features at each level for a pair of sentences .,model,Comparing with State-of-the-art Models,0,226,56,27,0,model : Comparing with State-of-the-art Models,0.889763779527559,0.8,0.6585365853658537
question-answering,3,They obtained their best performance by pretraining the model on a language modeling task ( the 3rd row of ) .,model,Comparing with State-of-the-art Models,0,227,57,28,0,model : Comparing with State-of-the-art Models,0.8937007874015748,0.8142857142857143,0.6829268292682927
question-answering,3,"However , their model heavily depends on the pretraining strategy .",model,Comparing with State-of-the-art Models,0,228,58,29,0,model : Comparing with State-of-the-art Models,0.8976377952755905,0.8285714285714286,0.7073170731707317
question-answering,3,"Without pretraining , they got a much worse performance ( the second row of ) .",model,Comparing with State-of-the-art Models,0,229,59,30,0,model : Comparing with State-of-the-art Models,0.9015748031496063,0.8428571428571429,0.7317073170731707
question-answering,3,proposed a similar model to .,model,Comparing with State-of-the-art Models,0,230,60,31,0,model : Comparing with State-of-the-art Models,0.905511811023622,0.8571428571428571,0.7560975609756098
question-answering,3,"Similarly , they also used a CNN model to extract features at multiple levels of granularity .",model,Comparing with State-of-the-art Models,0,231,61,32,0,model : Comparing with State-of-the-art Models,0.9094488188976378,0.8714285714285714,0.7804878048780488
question-answering,3,"Differently , they utilized some extra annotated resources , e.g. , embeddings from part - of - speech ( POS ) tags and PARAGRAM vectors trained from the Paraphrase Data base .",model,Comparing with State-of-the-art Models,0,232,62,33,0,model : Comparing with State-of-the-art Models,0.9133858267716536,0.8857142857142857,0.8048780487804879
question-answering,3,Their model outperformed without the need of pretraining ( the sixth row of ) .,model,Comparing with State-of-the-art Models,0,233,63,34,0,model : Comparing with State-of-the-art Models,0.9173228346456693,0.9,0.8292682926829268
question-answering,3,"However , the performance was reduced after removing the extra resources ( the fourth and fifth rows of ) .",model,Comparing with State-of-the-art Models,0,234,64,35,0,model : Comparing with State-of-the-art Models,0.9212598425196851,0.9142857142857143,0.8536585365853658
question-answering,3,applied their attention - based CNN model on this dataset .,model,Comparing with State-of-the-art Models,0,235,65,36,0,model : Comparing with State-of-the-art Models,0.9251968503937008,0.9285714285714286,0.8780487804878049
question-answering,3,"By adding a couple of sparse features and using a layerwise training strategy , they got a pretty good performance .",model,Comparing with State-of-the-art Models,0,236,66,37,0,model : Comparing with State-of-the-art Models,0.9291338582677166,0.9428571428571428,0.9024390243902439
question-answering,3,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",model,Comparing with State-of-the-art Models,1,237,67,38,0,model : Comparing with State-of-the-art Models,0.9330708661417323,0.9571428571428572,0.926829268292683
question-answering,3,"However , the best performance so far on this dataset is obtained by .",model,Comparing with State-of-the-art Models,0,238,68,39,0,model : Comparing with State-of-the-art Models,0.937007874015748,0.9714285714285714,0.9512195121951219
question-answering,3,"In their model , they just utilized several hand - crafted features in a Support Vector Machine ( SVM ) model .",model,Comparing with State-of-the-art Models,0,239,69,40,0,model : Comparing with State-of-the-art Models,0.9409448818897638,0.9857142857142858,0.975609756097561
question-answering,3,"Therefore , the deep learning methods still have along way to go for this task .",model,Comparing with State-of-the-art Models,0,240,70,41,0,model : Comparing with State-of-the-art Models,0.9448818897637795,1.0,1.0
question-answering,3,Related Work,related work,Related Work,0,241,1,1,0,related work : Related Work,0.9488188976377953,0.125,0.125
question-answering,3,The semantic matching functions in subsection 3.1 are inspired from the attention - based neural machine translation .,related work,Related Work,0,242,2,2,0,related work : Related Work,0.952755905511811,0.25,0.25
question-answering,3,"However , most of the previous work using the attention mechanism in only LSTM models .",related work,Related Work,0,243,3,3,0,related work : Related Work,0.9566929133858267,0.375,0.375
question-answering,3,Whereas our model introduces the attention mechanism into the CNN model .,related work,Related Work,0,244,4,4,0,related work : Related Work,0.9606299212598425,0.5,0.5
question-answering,3,similar work is the attention - based CNN model proposed by .,related work,Related Work,0,245,5,5,0,related work : Related Work,0.9645669291338582,0.625,0.625
question-answering,3,"They first build an attention matrix for a sentence pair , and then directly take the attention matrix as a new channel of the CNN model .",related work,Related Work,0,246,6,6,0,related work : Related Work,0.968503937007874,0.75,0.75
question-answering,3,"Differently , our model uses the attention matrix ( or similarity matrix ) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix , and then feeds these two matrixes into a two - channel CNN model .",related work,Related Work,0,247,7,7,0,related work : Related Work,0.9724409448818898,0.875,0.875
question-answering,3,The model can then focus much on the interactions between similar and dissimilar parts of a sentence pair .,related work,Related Work,0,248,8,8,0,related work : Related Work,0.9763779527559056,1.0,1.0
question-answering,3,Conclusion,conclusion,Conclusion,0,249,1,1,0,conclusion : Conclusion,0.9803149606299213,0.16666666666666666,0.16666666666666666
question-answering,3,"In this work , we proposed a model to assess sentence similarity by decomposing and composing lexical semantics .",conclusion,Conclusion,0,250,2,2,0,conclusion : Conclusion,0.984251968503937,0.3333333333333333,0.3333333333333333
question-answering,3,"To bridge the lexical gap problem , our model represents each word with its context vector .",conclusion,Conclusion,0,251,3,3,0,conclusion : Conclusion,0.9881889763779528,0.5,0.5
question-answering,3,"To extract features from both the similarity and dissimilarity of a sentence pair , we designed several methods to decompose the word vector into a similar component and a dissimilar component .",conclusion,Conclusion,0,252,4,4,0,conclusion : Conclusion,0.9921259842519685,0.6666666666666666,0.6666666666666666
question-answering,3,"To extract features at multiple levels of granularity , we employed a two - channel CNN model and equipped it with multiple types of ngram filters .",conclusion,Conclusion,0,253,5,5,0,conclusion : Conclusion,0.9960629921259843,0.8333333333333334,0.8333333333333334
question-answering,3,Experimental results show that our model is quite effective on both the answer sentence selection task and the paraphrase identification task .,conclusion,Conclusion,0,254,6,6,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,4,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,title,title,1,2,1,1,0,title : title,0.006872852233676976,1.0,1.0
question-answering,4,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.010309278350515464,0.1111111111111111,0.1111111111111111
question-answering,4,Understanding unstructured text is a major goal within natural language processing .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.013745704467353952,0.2222222222222222,0.2222222222222222
question-answering,4,Comprehension tests pose questions based on short text passages to evaluate such understanding .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.01718213058419244,0.3333333333333333,0.3333333333333333
question-answering,4,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.020618556701030927,0.4444444444444444,0.4444444444444444
question-answering,4,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.024054982817869417,0.5555555555555556,0.5555555555555556
question-answering,4,"We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.027491408934707903,0.6666666666666666,0.6666666666666666
question-answering,4,"The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.030927835051546393,0.7777777777777778,0.7777777777777778
question-answering,4,Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.03436426116838488,0.8888888888888888,0.8888888888888888
question-answering,4,"When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets a new state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.037800687285223365,1.0,1.0
question-answering,4,Introduction,introduction,introduction,0,12,1,1,0,introduction : introduction,0.041237113402061855,0.02702702702702703,0.02702702702702703
question-answering,4,"Humans learn in a variety of ways - by communication with each other , and by study , the reading of text .",introduction,introduction,0,13,2,2,0,introduction : introduction,0.044673539518900345,0.05405405405405406,0.05405405405405406
question-answering,4,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .",introduction,introduction,1,14,3,3,0,introduction : introduction,0.048109965635738834,0.08108108108108109,0.08108108108108109
question-answering,4,It has garnered significant attention from the machine learning research community in recent years .,introduction,introduction,0,15,4,4,0,introduction : introduction,0.05154639175257732,0.10810810810810811,0.10810810810810811
question-answering,4,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,introduction,introduction,1,16,5,5,0,introduction : introduction,0.054982817869415807,0.13513513513513514,0.13513513513513514
question-answering,4,"Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .",introduction,introduction,0,17,6,6,0,introduction : introduction,0.058419243986254296,0.16216216216216217,0.16216216216216217
question-answering,4,"Given a text passage and a question about its content , a system is tested on its ability to determine the correct answer .",introduction,introduction,0,18,7,7,0,introduction : introduction,0.061855670103092786,0.1891891891891892,0.1891891891891892
question-answering,4,"In this work , we focus on MCTest , a complex but data - limited comprehension benchmark , whose multiple - choice questions require not only extraction but also inference and limited reasoning .",introduction,introduction,0,19,8,8,0,introduction : introduction,0.06529209621993128,0.21621621621621623,0.21621621621621623
question-answering,4,"Inference and reasoning are important human skills that apply broadly , beyond language .",introduction,introduction,0,20,9,9,0,introduction : introduction,0.06872852233676977,0.24324324324324326,0.24324324324324326
question-answering,4,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,introduction,introduction,0,21,10,10,0,introduction : introduction,0.07216494845360824,0.2702702702702703,0.2702702702702703
question-answering,4,"There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",introduction,introduction,0,22,11,11,0,introduction : introduction,0.07560137457044673,0.2972972972972973,0.2972972972972973
question-answering,4,"Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .",introduction,introduction,0,23,12,12,0,introduction : introduction,0.07903780068728522,0.32432432432432434,0.32432432432432434
question-answering,4,"Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .",introduction,introduction,0,24,13,13,0,introduction : introduction,0.08247422680412371,0.35135135135135137,0.35135135135135137
question-answering,4,"Likewise , deep models learn to extract their own features , but this is a data - intensive process .",introduction,introduction,0,25,14,14,0,introduction : introduction,0.0859106529209622,0.3783783783783784,0.3783783783783784
question-answering,4,Our model learns to comprehend at a high level even when data is sparse .,introduction,introduction,0,26,15,15,0,introduction : introduction,0.08934707903780069,0.40540540540540543,0.40540540540540543
question-answering,4,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,introduction,introduction,1,27,16,16,0,introduction : introduction,0.09278350515463918,0.43243243243243246,0.43243243243243246
question-answering,4,We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .,introduction,introduction,0,28,17,17,0,introduction : introduction,0.09621993127147767,0.4594594594594595,0.4594594594594595
question-answering,4,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",introduction,introduction,1,29,18,18,0,introduction : introduction,0.09965635738831616,0.4864864864864865,0.4864864864864865
question-answering,4,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",introduction,introduction,1,30,19,19,0,introduction : introduction,0.10309278350515463,0.5135135135135135,0.5135135135135135
question-answering,4,"As in the semantic perspective , we consider matches over complete sentences .",introduction,introduction,0,31,20,20,0,introduction : introduction,0.10652920962199312,0.5405405405405406,0.5405405405405406
question-answering,4,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",introduction,introduction,1,32,21,21,0,introduction : introduction,0.10996563573883161,0.5675675675675675,0.5675675675675675
question-answering,4,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",introduction,introduction,1,33,22,22,0,introduction : introduction,0.1134020618556701,0.5945945945945946,0.5945945945945946
question-answering,4,Words are represented throughout by embedding vectors .,introduction,introduction,0,34,23,23,0,introduction : introduction,0.11683848797250859,0.6216216216216216,0.6216216216216216
question-answering,4,These distinct perspectives naturally form a hierarchy that we depict in .,introduction,introduction,0,35,24,24,0,introduction : introduction,0.12027491408934708,0.6486486486486487,0.6486486486486487
question-answering,4,"Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .",introduction,introduction,0,36,25,25,0,introduction : introduction,0.12371134020618557,0.6756756756756757,0.6756756756756757
question-answering,4,The perspectives of our model can be considered a type of feature .,introduction,introduction,0,37,26,26,0,introduction : introduction,0.12714776632302405,0.7027027027027027,0.7027027027027027
question-answering,4,"However , they are implemented by parametric differentiable functions .",introduction,introduction,0,38,27,27,0,introduction : introduction,0.13058419243986255,0.7297297297297297,0.7297297297297297
question-answering,4,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .",introduction,introduction,0,39,28,28,0,introduction : introduction,0.13402061855670103,0.7567567567567568,0.7567567567567568
question-answering,4,"Our model , significantly , can be trained end - to - end with backpropagation .",introduction,introduction,0,40,29,29,0,introduction : introduction,0.13745704467353953,0.7837837837837838,0.7837837837837838
question-answering,4,"To facilitate learning with limited data , we also develop a unique training scheme .",introduction,introduction,0,41,30,30,0,introduction : introduction,0.140893470790378,0.8108108108108109,0.8108108108108109
question-answering,4,We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .,introduction,introduction,0,42,31,31,0,introduction : introduction,0.14432989690721648,0.8378378378378378,0.8378378378378378
question-answering,4,"Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .",introduction,introduction,0,43,32,32,0,introduction : introduction,0.14776632302405499,0.8648648648648649,0.8648648648648649
question-answering,4,We call this technique training wheels .,introduction,introduction,0,44,33,33,0,introduction : introduction,0.15120274914089346,0.8918918918918919,0.8918918918918919
question-answering,4,Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .,introduction,introduction,0,45,34,34,0,introduction : introduction,0.15463917525773196,0.918918918918919,0.918918918918919
question-answering,4,"Models designed specifically for MCTest include those of , and more recently , , and .",introduction,introduction,0,46,35,35,0,introduction : introduction,0.15807560137457044,0.9459459459459459,0.9459459459459459
question-answering,4,"In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .",introduction,introduction,0,47,36,36,0,introduction : introduction,0.16151202749140894,0.972972972972973,0.972972972972973
question-answering,4,"Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .",introduction,introduction,0,48,37,37,0,introduction : introduction,0.16494845360824742,1.0,1.0
question-answering,4,The Problem,system description,The Problem,0,49,1,1,0,system description : The Problem,0.16838487972508592,0.1,0.1
question-answering,4,"In this section we borrow from , who laid out the MC problem nicely .",system description,The Problem,0,50,2,2,0,system description : The Problem,0.1718213058419244,0.2,0.2
question-answering,4,Machine comprehension requires machines to answer questions based on unstructured text .,system description,The Problem,0,51,3,3,0,system description : The Problem,0.17525773195876287,0.3,0.3
question-answering,4,This can be viewed as selecting the best answer from a set of candidates .,system description,The Problem,0,52,4,4,0,system description : The Problem,0.17869415807560138,0.4,0.4
question-answering,4,"In the multiple - choice case , candidate answers are predefined , but candidate answers may also be undefined yet restricted ( e.g. , to yes , no , or any noun phrase in the text ) .",system description,The Problem,0,53,5,5,0,system description : The Problem,0.18213058419243985,0.5,0.5
question-answering,4,"For each question q , let T be the unstructured text and A = {a i } the set of candidate answers to q .",system description,The Problem,0,54,6,6,0,system description : The Problem,0.18556701030927836,0.6,0.6
question-answering,4,The machine comprehension task reduces to selecting the answer that has the highest evidence given T .,system description,The Problem,0,55,7,7,0,system description : The Problem,0.18900343642611683,0.7,0.7
question-answering,4,"As in , we combine an answer and a question into a hypothesis , hi = f ( q , a i ) .",system description,The Problem,0,56,8,8,0,system description : The Problem,0.19243986254295534,0.8,0.8
question-answering,4,"To facilitate comparisons of the text with the hypotheses , we also breakdown the passage into sentences t j , T = {t j }.",system description,The Problem,0,57,9,9,0,system description : The Problem,0.1958762886597938,0.9,0.9
question-answering,4,"In our setting , q , a i , and t j each represent a sequence of embedding vectors , one for each word and punctuation mark in the respective item .",system description,The Problem,0,58,10,10,0,system description : The Problem,0.19931271477663232,1.0,1.0
question-answering,4,Related Work,related work,Related Work,0,59,1,1,0,related work : Related Work,0.2027491408934708,0.041666666666666664,0.041666666666666664
question-answering,4,Machine comprehension is currently a hot topic within the machine learning community .,related work,Related Work,0,60,2,2,0,related work : Related Work,0.20618556701030927,0.08333333333333333,0.08333333333333333
question-answering,4,"In this section we will focus on the best - performing models applied specifically to MCTest , since it is somewhat unique among MC datasets ( see Section 5 ) .",related work,Related Work,0,61,3,3,0,related work : Related Work,0.20962199312714777,0.125,0.125
question-answering,4,"Generally , models can be divided into two categories : those that use fixed , engineered features , and neural models .",related work,Related Work,0,62,4,4,0,related work : Related Work,0.21305841924398625,0.16666666666666666,0.16666666666666666
question-answering,4,The bulk of the work on MCTest falls into the former category .,related work,Related Work,0,63,5,5,0,related work : Related Work,0.21649484536082475,0.20833333333333334,0.20833333333333334
question-answering,4,"Manually engineered features often require significant effort on the part of a designer , and / or various auxiliary tools to extract them , and they can not be modified by training .",related work,Related Work,0,64,6,6,0,related work : Related Work,0.21993127147766323,0.25,0.25
question-answering,4,"On the other hand , neural models can be trained end - to - end and typically harness only a single feature : vectorrepresentations of words .",related work,Related Work,0,65,7,7,0,related work : Related Work,0.22336769759450173,0.2916666666666667,0.2916666666666667
question-answering,4,Word embeddings are fed into a complex and possibly deep neural network which processes and compares text to question and answer .,related work,Related Work,0,66,8,8,0,related work : Related Work,0.2268041237113402,0.3333333333333333,0.3333333333333333
question-answering,4,"Among deep models , mechanisms of attention and working memory are common , as in and .",related work,Related Work,0,67,9,9,0,related work : Related Work,0.23024054982817868,0.375,0.375
question-answering,4,"Feature - engineering models treated MCTest as a structured prediction problem , searching for a latent answerentailing structure connecting question , answer , and text .",related work,Related Work,0,68,10,10,0,related work : Related Work,0.23367697594501718,0.4166666666666667,0.4166666666666667
question-answering,4,This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text .,related work,Related Work,0,69,11,11,0,related work : Related Work,0.23711340206185566,0.4583333333333333,0.4583333333333333
question-answering,4,The process of ( latently ) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation .,related work,Related Work,0,70,12,12,0,related work : Related Work,0.24054982817869416,0.5,0.5
question-answering,4,The model uses event and entity coreference links across sentences along with a host of other features .,related work,Related Work,0,71,13,13,0,related work : Related Work,0.24398625429553264,0.5416666666666666,0.5416666666666666
question-answering,4,These include specifically trained word vectors for synonymy ; antonymy and class - inclusion relations from external data base sources ; dependencies and semantic role labels .,related work,Related Work,0,72,14,14,0,related work : Related Work,0.24742268041237114,0.5833333333333334,0.5833333333333334
question-answering,4,"The model is trained using a latent structural SVM extended to a multitask setting , so that questions are first classified using a pretrained top - level classifier .",related work,Related Work,0,73,15,15,0,related work : Related Work,0.2508591065292096,0.625,0.625
question-answering,4,This enables the system to use different processing strategies for different question categories .,related work,Related Work,0,74,16,16,0,related work : Related Work,0.2542955326460481,0.6666666666666666,0.6666666666666666
question-answering,4,The model also combines question and answer into a well - formed statement using the rules of Cucerzan and Agichtein ( 2005 ) .,related work,Related Work,0,75,17,17,0,related work : Related Work,0.25773195876288657,0.7083333333333334,0.7083333333333334
question-answering,4,"Our model is simpler than that of in terms of the features it takes in , the training procedure ( stochastic gradient descent vs. alternating minimization ) , question classification ( we use none ) , and question - answer combination ( simple concatenation or mean vs. a set of rules ) .",related work,Related Work,0,76,18,18,0,related work : Related Work,0.2611683848797251,0.75,0.75
question-answering,4,"augmented the baseline feature set from with features for syntax , frame semantics , coreference chains , and word embeddings .",related work,Related Work,0,77,19,19,0,related work : Related Work,0.2646048109965636,0.7916666666666666,0.7916666666666666
question-answering,4,They combined features using a linear latent - variable classifier trained to minimize a max - margin loss function .,related work,Related Work,0,78,20,20,0,related work : Related Work,0.26804123711340205,0.8333333333333334,0.8333333333333334
question-answering,4,"As in , questions and answers are combined using a set of manually written rules .",related work,Related Work,0,79,21,21,0,related work : Related Work,0.27147766323024053,0.875,0.875
question-answering,4,"The method of achieved the previous state of the art , but has significant complexity in terms of the feature set .",related work,Related Work,0,80,22,22,0,related work : Related Work,0.27491408934707906,0.9166666666666666,0.9166666666666666
question-answering,4,"Space does not permit a full description of all models in this category , but see also and .",related work,Related Work,0,81,23,23,0,related work : Related Work,0.27835051546391754,0.9583333333333334,0.9583333333333334
question-answering,4,"Despite its relative lack of features , the Parallel - Hierarchical model improves upon the featureengineered state of the art for MCTest by a small amount ( about 1 % absolute ) as detailed in Section 5 .",related work,Related Work,0,82,24,24,0,related work : Related Work,0.281786941580756,1.0,1.0
question-answering,4,Neural models,model,Neural models,0,83,1,1,0,model : Neural models,0.2852233676975945,0.008928571428571428,0.07692307692307693
question-answering,4,"Neural models have , to date , performed relatively poorly on MCTest .",model,Neural models,0,84,2,2,0,model : Neural models,0.28865979381443296,0.017857142857142856,0.15384615384615385
question-answering,4,This is because the dataset is sparse and complex .,model,Neural models,0,85,3,3,0,model : Neural models,0.2920962199312715,0.026785714285714284,0.23076923076923078
question-answering,4,investigated deep - learning approaches concurrently with the present work .,model,Neural models,0,86,4,4,0,model : Neural models,0.29553264604810997,0.03571428571428571,0.3076923076923077
question-answering,4,"They measured the performance of the Attentive Reader and the Neural Reasoner , both deep , end - to - end recurrent models with attention mechanisms , and also developed an attention - based convolutional network , the HABCNN .",model,Neural models,0,87,5,5,0,model : Neural models,0.29896907216494845,0.044642857142857144,0.38461538461538464
question-answering,4,"Their network operates on a hierarchy similar to our own , providing further evidence of the promise of hierarchical perspectives .",model,Neural models,0,88,6,6,0,model : Neural models,0.3024054982817869,0.05357142857142857,0.46153846153846156
question-answering,4,"Specifically , the HABCNN processes text at the sentence level and the snippet level , where the latter combines adjacent sentences ( as we do through an n-gram input ) .",model,Neural models,0,89,7,7,0,model : Neural models,0.30584192439862545,0.0625,0.5384615384615384
question-answering,4,Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network .,model,Neural models,0,90,8,8,0,model : Neural models,0.30927835051546393,0.07142857142857142,0.6153846153846154
question-answering,4,"This encoding modulates attention over sentence and snippet encodings , followed by maxpooling to determine the best matches between question , answer , and text .",model,Neural models,0,91,9,9,0,model : Neural models,0.3127147766323024,0.08035714285714286,0.6923076923076923
question-answering,4,"As in the present work , matching scores are given by cosine similarity .",model,Neural models,0,92,10,10,0,model : Neural models,0.3161512027491409,0.08928571428571429,0.7692307692307693
question-answering,4,The HABCNN also makes use of a question classifier .,model,Neural models,0,93,11,11,0,model : Neural models,0.31958762886597936,0.09821428571428571,0.8461538461538461
question-answering,4,"Despite the shared concepts between the HABCNN and our approach , the Parallel - Hierarchical model performs significantly better on MCTest ( more than 15 % absolute ) as detailed in Section 5 .",model,Neural models,0,94,12,12,0,model : Neural models,0.3230240549828179,0.10714285714285714,0.9230769230769231
question-answering,4,Other neural models tested in fare even worse .,model,Neural models,0,95,13,13,0,model : Neural models,0.32646048109965636,0.11607142857142858,1.0
question-answering,4,The Parallel - Hierarchical Model,model,The Parallel-Hierarchical Model,0,96,14,1,0,model : The Parallel-Hierarchical Model,0.32989690721649484,0.125,0.2
question-answering,4,Let us now define our machine comprehension model in full .,model,The Parallel-Hierarchical Model,0,97,15,2,0,model : The Parallel-Hierarchical Model,0.3333333333333333,0.13392857142857142,0.4
question-answering,4,"We first describe each of the perspectives separately , then describe how they are combined .",model,The Parallel-Hierarchical Model,0,98,16,3,0,model : The Parallel-Hierarchical Model,0.33676975945017185,0.14285714285714285,0.6
question-answering,4,"Below , we use subscripts to index elements of sequences , like word vectors , and superscripts to indicate whether elements come from the text , question , or answer .",model,The Parallel-Hierarchical Model,0,99,17,4,0,model : The Parallel-Hierarchical Model,0.3402061855670103,0.15178571428571427,0.8
question-answering,4,"In particular , we use the subscripts k , m , n , p to index sequences from the text , question , answer , and hypothesis , respectively , and superscripts t , q , a , h. We depict the model schematically in .",model,The Parallel-Hierarchical Model,0,100,18,5,0,model : The Parallel-Hierarchical Model,0.3436426116838488,0.16071428571428573,1.0
question-answering,4,Semantic Perspective,model,Semantic Perspective,0,101,19,1,0,model : Semantic Perspective,0.3470790378006873,0.16964285714285715,0.0625
question-answering,4,The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space .,model,Semantic Perspective,0,102,20,2,0,model : Semantic Perspective,0.35051546391752575,0.17857142857142858,0.125
question-answering,4,Each sen - tence of the text is a sequence of d-dimensional word vectors :,model,Semantic Perspective,0,103,21,3,0,model : Semantic Perspective,0.3539518900343643,0.1875,0.1875
question-answering,4,"The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two - layer network that implements weighted sum followed by an affine tranformation and a nonlinearity ; i.e. , The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .",model,Semantic Perspective,0,104,22,4,0,model : Semantic Perspective,0.35738831615120276,0.19642857142857142,0.25
question-answering,4,"The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two - layer network that implements weighted sum followed by an affine tranformation and a nonlinearity ; i.e. , The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .",model,Semantic Perspective,0,105,23,5,0,model : Semantic Perspective,0.36082474226804123,0.20535714285714285,0.3125
question-answering,4,The scalar ? k is a trainable weight associated to each word in the vocabulary .,model,Semantic Perspective,0,106,24,6,0,model : Semantic Perspective,0.3642611683848797,0.21428571428571427,0.375
question-answering,4,The scalar ? k is a trainable weight associated to each word in the vocabulary .,model,Semantic Perspective,0,107,25,7,0,model : Semantic Perspective,0.36769759450171824,0.22321428571428573,0.4375
question-answering,4,These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus .,model,Semantic Perspective,0,108,26,8,0,model : Semantic Perspective,0.3711340206185567,0.23214285714285715,0.5
question-answering,4,"They can , for example , learn to perform the function of stopword lists in a soft , trainable way , to nullify the contribution of unimportant filler words .",model,Semantic Perspective,0,109,27,9,0,model : Semantic Perspective,0.3745704467353952,0.24107142857142858,0.5625
question-answering,4,"The semantic representation of a hypothesis is formed analogously , except that we combine the question word vectors q m and answer word vectors an as a single sequence {h p } = {q m , an }.",model,Semantic Perspective,0,110,28,10,0,model : Semantic Perspective,0.37800687285223367,0.25,0.625
question-answering,4,"For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ? R Dd and bias vector b h A ? RD .",model,Semantic Perspective,0,111,29,11,0,model : Semantic Perspective,0.38144329896907214,0.25892857142857145,0.6875
question-answering,4,"For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ? R Dd and bias vector b h A ? RD .",model,Semantic Perspective,0,112,30,12,0,model : Semantic Perspective,0.3848797250859107,0.26785714285714285,0.75
question-answering,4,"For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ? R Dd and bias vector b h A ? RD .",model,Semantic Perspective,0,113,31,13,0,model : Semantic Perspective,0.38831615120274915,0.2767857142857143,0.8125
question-answering,4,These transformations map a text sentence and a hypothesis into a common space where they can be compared .,model,Semantic Perspective,0,114,32,14,0,model : Semantic Perspective,0.3917525773195876,0.2857142857142857,0.875
question-answering,4,"We compute the semantic match be-tween text sentence and hypothesis using the cosine similarity , M sem = cos ( s t , sh ) .",model,Semantic Perspective,0,115,33,15,0,model : Semantic Perspective,0.3951890034364261,0.29464285714285715,0.9375
question-answering,4,2 ),model,Semantic Perspective,0,116,34,16,0,model : Semantic Perspective,0.39862542955326463,0.30357142857142855,1.0
question-answering,4,Word - by - Word Perspective,model,Word-by-Word Perspective,0,117,35,1,0,model : Word-by-Word Perspective,0.4020618556701031,0.3125,0.125
question-answering,4,"The first step in building the word - by - word perspective is to transform word vectors from a text sentence , question , and answer through respective neural functions .",model,Word-by-Word Perspective,0,118,36,2,0,model : Word-by-Word Perspective,0.4054982817869416,0.32142857142857145,0.25
question-answering,4,"For the text , ? RD and f is again the leaky ReLU .",model,Word-by-Word Perspective,0,119,37,3,0,model : Word-by-Word Perspective,0.40893470790378006,0.33035714285714285,0.375
question-answering,4,"For the text , ? RD and f is again the leaky ReLU .",model,Word-by-Word Perspective,0,120,38,4,0,model : Word-by-Word Perspective,0.41237113402061853,0.3392857142857143,0.5
question-answering,4,"For the text , ? RD and f is again the leaky ReLU .",model,Word-by-Word Perspective,0,121,39,5,0,model : Word-by-Word Perspective,0.41580756013745707,0.3482142857142857,0.625
question-answering,4,We transform the question and the answer toq m and n analogously using distinct matrices and bias vectors .,model,Word-by-Word Perspective,0,122,40,6,0,model : Word-by-Word Perspective,0.41924398625429554,0.35714285714285715,0.75
question-answering,4,"In contrast with the semantic perspective , we keep the question and answer candidates separate in the wordby - word perspective .",model,Word-by-Word Perspective,0,123,41,7,0,model : Word-by-Word Perspective,0.422680412371134,0.36607142857142855,0.875
question-answering,4,"This is because matches to answer words are inherently more important than matches to question words , and we want our model to learn to use this property .",model,Word-by-Word Perspective,0,124,42,8,0,model : Word-by-Word Perspective,0.4261168384879725,0.375,1.0
question-answering,4,Sentential,model,Sentential,0,125,43,1,0,model : Sentential,0.42955326460481097,0.38392857142857145,0.09090909090909091
question-answering,4,"Inspired by the work of in paraphrase detection , we compute matches between hypotheses and text sentences at the word level .",model,Sentential,0,126,44,2,0,model : Sentential,0.4329896907216495,0.39285714285714285,0.18181818181818182
question-answering,4,This computation uses the cosine similarity as before :,model,Sentential,0,127,45,3,0,model : Sentential,0.436426116838488,0.4017857142857143,0.2727272727272727
question-answering,4,"ca kn = cos (t k , n ) .",model,Sentential,0,128,46,4,0,model : Sentential,0.43986254295532645,0.4107142857142857,0.36363636363636365
question-answering,4,The word - by - word match between a text sentence and question is determined by taking the maximum over k ( finding the text word that best matches each question word ) and then taking a weighted mean over m ( finding the average match over the full question ) :,model,Sentential,0,129,47,5,0,model : Sentential,0.44329896907216493,0.41964285714285715,0.45454545454545453
question-answering,4,"Here , ? m is the word weight for the question word and Z normalizes these weights to sum to one over the question .",model,Sentential,0,130,48,6,0,model : Sentential,0.44673539518900346,0.42857142857142855,0.5454545454545454
question-answering,4,"Here , ? m is the word weight for the question word and Z normalizes these weights to sum to one over the question .",model,Sentential,0,131,49,7,0,model : Sentential,0.45017182130584193,0.4375,0.6363636363636364
question-answering,4,"We define the match between a sentence and answer candidate , M a , analogously .",model,Sentential,0,132,50,8,0,model : Sentential,0.4536082474226804,0.44642857142857145,0.7272727272727273
question-answering,4,"Finally , we combine the matches to question and answer according to",model,Sentential,0,133,51,9,0,model : Sentential,0.4570446735395189,0.45535714285714285,0.8181818181818182
question-answering,4,Here the ? are trainable parameters that control the relative importance of the terms .,model,Sentential,0,134,52,10,0,model : Sentential,0.46048109965635736,0.4642857142857143,0.9090909090909091
question-answering,4,Here the ? are trainable parameters that control the relative importance of the terms .,model,Sentential,0,135,53,11,0,model : Sentential,0.4639175257731959,0.4732142857142857,1.0
question-answering,4,Sequential Sliding Window,model,Sequential Sliding Window,0,136,54,1,0,model : Sequential Sliding Window,0.46735395189003437,0.48214285714285715,0.09090909090909091
question-answering,4,The sequential sliding window is related to the original MCTest baseline by .,model,Sequential Sliding Window,0,137,55,2,0,model : Sequential Sliding Window,0.47079037800687284,0.49107142857142855,0.18181818181818182
question-answering,4,"Our sliding window decays from its focus word according to a Gaussian distribution , which we extend by assigning a trainable weight to each location in the window .",model,Sequential Sliding Window,0,138,56,3,0,model : Sequential Sliding Window,0.4742268041237113,0.5,0.2727272727272727
question-answering,4,This modification enables the window to use information about the distance between word matches ; the original baseline used distance information through a predefined function .,model,Sequential Sliding Window,0,139,57,4,0,model : Sequential Sliding Window,0.47766323024054985,0.5089285714285714,0.36363636363636365
question-answering,4,"The sliding window scans over the words of the text as one continuous sequence , without sentence breaks .",model,Sequential Sliding Window,0,140,58,5,0,model : Sequential Sliding Window,0.48109965635738833,0.5178571428571429,0.45454545454545453
question-answering,4,"Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .",model,Sequential Sliding Window,0,141,59,6,0,model : Sequential Sliding Window,0.4845360824742268,0.5267857142857143,0.5454545454545454
question-answering,4,"This weight is based on a word 's position in the window , which , given a window , depends on its global position k.",model,Sequential Sliding Window,0,142,60,7,0,model : Sequential Sliding Window,0.4879725085910653,0.5357142857142857,0.6363636363636364
question-answering,4,The cosine similarity is adapted as,model,Sequential Sliding Window,0,143,61,8,0,model : Sequential Sliding Window,0.49140893470790376,0.5446428571428571,0.7272727272727273
question-answering,4,for the question and analogously for the answer .,model,Sequential Sliding Window,0,144,62,9,0,model : Sequential Sliding Window,0.4948453608247423,0.5535714285714286,0.8181818181818182
question-answering,4,We initialize the location weights with a Gaussian and fine - tune them during training .,model,Sequential Sliding Window,0,145,63,10,0,model : Sequential Sliding Window,0.49828178694158076,0.5625,0.9090909090909091
question-answering,4,"The final matching score , denoted as M sws , is computed as in and with sq km replacing c q km .",model,Sequential Sliding Window,0,146,64,11,0,model : Sequential Sliding Window,0.5017182130584192,0.5714285714285714,1.0
question-answering,4,Dependency Sliding Window,model,Dependency Sliding Window,0,147,65,1,0,model : Dependency Sliding Window,0.5051546391752577,0.5803571428571429,0.04
question-answering,4,"The dependency sliding window operates identically to the linear sliding window , but on a different view of the text passage .",model,Dependency Sliding Window,0,148,66,2,0,model : Dependency Sliding Window,0.5085910652920962,0.5892857142857143,0.08
question-answering,4,The output of this component is M swd and is formed analogously to M sws .,model,Dependency Sliding Window,0,149,67,3,0,model : Dependency Sliding Window,0.5120274914089347,0.5982142857142857,0.12
question-answering,4,The dependency perspective uses the Stanford Dependency Parser as an auxiliary tool .,model,Dependency Sliding Window,0,150,68,4,0,model : Dependency Sliding Window,0.5154639175257731,0.6071428571428571,0.16
question-answering,4,"Thus , the dependency graph can be considered a fixed feature .",model,Dependency Sliding Window,0,151,69,5,0,model : Dependency Sliding Window,0.5189003436426117,0.6160714285714286,0.2
question-answering,4,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .",model,Dependency Sliding Window,0,152,70,6,0,model : Dependency Sliding Window,0.5223367697594502,0.625,0.24
question-answering,4,"However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .",model,Dependency Sliding Window,0,153,71,7,0,model : Dependency Sliding Window,0.5257731958762887,0.6339285714285714,0.28
question-answering,4,"Specifically , we run the Stanford Dependency Parser on each text sentence to build a dependency graph .",model,Dependency Sliding Window,0,154,72,8,0,model : Dependency Sliding Window,0.5292096219931272,0.6428571428571429,0.32
question-answering,4,"This graph has n w vertices , one for each word in the sentence .",model,Dependency Sliding Window,0,155,73,9,0,model : Dependency Sliding Window,0.5326460481099656,0.6517857142857143,0.36
question-answering,4,From the dependency graph we form the Laplacian matrix L ? R nwnw and determine its eigenvectors .,model,Dependency Sliding Window,0,156,74,10,0,model : Dependency Sliding Window,0.5360824742268041,0.6607142857142857,0.4
question-answering,4,From the dependency graph we form the Laplacian matrix L ? R nwnw and determine its eigenvectors .,model,Dependency Sliding Window,0,157,75,11,0,model : Dependency Sliding Window,0.5395189003436426,0.6696428571428571,0.44
question-answering,4,The second eigenvector u 2 of the Laplacian is known as the Fiedler vector .,model,Dependency Sliding Window,0,158,76,12,0,model : Dependency Sliding Window,0.5429553264604811,0.6785714285714286,0.48
question-answering,4,It is the solution to the minimization,model,Dependency Sliding Window,0,159,77,13,0,model : Dependency Sliding Window,0.5463917525773195,0.6875,0.52
question-answering,4,"where vi are the vertices of the graph , and ? ij is the weight of the edge from vertex i to vertex j.",model,Dependency Sliding Window,0,160,78,14,0,model : Dependency Sliding Window,0.5498281786941581,0.6964285714285714,0.56
question-answering,4,"where vi are the vertices of the graph , and ? ij is the weight of the edge from vertex i to vertex j.",model,Dependency Sliding Window,0,161,79,15,0,model : Dependency Sliding Window,0.5532646048109966,0.7053571428571429,0.6
question-answering,4,"The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close , modulated by the connection weights .",model,Dependency Sliding Window,0,162,80,16,0,model : Dependency Sliding Window,0.5567010309278351,0.7142857142857143,0.64
question-answering,4,1,model,Dependency Sliding Window,0,163,81,17,0,model : Dependency Sliding Window,0.5601374570446735,0.7232142857142857,0.68
question-answering,4,This enables us to reorder the words of a sentence based on their proximity in the dependency graph .,model,Dependency Sliding Window,0,164,82,18,0,model : Dependency Sliding Window,0.563573883161512,0.7321428571428571,0.72
question-answering,4,The reordering of the words is given by the ordered index set I = arg sort ( u 2 ) .,model,Dependency Sliding Window,0,165,83,19,0,model : Dependency Sliding Window,0.5670103092783505,0.7410714285714286,0.76
question-answering,4,"To give an example of how this works , consider the following sentence from MCTest and its dependency - based reordering :",model,Dependency Sliding Window,0,166,84,20,0,model : Dependency Sliding Window,0.570446735395189,0.75,0.8
question-answering,4,"Jenny , Mrs. Mustard 's helper , called the police .",model,Dependency Sliding Window,0,167,85,21,0,model : Dependency Sliding Window,0.5738831615120275,0.7589285714285714,0.84
question-answering,4,"the police , called Jenny helper , Mrs. 's Mustard .",model,Dependency Sliding Window,0,168,86,22,0,model : Dependency Sliding Window,0.5773195876288659,0.7678571428571429,0.88
question-answering,4,Sliding - window - based matching on the original sentence will answer the question,model,Dependency Sliding Window,0,169,87,23,0,model : Dependency Sliding Window,0.5807560137457045,0.7767857142857143,0.92
question-answering,4,Who called the police ? with Mrs. Mustard .,model,Dependency Sliding Window,0,170,88,24,0,model : Dependency Sliding Window,0.584192439862543,0.7857142857142857,0.96
question-answering,4,"The dependency reordering enables the window to determine the correct answer , Jenny .",model,Dependency Sliding Window,0,171,89,25,0,model : Dependency Sliding Window,0.5876288659793815,0.7946428571428571,1.0
question-answering,4,Combining Distributed,model,Combining Distributed Evidence,0,172,90,1,0,model : Combining Distributed Evidence,0.5910652920962199,0.8035714285714286,0.06666666666666667
question-answering,4,Evidence,model,Combining Distributed Evidence,0,173,91,2,0,model : Combining Distributed Evidence,0.5945017182130584,0.8125,0.13333333333333333
question-answering,4,It is important in comprehension to synthesize information found throughout a document .,model,Combining Distributed Evidence,0,174,92,3,0,model : Combining Distributed Evidence,0.5979381443298969,0.8214285714285714,0.2
question-answering,4,"MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone , but would instead require some form of inference or limited reasoning .",model,Combining Distributed Evidence,0,175,93,4,0,model : Combining Distributed Evidence,0.6013745704467354,0.8303571428571429,0.26666666666666666
question-answering,4,It therefore includes questions where the evidence for an answer spans several sentences .,model,Combining Distributed Evidence,0,176,94,5,0,model : Combining Distributed Evidence,0.6048109965635738,0.8392857142857143,0.3333333333333333
question-answering,4,"To perform synthesis , our model also takes in ngrams of sentences , i.e. , sentence pairs and triples strung together .",model,Combining Distributed Evidence,0,177,95,6,0,model : Combining Distributed Evidence,0.6082474226804123,0.8482142857142857,0.4
question-answering,4,"The model treats these exactly as it does single sentences , applying all functions detailed above .",model,Combining Distributed Evidence,0,178,96,7,0,model : Combining Distributed Evidence,0.6116838487972509,0.8571428571428571,0.4666666666666667
question-answering,4,later pooling operation combines scores across all n-grams ( including the singlesentence input ) .,model,Combining Distributed Evidence,0,179,97,8,0,model : Combining Distributed Evidence,0.6151202749140894,0.8660714285714286,0.5333333333333333
question-answering,4,This is described in the next subsection .,model,Combining Distributed Evidence,0,180,98,9,0,model : Combining Distributed Evidence,0.6185567010309279,0.875,0.6
question-answering,4,"With n-grams , the model can combine information distributed across contiguous sentences .",model,Combining Distributed Evidence,0,181,99,10,0,model : Combining Distributed Evidence,0.6219931271477663,0.8839285714285714,0.6666666666666666
question-answering,4,"In some cases , however , the required evidence is spread across distant sentences .",model,Combining Distributed Evidence,0,182,100,11,0,model : Combining Distributed Evidence,0.6254295532646048,0.8928571428571429,0.7333333333333333
question-answering,4,"To give our model some capacity to deal with this scenario , we take the top N sentences as scored by all the preceding functions , and then repeat the scoring computations viewing these top N as a single sentence .",model,Combining Distributed Evidence,0,183,101,12,0,model : Combining Distributed Evidence,0.6288659793814433,0.9017857142857143,0.8
question-answering,4,The reasoning behind these approaches can be explained well in a probabilistic setting .,model,Combining Distributed Evidence,0,184,102,13,0,model : Combining Distributed Evidence,0.6323024054982818,0.9107142857142857,0.8666666666666667
question-answering,4,"If we consider our similarity scores to model the likelihood of a text sentence given a hypothesis , p (t j |h i ) , then the n-gram and top N approaches model a joint probability p (t j 1 , t j 2 , . . . , t j k |h i ) .",model,Combining Distributed Evidence,0,185,103,14,0,model : Combining Distributed Evidence,0.6357388316151202,0.9196428571428571,0.9333333333333333
question-answering,4,We can not model the joint probability as a product of individual terms ( score values ) because distributed pieces of evidence are likely not independent .,model,Combining Distributed Evidence,0,186,104,15,0,model : Combining Distributed Evidence,0.6391752577319587,0.9285714285714286,1.0
question-answering,4,Combining Perspectives,model,Combining Perspectives,0,187,105,1,0,model : Combining Perspectives,0.6426116838487973,0.9375,0.125
question-answering,4,"We use a multilayer perceptron to combine M sem , M word , M swd , and M sws as a final matching score M i for each answer candidate .",model,Combining Perspectives,0,188,106,2,0,model : Combining Perspectives,0.6460481099656358,0.9464285714285714,0.25
question-answering,4,"This network also pools and combines the separate n-gram scores , and uses a linear activation function .",model,Combining Perspectives,0,189,107,3,0,model : Combining Perspectives,0.6494845360824743,0.9553571428571429,0.375
question-answering,4,Our over all training objective is to minimize the ranking loss,model,Combining Perspectives,0,190,108,4,0,model : Combining Perspectives,0.6529209621993127,0.9642857142857143,0.5
question-answering,4,"where is a constant margin , i * indexes the correct answer , and we take the maximum over i so that we are ranking the correct answer over the best - ranked incorrect answer ( of which there are three ) .",model,Combining Perspectives,0,191,109,5,0,model : Combining Perspectives,0.6563573883161512,0.9732142857142857,0.625
question-answering,4,This approach worked better than comparing the correct answer to the incorrect answers individually as in .,model,Combining Perspectives,0,192,110,6,0,model : Combining Perspectives,0.6597938144329897,0.9821428571428571,0.75
question-answering,4,"Our implementation of the Parallel - Hierarchical model , using the Keras framework , is available on Github .",model,Combining Perspectives,0,193,111,7,0,model : Combining Perspectives,0.6632302405498282,0.9910714285714286,0.875
question-answering,4,2,model,Combining Perspectives,0,194,112,8,0,model : Combining Perspectives,0.6666666666666666,1.0,1.0
question-answering,4,Training Wheels,training,Training Wheels,0,195,1,1,0,training : Training Wheels,0.6701030927835051,0.09090909090909091,0.09090909090909091
question-answering,4,"Before training , we initialized the neural - network components of our model to perform sensible heuristic functions .",training,Training Wheels,0,196,2,2,0,training : Training Wheels,0.6735395189003437,0.18181818181818182,0.18181818181818182
question-answering,4,Training did not converge on the small MCTest without this vital approach .,training,Training Wheels,0,197,3,3,0,training : Training Wheels,0.6769759450171822,0.2727272727272727,0.2727272727272727
question-answering,4,"Empirically , we found that we could achieve above 50 % accuracy on MCTest using a simple sum of word vectors followed by a dot product between the question sum and the hypothesis sum .",training,Training Wheels,0,198,4,4,0,training : Training Wheels,0.6804123711340206,0.36363636363636365,0.36363636363636365
question-answering,4,"Therefore , we initialized the network for the semantic perspective to perform this sum , by initializing A x as the identity matrix and bx A as the zero vector , x ? {t , h} .",training,Training Wheels,0,199,5,5,0,training : Training Wheels,0.6838487972508591,0.45454545454545453,0.45454545454545453
question-answering,4,Recall that the activation function is a ReLU so that positive outputs are unchanged .,training,Training Wheels,0,200,6,6,0,training : Training Wheels,0.6872852233676976,0.5454545454545454,0.5454545454545454
question-answering,4,"We also found basic word - matching scores to be helpful , so we initialized the word - by - word networks likewise .",training,Training Wheels,0,201,7,7,0,training : Training Wheels,0.6907216494845361,0.6363636363636364,0.6363636363636364
question-answering,4,"The network for perspectivecombination was initialized to perform a sum of individual scores , using a zero bias - vector and a weight matrix of ones , since we found that each perspective contributed positively to the over all result .",training,Training Wheels,0,202,8,8,0,training : Training Wheels,0.6941580756013745,0.7272727272727273,0.7272727272727273
question-answering,4,This training wheels approach is related to other techniques from the literature .,training,Training Wheels,0,203,9,9,0,training : Training Wheels,0.697594501718213,0.8181818181818182,0.8181818181818182
question-answering,4,"For instance , proposed the identity - matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation .",training,Training Wheels,0,204,10,10,0,training : Training Wheels,0.7010309278350515,0.9090909090909091,0.9090909090909091
question-answering,4,"In residual networks , shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model .",training,Training Wheels,0,205,11,11,0,training : Training Wheels,0.7044673539518901,1.0,1.0
question-answering,4,Experiments,experiment,Experiments,0,206,1,1,0,experiment : Experiments,0.7079037800687286,1.0,1.0
question-answering,4,The Dataset,dataset,The Dataset,0,207,1,1,0,dataset : The Dataset,0.711340206185567,0.125,0.125
question-answering,4,"MCTest is a collection of 660 elementary - level children 's stories and associated questions , written by human subjects .",dataset,The Dataset,0,208,2,2,0,dataset : The Dataset,0.7147766323024055,0.25,0.25
question-answering,4,"The stories are fictional , ensuring that the answer must be found in the text itself , and carefully limited to what a young child can understand .",dataset,The Dataset,0,209,3,3,0,dataset : The Dataset,0.718213058419244,0.375,0.375
question-answering,4,The more challenging variant consists of 500 stories with four multiple - choice questions each .,dataset,The Dataset,0,210,4,4,0,dataset : The Dataset,0.7216494845360825,0.5,0.5
question-answering,4,"Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .",dataset,The Dataset,0,211,5,5,0,dataset : The Dataset,0.7250859106529209,0.625,0.625
question-answering,4,MCTest is challenging because it is both complicated and small .,dataset,The Dataset,0,212,6,6,0,dataset : The Dataset,0.7285223367697594,0.75,0.75
question-answering,4,"As per , "" it is very difficult to train statistical models only on MCTest . """,dataset,The Dataset,0,213,7,7,0,dataset : The Dataset,0.7319587628865979,0.875,0.875
question-answering,4,"It s size limits the number of parameters that can be trained , and prevents learning any complex language modeling simultaneously with the capacity to answer questions .",dataset,The Dataset,0,214,8,8,0,dataset : The Dataset,0.7353951890034365,1.0,1.0
question-answering,4,Training and Model Details,training,Training and Model Details,0,215,1,1,0,training : Training and Model Details,0.738831615120275,0.038461538461538464,0.038461538461538464
question-answering,4,In this section we describe important details of the training procedure and model setup .,training,Training and Model Details,0,216,2,2,0,training : Training and Model Details,0.7422680412371134,0.07692307692307693,0.07692307692307693
question-answering,4,"For a complete list of hyperparameter settings , our stopword list , and other minutiae , we refer interested readers to our Github repository .",training,Training and Model Details,0,217,3,3,0,training : Training and Model Details,0.7457044673539519,0.11538461538461539,0.11538461538461539
question-answering,4,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .",training,Training and Model Details,1,218,4,4,0,training : Training and Model Details,0.7491408934707904,0.15384615384615385,0.15384615384615385
question-answering,4,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .",training,Training and Model Details,0,219,5,5,0,training : Training and Model Details,0.7525773195876289,0.19230769230769232,0.19230769230769232
question-answering,4,The vectors are 300 - dimensional ( d = 300 ) .,training,Training and Model Details,0,220,6,6,0,training : Training and Model Details,0.7560137457044673,0.23076923076923078,0.23076923076923078
question-answering,4,"We do not use a stopword list for the text passage , instead relying on the trainable word weights to ascribe global importance ratings to words .",training,Training and Model Details,0,221,7,7,0,training : Training and Model Details,0.7594501718213058,0.2692307692307692,0.2692307692307692
question-answering,4,These weights are initialized with the inverse document frequency ( IDF ) statistic computed over the MCTest corpus .,training,Training and Model Details,0,222,8,8,0,training : Training and Model Details,0.7628865979381443,0.3076923076923077,0.3076923076923077
question-answering,4,3,training,Training and Model Details,0,223,9,9,0,training : Training and Model Details,0.7663230240549829,0.34615384615384615,0.34615384615384615
question-answering,4,"However , we douse a short stopword list for questions .",training,Training and Model Details,0,224,10,10,0,training : Training and Model Details,0.7697594501718213,0.38461538461538464,0.38461538461538464
question-answering,4,"This list nullifies query words such as { Who , what , when , where , how} , along with conjugations of the verbs to do and to be .",training,Training and Model Details,0,225,11,11,0,training : Training and Model Details,0.7731958762886598,0.4230769230769231,0.4230769230769231
question-answering,4,"Following earlier methods , we use a heuristic to improve performance on negation questions .",training,Training and Model Details,0,226,12,12,0,training : Training and Model Details,0.7766323024054983,0.46153846153846156,0.46153846153846156
question-answering,4,"When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .",training,Training and Model Details,0,227,13,13,0,training : Training and Model Details,0.7800687285223368,0.5,0.5
question-answering,4,The most important technique for training the model was the training wheels approach .,training,Training and Model Details,0,228,14,14,0,training : Training and Model Details,0.7835051546391752,0.5384615384615384,0.5384615384615384
question-answering,4,"Without this , training was not effective at all .",training,Training and Model Details,0,229,15,15,0,training : Training and Model Details,0.7869415807560137,0.5769230769230769,0.5769230769230769
question-answering,4,The identity initialization requires that the network weight matrices are square ( d = D ) .,training,Training and Model Details,0,230,16,16,0,training : Training and Model Details,0.7903780068728522,0.6153846153846154,0.6153846153846154
question-answering,4,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",training,Training and Model Details,1,231,17,17,0,training : Training and Model Details,0.7938144329896907,0.6538461538461539,0.6538461538461539
question-answering,4,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .",training,Training and Model Details,0,232,18,18,0,training : Training and Model Details,0.7972508591065293,0.6923076923076923,0.6923076923076923
question-answering,4,Our best performing model held networks at the wordby - word level fixed .,training,Training and Model Details,0,233,19,19,0,training : Training and Model Details,0.8006872852233677,0.7307692307692307,0.7307692307692307
question-answering,4,"For combining distributed evidence , we used up to trigrams over sentences and our bestperforming model reiterated over the top two sentences ( N = 2 ) .",training,Training and Model Details,0,234,20,20,0,training : Training and Model Details,0.8041237113402062,0.7692307692307693,0.7692307692307693
question-answering,4,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .",training,Training and Model Details,1,235,21,21,0,training : Training and Model Details,0.8075601374570447,0.8076923076923077,0.8076923076923077
question-answering,4,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,training,Training and Model Details,1,236,22,22,0,training : Training and Model Details,0.8109965635738832,0.8461538461538461,0.8461538461538461
question-answering,4,"MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .",training,Training and Model Details,0,237,23,23,0,training : Training and Model Details,0.8144329896907216,0.8846153846153846,0.8846153846153846
question-answering,4,presents the performance of featureengineered and neural methods on the MCTest test set .,training,Training and Model Details,0,238,24,24,0,training : Training and Model Details,0.8178694158075601,0.9230769230769231,0.9230769230769231
question-answering,4,"Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .",training,Training and Model Details,0,239,25,25,0,training : Training and Model Details,0.8213058419243986,0.9615384615384616,0.9615384615384616
question-answering,4,"Clearly , MCTest - 160 is easier .",training,Training and Model Details,0,240,26,26,0,training : Training and Model Details,0.8247422680412371,1.0,1.0
question-answering,4,Results,result,Results,0,241,1,1,0,result : Results,0.8281786941580757,0.06666666666666667,0.5
question-answering,4,The first three rows represent featureengineered methods .,result,Results,0,242,2,2,0,result : Results,0.8316151202749141,0.13333333333333333,1.0
question-answering,4,+,result,Analysis and Discussion,0,243,3,1,0,result : Analysis and Discussion,0.8350515463917526,0.2,0.07692307692307693
question-answering,4,RTE is the best - performing variant of the original baseline published along with MCTest .,result,Analysis and Discussion,0,244,4,2,0,result : Analysis and Discussion,0.8384879725085911,0.26666666666666666,0.15384615384615385
question-answering,4,"It uses a lexical sliding window and distance - based measure , augmented with rules for recognizing textual entailment .",result,Analysis and Discussion,0,245,5,3,0,result : Analysis and Discussion,0.8419243986254296,0.3333333333333333,0.23076923076923078
question-answering,4,We described the methods of and in Section 3 .,result,Analysis and Discussion,0,246,6,4,0,result : Analysis and Discussion,0.845360824742268,0.4,0.3076923076923077
question-answering,4,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ? 1 % ) .",result,Analysis and Discussion,1,247,7,5,0,result : Analysis and Discussion,0.8487972508591065,0.4666666666666667,0.38461538461538464
question-answering,4,The method of achieves the best over all result on MCTest - 160 .,result,Analysis and Discussion,0,248,8,6,0,result : Analysis and Discussion,0.852233676975945,0.5333333333333333,0.46153846153846156
question-answering,4,We suspect this is because our neural method suffered from the relative lack of training data .,result,Analysis and Discussion,0,249,9,7,0,result : Analysis and Discussion,0.8556701030927835,0.6,0.5384615384615384
question-answering,4,The last four rows in are neural methods that we discussed in Section 3 .,result,Analysis and Discussion,0,250,10,8,0,result : Analysis and Discussion,0.8591065292096219,0.6666666666666666,0.6153846153846154
question-answering,4,Performance measures are taken from .,result,Analysis and Discussion,0,251,11,9,0,result : Analysis and Discussion,0.8625429553264605,0.7333333333333333,0.6923076923076923
question-answering,4,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,result,Analysis and Discussion,0,252,12,10,0,result : Analysis and Discussion,0.865979381443299,0.8,0.7692307692307693
question-answering,4,"The Neural Reasoner and the Attentive Reader are large , deep models with hundreds of thousands of parameters , so it is unsurprising that they performed poorly on MCTest .",result,Analysis and Discussion,0,253,13,11,0,result : Analysis and Discussion,0.8694158075601375,0.8666666666666667,0.8461538461538461
question-answering,4,"The specificallydesigned HABCNN fared better , its convolutional architecture cutting down on the parameter count .",result,Analysis and Discussion,0,254,14,12,0,result : Analysis and Discussion,0.872852233676976,0.9333333333333333,0.9230769230769231
question-answering,4,"Because there are similarities between our model and the HABCNN , we hypothesize that much of the performance difference is attributable to our training wheels methodology .",result,Analysis and Discussion,0,255,15,13,0,result : Analysis and Discussion,0.8762886597938144,1.0,1.0
question-answering,4,Analysis and Discussion,analysis,analysis,0,256,1,1,0,analysis : analysis,0.8797250859106529,0.03571428571428571,0.03571428571428571
question-answering,4,We measure the contribution of each component of the model by ablating it .,analysis,analysis,0,257,2,2,0,analysis : analysis,0.8831615120274914,0.07142857142857142,0.07142857142857142
question-answering,4,Results are given in .,analysis,analysis,0,258,3,3,0,analysis : analysis,0.8865979381443299,0.10714285714285714,0.10714285714285714
question-answering,4,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .",analysis,analysis,1,259,4,4,0,analysis : analysis,0.8900343642611683,0.14285714285714285,0.14285714285714285
question-answering,4,"Without this , the model has almost no Method MCTest - 160 accuracy ( % )",analysis,analysis,0,260,5,5,0,analysis : analysis,0.8934707903780069,0.17857142857142858,0.17857142857142858
question-answering,4,MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .,analysis,analysis,0,261,6,6,0,analysis : analysis,0.8969072164948454,0.21428571428571427,0.21428571428571427
question-answering,4,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",analysis,analysis,1,262,7,7,0,analysis : analysis,0.9003436426116839,0.25,0.25
question-answering,4,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",analysis,analysis,1,263,8,8,0,analysis : analysis,0.9037800687285223,0.2857142857142857,0.2857142857142857
question-answering,4,Simple word - by - word matching is obviously useful on MCTest .,analysis,analysis,1,264,9,9,0,analysis : analysis,0.9072164948453608,0.32142857142857145,0.32142857142857145
question-answering,4,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .",analysis,analysis,1,265,10,10,0,analysis : analysis,0.9106529209621993,0.35714285714285715,0.35714285714285715
question-answering,4,"On the other hand , the dependency - based sliding window makes only a minor contribution .",analysis,analysis,1,266,11,11,0,analysis : analysis,0.9140893470790378,0.39285714285714285,0.39285714285714285
question-answering,4,We found this surprising .,analysis,analysis,0,267,12,12,0,analysis : analysis,0.9175257731958762,0.42857142857142855,0.42857142857142855
question-answering,4,It maybe that linearization of the dependency graph removes too much of its information .,analysis,analysis,0,268,13,13,0,analysis : analysis,0.9209621993127147,0.4642857142857143,0.4642857142857143
question-answering,4,"Finally , the exogenous word weights make a significant contribution of almost 5 % .",analysis,analysis,1,269,14,14,0,analysis : analysis,0.9243986254295533,0.5,0.5
question-answering,4,"Analysis reveals that most of our system 's test failures occur on questions about quantity ( e.g. , How many ...? ) and temporal order ( e.g. , Who was invited last ? ) .",analysis,analysis,0,270,15,15,0,analysis : analysis,0.9278350515463918,0.5357142857142857,0.5357142857142857
question-answering,4,"Quantity questions makeup 9.5 % of our errors on the validation set , while order questions makeup 10.3 % .",analysis,analysis,0,271,16,16,0,analysis : analysis,0.9312714776632303,0.5714285714285714,0.5714285714285714
question-answering,4,"This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .",analysis,analysis,0,272,17,17,0,analysis : analysis,0.9347079037800687,0.6071428571428571,0.6071428571428571
question-answering,4,"Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .",analysis,analysis,0,273,18,18,0,analysis : analysis,0.9381443298969072,0.6428571428571429,0.6428571428571429
question-answering,4,The Parallel - Hierarchical model is simple .,analysis,analysis,0,274,19,19,0,analysis : analysis,0.9415807560137457,0.6785714285714286,0.6785714285714286
question-answering,4,It does no complex language or sequence modeling .,analysis,analysis,0,275,20,20,0,analysis : analysis,0.9450171821305842,0.7142857142857143,0.7142857142857143
question-answering,4,It s simplicity is a response to the limited data of MCTest .,analysis,analysis,0,276,21,21,0,analysis : analysis,0.9484536082474226,0.75,0.75
question-answering,4,"Nevertheless , the model achieves stateof - the - art results on the multi questions , which ( putatively ) require some limited reasoning .",analysis,analysis,0,277,22,22,0,analysis : analysis,0.9518900343642611,0.7857142857142857,0.7857142857142857
question-answering,4,Our model is able to handle them reasonably well just by stringing important sentences together .,analysis,analysis,0,278,23,23,0,analysis : analysis,0.9553264604810997,0.8214285714285714,0.8214285714285714
question-answering,4,"Thus , the model imitates reasoning with a heuristic .",analysis,analysis,0,279,24,24,0,analysis : analysis,0.9587628865979382,0.8571428571428571,0.8571428571428571
question-answering,4,"This suggests that , to learn true reasoning abilities , MCTest is too simple a dataset - and it is almost certainly too small for this goal .",analysis,analysis,0,280,25,25,0,analysis : analysis,0.9621993127147767,0.8928571428571429,0.8928571428571429
question-answering,4,"However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .",analysis,analysis,0,281,26,26,0,analysis : analysis,0.9656357388316151,0.9285714285714286,0.9285714285714286
question-answering,4,"If so , the Parallel - Hierarchical model is a good start on the former .",analysis,analysis,0,282,27,27,0,analysis : analysis,0.9690721649484536,0.9642857142857143,0.9642857142857143
question-answering,4,"Indeed , if we train the method exclusively on single questions then its results become even more impressive : we can achieve a test accuracy of 79.1 % on MCTest - 500 .",analysis,analysis,0,283,28,28,0,analysis : analysis,0.9725085910652921,1.0,1.0
question-answering,4,Conclusion,conclusion,Conclusion,0,284,1,1,0,conclusion : Conclusion,0.9759450171821306,0.125,0.125
question-answering,4,"We have presented the novel Parallel - Hierarchical model for machine comprehension , and evaluated it on the small but complex MCTest .",conclusion,Conclusion,0,285,2,2,0,conclusion : Conclusion,0.979381443298969,0.25,0.25
question-answering,4,"Our model achieves state - of - the - art results , outperforming several feature - engineered and neural approaches .",conclusion,Conclusion,0,286,3,3,0,conclusion : Conclusion,0.9828178694158075,0.375,0.375
question-answering,4,"Working with our model has emphasized to us the following ( not necessarily novel ) concepts , which we record hereto promote further empirical validation .",conclusion,Conclusion,0,287,4,4,0,conclusion : Conclusion,0.9862542955326461,0.5,0.5
question-answering,4,Good comprehension of language is supported by hierarchical levels of understanding ( Cf. ) .,conclusion,Conclusion,0,288,5,5,0,conclusion : Conclusion,0.9896907216494846,0.625,0.625
question-answering,4,Exogenous attention ( the trainable word weights ) maybe broadly helpful for NLP .,conclusion,Conclusion,0,289,6,6,0,conclusion : Conclusion,0.993127147766323,0.75,0.75
question-answering,4,"The training wheels approach , that is , initializing neural networks to perform sensible heuristics , appears helpful for small datasets .",conclusion,Conclusion,0,290,7,7,0,conclusion : Conclusion,0.9965635738831615,0.875,0.875
question-answering,4,"Reasoning over language is challenging , but easily simulated in some cases .",conclusion,Conclusion,0,291,8,8,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,5,Iterative Alternating Neural Attention for Machine Reading,title,title,1,2,1,1,0,title : title,0.00904977375565611,1.0,1.0
question-answering,5,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.013574660633484163,0.25,0.25
question-answering,5,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.01809954751131222,0.5,0.5
question-answering,5,"Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02262443438914027,0.75,0.75
question-answering,5,Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.027149321266968326,1.0,1.0
question-answering,5,Introduction,introduction,introduction,0,7,1,1,0,introduction : introduction,0.03167420814479638,0.043478260869565216,0.043478260869565216
question-answering,5,"Recently , the idea of training machine comprehension models that can read , understand , and answer questions about a text has come closer to reality principally through two factors .",introduction,introduction,0,8,2,2,0,introduction : introduction,0.03619909502262444,0.08695652173913043,0.08695652173913043
question-answering,5,"The first is the advent of deep learning techniques , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data .",introduction,introduction,0,9,3,3,0,introduction : introduction,0.04072398190045249,0.13043478260869565,0.13043478260869565
question-answering,5,"The second factor is the formulation of standard machine comprehension benchmarks based on Cloze - style queries , which permit fast integration loops between model conception and experimental evaluation .",introduction,introduction,0,10,4,4,0,introduction : introduction,0.04524886877828054,0.17391304347826086,0.17391304347826086
question-answering,5,Cloze - style queries are created by deleting a particular word in a natural - language statement .,introduction,introduction,0,11,5,5,0,introduction : introduction,0.049773755656108594,0.21739130434782608,0.21739130434782608
question-answering,5,The task is to guess which word was deleted .,introduction,introduction,0,12,6,6,0,introduction : introduction,0.05429864253393665,0.2608695652173913,0.2608695652173913
question-answering,5,"In a pragmatic approach , recent work formed such questions by extracting a sentence from a larger document .",introduction,introduction,0,13,7,7,0,introduction : introduction,0.058823529411764705,0.30434782608695654,0.30434782608695654
question-answering,5,"In contrast to considering a stand - alone statement , the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word .",introduction,introduction,0,14,8,8,0,introduction : introduction,0.06334841628959276,0.34782608695652173,0.34782608695652173
question-answering,5,Such contextual dependencies may also be injected by removing a word from a short human - crafted summary of a larger body of text .,introduction,introduction,0,15,9,9,0,introduction : introduction,0.06787330316742081,0.391304347826087,0.391304347826087
question-answering,5,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,introduction,introduction,0,16,10,10,0,introduction : introduction,0.07239819004524888,0.43478260869565216,0.43478260869565216
question-answering,5,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",introduction,introduction,0,17,11,11,0,introduction : introduction,0.07692307692307693,0.4782608695652174,0.4782608695652174
question-answering,5,The missing word is assumed to appear in the document .,introduction,introduction,0,18,12,12,0,introduction : introduction,0.08144796380090498,0.5217391304347826,0.5217391304347826
question-answering,5,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .",introduction,introduction,1,19,13,13,0,introduction : introduction,0.08597285067873303,0.5652173913043478,0.5652173913043478
question-answering,5,The model first reads the document and the query using a recurrent neural network .,introduction,introduction,1,20,14,14,0,introduction : introduction,0.09049773755656108,0.6086956521739131,0.6086956521739131
question-answering,5,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .",introduction,introduction,1,21,15,15,0,introduction : introduction,0.09502262443438914,0.6521739130434783,0.6521739130434783
question-answering,5,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .",introduction,introduction,1,22,16,16,0,introduction : introduction,0.09954751131221719,0.6956521739130435,0.6956521739130435
question-answering,5,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,introduction,introduction,1,23,17,17,0,introduction : introduction,0.10407239819004525,0.7391304347826086,0.7391304347826086
question-answering,5,"This permits our model to reason about different parts of the query in a sequential way , based on the information that has been gathered previously from the document .",introduction,introduction,0,24,18,18,0,introduction : introduction,0.1085972850678733,0.782608695652174,0.782608695652174
question-answering,5,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .",introduction,introduction,1,25,19,19,0,introduction : introduction,0.11312217194570136,0.8260869565217391,0.8260869565217391
question-answering,5,This paper makes the following contributions .,introduction,introduction,0,26,20,20,0,introduction : introduction,0.11764705882352941,0.8695652173913043,0.8695652173913043
question-answering,5,"We present a novel iterative , alternating attention mechanism that , unlike existing models , does not compress the query to a single representation , but instead alternates its attention between the query and the document to obtain a fine - grained query representation within a fixed computation time .",introduction,introduction,0,27,21,21,0,introduction : introduction,0.12217194570135746,0.9130434782608695,0.9130434782608695
question-answering,5,Our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes .,introduction,introduction,0,28,22,22,0,introduction : introduction,0.12669683257918551,0.9565217391304348,0.9565217391304348
question-answering,5,It obtains state - of - theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks .,introduction,introduction,0,29,23,23,0,introduction : introduction,0.13122171945701358,1.0,1.0
question-answering,5,Task Description,system description,Task Description,0,30,1,1,0,system description : Task Description,0.13574660633484162,0.014492753623188406,0.07142857142857142
question-answering,5,One of the advantages of using Cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention .,system description,Task Description,0,31,2,2,0,system description : Task Description,0.14027149321266968,0.028985507246376812,0.14285714285714285
question-answering,5,The CBT and corpora are two such datasets .,system description,Task Description,0,32,3,3,0,system description : Task Description,0.14479638009049775,0.043478260869565216,0.21428571428571427
question-answering,5,The CBT 1 corpus was generated from well - known children 's books available through Project Gutenberg .,system description,Task Description,0,33,4,4,0,system description : Task Description,0.1493212669683258,0.057971014492753624,0.2857142857142857
question-answering,5,Documents consist of 20 - sentence excerpts from these books .,system description,Task Description,0,34,5,5,0,system description : Task Description,0.15384615384615385,0.07246376811594203,0.35714285714285715
question-answering,5,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,system description,Task Description,0,35,6,6,0,system description : Task Description,0.1583710407239819,0.08695652173913043,0.42857142857142855
question-answering,5,The dataset is divided into four subsets depending on the type of the word replaced .,system description,Task Description,0,36,7,7,0,system description : Task Description,0.16289592760180996,0.10144927536231885,0.5
question-answering,5,"The subsets are named entity , common noun , verb , and preposition .",system description,Task Description,0,37,8,8,0,system description : Task Description,0.167420814479638,0.11594202898550725,0.5714285714285714
question-answering,5,"We will focus our evaluation solely on the first two subsets , i.e. CBT - NE ( named entity ) and CBT - CN ( common nouns ) , since the latter two are relatively simple as demonstrated by .",system description,Task Description,0,38,9,9,0,system description : Task Description,0.17194570135746606,0.13043478260869565,0.6428571428571429
question-answering,5,The CNN 2 corpus was generated from news articles available through the CNN website .,system description,Task Description,0,39,10,10,0,system description : Task Description,0.17647058823529413,0.14492753623188406,0.7142857142857143
question-answering,5,"The documents are given by the full articles themselves , which are accompanied by short , bullet - point summary statements .",system description,Task Description,0,40,11,11,0,system description : Task Description,0.18099547511312217,0.15942028985507245,0.7857142857142857
question-answering,5,"Instead of extracting a query from the articles themselves , the authors replace a named entity within each article summary with an anonymous placeholder token .",system description,Task Description,0,41,12,12,0,system description : Task Description,0.18552036199095023,0.17391304347826086,0.8571428571428571
question-answering,5,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ? A is",system description,Task Description,0,42,13,13,0,system description : Task Description,0.19004524886877827,0.18840579710144928,0.9285714285714286
question-answering,5,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ? A is",system description,Task Description,0,43,14,14,0,system description : Task Description,0.19457013574660634,0.2028985507246377,1.0
question-answering,5,Alternating Iterative Attention,system description,Alternating Iterative Attention,0,44,15,1,0,system description : Alternating Iterative Attention,0.19909502262443438,0.21739130434782608,0.125
question-answering,5,Our model is represented in .,system description,Alternating Iterative Attention,0,45,16,2,0,system description : Alternating Iterative Attention,0.20361990950226244,0.2318840579710145,0.25
question-answering,5,It s workflow has three steps .,system description,Alternating Iterative Attention,0,46,17,3,0,system description : Alternating Iterative Attention,0.2081447963800905,0.2463768115942029,0.375
question-answering,5,"First is the encoding phase , in which we compute a set of vector representations , acting as a memory of the content of the input document and query .",system description,Alternating Iterative Attention,0,47,18,4,0,system description : Alternating Iterative Attention,0.21266968325791855,0.2608695652173913,0.5
question-answering,5,"Next , the inference phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful .",system description,Alternating Iterative Attention,0,48,19,5,0,system description : Alternating Iterative Attention,0.2171945701357466,0.2753623188405797,0.625
question-answering,5,"To accomplish this , we use an iterative process that , at each iteration , alternates attentive memory accesses to the query and the document .",system description,Alternating Iterative Attention,0,49,20,6,0,system description : Alternating Iterative Attention,0.22171945701357465,0.2898550724637681,0.75
question-answering,5,"Finally , the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer .",system description,Alternating Iterative Attention,0,50,21,7,0,system description : Alternating Iterative Attention,0.22624434389140272,0.30434782608695654,0.875
question-answering,5,We describe each of the phases in the following sections .,system description,Alternating Iterative Attention,0,51,22,8,0,system description : Alternating Iterative Attention,0.23076923076923078,0.3188405797101449,1.0
question-answering,5,Bidirectional Encoding,system description,Bidirectional Encoding,0,52,23,1,0,system description : Bidirectional Encoding,0.23529411764705882,0.3333333333333333,0.045454545454545456
question-answering,5,"The input to the encoding phase is a sequence of words X = ( x 1 , . . . , x | X | ) , such as a document or a query , drawn from a vocabulary V .",system description,Bidirectional Encoding,0,53,24,2,0,system description : Bidirectional Encoding,0.2398190045248869,0.34782608695652173,0.09090909090909091
question-answering,5,Each word is represented by a continuous word embedding x ? Rd stored in a word embedding matrix X ? R | V |d .,system description,Bidirectional Encoding,0,54,25,3,0,system description : Bidirectional Encoding,0.24434389140271492,0.36231884057971014,0.13636363636363635
question-answering,5,Each word is represented by a continuous word embedding x ? Rd stored in a word embedding matrix X ? R | V |d .,system description,Bidirectional Encoding,0,55,26,4,0,system description : Bidirectional Encoding,0.248868778280543,0.37681159420289856,0.18181818181818182
question-answering,5,The sequence X is processed using a recurrent neural network encoder with gated recurrent units ( GRU ) .,system description,Bidirectional Encoding,0,56,27,5,0,system description : Bidirectional Encoding,0.25339366515837103,0.391304347826087,0.22727272727272727
question-answering,5,"For each position i in the input sequence , the GRU takes as input the word embedding xi and updates a hidden Figure 1 : Our model first encodes the query and the document by means of bidirectional GRU networks .",system description,Bidirectional Encoding,0,57,28,6,0,system description : Bidirectional Encoding,0.2579185520361991,0.4057971014492754,0.2727272727272727
question-answering,5,"Then , it deploys an iterative inference mechanism that alternates between attending query encodings ( 1 ) and document encodings ( 2 ) given the query attended state .",system description,Bidirectional Encoding,0,58,29,7,0,system description : Bidirectional Encoding,0.26244343891402716,0.42028985507246375,0.3181818181818182
question-answering,5,The results of the alternating attention is gated and fed back into the inference GRU .,system description,Bidirectional Encoding,0,59,30,8,0,system description : Bidirectional Encoding,0.2669683257918552,0.43478260869565216,0.36363636363636365
question-answering,5,"Even if the encodings are computed only once , the query representation is dynamic and changes throughout the inference process .",system description,Bidirectional Encoding,0,60,31,9,0,system description : Bidirectional Encoding,0.27149321266968324,0.4492753623188406,0.4090909090909091
question-answering,5,"After a fixed number of steps T , the weights of the document attention are used to estimate the probability of the answer P ( a|Q , D ) .",system description,Bidirectional Encoding,0,61,32,10,0,system description : Bidirectional Encoding,0.27601809954751133,0.463768115942029,0.45454545454545453
question-answering,5,by :,system description,Bidirectional Encoding,0,62,33,11,0,system description : Bidirectional Encoding,0.28054298642533937,0.4782608695652174,0.5
question-answering,5,"where hi , r i and u i ? Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ? R hd , H {r , u , h} ? R hh are the parameters of the GRU , ? is the sigmoid function and is the elementwise multiplication .",system description,Bidirectional Encoding,0,63,34,12,0,system description : Bidirectional Encoding,0.2850678733031674,0.4927536231884058,0.5454545454545454
question-answering,5,"where hi , r i and u i ? Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ? R hd , H {r , u , h} ? R hh are the parameters of the GRU , ? is the sigmoid function and is the elementwise multiplication .",system description,Bidirectional Encoding,0,64,35,13,0,system description : Bidirectional Encoding,0.2895927601809955,0.5072463768115942,0.5909090909090909
question-answering,5,"where hi , r i and u i ? Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ? R hd , H {r , u , h} ? R hh are the parameters of the GRU , ? is the sigmoid function and is the elementwise multiplication .",system description,Bidirectional Encoding,0,65,36,14,0,system description : Bidirectional Encoding,0.29411764705882354,0.5217391304347826,0.6363636363636364
question-answering,5,"where hi , r i and u i ? Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ? R hd , H {r , u , h} ? R hh are the parameters of the GRU , ? is the sigmoid function and is the elementwise multiplication .",system description,Bidirectional Encoding,0,66,37,15,0,system description : Bidirectional Encoding,0.2986425339366516,0.5362318840579711,0.6818181818181818
question-answering,5,"where hi , r i and u i ? Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ? R hd , H {r , u , h} ? R hh are the parameters of the GRU , ? is the sigmoid function and is the elementwise multiplication .",system description,Bidirectional Encoding,0,67,38,16,0,system description : Bidirectional Encoding,0.3031674208144796,0.5507246376811594,0.7272727272727273
question-answering,5,The hidden state hi acts as a representation of the word x i in the context of the preceding sequence inputs x < i .,system description,Bidirectional Encoding,0,68,39,17,0,system description : Bidirectional Encoding,0.3076923076923077,0.5652173913043478,0.7727272727272727
question-answering,5,"In order to incorporate information from the future tokens x > i , we choose to process the sequence in reverse with an additional GRU .",system description,Bidirectional Encoding,0,69,40,18,0,system description : Bidirectional Encoding,0.31221719457013575,0.5797101449275363,0.8181818181818182
question-answering,5,"Therefore , the encoding phase maps each token xi to a contextual representation given by the concatenation of the forward and backward GRU hidden",system description,Bidirectional Encoding,0,70,41,19,0,system description : Bidirectional Encoding,0.3167420814479638,0.5942028985507246,0.8636363636363636
question-answering,5,We denote byq i ? R 2h andd i ? R 2h the contextual encodings for word i in the query Q and the document D respectively .,system description,Bidirectional Encoding,0,71,42,20,0,system description : Bidirectional Encoding,0.3212669683257919,0.6086956521739131,0.9090909090909091
question-answering,5,We denote byq i ? R 2h andd i ? R 2h the contextual encodings for word i in the query Q and the document D respectively .,system description,Bidirectional Encoding,0,72,43,21,0,system description : Bidirectional Encoding,0.3257918552036199,0.6231884057971014,0.9545454545454546
question-answering,5,We denote byq i ? R 2h andd i ? R 2h the contextual encodings for word i in the query Q and the document D respectively .,system description,Bidirectional Encoding,0,73,44,22,0,system description : Bidirectional Encoding,0.33031674208144796,0.6376811594202898,1.0
question-answering,5,Iterative Alternating Attention,system description,Iterative Alternating Attention,0,74,45,1,0,system description : Iterative Alternating Attention,0.334841628959276,0.6521739130434783,0.058823529411764705
question-answering,5,This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer .,system description,Iterative Alternating Attention,0,75,46,2,0,system description : Iterative Alternating Attention,0.3393665158371041,0.6666666666666666,0.11764705882352941
question-answering,5,The inference is modelled by an additional recurrent GRU network .,system description,Iterative Alternating Attention,0,76,47,3,0,system description : Iterative Alternating Attention,0.3438914027149321,0.6811594202898551,0.17647058823529413
question-answering,5,The recurrent network iteratively performs an alternating search step to gather information that maybe useful to predict the answer .,system description,Iterative Alternating Attention,0,77,48,4,0,system description : Iterative Alternating Attention,0.34841628959276016,0.6956521739130435,0.23529411764705882
question-answering,5,"In particular , at each time step : ( 1 ) it performs an attentive read on the query encodings , resulting in a query glimpse , qt , and ( 2 ) given the current query glimpse , it extracts a conditional document glimpse , d t , representing the parts of the document thatare relevant to the current query glimpse .",system description,Iterative Alternating Attention,0,78,49,5,0,system description : Iterative Alternating Attention,0.35294117647058826,0.7101449275362319,0.29411764705882354
question-answering,5,"In turn , both attentive reads are conditioned on the previous hidden state of the inference GRU s t ?1 , summarizing the information that has been gathered from the query and the document up to time t.",system description,Iterative Alternating Attention,0,79,50,6,0,system description : Iterative Alternating Attention,0.3574660633484163,0.7246376811594203,0.35294117647058826
question-answering,5,The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process .,system description,Iterative Alternating Attention,0,80,51,7,0,system description : Iterative Alternating Attention,0.36199095022624433,0.7391304347826086,0.4117647058823529
question-answering,5,Query Attentive Read,system description,Iterative Alternating Attention,0,81,52,8,0,system description : Iterative Alternating Attention,0.3665158371040724,0.7536231884057971,0.47058823529411764
question-answering,5,"Given the query encodings {q i } , we formulate a query glimpse qt at timestep t by :",system description,Iterative Alternating Attention,0,82,53,9,0,system description : Iterative Alternating Attention,0.37104072398190047,0.7681159420289855,0.5294117647058824
question-answering,5,"where q i , tare the query attention weights and A q ? R 2hs , where sis the dimensionality of the inference GRU state , and a q ? R 2 h .",system description,Iterative Alternating Attention,0,83,54,10,0,system description : Iterative Alternating Attention,0.3755656108597285,0.782608695652174,0.5882352941176471
question-answering,5,"where q i , tare the query attention weights and A q ? R 2hs , where sis the dimensionality of the inference GRU state , and a q ? R 2 h .",system description,Iterative Alternating Attention,0,84,55,11,0,system description : Iterative Alternating Attention,0.38009049773755654,0.7971014492753623,0.6470588235294118
question-answering,5,"where q i , tare the query attention weights and A q ? R 2hs , where sis the dimensionality of the inference GRU state , and a q ? R 2 h .",system description,Iterative Alternating Attention,0,85,56,12,0,system description : Iterative Alternating Attention,0.38461538461538464,0.8115942028985508,0.7058823529411765
question-answering,5,"The attention we use here is similar to the formulation used in , but with two differences .",system description,Iterative Alternating Attention,0,86,57,13,0,system description : Iterative Alternating Attention,0.3891402714932127,0.8260869565217391,0.7647058823529411
question-answering,5,"First , we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step .",system description,Iterative Alternating Attention,0,87,58,14,0,system description : Iterative Alternating Attention,0.3936651583710407,0.8405797101449275,0.8235294117647058
question-answering,5,This simple bilinear attention has been successfully used in .,system description,Iterative Alternating Attention,0,88,59,15,0,system description : Iterative Alternating Attention,0.39819004524886875,0.855072463768116,0.8823529411764706
question-answering,5,"Second , we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t?1 .",system description,Iterative Alternating Attention,0,89,60,16,0,system description : Iterative Alternating Attention,0.40271493212669685,0.8695652173913043,0.9411764705882353
question-answering,5,This is similar to what is achieved by the original attention mechanism proposed in without the burden of the additional tanh layer .,system description,Iterative Alternating Attention,0,90,61,17,0,system description : Iterative Alternating Attention,0.4072398190045249,0.8840579710144928,1.0
question-answering,5,Document Attentive Read,system description,Document Attentive Read,0,91,62,1,0,system description : Document Attentive Read,0.4117647058823529,0.8985507246376812,0.125
question-answering,5,The alternating attention continues by probing the document given the current query glimpse qt .,system description,Document Attentive Read,0,92,63,2,0,system description : Document Attentive Read,0.416289592760181,0.9130434782608695,0.25
question-answering,5,"In particular , the document attention weights are computed based on both the previous search state and the currently selected query glimpse qt :",system description,Document Attentive Read,0,93,64,3,0,system description : Document Attentive Read,0.42081447963800905,0.927536231884058,0.375
question-answering,5,"where d i , tare the attention weights for each word in the document and A d ? R 2 h ( s + 2h ) and ad ? R 2 h .",system description,Document Attentive Read,0,94,65,4,0,system description : Document Attentive Read,0.4253393665158371,0.9420289855072463,0.5
question-answering,5,"where d i , tare the attention weights for each word in the document and A d ? R 2 h ( s + 2h ) and ad ? R 2 h .",system description,Document Attentive Read,0,95,66,5,0,system description : Document Attentive Read,0.4298642533936652,0.9565217391304348,0.625
question-answering,5,"where d i , tare the attention weights for each word in the document and A d ? R 2 h ( s + 2h ) and ad ? R 2 h .",system description,Document Attentive Read,0,96,67,6,0,system description : Document Attentive Read,0.4343891402714932,0.9710144927536232,0.75
question-answering,5,Note that the document attention is also conditioned on s t?1 .,system description,Document Attentive Read,0,97,68,7,0,system description : Document Attentive Read,0.43891402714932126,0.9855072463768116,0.875
question-answering,5,"This allows the model to perform transitive reasoning on the document side , i.e. to use previously obtained document information to bias future attended locations , which is particularly important for natural language inference tasks .",system description,Document Attentive Read,0,98,69,8,0,system description : Document Attentive Read,0.4434389140271493,1.0,1.0
question-answering,5,Gating Search Results,result,Gating Search Results,0,99,1,1,0,result : Gating Search Results,0.4479638009049774,0.05555555555555555,0.07692307692307693
question-answering,5,"In order to update its recurrent state , the inference GRU may evolve on the basis of the information gathered from the current inference step , i.e.",result,Gating Search Results,0,100,2,2,0,result : Gating Search Results,0.45248868778280543,0.1111111111111111,0.15384615384615385
question-answering,5,"However , the current query glimpse maybe too general or the document may not contain the information specified in the query glimpse , i.e. the query or the document attention weights maybe nearly uniform .",result,Gating Search Results,0,101,3,3,0,result : Gating Search Results,0.45701357466063347,0.16666666666666666,0.23076923076923078
question-answering,5,We include a gating mechanism that is designed to reset the current query and document glimpses in the case that the current search is not fruitful .,result,Gating Search Results,0,102,4,4,0,result : Gating Search Results,0.46153846153846156,0.2222222222222222,0.3076923076923077
question-answering,5,"Formally , we implement a gating mech -",result,Gating Search Results,0,103,5,5,0,result : Gating Search Results,0.4660633484162896,0.2777777777777778,0.38461538461538464
question-answering,5,where is the element - wise multiplication and g :,result,Gating Search Results,0,104,6,6,0,result : Gating Search Results,0.47058823529411764,0.3333333333333333,0.46153846153846156
question-answering,5,s+6h ? R 2 h .,result,Gating Search Results,0,105,7,7,0,result : Gating Search Results,0.4751131221719457,0.3888888888888889,0.5384615384615384
question-answering,5,s+6h ? R 2 h .,result,Gating Search Results,0,106,8,8,0,result : Gating Search Results,0.4796380090497738,0.4444444444444444,0.6153846153846154
question-answering,5,The gate g takes the form of a 2 - layer feed - forward network with sigmoid output unit activation .,result,Gating Search Results,0,107,9,9,0,result : Gating Search Results,0.4841628959276018,0.5,0.6923076923076923
question-answering,5,"The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses , making it easier to determine the degree of matching between them .",result,Gating Search Results,0,108,10,10,0,result : Gating Search Results,0.48868778280542985,0.5555555555555556,0.7692307692307693
question-answering,5,"Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. , .",result,Gating Search Results,0,109,11,11,0,result : Gating Search Results,0.49321266968325794,0.6111111111111112,0.8461538461538461
question-answering,5,"Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. , .",result,Gating Search Results,0,110,12,12,0,result : Gating Search Results,0.497737556561086,0.6666666666666666,0.9230769230769231
question-answering,5,"Intuitively , the model reviews the query glimpse with respect to the contents of the document glimpse and vice versa .",result,Gating Search Results,0,111,13,13,0,result : Gating Search Results,0.502262443438914,0.7222222222222222,1.0
question-answering,5,Answer Prediction,result,Answer Prediction,0,112,14,1,0,result : Answer Prediction,0.5067873303167421,0.7777777777777778,0.2
question-answering,5,"After a fixed number of time - steps T , the document attention weights obtained in the last search step d i , T are used to predict the probability of the answer given the document and the query P ( a|Q , D ) .",result,Answer Prediction,0,113,15,2,0,result : Answer Prediction,0.5113122171945701,0.8333333333333334,0.4
question-answering,5,"Formally , we follow and apply the "" pointer - sum "" loss :",result,Answer Prediction,0,114,16,3,0,result : Answer Prediction,0.5158371040723982,0.8888888888888888,0.6
question-answering,5,"where I ( a , D ) is a set of positions where a occurs in the document .",result,Answer Prediction,0,115,17,4,0,result : Answer Prediction,0.5203619909502263,0.9444444444444444,0.8
question-answering,5,"The model is trained to maximize log P ( a|Q , D ) over the training corpus .",result,Answer Prediction,0,116,18,5,0,result : Answer Prediction,0.5248868778280543,1.0,1.0
question-answering,5,Training Details,training,Training Details,0,117,1,1,0,training : Training Details,0.5294117647058824,0.07692307692307693,0.07692307692307693
question-answering,5,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",training,Training Details,1,118,2,2,0,training : Training Details,0.5339366515837104,0.15384615384615385,0.15384615384615385
question-answering,5,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",training,Training Details,1,119,3,3,0,training : Training Details,0.5384615384615384,0.23076923076923078,0.23076923076923078
question-answering,5,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .",training,Training Details,1,120,4,4,0,training : Training Details,0.5429864253393665,0.3076923076923077,0.3076923076923077
question-answering,5,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",training,Training Details,1,121,5,5,0,training : Training Details,0.5475113122171946,0.38461538461538464,0.38461538461538464
question-answering,5,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",training,Training Details,1,122,6,6,0,training : Training Details,0.5520361990950227,0.46153846153846156,0.46153846153846156
question-answering,5,the inputs to both the query and the document attention mechanisms .,training,Training Details,0,123,7,7,0,training : Training Details,0.5565610859728507,0.5384615384615384,0.5384615384615384
question-answering,5,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .",training,Training Details,0,124,8,8,0,training : Training Details,0.5610859728506787,0.6153846153846154,0.6153846153846154
question-answering,5,"Our model is implemented in Theano , using the Keras library .",training,Training Details,1,125,9,9,0,training : Training Details,0.5656108597285068,0.6923076923076923,0.6923076923076923
question-answering,5,"Computational Complexity Similar to previous state - of - the - art models which use a bidirectional encoder , the major bottleneck of our method is computing the document and query encodings .",training,Training Details,0,126,10,10,0,training : Training Details,0.5701357466063348,0.7692307692307693,0.7692307692307693
question-answering,5,"The alternating attention mechanism runs only for a fixed number of steps ( T = 8 in our tests ) , which is orders of magnitude smaller than a typical document or query in our datasets ( see ) .",training,Training Details,0,127,11,11,0,training : Training Details,0.5746606334841629,0.8461538461538461,0.8461538461538461
question-answering,5,The repeated attentions each require a softmax over ? 1000 locations which is typically fast on recent GPU architectures .,training,Training Details,0,128,12,12,0,training : Training Details,0.579185520361991,0.9230769230769231,0.9230769230769231
question-answering,5,"Thus , our computation cost is comparable to , but we outperform the latter models on the datasets tested .",training,Training Details,0,129,13,13,0,training : Training Details,0.583710407239819,1.0,1.0
question-answering,5,Results,result,Results,0,130,1,1,0,result : Results,0.5882352941176471,0.25,0.3333333333333333
question-answering,5,"We report the results of our model on the CBT - CN , CBT - NE and CNN datasets , previously described in Section 2 . reports our results on the CBT - CN and CBT - NE dataset .",result,Results,0,131,2,2,0,result : Results,0.5927601809954751,0.5,0.6666666666666666
question-answering,5,"The Humans , LSTMs and Memory Networks ( Mem NNs ) results are taken from and the Attention - Sum Reader ( AS Reader ) is a state - of - the - art result recently obtained by .",result,Results,0,132,3,3,0,result : Results,0.5972850678733032,0.75,1.0
question-answering,5,CBT,result,CBT,1,133,4,1,0,result : CBT,0.6018099547511312,1.0,1.0
question-answering,5,Main result,result,result,0,134,1,1,0,result : result,0.6063348416289592,0.02702702702702703,0.06666666666666667
question-answering,5,Our model ( line 7 ) sets a new stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,result,result,1,135,2,2,0,result : result,0.6108597285067874,0.05405405405405406,0.13333333333333333
question-answering,5,This performance gap is only partially reflected on the CBT - NE dataset .,result,result,0,136,3,3,0,result : result,0.6153846153846154,0.08108108108108109,0.2
question-answering,5,"We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set , which sits on par with the best baseline .",result,result,0,137,4,4,0,result : result,0.6199095022624435,0.10810810810810811,0.26666666666666666
question-answering,5,"In CBT - NE , the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun .",result,result,0,138,5,5,0,result : result,0.6244343891402715,0.13513513513513514,0.3333333333333333
question-answering,5,We found that approximatively 27.5 % of validation examples and 29.6 % of test examples contain an answer that has never been predicted in the training set .,result,result,0,139,6,6,0,result : result,0.6289592760180995,0.16216216216216217,0.4
question-answering,5,"These numbers are considerably lower for the CBT - CN , for which only 2.5 % and 4.6 % of validation and test examples respectively contain an answer that has not been previously seen .",result,result,0,140,7,7,0,result : result,0.6334841628959276,0.1891891891891892,0.4666666666666667
question-answering,5,Ensembles Fusing multiple models generally achieves better generalization .,result,result,0,141,8,8,0,result : result,0.6380090497737556,0.21621621621621623,0.5333333333333333
question-answering,5,"In order to investigate whether this could help achieving better held - out performance on CBT - NE , we adopt a simple strategy and average the predictions of 5 models trained with different random seeds ( line 9 , 3 from and 4 from .",result,result,0,142,9,9,0,result : result,0.6425339366515838,0.24324324324324326,0.6
question-answering,5,improvements over the single model and sits at 74.1 on validation and 71.0 on test .,result,result,0,143,10,10,0,result : result,0.6470588235294118,0.2702702702702703,0.6666666666666666
question-answering,5,Fixed query attention,result,result,0,144,11,11,0,result : result,0.6515837104072398,0.2972972972972973,0.7333333333333333
question-answering,5,"In order to measure the impact of the query attention step in our model , we constrain the query attention weights q i , t to be uniform , i.e. q i , t = 1 / | Q | , for all t = 1 , . . . , T ( line 6 ) .",result,result,0,145,12,12,0,result : result,0.6561085972850679,0.32432432432432434,0.8
question-answering,5,This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work .,result,result,0,146,13,13,0,result : result,0.6606334841628959,0.35135135135135137,0.8666666666666667
question-answering,5,"By comparing line 6 and line 7 , we see that the query attention mechanism allows improvements up to 2.3 points in validation and 4.9 points in test with respect to fixing the query representation throughout the search process .",result,result,0,147,14,14,0,result : result,0.665158371040724,0.3783783783783784,0.9333333333333333
question-answering,5,similar scenario was observed on the CNN dataset ..,result,result,0,148,15,15,0,result : result,0.669683257918552,0.40540540540540543,1.0
question-answering,5,CNN,result,CNN,1,149,16,1,0,result : CNN,0.6742081447963801,0.43243243243243246,1.0
question-answering,5,Main result,result,Main result,0,150,17,1,0,result : Main result,0.6787330316742082,0.4594594594594595,0.047619047619047616
question-answering,5,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,result,Main result,1,151,18,2,0,result : Main result,0.6832579185520362,0.4864864864864865,0.09523809523809523
question-answering,5,We also report the very recent results of the Stanford AR system that came to our attention during the writeup of this article ) ( line 9 ) .,result,Main result,0,152,19,3,0,result : Main result,0.6877828054298643,0.5135135135135135,0.14285714285714285
question-answering,5,Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test .,result,Main result,0,153,20,4,0,result : Main result,0.6923076923076923,0.5405405405405406,0.19047619047619047
question-answering,5,We note that the latter comparison maybe influenced by different training and initialization strategies .,result,Main result,0,154,21,5,0,result : Main result,0.6968325791855203,0.5675675675675675,0.23809523809523808
question-answering,5,"First , Stanford AS uses Glo Ve embeddings , pre-trained from a large external corpus .",result,Main result,0,155,22,6,0,result : Main result,0.7013574660633484,0.5945945945945946,0.2857142857142857
question-answering,5,"Second , the system normalizes the output probabilities only over the candidate answers in the document .",result,Main result,0,156,23,7,0,result : Main result,0.7058823529411765,0.6216216216216216,0.3333333333333333
question-answering,5,Ensembles,result,Main result,0,157,24,8,0,result : Main result,0.7104072398190046,0.6486486486486487,0.38095238095238093
question-answering,5,We also report the results using ensembled models .,result,Main result,0,158,25,9,0,result : Main result,0.7149321266968326,0.6756756756756757,0.42857142857142855
question-answering,5,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",result,Main result,1,159,26,10,0,result : Main result,0.7194570135746606,0.7027027027027027,0.47619047619047616
question-answering,5,Category analysis classified a sample of 100 CNN stories based on the type of inference required to guess the answer .,result,Main result,0,160,27,11,0,result : Main result,0.7239819004524887,0.7297297297297297,0.5238095238095238
question-answering,5,"Categories that only require local context matching around the placeholder and the answer in the text are Exact Match , Paraphrasing , and Partial Clue , while those which require higher reasoning skills are Multiple Sentences and Ambiguous .",result,Main result,0,161,28,12,0,result : Main result,0.7285067873303167,0.7567567567567568,0.5714285714285714
question-answering,5,"For example , in Exact Match examples , the question placeholder and the answer in the document share several neighboring exact words .",result,Main result,0,162,29,13,0,result : Main result,0.7330316742081447,0.7837837837837838,0.6190476190476191
question-answering,5,Category - specific results are reported in : Per-category performance of the Stanford AR and our system .,result,Main result,0,163,30,14,0,result : Main result,0.7375565610859729,0.8108108108108109,0.6666666666666666
question-answering,5,"The first three categories require local context matching , the next two global context matching and coreference errors are unanswerable questions .",result,Main result,0,164,31,15,0,result : Main result,0.7420814479638009,0.8378378378378378,0.7142857142857143
question-answering,5,"tackled by the neural models , which perform similarly .",result,Main result,0,165,32,16,0,result : Main result,0.746606334841629,0.8648648648648649,0.7619047619047619
question-answering,5,It seems that the iterative alternating attention inference is better able to solve more difficult examples such as Ambiguous / Hard .,result,Main result,0,166,33,17,0,result : Main result,0.751131221719457,0.8918918918918919,0.8095238095238095
question-answering,5,"One hypothesis is that , in contrast to Stanford AR , which uses only one fixedquery attention step , our iterative attention may better explore the documents and queries .",result,Main result,0,167,34,18,0,result : Main result,0.755656108597285,0.918918918918919,0.8571428571428571
question-answering,5,"Finally , Coreference Errors ( ? 25 % of the corpus ) includes examples with critical coreference resolution errors which may make the questions "" unanswerable "" .",result,Main result,0,168,35,19,0,result : Main result,0.7601809954751131,0.9459459459459459,0.9047619047619048
question-answering,5,This is a barrier to achieving accuracies considerably above 75 % .,result,Main result,0,169,36,20,0,result : Main result,0.7647058823529411,0.972972972972973,0.9523809523809523
question-answering,5,"If this estimate is accurate , our ensemble model ( 76.1 % ) maybe approaching near-optimal performance on this dataset .",result,Main result,0,170,37,21,0,result : Main result,0.7692307692307693,1.0,1.0
question-answering,5,Discussion,discussion,Discussion,0,171,1,1,0,discussion : Discussion,0.7737556561085973,0.043478260869565216,0.043478260869565216
question-answering,5,We inspect the query and document attention weights for an example article from the CNN dataset .,discussion,Discussion,0,172,2,2,0,discussion : Discussion,0.7782805429864253,0.08695652173913043,0.08695652173913043
question-answering,5,"The title of the article is "" Dante turns in his grave as Italian language declines "" , and it discusses the decline of Italian language in schools .",discussion,Discussion,0,173,3,3,0,discussion : Discussion,0.7828054298642534,0.13043478260869565,0.13043478260869565
question-answering,5,"The plot is shown in . 2 , where locations attended to in the query and document are in the left and right column respectively .",discussion,Discussion,0,174,4,4,0,discussion : Discussion,0.7873303167420814,0.17391304347826086,0.17391304347826086
question-answering,5,"Each row corresponds to an inference timestep 1 ? t ? 8 . At the first step , the query attention focuses on the placeholder token , as its local context is generally important to discriminate the answer .",discussion,Discussion,0,175,5,5,0,discussion : Discussion,0.7918552036199095,0.21739130434782608,0.21739130434782608
question-answering,5,"Each row corresponds to an inference timestep 1 ? t ? 8 . At the first step , the query attention focuses on the placeholder token , as its local context is generally important to discriminate the answer .",discussion,Discussion,0,176,6,6,0,discussion : Discussion,0.7963800904977375,0.2608695652173913,0.2608695652173913
question-answering,5,"The model first focuses on @entity148 , which corresponds to "" Greek "" in this The approach to teaching @entity6 in @placeholder schools needs a makeover , she says : Visualization of the alternated attention mechanism for an article in CNN , treating about the decline of the Italian language in schools .",discussion,Discussion,0,177,7,7,0,discussion : Discussion,0.8009049773755657,0.30434782608695654,0.30434782608695654
question-answering,5,The title of the plot is the query .,discussion,Discussion,0,178,8,8,0,discussion : Discussion,0.8054298642533937,0.34782608695652173,0.34782608695652173
question-answering,5,Each row correspond to a timestep .,discussion,Discussion,0,179,9,9,0,discussion : Discussion,0.8099547511312217,0.391304347826087,0.391304347826087
question-answering,5,"The target is @entity3 which corresponds to the word "" Italian "" .",discussion,Discussion,0,180,10,10,0,discussion : Discussion,0.8144796380090498,0.43478260869565216,0.43478260869565216
question-answering,5,across document locations ) .,discussion,Discussion,0,181,11,11,0,discussion : Discussion,0.8190045248868778,0.4782608695652174,0.4782608695652174
question-answering,5,"At t = 2 , the query attention moves towards "" schools "" and the model hesitates between "" Italian "" and "" European Union "" ( @entity28 , see step 3 ) , both of which may satisfy the query .",discussion,Discussion,0,182,12,12,0,discussion : Discussion,0.8235294117647058,0.5217391304347826,0.5217391304347826
question-answering,5,"At step 3 , the most likely candidates are "" European Union "" and "" Rome "" ( @entity159 ) .",discussion,Discussion,0,183,13,13,0,discussion : Discussion,0.8280542986425339,0.5652173913043478,0.5652173913043478
question-answering,5,"As the timesteps unfold , the model learns that "" needs "" maybe important to infer the correct entity , i.e. "" Italian "" .",discussion,Discussion,0,184,14,14,0,discussion : Discussion,0.832579185520362,0.6086956521739131,0.6086956521739131
question-answering,5,"The query sits on the same attended location , while the document attention evolves to become more confident about the answer .",discussion,Discussion,0,185,15,15,0,discussion : Discussion,0.8371040723981901,0.6521739130434783,0.6521739130434783
question-answering,5,"We find that , across CBT and CNN examples , the query attention wanders near or focuses on the placeholder location , attempting to discriminate its identity using only local context .",discussion,Discussion,0,186,16,16,0,discussion : Discussion,0.8416289592760181,0.6956521739130435,0.6956521739130435
question-answering,5,"For these particular datasets , the majority of questions can be answered after attending only to the words directly neighbouring the placeholder .",discussion,Discussion,0,187,17,17,0,discussion : Discussion,0.8461538461538461,0.7391304347826086,0.7391304347826086
question-answering,5,"This aligns with the findings of concerning CNN , which state that the required reasoning and inference levels for this dataset are quite simple .",discussion,Discussion,0,188,18,18,0,discussion : Discussion,0.8506787330316742,0.782608695652174,0.782608695652174
question-answering,5,"It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words , and thereby necessitates deeper query exploration .",discussion,Discussion,0,189,19,19,0,discussion : Discussion,0.8552036199095022,0.8260869565217391,0.8260869565217391
question-answering,5,"Finally , across this work we fixed the number of inference steps T .",discussion,Discussion,0,190,20,20,0,discussion : Discussion,0.8597285067873304,0.8695652173913043,0.8695652173913043
question-answering,5,We found that using 8 timesteps works well consistently across the tested datasets .,discussion,Discussion,0,191,21,21,0,discussion : Discussion,0.8642533936651584,0.9130434782608695,0.9130434782608695
question-answering,5,"However , we hypothesize that more ( fewer ) timesteps would benefit harder ( easier ) examples .",discussion,Discussion,0,192,22,22,0,discussion : Discussion,0.8687782805429864,0.9565217391304348,0.9565217391304348
question-answering,5,straight - forward extension of the model would be to dynamically select the number of inference steps conditioned on each example .,discussion,Discussion,0,193,23,23,0,discussion : Discussion,0.8733031674208145,1.0,1.0
question-answering,5,Related Works,related work,Related Works,0,194,1,1,0,related work : Related Works,0.8778280542986425,0.05,0.05
question-answering,5,Neural attention models have been applied recently to a smrgsbord of machine learning and natural language processing problems .,related work,Related Works,0,195,2,2,0,related work : Related Works,0.8823529411764706,0.1,0.1
question-answering,5,"These include , but are not limited to , handwriting recognition , digit classification , machine translation , question answering and caption generation .",related work,Related Works,0,196,3,3,0,related work : Related Works,0.8868778280542986,0.15,0.15
question-answering,5,"In general , attention models keep a memory of states that can be accessed at will by learned attention policies .",related work,Related Works,0,197,4,4,0,related work : Related Works,0.8914027149321267,0.2,0.2
question-answering,5,"In our case , the memory is represented by the set of document and query contextual encodings .",related work,Related Works,0,198,5,5,0,related work : Related Works,0.8959276018099548,0.25,0.25
question-answering,5,"Our model is closely related to , which were also applied to question answering .",related work,Related Works,0,199,6,6,0,related work : Related Works,0.9004524886877828,0.3,0.3
question-answering,5,"The pointer - style attention mechanism that we use to perform the final answer prediction has been proposed by , which in turn was based on the earlier Pointer Networks of .",related work,Related Works,0,200,7,7,0,related work : Related Works,0.9049773755656109,0.35,0.35
question-answering,5,"However , differently from our work , perform only one attention step and embed the query into a single vector representation , corresponding to the concatenation of the last state of the forward and backward GRU networks .",related work,Related Works,0,201,8,8,0,related work : Related Works,0.9095022624434389,0.4,0.4
question-answering,5,"To our knowledge , embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models .",related work,Related Works,0,202,9,9,0,related work : Related Works,0.9140271493212669,0.45,0.45
question-answering,5,"In our model , the repeated , tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer , and then to focus on the parts of the document thatare most salient to the currently - attended query components .",related work,Related Works,0,203,10,10,0,related work : Related Works,0.918552036199095,0.5,0.5
question-answering,5,similar attempt in attending different components of the query maybe found in .,related work,Related Works,0,204,11,11,0,related work : Related Works,0.9230769230769231,0.55,0.55
question-answering,5,"In that model , the document is processed once for each query word .",related work,Related Works,0,205,12,12,0,related work : Related Works,0.9276018099547512,0.6,0.6
question-answering,5,"This can be computationally intractable for large documents , since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times .",related work,Related Works,0,206,13,13,0,related work : Related Works,0.9321266968325792,0.65,0.65
question-answering,5,"In contrast , our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps .",related work,Related Works,0,207,14,14,0,related work : Related Works,0.9366515837104072,0.7,0.7
question-answering,5,The inference network is responsible for making sense of the current attention step with respect to what has been gathered before .,related work,Related Works,0,208,15,15,0,related work : Related Works,0.9411764705882353,0.75,0.75
question-answering,5,"In addition to achieving state - of the - art performance , this technique may also prove to be more scalable than alternative query attention models .",related work,Related Works,0,209,16,16,0,related work : Related Works,0.9457013574660633,0.8,0.8
question-answering,5,"Finally , our iterative inference process shares similarities to the iterative hops in Memory Networks .",related work,Related Works,0,210,17,17,0,related work : Related Works,0.9502262443438914,0.85,0.85
question-answering,5,"In that model , the query representation is updated iteratively from hop to hop , although its different components are not attended to separately .",related work,Related Works,0,211,18,18,0,related work : Related Works,0.9547511312217195,0.9,0.9
question-answering,5,"Moreover , we substitute the simple linear update with a GRU network .",related work,Related Works,0,212,19,19,0,related work : Related Works,0.9592760180995475,0.95,0.95
question-answering,5,The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep .,related work,Related Works,0,213,20,20,0,related work : Related Works,0.9638009049773756,1.0,1.0
question-answering,5,Conclusion,conclusion,Conclusion,0,214,1,1,0,conclusion : Conclusion,0.9683257918552036,0.125,0.125
question-answering,5,We presented an iterative neural attention model and applied it to machine comprehension tasks .,conclusion,Conclusion,0,215,2,2,0,conclusion : Conclusion,0.9728506787330317,0.25,0.25
question-answering,5,"Our architecture deploys a novel alternating attention mechanism , and tightly integrates successful ideas from past works in machine reading comprehension to obtain state - of - the - art results on three datasets .",conclusion,Conclusion,0,216,3,3,0,conclusion : Conclusion,0.9773755656108597,0.375,0.375
question-answering,5,The iterative alternating attention mechanism continually refines its view of the query and document while aggregating the information required to answer a query .,conclusion,Conclusion,0,217,4,4,0,conclusion : Conclusion,0.9819004524886877,0.5,0.5
question-answering,5,Multiple future research directions maybe envisioned .,conclusion,Conclusion,0,218,5,5,0,conclusion : Conclusion,0.9864253393665159,0.625,0.625
question-answering,5,We plan to dynamically select the optimal number of inference steps required for each example .,conclusion,Conclusion,0,219,6,6,0,conclusion : Conclusion,0.9909502262443439,0.75,0.75
question-answering,5,"Moreover , we suspect that shifting towards stochastic attention should permit us to learn more interesting search policies .",conclusion,Conclusion,0,220,7,7,0,conclusion : Conclusion,0.995475113122172,0.875,0.875
question-answering,5,"Finally , we believe that our model is fully general and maybe applied in a straightforward way to other tasks such as information retrieval .",conclusion,Conclusion,0,221,8,8,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,6,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,title,title,1,2,1,1,0,title : title,0.006060606060606061,1.0,1.0
question-answering,6,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.00909090909090909,0.16666666666666666,0.16666666666666666
question-answering,6,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.012121212121212121,0.3333333333333333,0.3333333333333333
question-answering,6,"We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.015151515151515152,0.5,0.5
question-answering,6,"QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.01818181818181818,0.6666666666666666,0.6666666666666666
question-answering,6,"Our experiments show that QRN produces the state - of - the - art results in bAbI QA and dialog tasks , and in are al goal - oriented dialog dataset .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.021212121212121213,0.8333333333333334,0.8333333333333334
question-answering,6,"In addition , QRN formulation allows parallelization on RNN 's time axis , saving an order of magnitude in time complexity for training and inference .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.024242424242424242,1.0,1.0
question-answering,6,INTRODUCTION,introduction,introduction,0,9,1,1,0,introduction : introduction,0.02727272727272727,0.03125,0.03125
question-answering,6,"In this paper , we address the problem of question answering ( QA ) when reasoning over multiple facts is required .",introduction,introduction,0,10,2,2,0,introduction : introduction,0.030303030303030304,0.0625,0.0625
question-answering,6,"For example , consider we know that Frogs eat insects and Flies are insects .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.03333333333333333,0.09375,0.09375
question-answering,6,Then answering Do frogs eat flies ? requires reasoning over both of the above facts .,introduction,introduction,0,12,4,4,0,introduction : introduction,0.03636363636363636,0.125,0.125
question-answering,6,Then answering Do frogs eat flies ? requires reasoning over both of the above facts .,introduction,introduction,0,13,5,5,0,introduction : introduction,0.03939393939393939,0.15625,0.15625
question-answering,6,"Question answering , more specifically context - based QA , has been extensively studied in machine comprehension tasks .",introduction,introduction,0,14,6,6,0,introduction : introduction,0.04242424242424243,0.1875,0.1875
question-answering,6,"However , most of the datasets are primarily focused on lexical and syntactic understanding , and hardly concentrate on inference over multiple facts .",introduction,introduction,0,15,7,7,0,introduction : introduction,0.045454545454545456,0.21875,0.21875
question-answering,6,"Recently , several datasets aimed for testing multi-hop reasoning have emerged ; among them are story - based QA and the dialog task .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.048484848484848485,0.25,0.25
question-answering,6,"Recurrent Neural Network ( RNN ) and its variants , such as Long Short - Term Memory ( LSTM ) and Gated Recurrent Unit ( GRU ) , are popular choices for modeling natural language .",introduction,introduction,0,17,9,9,0,introduction : introduction,0.051515151515151514,0.28125,0.28125
question-answering,6,"However , when used for multi-hop reasoning in question answering , purely RNN - based models have shown to perform poorly .",introduction,introduction,0,18,10,10,0,introduction : introduction,0.05454545454545454,0.3125,0.3125
question-answering,6,This is largely due to the fact that RNN 's internal memory is inherently unstable over along term .,introduction,introduction,0,19,11,11,0,introduction : introduction,0.05757575757575758,0.34375,0.34375
question-answering,6,"For this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.06060606060606061,0.375,0.375
question-answering,6,The attention mechanism allows these models to focus on a single sentence in each layer .,introduction,introduction,0,21,13,13,0,introduction : introduction,0.06363636363636363,0.40625,0.40625
question-answering,6,They can sequentially read multiple relevant sentences from the memory with multiple layers to perform multi-hop reasoning .,introduction,introduction,0,22,14,14,0,introduction : introduction,0.06666666666666667,0.4375,0.4375
question-answering,6,"However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them .",introduction,introduction,0,23,15,15,0,introduction : introduction,0.0696969696969697,0.46875,0.46875
question-answering,6,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",introduction,introduction,1,24,16,16,0,introduction : introduction,0.07272727272727272,0.5,0.5
question-answering,6,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",introduction,introduction,1,25,17,17,0,introduction : introduction,0.07575757575757576,0.53125,0.53125
question-answering,6,"For instance in , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story .",introduction,introduction,0,26,18,18,0,introduction : introduction,0.07878787878787878,0.5625,0.5625
question-answering,6,"After observing the first sentence , Sandra got the apple there , QRN transforms the original question to a reduced query",introduction,introduction,0,27,19,19,0,introduction : introduction,0.08181818181818182,0.59375,0.59375
question-answering,6,"Where is Sandra ? , which is presumably . ? and ? are update gate and reduce functions , respectively .? is assigned to be h 2 5 , the local query at the last time step in the last layer .",introduction,introduction,0,28,20,20,0,introduction : introduction,0.08484848484848485,0.625,0.625
question-answering,6,"Where is Sandra ? , which is presumably . ? and ? are update gate and reduce functions , respectively .? is assigned to be h 2 5 , the local query at the last time step in the last layer .",introduction,introduction,0,29,21,21,0,introduction : introduction,0.08787878787878788,0.65625,0.65625
question-answering,6,"Where is Sandra ? , which is presumably . ? and ? are update gate and reduce functions , respectively .? is assigned to be h 2 5 , the local query at the last time step in the last layer .",introduction,introduction,0,30,22,22,0,introduction : introduction,0.09090909090909091,0.6875,0.6875
question-answering,6,"Also , red-colored text is the inferred meanings of the vectors ( see. easier to answer than the original question given the context provided by the first sentence .",introduction,introduction,0,31,23,23,0,introduction : introduction,0.09393939393939393,0.71875,0.71875
question-answering,6,"Unlike RNN - based models , QRN 's candidate state ( h t in ) does not depend on the previous hidden state ( h t?1 ) .",introduction,introduction,0,32,24,24,0,introduction : introduction,0.09696969696969697,0.75,0.75
question-answering,6,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .",introduction,introduction,0,33,25,25,0,introduction : introduction,0.1,0.78125,0.78125
question-answering,6,"In short , the main contribution of QRN is threefold .",introduction,introduction,0,34,26,26,0,introduction : introduction,0.10303030303030303,0.8125,0.8125
question-answering,6,"First , QRN is a simple variant of RNN that reduces the query given the context sentences in a differentiable manner .",introduction,introduction,0,35,27,27,0,introduction : introduction,0.10606060606060606,0.84375,0.84375
question-answering,6,"Second , QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",introduction,introduction,0,36,28,28,0,introduction : introduction,0.10909090909090909,0.875,0.875
question-answering,6,"Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) .",introduction,introduction,0,37,29,29,0,introduction : introduction,0.11212121212121212,0.90625,0.90625
question-answering,6,"Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",introduction,introduction,0,38,30,30,0,introduction : introduction,0.11515151515151516,0.9375,0.9375
question-answering,6,"In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency .",introduction,introduction,0,39,31,31,0,introduction : introduction,0.11818181818181818,0.96875,0.96875
question-answering,6,We experimentally demonstrate these contributions by achieving the state - of - the - art results on story - based QA and interactive dialog datasets .,introduction,introduction,0,40,32,32,0,introduction : introduction,0.12121212121212122,1.0,1.0
question-answering,6,MODEL,model,MODEL,0,41,1,1,0,model : MODEL,0.12424242424242424,0.010416666666666666,0.038461538461538464
question-answering,6,"In story - based QA ( or dialog dataset ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user 's last utterance in the dialog ) .",model,MODEL,0,42,2,2,0,model : MODEL,0.12727272727272726,0.020833333333333332,0.07692307692307693
question-answering,6,The output is the predicted answer to the question in natural language ( the system 's next utterance in the dialog ) .,model,MODEL,0,43,3,3,0,model : MODEL,0.1303030303030303,0.03125,0.11538461538461539
question-answering,6,The only supervision provided during training is the answer to the question .,model,MODEL,0,44,4,4,0,model : MODEL,0.13333333333333333,0.041666666666666664,0.15384615384615385
question-answering,6,"In this paper we particularly focus on end - to - end solutions , i.e. , the only supervision comes from questions and answers , and we restrain from using manually defined rules or external language resources , such as lexicon or dependency parser .",model,MODEL,0,45,5,5,0,model : MODEL,0.13636363636363635,0.052083333333333336,0.19230769230769232
question-answering,6,"Let x 1 , . . . , x",model,MODEL,0,46,6,6,0,model : MODEL,0.1393939393939394,0.0625,0.23076923076923078
question-answering,6,"denote the sequence of sentences , where T is the number of sentences in the story , and let q denote the question .",model,MODEL,0,47,7,7,0,model : MODEL,0.14242424242424243,0.07291666666666667,0.2692307692307692
question-answering,6,"Let ? denote the predicted answer , and y denote the true answer .",model,MODEL,0,48,8,8,0,model : MODEL,0.14545454545454545,0.08333333333333333,0.3076923076923077
question-answering,6,"Let ? denote the predicted answer , and y denote the true answer .",model,MODEL,0,49,9,9,0,model : MODEL,0.1484848484848485,0.09375,0.34615384615384615
question-answering,6,"Our proposed system for end - to - end QA task is divided into three modules ( ) : input module , QRN layers , and output module .",model,MODEL,0,50,10,10,0,model : MODEL,0.15151515151515152,0.10416666666666667,0.38461538461538464
question-answering,6,Input module .,model,MODEL,0,51,11,11,0,model : MODEL,0.15454545454545454,0.11458333333333333,0.4230769230769231
question-answering,6,"Input module maps each sentence x t and the question q to d-dimensional vector space , x t ? Rd and qt ? Rd .",model,MODEL,0,52,12,12,0,model : MODEL,0.15757575757575756,0.125,0.46153846153846156
question-answering,6,"Input module maps each sentence x t and the question q to d-dimensional vector space , x t ? Rd and qt ? Rd .",model,MODEL,0,53,13,13,0,model : MODEL,0.1606060606060606,0.13541666666666666,0.5
question-answering,6,"Input module maps each sentence x t and the question q to d-dimensional vector space , x t ? Rd and qt ? Rd .",model,MODEL,0,54,14,14,0,model : MODEL,0.16363636363636364,0.14583333333333334,0.5384615384615384
question-answering,6,We adopt a previous solution for the input module ( details in Section 5 ) .,model,MODEL,0,55,15,15,0,model : MODEL,0.16666666666666666,0.15625,0.5769230769230769
question-answering,6,QRN layers .,model,MODEL,0,56,16,16,0,model : MODEL,0.1696969696969697,0.16666666666666666,0.6153846153846154
question-answering,6,"QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , ? ? Rd . A QRN layer refers to the recurrent application of a QRN unit , which can be considered as a variant of RNN with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space .",model,MODEL,0,57,17,17,0,model : MODEL,0.17272727272727273,0.17708333333333334,0.6538461538461539
question-answering,6,"QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , ? ? Rd . A QRN layer refers to the recurrent application of a QRN unit , which can be considered as a variant of RNN with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space .",model,MODEL,0,58,18,18,0,model : MODEL,0.17575757575757575,0.1875,0.6923076923076923
question-answering,6,"The details of the QRN module is explained throughout this section ( 2.1 , 2.2 ) .",model,MODEL,0,59,19,19,0,model : MODEL,0.1787878787878788,0.19791666666666666,0.7307692307692307
question-answering,6,Output module .,model,MODEL,0,60,20,20,0,model : MODEL,0.18181818181818182,0.20833333333333334,0.7692307692307693
question-answering,6,Output module maps ? obtained from QRN to a natural language answer ?.,model,MODEL,0,61,21,21,0,model : MODEL,0.18484848484848485,0.21875,0.8076923076923077
question-answering,6,Output module maps ? obtained from QRN to a natural language answer ?.,model,MODEL,0,62,22,22,0,model : MODEL,0.18787878787878787,0.22916666666666666,0.8461538461538461
question-answering,6,"Similar to the input module , we adopt a standard solution for the output module ( details in Section 5 ) .",model,MODEL,0,63,23,23,0,model : MODEL,0.19090909090909092,0.23958333333333334,0.8846153846153846
question-answering,6,"We first formally define the base model of a QRN unit , and then we explain how we connect the input and output modules to it ( Section 2.1 ) .",model,MODEL,0,64,24,24,0,model : MODEL,0.19393939393939394,0.25,0.9230769230769231
question-answering,6,We also present a few extensions to the network that can improve QRN 's performance ( Section 2.2 ) .,model,MODEL,0,65,25,25,0,model : MODEL,0.19696969696969696,0.2604166666666667,0.9615384615384616
question-answering,6,"Finally , we show that QRN can be parallelized overtime , giving computational advantage over most RNN - based models by one order of magnitude ( Section 3 ) .",model,MODEL,0,66,26,26,0,model : MODEL,0.2,0.2708333333333333,1.0
question-answering,6,QRN UNIT,model,QRN UNIT,0,67,27,1,0,model : QRN UNIT,0.20303030303030303,0.28125,0.05263157894736842
question-answering,6,"As an RNN - based model , QRN is a single recurrent unit that updates its hidden state ( reduced query ) through time and layers .",model,QRN UNIT,0,68,28,2,0,model : QRN UNIT,0.20606060606060606,0.2916666666666667,0.10526315789473684
question-answering,6,"depicts the schematic structure of a QRN unit , and demonstrates how layers are stacked .",model,QRN UNIT,0,69,29,3,0,model : QRN UNIT,0.20909090909090908,0.3020833333333333,0.15789473684210525
question-answering,6,"QRN unit accepts two inputs ( local query vector qt ? Rd and sentence vector x t ? Rd ) , and two outputs ( reduced query vector ht ? Rd , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) .",model,QRN UNIT,0,70,30,4,0,model : QRN UNIT,0.21212121212121213,0.3125,0.21052631578947367
question-answering,6,"QRN unit accepts two inputs ( local query vector qt ? Rd and sentence vector x t ? Rd ) , and two outputs ( reduced query vector ht ? Rd , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) .",model,QRN UNIT,0,71,31,5,0,model : QRN UNIT,0.21515151515151515,0.3229166666666667,0.2631578947368421
question-answering,6,"QRN unit accepts two inputs ( local query vector qt ? Rd and sentence vector x t ? Rd ) , and two outputs ( reduced query vector ht ? Rd , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) .",model,QRN UNIT,0,72,32,6,0,model : QRN UNIT,0.21818181818181817,0.3333333333333333,0.3157894736842105
question-answering,6,The local query vector is not necessarily identical to the original query ( question ) vector q .,model,QRN UNIT,0,73,33,7,0,model : QRN UNIT,0.22121212121212122,0.34375,0.3684210526315789
question-answering,6,"In order to compute the outputs , we use update gate function ? :",model,QRN UNIT,0,74,34,8,0,model : QRN UNIT,0.22424242424242424,0.3541666666666667,0.42105263157894735
question-answering,6,"Intuitively , the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state .",model,QRN UNIT,0,75,35,9,0,model : QRN UNIT,0.22727272727272727,0.3645833333333333,0.47368421052631576
question-answering,6,The reduce function transforms the local query input to a candidate state which is a new reduced ( easier ) query given the sentence .,model,QRN UNIT,0,76,36,10,0,model : QRN UNIT,0.23030303030303031,0.375,0.5263157894736842
question-answering,6,The outputs are calculated with the following equations :,model,QRN UNIT,0,77,37,11,0,model : QRN UNIT,0.23333333333333334,0.3854166666666667,0.5789473684210527
question-answering,6,"where z t is the scalar update gate , h t is the candidate reduced query , and ht is the final reduced query at time step t , ? ( ) is sigmoid activation , tanh ( ) is hyperboolic tangent activation ( applied element - wise ) , is element - wise vector multiplication , and [ ; ] is vector concatenation along the row .",model,QRN UNIT,0,78,38,12,0,model : QRN UNIT,0.23636363636363636,0.3958333333333333,0.631578947368421
question-answering,6,"where z t is the scalar update gate , h t is the candidate reduced query , and ht is the final reduced query at time step t , ? ( ) is sigmoid activation , tanh ( ) is hyperboolic tangent activation ( applied element - wise ) , is element - wise vector multiplication , and [ ; ] is vector concatenation along the row .",model,QRN UNIT,0,79,39,13,0,model : QRN UNIT,0.23939393939393938,0.40625,0.6842105263157895
question-answering,6,"As a base case , h 0 = 0 .",model,QRN UNIT,0,80,40,14,0,model : QRN UNIT,0.24242424242424243,0.4166666666666667,0.7368421052631579
question-answering,6,"Here we have explicitly defined ? and ? , but they can be any reasonable differentiable functions .",model,QRN UNIT,0,81,41,15,0,model : QRN UNIT,0.24545454545454545,0.4270833333333333,0.7894736842105263
question-answering,6,"Here we have explicitly defined ? and ? , but they can be any reasonable differentiable functions .",model,QRN UNIT,0,82,42,16,0,model : QRN UNIT,0.24848484848484848,0.4375,0.8421052631578947
question-answering,6,The update gate is similar to the global attention mechanism in that it measures the similarity between the sentence ( a memory slot ) and the query .,model,QRN UNIT,0,83,43,17,0,model : QRN UNIT,0.2515151515151515,0.4479166666666667,0.8947368421052632
question-answering,6,"However , a significant difference is that the update gate is computed using sigmoid ( ? ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using softmax function over the entire memory ( hence globally defined ) .",model,QRN UNIT,0,84,44,18,0,model : QRN UNIT,0.2545454545454545,0.4583333333333333,0.9473684210526315
question-answering,6,The update gate can be rather considered as local sigmoid attention .,model,QRN UNIT,0,85,45,19,0,model : QRN UNIT,0.25757575757575757,0.46875,1.0
question-answering,6,Stacking layers,model,Stacking layers,0,86,46,1,0,model : Stacking layers,0.2606060606060606,0.4791666666666667,0.045454545454545456
question-answering,6,"We just showed the single - layer case of QRN , but QRN with multiple layers is able to perform reasoning over multiple facts more effectively , as shown in the example of .",model,Stacking layers,0,87,47,2,0,model : Stacking layers,0.2636363636363636,0.4895833333333333,0.09090909090909091
question-answering,6,"In order to stack several layers of QRN , the outputs of the current layer are used as the inputs to the next layer .",model,Stacking layers,0,88,48,3,0,model : Stacking layers,0.26666666666666666,0.5,0.13636363636363635
question-answering,6,"That is , using superscript k to denote the current layer 's index ( assuming 1 - based indexing ) , we let q k + 1 t = h kt .",model,Stacking layers,0,89,49,4,0,model : Stacking layers,0.2696969696969697,0.5104166666666666,0.18181818181818182
question-answering,6,"Note that x t is passed to the next layer without any modification , so we do not put a layer index on it .",model,Stacking layers,0,90,50,5,0,model : Stacking layers,0.2727272727272727,0.5208333333333334,0.22727272727272727
question-answering,6,Bi-direction .,model,Stacking layers,0,91,51,6,0,model : Stacking layers,0.27575757575757576,0.53125,0.2727272727272727
question-answering,6,"So far we have assumed that QRN only needs to look at past sentences , whereas oftentimes , query answers can depend on future sentences .",model,Stacking layers,0,92,52,7,0,model : Stacking layers,0.2787878787878788,0.5416666666666666,0.3181818181818182
question-answering,6,"For instance , consider a sentence "" John dropped the football . "" at time t.",model,Stacking layers,0,93,53,8,0,model : Stacking layers,0.2818181818181818,0.5520833333333334,0.36363636363636365
question-answering,6,"Then , even if there is no mention about the "" football "" in the past ( at time i < t ) , it can be implied that "" John "" has the "" football "" at the current time t.",model,Stacking layers,0,94,54,9,0,model : Stacking layers,0.28484848484848485,0.5625,0.4090909090909091
question-answering,6,"In order to incorporate the future dependency , we obtain ? ? ht and ? ? ht in both forward and backward directions , respectively , using Equation",model,Stacking layers,0,95,55,10,0,model : Stacking layers,0.2878787878787879,0.5729166666666666,0.45454545454545453
question-answering,6,"In order to incorporate the future dependency , we obtain ? ? ht and ? ? ht in both forward and backward directions , respectively , using Equation",model,Stacking layers,0,96,56,11,0,model : Stacking layers,0.2909090909090909,0.5833333333333334,0.5
question-answering,6,"In order to incorporate the future dependency , we obtain ? ? ht and ? ? ht in both forward and backward directions , respectively , using Equation",model,Stacking layers,0,97,57,12,0,model : Stacking layers,0.29393939393939394,0.59375,0.5454545454545454
question-answering,6,3 .,model,Stacking layers,0,98,58,13,0,model : Stacking layers,0.296969696969697,0.6041666666666666,0.5909090909090909
question-answering,6,We then add them together to get qt for the next layer .,model,Stacking layers,0,99,59,14,0,model : Stacking layers,0.3,0.6145833333333334,0.6363636363636364
question-answering,6,"That is , are shared between the two directions .",model,Stacking layers,0,100,60,15,0,model : Stacking layers,0.30303030303030304,0.625,0.6818181818181818
question-answering,6,Connecting input and output modules .,model,Stacking layers,0,101,61,16,0,model : Stacking layers,0.30606060606060603,0.6354166666666666,0.7272727272727273
question-answering,6,depicts how QRN is connected with the input and output modules .,model,Stacking layers,0,102,62,17,0,model : Stacking layers,0.3090909090909091,0.6458333333333334,0.7727272727272727
question-answering,6,"In the first layer of QRN , q 1 t = q for all t , where q is obtained from the input module by processing the natural language question input q. x t is also obtained from x t by the same input module .",model,Stacking layers,0,103,63,18,0,model : Stacking layers,0.31212121212121213,0.65625,0.8181818181818182
question-answering,6,The output at the last time step in the last layer is passed to the output module .,model,Stacking layers,0,104,64,19,0,model : Stacking layers,0.3151515151515151,0.6666666666666666,0.8636363636363636
question-answering,6,"That is , y = h K t where K represent the number of layers in the network .",model,Stacking layers,0,105,65,20,0,model : Stacking layers,0.3181818181818182,0.6770833333333334,0.9090909090909091
question-answering,6,Then the output module gives the predicted answer ? in natural language .,model,Stacking layers,0,106,66,21,0,model : Stacking layers,0.3212121212121212,0.6875,0.9545454545454546
question-answering,6,Then the output module gives the predicted answer ? in natural language .,model,Stacking layers,0,107,67,22,0,model : Stacking layers,0.3242424242424242,0.6979166666666666,1.0
question-answering,6,EXTENSIONS,model,EXTENSIONS,0,108,68,1,0,model : EXTENSIONS,0.32727272727272727,0.7083333333333334,0.0625
question-answering,6,"Here we introduce a few extensions of QRN , and later in our experiments , we test QRN 's performance with and without each of these extensions .",model,EXTENSIONS,0,109,69,2,0,model : EXTENSIONS,0.3303030303030303,0.71875,0.125
question-answering,6,Reset gate .,model,EXTENSIONS,0,110,70,3,0,model : EXTENSIONS,0.3333333333333333,0.7291666666666666,0.1875
question-answering,6,"Inspired by GRU , we found that it is useful to allow the QRN unit to reset ( nullify ) the candidate reduced query ( i.e. , h t ) when necessary .",model,EXTENSIONS,0,111,71,4,0,model : EXTENSIONS,0.33636363636363636,0.7395833333333334,0.25
question-answering,6,For this we use a reset gate function ? :,model,EXTENSIONS,0,112,72,5,0,model : EXTENSIONS,0.3393939393939394,0.75,0.3125
question-answering,6,which can be defined similarly to the update gate function :,model,EXTENSIONS,0,113,73,6,0,model : EXTENSIONS,0.3424242424242424,0.7604166666666666,0.375
question-answering,6,"where W ( r ) ? R 1d is a weight matrix , and b ( r ) ? R is a bias term .",model,EXTENSIONS,0,114,74,7,0,model : EXTENSIONS,0.34545454545454546,0.7708333333333334,0.4375
question-answering,6,"where W ( r ) ? R 1d is a weight matrix , and b ( r ) ? R is a bias term .",model,EXTENSIONS,0,115,75,8,0,model : EXTENSIONS,0.3484848484848485,0.78125,0.5
question-answering,6,"where W ( r ) ? R 1d is a weight matrix , and b ( r ) ? R is a bias term .",model,EXTENSIONS,0,116,76,9,0,model : EXTENSIONS,0.3515151515151515,0.7916666666666666,0.5625
question-answering,6,Equation 3 is rewritten as,model,EXTENSIONS,0,117,77,10,0,model : EXTENSIONS,0.35454545454545455,0.8020833333333334,0.625
question-answering,6,Note that we do not use the reset gate in the last layer .,model,EXTENSIONS,0,118,78,11,0,model : EXTENSIONS,0.3575757575757576,0.8125,0.6875
question-answering,6,Vector gates .,model,EXTENSIONS,0,119,79,12,0,model : EXTENSIONS,0.3606060606060606,0.8229166666666666,0.75
question-answering,6,"As in LSTM and GRU , update and reset gates can be vectors instead of scalar values for fine - controlled gating .",model,EXTENSIONS,0,120,80,13,0,model : EXTENSIONS,0.36363636363636365,0.8333333333333334,0.8125
question-answering,6,"For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d .",model,EXTENSIONS,0,121,81,14,0,model : EXTENSIONS,0.36666666666666664,0.84375,0.875
question-answering,6,"Then we obtain z t , rt ? Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",model,EXTENSIONS,0,122,82,15,0,model : EXTENSIONS,0.3696969696969697,0.8541666666666666,0.9375
question-answering,6,"Then we obtain z t , rt ? Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",model,EXTENSIONS,0,123,83,16,0,model : EXTENSIONS,0.37272727272727274,0.8645833333333334,1.0
question-answering,6,PARALLELIZATION,model,PARALLELIZATION,0,124,84,1,0,model : PARALLELIZATION,0.37575757575757573,0.875,0.07692307692307693
question-answering,6,An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time .,model,PARALLELIZATION,0,125,85,2,0,model : PARALLELIZATION,0.3787878787878788,0.8854166666666666,0.15384615384615385
question-answering,6,"This is in contrast with most RNN - based models that can not be parallelized , where computing the candidate hidden state at time t explicitly requires the previous hidden state .",model,PARALLELIZATION,0,126,86,3,0,model : PARALLELIZATION,0.38181818181818183,0.8958333333333334,0.23076923076923078
question-answering,6,"In QRN , the final reduced queries ( h t ) can be decomposed into computing over candidate reduced queries ( h t ) , without looking at the previous reduced query .",model,PARALLELIZATION,0,127,87,4,0,model : PARALLELIZATION,0.38484848484848483,0.90625,0.3076923076923077
question-answering,6,Here we primarily show that the query update in Equation 3 can be parallelized by rewriting the equation with matrix operations .,model,PARALLELIZATION,0,128,88,5,0,model : PARALLELIZATION,0.3878787878787879,0.9166666666666666,0.38461538461538464
question-answering,6,The extension to Equation 5 is straightforward .,model,PARALLELIZATION,0,129,89,6,0,model : PARALLELIZATION,0.39090909090909093,0.9270833333333334,0.46153846153846156
question-answering,6,The proof for QRN with vector gates is shown in Appendix B .,model,PARALLELIZATION,0,130,90,7,0,model : PARALLELIZATION,0.3939393939393939,0.9375,0.5384615384615384
question-answering,6,The recursive definition of Equation 3 can be explicitly written as,model,PARALLELIZATION,0,131,91,8,0,model : PARALLELIZATION,0.396969696969697,0.9479166666666666,0.6153846153846154
question-answering,6,Let bi = log ( 1 ? z i ) for brevity .,model,PARALLELIZATION,0,132,92,9,0,model : PARALLELIZATION,0.4,0.9583333333333334,0.6923076923076923
question-answering,6,Then we can rewrite Equation 7 as the following equation :,model,PARALLELIZATION,0,133,93,10,0,model : PARALLELIZATION,0.403030303030303,0.96875,0.7692307692307693
question-answering,6,Figure,model,PARALLELIZATION,0,134,94,11,0,model : PARALLELIZATION,0.40606060606060607,0.9791666666666666,0.8461538461538461
question-answering,6,": The schematics of QRN and the two state - of - the - art models , End - to - End Memory Networks and Improved Dynamic Memory Networks ( DMN + ) , simplified to emphasize the differences among the models .",model,PARALLELIZATION,0,135,95,12,0,model : PARALLELIZATION,0.4090909090909091,0.9895833333333334,0.9230769230769231
question-answering,6,"AGRU is a variant of GRU where the update gate is replaced with soft attention , proposed by .",model,PARALLELIZATION,0,136,96,13,0,model : PARALLELIZATION,0.4121212121212121,1.0,1.0
question-answering,6,RELATED WORK,related work,RELATED WORK,0,137,1,1,0,related work : RELATED WORK,0.41515151515151516,0.04,0.04
question-answering,6,"QRN is inspired by RNN - based models with gating mechanism , such as LSTM and GRU .",related work,RELATED WORK,0,138,2,2,0,related work : RELATED WORK,0.41818181818181815,0.08,0.08
question-answering,6,"While GRU and LSTM use the previous hidden state and the current input to obtain the candidate hidden state , QRN only uses the current two inputs to obtain the candidate reduced query ( equivalent to candidate hidden state ) .",related work,RELATED WORK,0,139,3,3,0,related work : RELATED WORK,0.4212121212121212,0.12,0.12
question-answering,6,"We conjecture that this not only gives computational advantage via parallelization , but also makes training easier , i.e. , avoiding vanishing gradient ( which is critical for long - term dependency ) , overfitting ( by simplifying the model ) , and converging to local minima .",related work,RELATED WORK,0,140,4,4,0,related work : RELATED WORK,0.42424242424242425,0.16,0.16
question-answering,6,"The idea of structurally simplifying ( constraining ) RNNs for learning longer - term patterns has been explored in recent previous work , such as Structurally Constrained Recurrent Network and Strongly - Typed Recurrent Neural Network ( STRNN ) .",related work,RELATED WORK,0,141,5,5,0,related work : RELATED WORK,0.42727272727272725,0.2,0.2
question-answering,6,"QRN is similar to STRNN in that both architectures use gating mechanism , and the gates and the candidate hidden states do not depend on the previous hidden states , which simplifies the recurrent relation .",related work,RELATED WORK,0,142,6,6,0,related work : RELATED WORK,0.4303030303030303,0.24,0.24
question-answering,6,"However , QRN can be distinguished from STRNN in three ways .",related work,RELATED WORK,0,143,7,7,0,related work : RELATED WORK,0.43333333333333335,0.28,0.28
question-answering,6,"First , QRN 's update gate simulates attention mechanism , measuring the relevance between the input sentence and query .",related work,RELATED WORK,0,144,8,8,0,related work : RELATED WORK,0.43636363636363634,0.32,0.32
question-answering,6,"On the other hand , the gates in STRNN can be considered as the simplification of LSTM / GRU by removing their dependency on previous hidden state .",related work,RELATED WORK,0,145,9,9,0,related work : RELATED WORK,0.4393939393939394,0.36,0.36
question-answering,6,"Second , QRN is an RNN that is natively compatible with context - based QA tasks , where the QRN unit accepts two inputs , i.e. each context sentence and query .",related work,RELATED WORK,0,146,10,10,0,related work : RELATED WORK,0.44242424242424244,0.4,0.4
question-answering,6,This is distinct from STRNN which has only one input .,related work,RELATED WORK,0,147,11,11,0,related work : RELATED WORK,0.44545454545454544,0.44,0.44
question-answering,6,"Third , we show that QRN is timewise - parallelizable on GPUs .",related work,RELATED WORK,0,148,12,12,0,related work : RELATED WORK,0.4484848484848485,0.48,0.48
question-answering,6,Our parallelization algorithm is also applicable to STRNN .,related work,RELATED WORK,0,149,13,13,0,related work : RELATED WORK,0.45151515151515154,0.52,0.52
question-answering,6,End - to - end Memory Network ( N2N ) uses external memory with multi - layer attention mechanism to focus on sentences thatare relevant to the question .,related work,RELATED WORK,0,150,14,14,0,related work : RELATED WORK,0.45454545454545453,0.56,0.56
question-answering,6,There are two key differences between N2N and our QRN .,related work,RELATED WORK,0,151,15,15,0,related work : RELATED WORK,0.4575757575757576,0.6,0.6
question-answering,6,"First , N2N summarizes the entire memory in each layer to control the attention in the next layer ( circle nodes in ) .",related work,RELATED WORK,0,152,16,16,0,related work : RELATED WORK,0.46060606060606063,0.64,0.64
question-answering,6,"Instead , QRN does not have any controller node ) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit .",related work,RELATED WORK,0,153,17,17,0,related work : RELATED WORK,0.4636363636363636,0.68,0.68
question-answering,6,"Second , N2N adds time - dependent trainable weights to the sentence representations to model the time dependency of the sentences ( as discussed in Section 1 ) .",related work,RELATED WORK,0,154,18,18,0,related work : RELATED WORK,0.4666666666666667,0.72,0.72
question-answering,6,QRN does not need such additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency .,related work,RELATED WORK,0,155,19,19,0,related work : RELATED WORK,0.4696969696969697,0.76,0.76
question-answering,6,Neural Reasoner and Gated End - toend Memory Network ) are variants of MemN2N that share its fundamental characteristics .,related work,RELATED WORK,0,156,20,20,0,related work : RELATED WORK,0.4727272727272727,0.8,0.8
question-answering,6,Improved Dynamic Memory Network ( DMN + ) uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences .,related work,RELATED WORK,0,157,21,21,0,related work : RELATED WORK,0.47575757575757577,0.84,0.84
question-answering,6,"It consists of two distinct GRUs , one for the time axis ( rectangle nodes in ) and one for the layer axis ( circle nodes in ) .",related work,RELATED WORK,0,158,22,22,0,related work : RELATED WORK,0.47878787878787876,0.88,0.88
question-answering,6,Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights .,related work,RELATED WORK,0,159,23,23,0,related work : RELATED WORK,0.4818181818181818,0.92,0.92
question-answering,6,"DMN + uses the time - axis GRU to summarizes the entire memory in each layer , and then the layer - axis GRU controls the attention weights in each layer .",related work,RELATED WORK,0,160,24,24,0,related work : RELATED WORK,0.48484848484848486,0.96,0.96
question-answering,6,"In contrast , QRN is simply a single recurrent unit without any controller node .",related work,RELATED WORK,0,161,25,25,0,related work : RELATED WORK,0.48787878787878786,1.0,1.0
question-answering,6,EXPERIMENTS,experiment,EXPERIMENTS,0,162,1,1,0,experiment : EXPERIMENTS,0.4909090909090909,0.05263157894736842,0.05263157894736842
question-answering,6,DATA bAb,experiment,EXPERIMENTS,0,163,2,2,0,experiment : EXPERIMENTS,0.49393939393939396,0.10526315789473684,0.10526315789473684
question-answering,6,"story - based QA dataset bAb I story - based QA dataset is composed of 20 different tasks ( Appendix A ) , each of which has 1,000 ( 1 k ) synthetically - generated story - question pair .",experiment,EXPERIMENTS,0,164,3,3,0,experiment : EXPERIMENTS,0.49696969696969695,0.15789473684210525,0.15789473684210525
question-answering,6,story can be as short as two sentences and as long as 200 + sentences .,experiment,EXPERIMENTS,0,165,4,4,0,experiment : EXPERIMENTS,0.5,0.21052631578947367,0.21052631578947367
question-answering,6,system is evaluated on the accuracy of getting the correct answers to the questions .,experiment,EXPERIMENTS,0,166,5,5,0,experiment : EXPERIMENTS,0.503030303030303,0.2631578947368421,0.2631578947368421
question-answering,6,"The answers are single words or lists ( e.g. "" football , apple "" ) .",experiment,EXPERIMENTS,0,167,6,6,0,experiment : EXPERIMENTS,0.5060606060606061,0.3157894736842105,0.3157894736842105
question-answering,6,Answering questions in each task requires selecting a set of relevant sentences and applying different kinds of logical reasoning over them .,experiment,EXPERIMENTS,0,168,7,7,0,experiment : EXPERIMENTS,0.509090909090909,0.3684210526315789,0.3684210526315789
question-answering,6,"The dataset also includes 10 k training data ( for each task ) , which allows training more complex models .",experiment,EXPERIMENTS,0,169,8,8,0,experiment : EXPERIMENTS,0.5121212121212121,0.42105263157894735,0.42105263157894735
question-answering,6,Note that DMN + only reports on the 10k dataset .,experiment,EXPERIMENTS,0,170,9,9,0,experiment : EXPERIMENTS,0.5151515151515151,0.47368421052631576,0.47368421052631576
question-answering,6,bAb,experiment,EXPERIMENTS,0,171,10,10,0,experiment : EXPERIMENTS,0.5181818181818182,0.5263157894736842,0.5263157894736842
question-answering,6,"dialog dataset bAb I dialog dataset consists of 5 different tasks , each of which has 1 k synthetically - generated goal - oriented dialogs between a user and the system in the domain of restaurant reservation .",experiment,EXPERIMENTS,0,172,11,11,0,experiment : EXPERIMENTS,0.5212121212121212,0.5789473684210527,0.5789473684210527
question-answering,6,Each dialog is as long as 96 utterances and comes with external knowledge base ( KB ) providing information of each restaurant .,experiment,EXPERIMENTS,0,173,12,12,0,experiment : EXPERIMENTS,0.5242424242424243,0.631578947368421,0.631578947368421
question-answering,6,"The authors also provide Out - Of - Vocabulary ( OOV ) version of the dataset , where many of the words and KB keywords in test data are not seen during training .",experiment,EXPERIMENTS,0,174,13,13,0,experiment : EXPERIMENTS,0.5272727272727272,0.6842105263157895,0.6842105263157895
question-answering,6,"system is evaluated on the accuracy of its response to each utterance of the user , choosing from up to 2500 possible candidate responses .",experiment,EXPERIMENTS,0,175,14,14,0,experiment : EXPERIMENTS,0.5303030303030303,0.7368421052631579,0.7368421052631579
question-answering,6,system is required not only to understand the user 's request but also refer to previous conversations in order to obtain the context information of the current conversation .,experiment,EXPERIMENTS,0,176,15,15,0,experiment : EXPERIMENTS,0.5333333333333333,0.7894736842105263,0.7894736842105263
question-answering,6,"DSTC2 ( Task 6 ) dialog dataset transformed the Second Dialog State Tracking Challenge ( DSTC2 ) dataset into the same format as the bAbI dialog dataset , for the measurement of performance on are al dataset .",experiment,EXPERIMENTS,0,177,16,16,0,experiment : EXPERIMENTS,0.5363636363636364,0.8421052631578947,0.8421052631578947
question-answering,6,"Each dialog can be as long as 800 + utterances , and a system needs to choose from 2407 possible candidate responses for each utterance of the user .",experiment,EXPERIMENTS,0,178,17,17,0,experiment : EXPERIMENTS,0.5393939393939394,0.8947368421052632,0.8947368421052632
question-answering,6,"Note that the evaluation metric of the original DSTC2 is different from that of the transformed DSTC2 , so previous work on the original DSTC2 should not be directly compared to our work .",experiment,EXPERIMENTS,0,179,18,18,0,experiment : EXPERIMENTS,0.5424242424242425,0.9473684210526315,0.9473684210526315
question-answering,6,"We will refer to this transformed DSTC2 dataset by "" Task 6 "" of dialog dataset .",experiment,EXPERIMENTS,0,180,19,19,0,experiment : EXPERIMENTS,0.5454545454545454,1.0,1.0
question-answering,6,MODEL DETAILS,model,MODEL DETAILS,0,181,1,1,0,model : MODEL DETAILS,0.5484848484848485,0.012658227848101266,0.029411764705882353
question-answering,6,Input Module .,model,MODEL DETAILS,0,182,2,2,0,model : MODEL DETAILS,0.5515151515151515,0.02531645569620253,0.058823529411764705
question-answering,6,"In the input module , we are given sentences ( previous conversations in dialog ) x t and a question ( most recent user utterance ) q , and we want to obtain their vector representations , x t , q ? Rd .",model,MODEL DETAILS,0,183,3,3,0,model : MODEL DETAILS,0.5545454545454546,0.0379746835443038,0.08823529411764706
question-answering,6,"In the input module , we are given sentences ( previous conversations in dialog ) x t and a question ( most recent user utterance ) q , and we want to obtain their vector representations , x t , q ? Rd .",model,MODEL DETAILS,0,184,4,4,0,model : MODEL DETAILS,0.5575757575757576,0.05063291139240506,0.11764705882352941
question-answering,6,We use a trainable embedding matrix A ? R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ? Rd .,model,MODEL DETAILS,0,185,5,5,0,model : MODEL DETAILS,0.5606060606060606,0.06329113924050633,0.14705882352941177
question-answering,6,We use a trainable embedding matrix A ? R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ? Rd .,model,MODEL DETAILS,0,186,6,6,0,model : MODEL DETAILS,0.5636363636363636,0.0759493670886076,0.17647058823529413
question-answering,6,We use a trainable embedding matrix A ? R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ? Rd .,model,MODEL DETAILS,0,187,7,7,0,model : MODEL DETAILS,0.5666666666666667,0.08860759493670886,0.20588235294117646
question-answering,6,Then the sentence representation x t is obtained by Position Encoder .,model,MODEL DETAILS,0,188,8,8,0,model : MODEL DETAILS,0.5696969696969697,0.10126582278481013,0.23529411764705882
question-answering,6,The same encoder with the same embedding matrix is also used to obtain the question vector q from q.,model,MODEL DETAILS,0,189,9,9,0,model : MODEL DETAILS,0.5727272727272728,0.11392405063291139,0.2647058823529412
question-answering,6,Output Module for story - based QA .,model,MODEL DETAILS,0,190,10,10,0,model : MODEL DETAILS,0.5757575757575758,0.12658227848101267,0.29411764705882354
question-answering,6,"In the output module , we are given the vector representation of the predicted answer ? and we want to obtain the natural language form of the answer , ?.",model,MODEL DETAILS,0,191,11,11,0,model : MODEL DETAILS,0.5787878787878787,0.13924050632911392,0.3235294117647059
question-answering,6,"In the output module , we are given the vector representation of the predicted answer ? and we want to obtain the natural language form of the answer , ?.",model,MODEL DETAILS,0,192,12,12,0,model : MODEL DETAILS,0.5818181818181818,0.1518987341772152,0.35294117647058826
question-answering,6,"We use a V - way single - layer softmax classifier to map ? to a V - dimensional sparse vector , v = softmax W ( y ) ? ? RV , where W ( y ) ? RV d is a weight matrix .",model,MODEL DETAILS,0,193,13,13,0,model : MODEL DETAILS,0.5848484848484848,0.16455696202531644,0.38235294117647056
question-answering,6,"We use a V - way single - layer softmax classifier to map ? to a V - dimensional sparse vector , v = softmax W ( y ) ? ? RV , where W ( y ) ? RV d is a weight matrix .",model,MODEL DETAILS,0,194,14,14,0,model : MODEL DETAILS,0.5878787878787879,0.17721518987341772,0.4117647058823529
question-answering,6,"We use a V - way single - layer softmax classifier to map ? to a V - dimensional sparse vector , v = softmax W ( y ) ? ? RV , where W ( y ) ? RV d is a weight matrix .",model,MODEL DETAILS,0,195,15,15,0,model : MODEL DETAILS,0.5909090909090909,0.189873417721519,0.4411764705882353
question-answering,6,Then the final answer ? is simply the argmax word inv .,model,MODEL DETAILS,0,196,16,16,0,model : MODEL DETAILS,0.593939393939394,0.20253164556962025,0.47058823529411764
question-answering,6,Then the final answer ? is simply the argmax word inv .,model,MODEL DETAILS,0,197,17,17,0,model : MODEL DETAILS,0.5969696969696969,0.21518987341772153,0.5
question-answering,6,"To handle questions with multiple - word answers , we consider each of them as a single word that contains punctuations such as space and comma , and put it in the vocabulary .",model,MODEL DETAILS,0,198,18,18,0,model : MODEL DETAILS,0.6,0.22784810126582278,0.5294117647058824
question-answering,6,Output Module for dialog .,model,MODEL DETAILS,0,199,19,19,0,model : MODEL DETAILS,0.603030303030303,0.24050632911392406,0.5588235294117647
question-answering,6,"We use a fixed number single - layer softmax classifiers , each of which is similar to that of the sotry - based QA model , to sequentially output each word of the system 's response .",model,MODEL DETAILS,0,200,20,20,0,model : MODEL DETAILS,0.6060606060606061,0.25316455696202533,0.5882352941176471
question-answering,6,"While it is similar in spirit to the RNN decoder , our output module does not have a recurrent hidden state or gating mechanism .",model,MODEL DETAILS,0,201,21,21,0,model : MODEL DETAILS,0.6090909090909091,0.26582278481012656,0.6176470588235294
question-answering,6,"Instead , it solely uses the final ouptut of the QRN , ? , and the current word output to influence the prediction of the next word among possible candidates .",model,MODEL DETAILS,0,202,22,22,0,model : MODEL DETAILS,0.6121212121212121,0.27848101265822783,0.6470588235294118
question-answering,6,Training .,model,MODEL DETAILS,0,203,23,23,0,model : MODEL DETAILS,0.6151515151515151,0.2911392405063291,0.6764705882352942
question-answering,6,We withhold 10 % of the training for development .,model,MODEL DETAILS,1,204,24,24,0,model : MODEL DETAILS,0.6181818181818182,0.3037974683544304,0.7058823529411765
question-answering,6,We use the hidden state size of 50 by deafult .,model,MODEL DETAILS,1,205,25,25,0,model : MODEL DETAILS,0.6212121212121212,0.31645569620253167,0.7352941176470589
question-answering,6,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",model,MODEL DETAILS,1,206,26,26,0,model : MODEL DETAILS,0.6242424242424243,0.3291139240506329,0.7647058823529411
question-answering,6,The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,model,MODEL DETAILS,0,207,27,27,0,model : MODEL DETAILS,0.6272727272727273,0.34177215189873417,0.7941176470588235
question-answering,6,"Weights in the QRN unit are initialized using techniques by , and are tied across the layers .",model,MODEL DETAILS,0,208,28,28,0,model : MODEL DETAILS,0.6303030303030303,0.35443037974683544,0.8235294117647058
question-answering,6,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,model,MODEL DETAILS,0,209,29,29,0,model : MODEL DETAILS,0.6333333333333333,0.3670886075949367,0.8529411764705882
question-answering,6,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,model,MODEL DETAILS,1,210,30,30,0,model : MODEL DETAILS,0.6363636363636364,0.379746835443038,0.8823529411764706
question-answering,6,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,model,MODEL DETAILS,1,211,31,31,0,model : MODEL DETAILS,0.6393939393939394,0.3924050632911392,0.9117647058823529
question-answering,6,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",model,MODEL DETAILS,1,212,32,32,0,model : MODEL DETAILS,0.6424242424242425,0.4050632911392405,0.9411764705882353
question-answering,6,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,model,MODEL DETAILS,1,213,33,33,0,model : MODEL DETAILS,0.6454545454545455,0.4177215189873418,0.9705882352941176
question-answering,6,"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .",model,MODEL DETAILS,0,214,34,34,0,model : MODEL DETAILS,0.6484848484848484,0.43037974683544306,1.0
question-answering,6,RESULTS .,model,RESULTS.,0,215,35,1,0,model : RESULTS.,0.6515151515151515,0.4430379746835443,0.022222222222222223
question-answering,6,We compare our model with baselines and previous state - of - the - art models on story - based and dialog tasks .,model,RESULTS.,0,216,36,2,0,model : RESULTS.,0.6545454545454545,0.45569620253164556,0.044444444444444446
question-answering,6,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .",model,RESULTS.,0,217,37,3,0,model : RESULTS.,0.6575757575757576,0.46835443037974683,0.06666666666666667
question-answering,6,Story - based QA .,model,RESULTS.,1,218,38,4,0,model : RESULTS.,0.6606060606060606,0.4810126582278481,0.08888888888888889
question-answering,6,Table 1 ( top ) reports the summary of results of our model ( QRN ) and previous work on b AbI QA ( task - wise results are shown in in Appendix ) .,model,RESULTS.,0,219,39,5,0,model : RESULTS.,0.6636363636363637,0.4936708860759494,0.1111111111111111
question-answering,6,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .",model,RESULTS.,1,220,40,6,0,model : RESULTS.,0.6666666666666666,0.5063291139240507,0.13333333333333333
question-answering,6,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .",model,RESULTS.,1,221,41,7,0,model : RESULTS.,0.6696969696969697,0.5189873417721519,0.15555555555555556
question-answering,6,Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .,model,RESULTS.,1,222,42,8,0,model : RESULTS.,0.6727272727272727,0.5316455696202531,0.17777777777777778
question-answering,6,"As done in previous work , we also report results when we use ' Match ' for dialogs .",model,RESULTS.,0,223,43,9,0,model : RESULTS.,0.6757575757575758,0.5443037974683544,0.2
question-answering,6,Match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on Appendix ) .,model,RESULTS.,0,224,44,10,0,model : RESULTS.,0.6787878787878788,0.5569620253164557,0.2222222222222222
question-answering,6,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,model,RESULTS.,1,225,45,11,0,model : RESULTS.,0.6818181818181818,0.569620253164557,0.24444444444444444
question-answering,6,Ablations .,model,RESULTS.,0,226,46,12,0,model : RESULTS.,0.6848484848484848,0.5822784810126582,0.26666666666666666
question-answering,6,"We test four types of ablations ( also discussed in Section 2.2 ) : number of layers ( 1 , 2 , 3 , or 6 ) , reset gate ( r ) , and gate vectorization ( v ) and the dimension of the hidden vector ( 50 , 100 ) .",model,RESULTS.,0,227,47,13,0,model : RESULTS.,0.6878787878787879,0.5949367088607594,0.28888888888888886
question-answering,6,We show a subset of combinations of the ablations for bAbI QA in ; other combinations performed poorly and / or did not give interesting observations .,model,RESULTS.,0,228,48,14,0,model : RESULTS.,0.6909090909090909,0.6075949367088608,0.3111111111111111
question-answering,6,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .",model,RESULTS.,1,229,49,15,0,model : RESULTS.,0.693939393939394,0.620253164556962,0.3333333333333333
question-answering,6,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .",model,RESULTS.,0,230,50,16,0,model : RESULTS.,0.696969696969697,0.6329113924050633,0.35555555555555557
question-answering,6,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .",model,RESULTS.,0,231,51,17,0,model : RESULTS.,0.7,0.6455696202531646,0.37777777777777777
question-answering,6,b ) Adding the reset gate helps .,model,RESULTS.,1,232,52,18,0,model : RESULTS.,0.703030303030303,0.6582278481012658,0.4
question-answering,6,"c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .",model,RESULTS.,1,233,53,19,0,model : RESULTS.,0.706060606060606,0.6708860759493671,0.4222222222222222
question-answering,6,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",model,RESULTS.,1,234,54,20,0,model : RESULTS.,0.7090909090909091,0.6835443037974683,0.4444444444444444
question-answering,6,"d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .",model,RESULTS.,0,235,55,21,0,model : RESULTS.,0.7121212121212122,0.6962025316455697,0.4666666666666667
question-answering,6,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,model,RESULTS.,0,236,56,22,0,model : RESULTS.,0.7151515151515152,0.7088607594936709,0.4888888888888889
question-answering,6,We implement QRN with and without parallelization in TensorFlow ) on a single Titan X GPU to qunaitify the computational gain of the parallelization .,model,RESULTS.,0,237,57,23,0,model : RESULTS.,0.7181818181818181,0.7215189873417721,0.5111111111111111
question-answering,6,"For QRN without parallelization , we use the RNN library provided by TensorFlow .",model,RESULTS.,0,238,58,24,0,model : RESULTS.,0.7212121212121212,0.7341772151898734,0.5333333333333333
question-answering,6,QRN with parallelization gives 6.2 times faster training and inference than QRN without parallelization on average .,model,RESULTS.,0,239,59,25,0,model : RESULTS.,0.7242424242424242,0.7468354430379747,0.5555555555555556
question-answering,6,We expect that the speedup can be even higher for datasets with larger context .,model,RESULTS.,0,240,60,26,0,model : RESULTS.,0.7272727272727273,0.759493670886076,0.5777777777777777
question-answering,6,Interpretations .,model,RESULTS.,0,241,61,27,0,model : RESULTS.,0.7303030303030303,0.7721518987341772,0.6
question-answering,6,An advantage of QRN is that the intermediate query updates are interpretable .,model,RESULTS.,0,242,62,28,0,model : RESULTS.,0.7333333333333333,0.7848101265822784,0.6222222222222222
question-answering,6,"shows intermediate local queries ( q kt ) interpreted in natural language , such as "" Where is Sandra ? "" .",model,RESULTS.,0,243,63,29,0,model : RESULTS.,0.7363636363636363,0.7974683544303798,0.6444444444444445
question-answering,6,"In order to obtain these , we place a decoder on the input question embedding q and add it s loss for recovering the question to the classification loss ( similarly to ) .",model,RESULTS.,0,244,64,30,0,model : RESULTS.,0.7393939393939394,0.810126582278481,0.6666666666666666
question-answering,6,We then use the same decoder to decode the intermediate queries .,model,RESULTS.,0,245,65,31,0,model : RESULTS.,0.7424242424242424,0.8227848101265823,0.6888888888888889
question-answering,6,This helps us understand the flow of information in the networks .,model,RESULTS.,0,246,66,32,0,model : RESULTS.,0.7454545454545455,0.8354430379746836,0.7111111111111111
question-answering,6,"In , the question Where is apple ? is transformed into",model,RESULTS.,0,247,67,33,0,model : RESULTS.,0.7484848484848485,0.8481012658227848,0.7333333333333333
question-answering,6,"In , the question Where is apple ? is transformed into",model,RESULTS.,0,248,68,34,0,model : RESULTS.,0.7515151515151515,0.8607594936708861,0.7555555555555555
question-answering,6,"Where is Sandra ? at t = 1 . At t = 2 , as Sandra dropped the apple , the apple is no more relevant to Sandra .",model,RESULTS.,0,249,69,35,0,model : RESULTS.,0.7545454545454545,0.8734177215189873,0.7777777777777778
question-answering,6,"We obtain Where is Daniel ? at time t = 3 , and it is propagated until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query .",model,RESULTS.,0,250,70,36,0,model : RESULTS.,0.7575757575757576,0.8860759493670886,0.8
question-answering,6,"We obtain Where is Daniel ? at time t = 3 , and it is propagated until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query .",model,RESULTS.,0,251,71,37,0,model : RESULTS.,0.7606060606060606,0.8987341772151899,0.8222222222222222
question-answering,6,Visualization .,model,RESULTS.,0,252,72,38,0,model : RESULTS.,0.7636363636363637,0.9113924050632911,0.8444444444444444
question-answering,6,shows vizualization of the ( scalar ) magnitudes of update and reset gates on story sentences and dialog utterances .,model,RESULTS.,0,253,73,39,0,model : RESULTS.,0.7666666666666667,0.9240506329113924,0.8666666666666667
question-answering,6,More visualizations are shown in Appendices : and .,model,RESULTS.,0,254,74,40,0,model : RESULTS.,0.7696969696969697,0.9367088607594937,0.8888888888888888
question-answering,6,"In , we observe high values on facts that provide information to answer question ( the system 's next utterance for dialog ) .",model,RESULTS.,0,255,75,41,0,model : RESULTS.,0.7727272727272727,0.9493670886075949,0.9111111111111111
question-answering,6,"In QA Task 2 example ( top left ) , we observe high update gate values in the first layer on facts that state who has the apple , and in the second layer , the high update gate values are on those that inform where that person went to .",model,RESULTS.,0,256,76,42,0,model : RESULTS.,0.7757575757575758,0.9620253164556962,0.9333333333333333
question-answering,6,"We also observe that the forward reset gate at t = 2 in the first layer ( ? ? r 1 2 ) is low , which is signifying that apple no more belongs to Sandra .",model,RESULTS.,0,257,77,43,0,model : RESULTS.,0.7787878787878788,0.9746835443037974,0.9555555555555556
question-answering,6,"In dialog Task 3 ( bottom left ) , the model is able to infer that three restaurants are already recommended so that it can recommend another one .",model,RESULTS.,0,258,78,44,0,model : RESULTS.,0.7818181818181819,0.9873417721518988,0.9777777777777777
question-answering,6,"In dialog Task 6 ( bottom ) , the model focuses on the sentences containing Spanish , and does not concentrate much on other facts such as I do n't care .",model,RESULTS.,0,259,79,45,0,model : RESULTS.,0.7848484848484848,1.0,1.0
question-answering,6,CONCLUSION,conclusion,CONCLUSION,0,260,1,1,0,conclusion : CONCLUSION,0.7878787878787878,0.14285714285714285,0.14285714285714285
question-answering,6,"In this paper , we introduce Query - Reduction Network ( QRN ) to answer context - based questions and carry out conversations with users that require multi-hop reasoning .",conclusion,CONCLUSION,0,261,2,2,0,conclusion : CONCLUSION,0.7909090909090909,0.2857142857142857,0.2857142857142857
question-answering,6,We show the state - of - theart results in the three datasets of story - based QA and dialog .,conclusion,CONCLUSION,0,262,3,3,0,conclusion : CONCLUSION,0.793939393939394,0.42857142857142855,0.42857142857142855
question-answering,6,We model a story or a dialog as a sequence of state - changing triggers and compute the final answer to the question or the system 's next utterance by recurrently updating ( or reducing ) the query .,conclusion,CONCLUSION,0,263,4,4,0,conclusion : CONCLUSION,0.796969696969697,0.5714285714285714,0.5714285714285714
question-answering,6,"QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",conclusion,CONCLUSION,0,264,5,5,0,conclusion : CONCLUSION,0.8,0.7142857142857143,0.7142857142857143
question-answering,6,"It addresses the long - term dependency problem of most RNNs by simplifying the recurrent update , in which the candidate hidden state ( reduced query ) does not depend on the previous state .",conclusion,CONCLUSION,0,265,6,6,0,conclusion : CONCLUSION,0.803030303030303,0.8571428571428571,0.8571428571428571
question-answering,6,"Moreover , QRN can be parallelized and can address the well - known problem of RNN 's vanishing gradients .",conclusion,CONCLUSION,0,266,7,7,0,conclusion : CONCLUSION,0.806060606060606,1.0,1.0
question-answering,6,TASK - WISE RESULTS,TASK-WISE RESULTS,TASK-WISE RESULTS,0,267,1,1,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8090909090909091,0.047619047619047616,0.1111111111111111
question-answering,6,"Here we provide detailed per- task breakdown of our results in QA ) and dialog datasets . error rates ( % ) of QRN and previous work : LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMemN2N ) .",TASK-WISE RESULTS,TASK-WISE RESULTS,0,268,2,2,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8121212121212121,0.09523809523809523,0.2222222222222222
question-answering,6,Results within each task of Differentiable Neural Computer ( DNC ) were not provided in its paper ) .,TASK-WISE RESULTS,TASK-WISE RESULTS,0,269,3,3,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8151515151515152,0.14285714285714285,0.3333333333333333
question-answering,6,"For QRN , a number in the front ( 1 , 2 , 3 , 6 ) indicates the number of layers .",TASK-WISE RESULTS,TASK-WISE RESULTS,0,270,4,4,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8181818181818182,0.19047619047619047,0.4444444444444444
question-answering,6,"number in the back indicates the dimension of hidden vector , while the default value is 50 .",TASK-WISE RESULTS,TASK-WISE RESULTS,0,271,5,5,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8212121212121212,0.23809523809523808,0.5555555555555556
question-answering,6,"r ' indicates that the reset gate is used , and ' v ' indicates that the gates were vectorized .",TASK-WISE RESULTS,TASK-WISE RESULTS,0,272,6,6,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8242424242424242,0.2857142857142857,0.6666666666666666
question-answering,6,'*' indicates joint training . ) and Gated End - to - end Memory Networks ( GMem N2N ) .,TASK-WISE RESULTS,TASK-WISE RESULTS,0,273,7,7,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8272727272727273,0.3333333333333333,0.7777777777777778
question-answering,6,"For QRN , a number in the front ( 1 , 2 , 3 , 6 ) indicates the number of layers and a number in the back ( 100 ) indicates the dimension of hidden vector , while the default value is 50 .",TASK-WISE RESULTS,TASK-WISE RESULTS,0,274,8,8,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8303030303030303,0.38095238095238093,0.8888888888888888
question-answering,6,"r ' indicates that the reset gate is used , ' v ' indicates that the gates were vectorized , and '+ ' indicates that ' match ' was used .",TASK-WISE RESULTS,TASK-WISE RESULTS,0,275,9,9,0,TASK-WISE RESULTS : TASK-WISE RESULTS,0.8333333333333334,0.42857142857142855,1.0
question-answering,6,VECTOR GATE PARALLELIZATION,TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,276,10,1,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8363636363636363,0.47619047619047616,0.08333333333333333
question-answering,6,"For vector gates , we have z t ? Rd instead of z t ? R .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,277,11,2,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8393939393939394,0.5238095238095238,0.16666666666666666
question-answering,6,"For vector gates , we have z t ? Rd instead of z t ? R .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,278,12,3,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8424242424242424,0.5714285714285714,0.25
question-answering,6,"For vector gates , we have z t ? Rd instead of z t ? R .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,279,13,4,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8454545454545455,0.6190476190476191,0.3333333333333333
question-answering,6,Therefore the following equation replaces Equation :,TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,280,14,5,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8484848484848485,0.6666666666666666,0.4166666666666667
question-answering,6,where z j k is the k - th column vector of z j .,TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,281,15,6,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8515151515151516,0.7142857142857143,0.5
question-answering,6,Let b ij = log ( 1 ? z i j ) for brevity .,TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,282,16,7,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8545454545454545,0.7619047619047619,0.5833333333333334
question-answering,6,Let b ij = log ( 1 ? z i j ) for brevity .,TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,283,17,8,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8575757575757575,0.8095238095238095,0.6666666666666666
question-answering,6,"Then , we can rewrite Equation 8 as following :",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,284,18,9,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8606060606060606,0.8571428571428571,0.75
question-answering,6,"where L , L ? R T T are lower and strictly lower triangular matrices of 1's are tiled across the column .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,285,19,10,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8636363636363636,0.9047619047619048,0.8333333333333334
question-answering,6,"= [ z 1 , . . . , z d ] ? R T d .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,286,20,11,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8666666666666667,0.9523809523809523,0.9166666666666666
question-answering,6,"= [ z 1 , . . . , z d ] ? R T d .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,287,21,12,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8696969696969697,1.0,1.0
question-answering,6,MODEL DETAILS,model,MODEL DETAILS,0,288,1,1,0,model : MODEL DETAILS,0.8727272727272727,0.023255813953488372,0.14285714285714285
question-answering,6,Match .,model,MODEL DETAILS,0,289,2,2,0,model : MODEL DETAILS,0.8757575757575757,0.046511627906976744,0.2857142857142857
question-answering,6,"While similar in spirit , our ' Match ' model is slightly different from previous work ( Bordes and .",model,MODEL DETAILS,0,290,3,3,0,model : MODEL DETAILS,0.8787878787878788,0.06976744186046512,0.42857142857142855
question-answering,6,"We use answer candidate embedding matrix , and add 2 dimension of 0 - 1 matrix which expresses whether the answer candidate matches with any word in the paragraph and the question .",model,MODEL DETAILS,0,291,4,4,0,model : MODEL DETAILS,0.8818181818181818,0.09302325581395349,0.5714285714285714
question-answering,6,"In other words , the softmax is computed b? v = softmax W [ W ( y ) ; M ( y ) ]? ? RV , where W ? R dd and W ( y ) ? RV ( d?2 ) are trainable weight matrices , and M ( y ) ? RV 2 is the 0 - 1 match matrix .",model,MODEL DETAILS,0,292,5,5,0,model : MODEL DETAILS,0.8848484848484849,0.11627906976744186,0.7142857142857143
question-answering,6,"In other words , the softmax is computed b? v = softmax W [ W ( y ) ; M ( y ) ]? ? RV , where W ? R dd and W ( y ) ? RV ( d?2 ) are trainable weight matrices , and M ( y ) ? RV 2 is the 0 - 1 match matrix .",model,MODEL DETAILS,0,293,6,6,0,model : MODEL DETAILS,0.8878787878787879,0.13953488372093023,0.8571428571428571
question-answering,6,"In other words , the softmax is computed b? v = softmax W [ W ( y ) ; M ( y ) ]? ? RV , where W ? R dd and W ( y ) ? RV ( d?2 ) are trainable weight matrices , and M ( y ) ? RV 2 is the 0 - 1 match matrix .",model,MODEL DETAILS,0,294,7,7,0,model : MODEL DETAILS,0.8909090909090909,0.16279069767441862,1.0
question-answering,6,VISUALIZATIONS,model,VISUALIZATIONS,0,295,8,1,0,model : VISUALIZATIONS,0.8939393939393939,0.18604651162790697,0.027777777777777776
question-answering,6,Visualization of Story - based QA . shows visualization of models for story - based QA tasks .,model,VISUALIZATIONS,0,296,9,2,0,model : VISUALIZATIONS,0.896969696969697,0.20930232558139536,0.05555555555555555
question-answering,6,"In the task 3 ( left ) , the model focuses on the facts that contain ' football ' in the first layer , and found out where Mary journeyed to before the bathroom in the second layer .",model,VISUALIZATIONS,0,297,10,3,0,model : VISUALIZATIONS,0.9,0.23255813953488372,0.08333333333333333
question-answering,6,"In task 7 ( right ) , the model focuses on the facts that provide information about the location of Sandra . 0.00 0.87 1.00 1.00 I 'm on it .",model,VISUALIZATIONS,0,298,11,4,0,model : VISUALIZATIONS,0.9030303030303031,0.2558139534883721,0.1111111111111111
question-answering,6,0.97 0.38 0.00,model,VISUALIZATIONS,0,299,12,5,0,model : VISUALIZATIONS,0.906060606060606,0.27906976744186046,0.1388888888888889
question-answering,6,How many people would you in your party .,model,VISUALIZATIONS,0,300,13,6,0,model : VISUALIZATIONS,0.9090909090909091,0.3023255813953488,0.16666666666666666
question-answering,6,1.00 0.00 0.41,model,VISUALIZATIONS,0,301,14,7,0,model : VISUALIZATIONS,0.9121212121212121,0.32558139534883723,0.19444444444444445
question-answering,6,For four people please .,model,VISUALIZATIONS,0,302,15,8,0,model : VISUALIZATIONS,0.9151515151515152,0.3488372093023256,0.2222222222222222
question-answering,6,Which price range are you looking for .,model,VISUALIZATIONS,0,303,16,9,0,model : VISUALIZATIONS,0.9181818181818182,0.37209302325581395,0.25
question-answering,6,Layer 1 Layer 2 Task 1 Issuing API calls z 1 ? ? r 1 ? ? r 1 z,model,VISUALIZATIONS,0,304,17,10,0,model : VISUALIZATIONS,0.9212121212121213,0.3953488372093023,0.2777777777777778
question-answering,6,Can you make a restaurant reservation for eight in a cheap price range in madrid 0.00 1.00 0.93 1.00 I 'm on it .,model,VISUALIZATIONS,0,305,18,11,0,model : VISUALIZATIONS,0.9242424242424242,0.4186046511627907,0.3055555555555556
question-answering,6,1.00 0.74 0.00,model,VISUALIZATIONS,0,306,19,12,0,model : VISUALIZATIONS,0.9272727272727272,0.4418604651162791,0.3333333333333333
question-answering,6,Any preference on a type of cuisine .,model,VISUALIZATIONS,0,307,20,13,0,model : VISUALIZATIONS,0.9303030303030303,0.46511627906976744,0.3611111111111111
question-answering,6,0.11 1.00 0.01,model,VISUALIZATIONS,0,308,21,14,0,model : VISUALIZATIONS,0.9333333333333333,0.4883720930232558,0.3888888888888889
question-answering,6,love british food .,model,VISUALIZATIONS,0,309,22,15,0,model : VISUALIZATIONS,0.9363636363636364,0.5116279069767442,0.4166666666666667
question-answering,6,0.99 0.99 0.57,model,VISUALIZATIONS,0,310,23,16,0,model : VISUALIZATIONS,0.9393939393939394,0.5348837209302325,0.4444444444444444
question-answering,6,Okay let me look into some options for you .,model,VISUALIZATIONS,0,311,24,17,0,model : VISUALIZATIONS,0.9424242424242424,0.5581395348837209,0.4722222222222222
question-answering,6,0.00 0.00 0.02 < SILENCE > API CALL british madrid eight cheap Layer 1 Layer 2 Task 4 Providing extra-information z 1 ? ? r 1 ? ? r 1 z 2 resto-paris-expen-spanish-8stars,model,VISUALIZATIONS,0,312,25,18,0,model : VISUALIZATIONS,0.9454545454545454,0.5813953488372093,0.5
question-answering,6,R-phone resto-paris -expen-spanish-8stars-phone 0.71 0.84 0.99 0.36 resto-paris-expen-spanish-8stars,model,VISUALIZATIONS,0,313,26,19,0,model : VISUALIZATIONS,0.9484848484848485,0.6046511627906976,0.5277777777777778
question-answering,6,R-address resto-paris - expen-spanish-8stars-address 1.00 0.99 1.00 1.00 resto-paris-expen-spanish-8stars R-location paris 0.05 0.01 1.00 0.00 resto-paris-expen-spanish-8stars R-number four 0.02 0.95 0.97 0.00 resto-paris-expen-spanish-8stars,model,VISUALIZATIONS,0,314,27,20,0,model : VISUALIZATIONS,0.9515151515151515,0.627906976744186,0.5555555555555556
question-answering,6,R-price expensive 0.00 0.05 0.92 0.00 resto-paris-expen-spanish-8stars,model,VISUALIZATIONS,0,315,28,21,0,model : VISUALIZATIONS,0.9545454545454546,0.6511627906976745,0.5833333333333334
question-answering,6,R-rating 8 0.38 0.91 1.00 0.10,model,VISUALIZATIONS,0,316,29,22,0,model : VISUALIZATIONS,0.9575757575757575,0.6744186046511628,0.6111111111111112
question-answering,6,What do you think of this option : resto-paris-expen-spanish-8stars 0.90 0.93 0.99 1.00,model,VISUALIZATIONS,0,317,30,23,0,model : VISUALIZATIONS,0.9606060606060606,0.6976744186046512,0.6388888888888888
question-answering,6,Let 's do it .,model,VISUALIZATIONS,0,318,31,24,0,model : VISUALIZATIONS,0.9636363636363636,0.7209302325581395,0.6666666666666666
question-answering,6,0.00 1.00 0.00,model,VISUALIZATIONS,0,319,32,25,0,model : VISUALIZATIONS,0.9666666666666667,0.7441860465116279,0.6944444444444444
question-answering,6,Great let me do the reservation .,model,VISUALIZATIONS,0,320,33,26,0,model : VISUALIZATIONS,0.9696969696969697,0.7674418604651163,0.7222222222222222
question-answering,6,0.99 0.97 0.00,model,VISUALIZATIONS,0,321,34,27,0,model : VISUALIZATIONS,0.9727272727272728,0.7906976744186046,0.75
question-answering,6,Do you have its address .,model,VISUALIZATIONS,0,322,35,28,0,model : VISUALIZATIONS,0.9757575757575757,0.813953488372093,0.7777777777777778
question-answering,6,Here it is : resto - paris - expen-spanish -8stars - address : Visualization of update and reset gates in QRN ' 2 r ' model for on several tasks of bAbI dialog and DSTC2 dialog .,model,VISUALIZATIONS,0,323,36,29,0,model : VISUALIZATIONS,0.9787878787878788,0.8372093023255814,0.8055555555555556
question-answering,6,We do not put reset gate in the last layer .,model,VISUALIZATIONS,0,324,37,30,0,model : VISUALIZATIONS,0.9818181818181818,0.8604651162790697,0.8333333333333334
question-answering,6,"Note that we only show some of recent sentences here , even the dialog has more sentences .",model,VISUALIZATIONS,0,325,38,31,0,model : VISUALIZATIONS,0.9848484848484849,0.8837209302325582,0.8611111111111112
question-answering,6,Visualization of Dialog .,model,VISUALIZATIONS,0,326,39,32,0,model : VISUALIZATIONS,0.9878787878787879,0.9069767441860465,0.8888888888888888
question-answering,6,shows visualization of models for dialog tasks .,model,VISUALIZATIONS,0,327,40,33,0,model : VISUALIZATIONS,0.990909090909091,0.9302325581395349,0.9166666666666666
question-answering,6,"In the first dialog of task 1 , the model focuses on the user utterance that mentions the user 's desired cuisine and location , and the current query ( user 's last utterance ) informs the system of the number of people , so the system is able to learn that it now needs to ask the user about the desired price range .",model,VISUALIZATIONS,0,328,41,34,0,model : VISUALIZATIONS,0.9939393939393939,0.9534883720930233,0.9444444444444444
question-answering,6,"In the second dialog of task 1 , the model focuses on the facts that provide information about the requests of the user .",model,VISUALIZATIONS,0,329,42,35,0,model : VISUALIZATIONS,0.996969696969697,0.9767441860465116,0.9722222222222222
question-answering,6,"In task 4 ( third ) , the model focuses on what restaurant a user is talking about and the information about the restaurant .",model,VISUALIZATIONS,0,330,43,36,0,model : VISUALIZATIONS,1.0,1.0,1.0
question-answering,7,Neural Semantic Encoders,title,title,0,2,1,1,0,title : title,0.007272727272727273,1.0,1.0
question-answering,7,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01090909090909091,0.034482758620689655,0.034482758620689655
question-answering,7,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.014545454545454545,0.06896551724137931,0.06896551724137931
question-answering,7,"NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01818181818181818,0.10344827586206896,0.10344827586206896
question-answering,7,NSE can also access 1 multiple and shared memories .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.02181818181818182,0.13793103448275862,0.13793103448275862
question-answering,7,"In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.025454545454545455,0.1724137931034483,0.1724137931034483
question-answering,7,"For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02909090909090909,0.20689655172413793,0.20689655172413793
question-answering,7,Recurrent neural networks ( RNNs ) have been successful for modeling sequences,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03272727272727273,0.2413793103448276,0.2413793103448276
question-answering,7,1 ] .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.03636363636363636,0.27586206896551724,0.27586206896551724
question-answering,7,"Particularly , RNNs equipped with internal short memories , such as long short - term memories ( LSTM )",abstract,abstract,0,11,9,9,0,abstract : abstract,0.04,0.3103448275862069,0.3103448275862069
question-answering,7,"2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.04363636363636364,0.3448275862068966,0.3448275862068966
question-answering,7,LSTM is powerful because it learns to control it s short term memories .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.04727272727272727,0.3793103448275862,0.3793103448275862
question-answering,7,"However , the short term memories in LSTM are a part of the training parameters .",abstract,abstract,0,14,12,12,0,abstract : abstract,0.05090909090909091,0.41379310344827586,0.41379310344827586
question-answering,7,This imposes some practical difficulties in training and modeling long sequences with LSTM .,abstract,abstract,0,15,13,13,0,abstract : abstract,0.05454545454545454,0.4482758620689655,0.4482758620689655
question-answering,7,Recently several studies have explored ways of extending the neural networks with an external memory [ 5 ] [ 6 ] [ 7 ] .,abstract,abstract,0,16,14,14,0,abstract : abstract,0.05818181818181818,0.4827586206896552,0.4827586206896552
question-answering,7,"Unlike LSTM , the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted .",abstract,abstract,0,17,15,15,0,abstract : abstract,0.06181818181818182,0.5172413793103449,0.5172413793103449
question-answering,7,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,abstract,abstract,1,18,16,16,0,abstract : abstract,0.06545454545454546,0.5517241379310345,0.5517241379310345
question-answering,7,NSE offers several desirable properties .,abstract,abstract,0,19,17,17,0,abstract : abstract,0.06909090909090909,0.5862068965517241,0.5862068965517241
question-answering,7,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,abstract,abstract,1,20,18,18,0,abstract : abstract,0.07272727272727272,0.6206896551724138,0.6206896551724138
question-answering,7,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",abstract,abstract,1,21,19,19,0,abstract : abstract,0.07636363636363637,0.6551724137931034,0.6551724137931034
question-answering,7,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,abstract,abstract,1,22,20,20,0,abstract : abstract,0.08,0.6896551724137931,0.6896551724137931
question-answering,7,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,abstract,abstract,1,23,21,21,0,abstract : abstract,0.08363636363636363,0.7241379310344828,0.7241379310344828
question-answering,7,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",abstract,abstract,1,24,22,22,0,abstract : abstract,0.08727272727272728,0.7586206896551724,0.7586206896551724
question-answering,7,We evaluate NSE on five different real tasks .,abstract,abstract,0,25,23,23,0,abstract : abstract,0.09090909090909091,0.7931034482758621,0.7931034482758621
question-answering,7,"For four of them , our models set new state - of - theart results .",abstract,abstract,0,26,24,24,0,abstract : abstract,0.09454545454545454,0.8275862068965517,0.8275862068965517
question-answering,7,Our results suggest that a NN model with the shared memory between encoder and decoder is a promising approach for sequence transduction problems such as machine translation and abstractive summarization .,abstract,abstract,0,27,25,25,0,abstract : abstract,0.09818181818181818,0.8620689655172413,0.8620689655172413
question-answering,7,"In particular , we observe that the attention - based neural machine translation can be further improved by shared - memory models .",abstract,abstract,0,28,26,26,0,abstract : abstract,0.10181818181818182,0.896551724137931,0.896551724137931
question-answering,7,We also analyze memory access pattern and compositionality in NSE and show that our model captures semantic and syntactic structures of input sentence .,abstract,abstract,0,29,27,27,0,abstract : abstract,0.10545454545454545,0.9310344827586207,0.9310344827586207
question-answering,7,1,abstract,abstract,0,30,28,28,0,abstract : abstract,0.10909090909090909,0.9655172413793104,0.9655172413793104
question-answering,7,"By access we mean changing the memory states by the read , compose and write operations .",abstract,abstract,0,31,29,29,0,abstract : abstract,0.11272727272727273,1.0,1.0
question-answering,7,Related Work,related work,related work,0,32,1,1,0,related work : related work,0.11636363636363636,0.024390243902439025,0.024390243902439025
question-answering,7,One of the pioneering work that attempts to extend deep neural networks with an external memory is Neural Turing Machines ( NTM ) .,related work,related work,0,33,2,2,0,related work : related work,0.12,0.04878048780487805,0.04878048780487805
question-answering,7,NTM implements a centralized controller and a fixed - sized random access memory .,related work,related work,0,34,3,3,0,related work : related work,0.12363636363636364,0.07317073170731707,0.07317073170731707
question-answering,7,The NTM memory is addressable by both content ( i.e. soft attention ) and location based access mechanisms .,related work,related work,0,35,4,4,0,related work : related work,0.12727272727272726,0.0975609756097561,0.0975609756097561
question-answering,7,The authors evaluated NTM on algorithmic tasks such as copying and sorting sequences .,related work,related work,0,36,5,5,0,related work : related work,0.13090909090909092,0.12195121951219512,0.12195121951219512
question-answering,7,Comparison with Neural Turing Machines : NSE addresses certain drawbacks of NTM .,related work,related work,0,37,6,6,0,related work : related work,0.13454545454545455,0.14634146341463414,0.14634146341463414
question-answering,7,"NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach .",related work,related work,0,38,7,7,0,related work : related work,0.13818181818181818,0.17073170731707318,0.17073170731707318
question-answering,7,"The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation .",related work,related work,0,39,8,8,0,related work : related work,0.14181818181818182,0.1951219512195122,0.1951219512195122
question-answering,7,"In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e. read - write ) in order to process the memory entries and input information .",related work,related work,0,40,9,9,0,related work : related work,0.14545454545454545,0.21951219512195122,0.21951219512195122
question-answering,7,The main advantage of NSE over NTM is in its memory update .,related work,related work,0,41,10,10,0,related work : related work,0.14909090909090908,0.24390243902439024,0.24390243902439024
question-answering,7,"Despite its sophisticated addressing mechanism , the NTM controller does not have mechanism to avoid information collision in the memory .",related work,related work,0,42,11,11,0,related work : related work,0.15272727272727274,0.2682926829268293,0.2682926829268293
question-answering,7,Particularly the NTM controller emits two separate set of access weights ( i.e. read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to .,related work,related work,0,43,12,12,0,related work : related work,0.15636363636363637,0.2926829268292683,0.2926829268292683
question-answering,7,Moreover the fixed - size memory in NTM has no memory allocation or de-allocation protocol .,related work,related work,0,44,13,13,0,related work : related work,0.16,0.3170731707317073,0.3170731707317073
question-answering,7,"Therefore unless the controller is intelligent enough to track the previous read / write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales .",related work,related work,0,45,14,14,0,related work : related work,0.16363636363636364,0.34146341463414637,0.34146341463414637
question-answering,7,We think that this is a potential reason that makes NTM hard to train and makes the training not stable .,related work,related work,0,46,15,15,0,related work : related work,0.16727272727272727,0.36585365853658536,0.36585365853658536
question-answering,7,We also note that the effectiveness of the location based addressing introduced in NTM is unclear .,related work,related work,0,47,16,16,0,related work : related work,0.1709090909090909,0.3902439024390244,0.3902439024390244
question-answering,7,"In NSE , we introduce a novel and systematic memory update approach based on the soft attention mechanism .",related work,related work,0,48,17,17,0,related work : related work,0.17454545454545456,0.4146341463414634,0.4146341463414634
question-answering,7,NSE writes new information to the most recently read memory locations .,related work,related work,0,49,18,18,0,related work : related work,0.1781818181818182,0.43902439024390244,0.43902439024390244
question-answering,7,This is accomplished by sharing the same memory key vector between the read and write modules .,related work,related work,0,50,19,19,0,related work : related work,0.18181818181818182,0.4634146341463415,0.4634146341463415
question-answering,7,The NSE memory update is scalable and potentially more robust to train .,related work,related work,0,51,20,20,0,related work : related work,0.18545454545454546,0.4878048780487805,0.4878048780487805
question-answering,7,"NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed .",related work,related work,0,52,21,21,0,related work : related work,0.1890909090909091,0.5121951219512195,0.5121951219512195
question-answering,7,The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation or de-allocation protocols .,related work,related work,0,53,22,22,0,related work : related work,0.19272727272727272,0.5365853658536586,0.5365853658536586
question-answering,7,Each memory location of the NSE memory stores a token representation in input sequence during encoding .,related work,related work,0,54,23,23,0,related work : related work,0.19636363636363635,0.5609756097560976,0.5609756097560976
question-answering,7,"This provides NSE with an anytime - access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention - based encoders .",related work,related work,0,55,24,24,0,related work : related work,0.2,0.5853658536585366,0.5853658536585366
question-answering,7,"Lastly , NTM addresses small algorithmic problems while NSE focuses on a set of large - scale language understanding tasks .",related work,related work,0,56,25,25,0,related work : related work,0.20363636363636364,0.6097560975609756,0.6097560975609756
question-answering,7,The RNNSearch model proposed in can be seen as a variation of memory augmented networks due to its ability to read the historic output states of RNNs with soft attention .,related work,related work,0,57,26,26,0,related work : related work,0.20727272727272728,0.6341463414634146,0.6341463414634146
question-answering,7,The work of combines the soft attention with Memory Networks ( Mem NNs ) .,related work,related work,0,58,27,27,0,related work : related work,0.2109090909090909,0.6585365853658537,0.6585365853658537
question-answering,7,"Similar to RNNSearch , MemNNs are designed with non-writable memories .",related work,related work,0,59,28,28,0,related work : related work,0.21454545454545454,0.6829268292682927,0.6829268292682927
question-answering,7,It constructs layered memory representations and showed promising results on both artificial and real question answering tasks .,related work,related work,0,60,29,29,0,related work : related work,0.21818181818181817,0.7073170731707317,0.7073170731707317
question-answering,7,We note that RNNSearch and MemNNs avoid the memory update and management overhead by simply using a non-writable memory storage .,related work,related work,0,61,30,30,0,related work : related work,0.22181818181818183,0.7317073170731707,0.7317073170731707
question-answering,7,Another variation of MemNNs is Dynamic Memory Network that is equipped with an episodic memory and seems to be flexible in different settings .,related work,related work,0,62,31,31,0,related work : related work,0.22545454545454546,0.7560975609756098,0.7560975609756098
question-answering,7,"Although NSE differs from other memory - augumented NN models in many aspects , they all use soft attention mechanism with a type of similarity measures to retrieve relevant information from the external memory .",related work,related work,0,63,32,32,0,related work : related work,0.2290909090909091,0.7804878048780488,0.7804878048780488
question-answering,7,"For example , NTM implements cosine similarity and MemNNs use vector dot product .",related work,related work,0,64,33,33,0,related work : related work,0.23272727272727273,0.8048780487804879,0.8048780487804879
question-answering,7,NSE uses the vector dot product for the similarity measure in NSE because it is faster to compute .,related work,related work,0,65,34,34,0,related work : related work,0.23636363636363636,0.8292682926829268,0.8292682926829268
question-answering,7,"Other related work includes Neural Program - Interpreters , which learns to run sub-programs and to compose them for high - level programs .",related work,related work,0,66,35,35,0,related work : related work,0.24,0.8536585365853658,0.8536585365853658
question-answering,7,It uses execution traces to provide the full supervision .,related work,related work,0,67,36,36,0,related work : related work,0.24363636363636362,0.8780487804878049,0.8780487804878049
question-answering,7,Researchers have also explored ways to add unbounded memory to LSTM using a particular data structure .,related work,related work,0,68,37,37,0,related work : related work,0.24727272727272728,0.9024390243902439,0.9024390243902439
question-answering,7,"Although this type of architecture provides a flexible capacity to store information , the memory access is constrained by the data structure used for the memory bank , such as stack and queue .",related work,related work,0,69,38,38,0,related work : related work,0.2509090909090909,0.926829268292683,0.926829268292683
question-answering,7,Overall it is expensive to train and to scale the previously proposed memory - based models .,related work,related work,0,70,39,39,0,related work : related work,0.2545454545454545,0.9512195121951219,0.9512195121951219
question-answering,7,Most models required a set of clever engineering tricks to work successfully .,related work,related work,0,71,40,40,0,related work : related work,0.2581818181818182,0.975609756097561,0.975609756097561
question-answering,7,Most of the aforementioned memory augmented neural networks have been tested on synthetic tasks whereas in this paper we evaluated NSE on a wide range of real and large - scale natural language applications .,related work,related work,0,72,41,41,0,related work : related work,0.26181818181818184,1.0,1.0
question-answering,7,Proposed Approach,approach,Proposed Approach,0,73,1,1,0,approach : Proposed Approach,0.26545454545454544,1.0,1.0
question-answering,7,Our training set consists,training,training,0,74,1,1,0,training : training,0.2690909090909091,0.02040816326530612,0.125
question-answering,7,". , w i Ti of tokens while the output Y i can be either a single target or a sequence .",training,training,0,75,2,2,0,training : training,0.2727272727272727,0.04081632653061224,0.25
question-answering,7,We transform each input token wt to its word embedding x t .,training,training,0,76,3,3,0,training : training,0.27636363636363637,0.061224489795918366,0.375
question-answering,7,"Our Neural Semantic Encoders ( NSE ) model has four main components : read , compose and write modules and an encoding memory M ? R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",training,training,0,77,4,4,0,training : training,0.28,0.08163265306122448,0.5
question-answering,7,"Our Neural Semantic Encoders ( NSE ) model has four main components : read , compose and write modules and an encoding memory M ? R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",training,training,0,78,5,5,0,training : training,0.28363636363636363,0.10204081632653061,0.625
question-answering,7,Each memory slot vector mt ? R k corresponds to the vector representation of information about word wt in memory .,training,training,0,79,6,6,0,training : training,0.2872727272727273,0.12244897959183673,0.75
question-answering,7,Each memory slot vector mt ? R k corresponds to the vector representation of information about word wt in memory .,training,training,0,80,7,7,0,training : training,0.2909090909090909,0.14285714285714285,0.875
question-answering,7,"In particular , the memory is initialized by the embedding vectors {x t } l t=1 and is evolved overtime , through read , compose and write operations .",training,training,0,81,8,8,0,training : training,0.29454545454545455,0.16326530612244897,1.0
question-answering,7,"Read , Compose and Write",training,"Read, Compose and Write",0,82,9,1,0,"training : Read, Compose and Write",0.29818181818181816,0.1836734693877551,0.03571428571428571
question-answering,7,NSE performs three main operations in every time step .,training,"Read, Compose and Write",0,83,10,2,0,"training : Read, Compose and Write",0.3018181818181818,0.20408163265306123,0.07142857142857142
question-answering,7,"After initializing the memory slots with the corresponding input representations , NSE processes an embedding vector x t and retrieves a memory slot m r,t that is expected to be associatively coherent ( i.e. semantically associated ) with the current input word wt .",training,"Read, Compose and Write",0,84,11,3,0,"training : Read, Compose and Write",0.3054545454545455,0.22448979591836735,0.10714285714285714
question-answering,7,The slot location r ( ranging from 1 to l ) is defined by a key vector z t which the read module emits by attending over the memory slots .,training,"Read, Compose and Write",0,85,12,4,0,"training : Read, Compose and Write",0.3090909090909091,0.24489795918367346,0.14285714285714285
question-answering,7,The compose module implements a composition operation that combines the memory slot with the current input .,training,"Read, Compose and Write",0,86,13,5,0,"training : Read, Compose and Write",0.31272727272727274,0.2653061224489796,0.17857142857142858
question-answering,7,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,training,"Read, Compose and Write",0,87,14,6,0,"training : Read, Compose and Write",0.31636363636363635,0.2857142857142857,0.21428571428571427
question-answering,7,"Instead of composing the raw embedding vector x t , we use the hidden state o t produced by the read module at time t",training,"Read, Compose and Write",0,88,15,7,0,"training : Read, Compose and Write",0.32,0.30612244897959184,0.25
question-answering,7,"where 1 is a matrix of ones , ? denotes the outer product which duplicates it s left vector l or k times to form a matrix .",training,"Read, Compose and Write",0,89,16,8,0,"training : Read, Compose and Write",0.3236363636363636,0.32653061224489793,0.2857142857142857
question-answering,7,"where 1 is a matrix of ones , ? denotes the outer product which duplicates it s left vector l or k times to form a matrix .",training,"Read, Compose and Write",0,90,17,9,0,"training : Read, Compose and Write",0.32727272727272727,0.3469387755102041,0.32142857142857145
question-answering,7,The read function f LST,training,"Read, Compose and Write",0,91,18,10,0,"training : Read, Compose and Write",0.33090909090909093,0.3673469387755102,0.35714285714285715
question-answering,7,Mr sequentially maps the word embeddings to the internal space of the memory M t?1 .,training,"Read, Compose and Write",0,92,19,11,0,"training : Read, Compose and Write",0.33454545454545453,0.3877551020408163,0.39285714285714285
question-answering,7,Then Equation 2 looks for the slots related to the input by computing association degree between each memory slot and the hidden state o t .,training,"Read, Compose and Write",0,93,20,12,0,"training : Read, Compose and Write",0.3381818181818182,0.40816326530612246,0.42857142857142855
question-answering,7,We calculate the association degree by the dot product and transform this scores to the fuzzy key vector z t by normalizing with sof tmax function .,training,"Read, Compose and Write",0,94,21,13,0,"training : Read, Compose and Write",0.3418181818181818,0.42857142857142855,0.4642857142857143
question-answering,7,"Since our key vector is fuzzy , the slot to be composed is retrieved by taking weighted sum of the all slots as in Equation .",training,"Read, Compose and Write",0,95,22,14,0,"training : Read, Compose and Write",0.34545454545454546,0.4489795918367347,0.5
question-answering,7,This process can also be seen as the soft attention mechanism .,training,"Read, Compose and Write",0,96,23,15,0,"training : Read, Compose and Write",0.3490909090909091,0.46938775510204084,0.5357142857142857
question-answering,7,"In Equation 4 and 5 , we compose and process the retrieved slot with the current hidden state and map the resulting vector to the encoder output space .",training,"Read, Compose and Write",0,97,24,16,0,"training : Read, Compose and Write",0.3527272727272727,0.4897959183673469,0.5714285714285714
question-answering,7,"Finally , we write the new representation to the memory location pointed by the key vector in where the key vector z t emitted by the read module is reused to inform the write module of the most recently read slots .",training,"Read, Compose and Write",0,98,25,17,0,"training : Read, Compose and Write",0.3563636363636364,0.5102040816326531,0.6071428571428571
question-answering,7,First the slot information that was retrieved is erased and then the new representation is located .,training,"Read, Compose and Write",0,99,26,18,0,"training : Read, Compose and Write",0.36,0.5306122448979592,0.6428571428571429
question-answering,7,NSE performs this iterative process until all words in the input sequence are read .,training,"Read, Compose and Write",0,100,27,19,0,"training : Read, Compose and Write",0.36363636363636365,0.5510204081632653,0.6785714285714286
question-answering,7,The encoding memories { M } T t=1 and output states {h} T t=1 are further used for the tasks .,training,"Read, Compose and Write",0,101,28,20,0,"training : Read, Compose and Write",0.36727272727272725,0.5714285714285714,0.7142857142857143
question-answering,7,"Although NSE reads a single word at a time , it has an anytime - access to the entire sequence stored in the encoding memory .",training,"Read, Compose and Write",0,102,29,21,0,"training : Read, Compose and Write",0.3709090909090909,0.5918367346938775,0.75
question-answering,7,"With the encoding memory , NSE maintains a mental image of the input sequence .",training,"Read, Compose and Write",0,103,30,22,0,"training : Read, Compose and Write",0.37454545454545457,0.6122448979591837,0.7857142857142857
question-answering,7,The memory is initialized with the raw embedding vector at time t = 0 .,training,"Read, Compose and Write",0,104,31,23,0,"training : Read, Compose and Write",0.3781818181818182,0.6326530612244898,0.8214285714285714
question-answering,7,We term such a freshly initialized memory a baby memory .,training,"Read, Compose and Write",0,105,32,24,0,"training : Read, Compose and Write",0.38181818181818183,0.6530612244897959,0.8571428571428571
question-answering,7,"As NSE reads more input content in time , the baby memory evolves and refines the encoded mental image .",training,"Read, Compose and Write",0,106,33,25,0,"training : Read, Compose and Write",0.38545454545454544,0.673469387755102,0.8928571428571429
question-answering,7,functions are neural networks and are the training parameters in our NSE .,training,"Read, Compose and Write",0,107,34,26,0,"training : Read, Compose and Write",0.3890909090909091,0.6938775510204082,0.9285714285714286
question-answering,7,"As the name suggests , we use LSTM and multi -layer perceptron ( MLP ) in this paper .",training,"Read, Compose and Write",0,108,35,27,0,"training : Read, Compose and Write",0.3927272727272727,0.7142857142857143,0.9642857142857143
question-answering,7,"Since NSE is fully differentiable , it can be trained with any gradient descent optimizer .",training,"Read, Compose and Write",0,109,36,28,0,"training : Read, Compose and Write",0.39636363636363636,0.7346938775510204,1.0
question-answering,7,Shared and Multiple Memory Accesses,training,Shared and Multiple Memory Accesses,0,110,37,1,0,training : Shared and Multiple Memory Accesses,0.4,0.7551020408163265,0.07692307692307693
question-answering,7,"For sequence to sequence transduction tasks like question answering , natural language inference and machine translation , it is beneficial to access other relevant memories in addition to its own one .",training,Shared and Multiple Memory Accesses,0,111,38,2,0,training : Shared and Multiple Memory Accesses,0.4036363636363636,0.7755102040816326,0.15384615384615385
question-answering,7,The shared or the multiple memory access allows a set of NSEs to exchange knowledge representations and to communicate with each other to accomplish a particular task throughout the encoding memory .,training,Shared and Multiple Memory Accesses,0,112,39,3,0,training : Shared and Multiple Memory Accesses,0.4072727272727273,0.7959183673469388,0.23076923076923078
question-answering,7,"NSE can be extended easily , so that it is able to read from and write to multiple memories simultaneously or multiple NSEs are able to access a shared memory .",training,Shared and Multiple Memory Accesses,0,113,40,4,0,training : Shared and Multiple Memory Accesses,0.4109090909090909,0.8163265306122449,0.3076923076923077
question-answering,7,b ) depicts a high - level architectural diagram of a multiple memory access - NSE ( MMA - NSE ) .,training,Shared and Multiple Memory Accesses,0,114,41,5,0,training : Shared and Multiple Memory Accesses,0.41454545454545455,0.8367346938775511,0.38461538461538464
question-answering,7,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,training,Shared and Multiple Memory Accesses,0,115,42,6,0,training : Shared and Multiple Memory Accesses,0.41818181818181815,0.8571428571428571,0.46153846153846156
question-answering,7,"Given a shared memory Mn ? R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",training,Shared and Multiple Memory Accesses,0,116,43,7,0,training : Shared and Multiple Memory Accesses,0.4218181818181818,0.8775510204081632,0.5384615384615384
question-answering,7,"Given a shared memory Mn ? R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",training,Shared and Multiple Memory Accesses,0,117,44,8,0,training : Shared and Multiple Memory Accesses,0.4254545454545455,0.8979591836734694,0.6153846153846154
question-answering,7,and this is almost the same as standard NSE .,training,Shared and Multiple Memory Accesses,0,118,45,9,0,training : Shared and Multiple Memory Accesses,0.4290909090909091,0.9183673469387755,0.6923076923076923
question-answering,7,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,training,Shared and Multiple Memory Accesses,0,119,46,10,0,training : Shared and Multiple Memory Accesses,0.43272727272727274,0.9387755102040817,0.7692307692307693
question-answering,7,"In MMA - NSE , the different memory slots are retrieved from the shared memories depending on their encoded semantic representations .",training,Shared and Multiple Memory Accesses,0,120,47,11,0,training : Shared and Multiple Memory Accesses,0.43636363636363634,0.9591836734693877,0.8461538461538461
question-answering,7,They are then composed together with the current input and written back to their corresponding slots .,training,Shared and Multiple Memory Accesses,0,121,48,12,0,training : Shared and Multiple Memory Accesses,0.44,0.9795918367346939,0.9230769230769231
question-answering,7,Note that MMA - NSE is capable of accessing a variable number of relevant shared memories once a composition function that takes in dynamic inputs is chosen .,training,Shared and Multiple Memory Accesses,0,122,49,13,0,training : Shared and Multiple Memory Accesses,0.44363636363636366,1.0,1.0
question-answering,7,Experiments,experiment,Experiments,0,123,1,1,0,experiment : Experiments,0.44727272727272727,0.012987012987012988,0.08333333333333333
question-answering,7,"We describe in this section experiments on five different tasks , in order to show that NSE can be effective and flexible in different settings .",experiment,Experiments,0,124,2,2,0,experiment : Experiments,0.4509090909090909,0.025974025974025976,0.16666666666666666
question-answering,7,"We report results on natural language inference , question answering ( QA ) , sentence classification , document sentiment analysis and machine translation .",experiment,Experiments,0,125,3,3,0,experiment : Experiments,0.45454545454545453,0.03896103896103896,0.25
question-answering,7,All five tasks challenge a model in terms of language understanding and semantic reasoning .,experiment,Experiments,0,126,4,4,0,experiment : Experiments,0.4581818181818182,0.05194805194805195,0.3333333333333333
question-answering,7,The models are trained using Adam with hyperparameters selected on development set .,experiment,Experiments,1,127,5,5,0,experiment : Experiments,0.4618181818181818,0.06493506493506493,0.4166666666666667
question-answering,7,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,experiment,Experiments,1,128,6,6,0,experiment : Experiments,0.46545454545454545,0.07792207792207792,0.5
question-answering,7,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,experiment,Experiments,1,129,7,7,0,experiment : Experiments,0.4690909090909091,0.09090909090909091,0.5833333333333334
question-answering,7,The word embeddings are fixed during training .,experiment,Experiments,0,130,8,8,0,experiment : Experiments,0.4727272727272727,0.1038961038961039,0.6666666666666666
question-answering,7,The embeddings for out - of - vocabulary words were set to zero vector .,experiment,Experiments,0,131,9,9,0,experiment : Experiments,0.4763636363636364,0.11688311688311688,0.75
question-answering,7,We crop or pad the input sequence to a fixed length .,experiment,Experiments,1,132,10,10,0,experiment : Experiments,0.48,0.12987012987012986,0.8333333333333334
question-answering,7,padding vector was inserted when padding .,experiment,Experiments,0,133,11,11,0,experiment : Experiments,0.48363636363636364,0.14285714285714285,0.9166666666666666
question-answering,7,The models were regularized by using dropouts and an l 2 weight decay .,experiment,Experiments,1,134,12,12,0,experiment : Experiments,0.48727272727272725,0.15584415584415584,1.0
question-answering,7,Natural Language Inference,experiment,Natural Language Inference,1,135,13,1,0,experiment : Natural Language Inference,0.4909090909090909,0.16883116883116883,0.037037037037037035
question-answering,7,The natural language inference is one of the main tasks in language understanding .,experiment,Natural Language Inference,0,136,14,2,0,experiment : Natural Language Inference,0.49454545454545457,0.18181818181818182,0.07407407407407407
question-answering,7,This task tests the ability of a model to reason about the semantic relationship between two sentences .,experiment,Natural Language Inference,0,137,15,3,0,experiment : Natural Language Inference,0.49818181818181817,0.19480519480519481,0.1111111111111111
question-answering,7,"In order to perform well on the task , NSE should be able to capture sentence semantics and be able to reason the relation between a sentence pair , i.e. , whether a premise - hypothesis pair is entailing , contradictory or neutral .",experiment,Natural Language Inference,0,138,16,4,0,experiment : Natural Language Inference,0.5018181818181818,0.2077922077922078,0.14814814814814814
question-answering,7,"We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train / dev / test sets and target label indicating their relation .",experiment,Natural Language Inference,0,139,17,5,0,experiment : Natural Language Inference,0.5054545454545455,0.22077922077922077,0.18518518518518517
question-answering,7,"Following the setting in the NSE output for each sentence was the input to a MLP , where the input layer computes the concatenation [ h pl ; h h l ] , absolute difference hp l ? h h land elementwise product hp l h h l of the two sentence representations .",experiment,Natural Language Inference,0,140,18,6,0,experiment : Natural Language Inference,0.509090909090909,0.23376623376623376,0.2222222222222222
question-answering,7,"Following the setting in the NSE output for each sentence was the input to a MLP , where the input layer computes the concatenation [ h pl ; h h l ] , absolute difference hp l ? h h land elementwise product hp l h h l of the two sentence representations .",experiment,Natural Language Inference,0,141,19,7,0,experiment : Natural Language Inference,0.5127272727272727,0.24675324675324675,0.25925925925925924
question-answering,7,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .",experiment,Natural Language Inference,1,142,20,8,0,experiment : Natural Language Inference,0.5163636363636364,0.2597402597402597,0.2962962962962963
question-answering,7,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",experiment,Natural Language Inference,1,143,21,9,0,experiment : Natural Language Inference,0.52,0.2727272727272727,0.3333333333333333
question-answering,7,The write / read neural nets and the last linear layer were regularized by using 30 % dropouts .,experiment,Natural Language Inference,0,144,22,10,0,experiment : Natural Language Inference,0.5236363636363637,0.2857142857142857,0.37037037037037035
question-answering,7,We evaluated three different variations of NSE show in .,experiment,Natural Language Inference,0,145,23,11,0,experiment : Natural Language Inference,0.5272727272727272,0.2987012987012987,0.4074074074074074
question-answering,7,The NSE model encodes each sentence simultaneously by using a separate memory for each sentence .,experiment,Natural Language Inference,0,146,24,12,0,experiment : Natural Language Inference,0.5309090909090909,0.3116883116883117,0.4444444444444444
question-answering,7,The second model - MMA - NSE first encodes the premise and then the hypothesis sentence by sharing the premise encoded memory in addition to the hypothesis memory .,experiment,Natural Language Inference,0,147,25,13,0,experiment : Natural Language Inference,0.5345454545454545,0.3246753246753247,0.48148148148148145
question-answering,7,"For the third model , we use inter-sentence attention which selectively reconstructs the premise representation .",experiment,Natural Language Inference,0,148,26,14,0,experiment : Natural Language Inference,0.5381818181818182,0.33766233766233766,0.5185185185185185
question-answering,7,shows the results of our models along with the results of published methods for the task .,experiment,Natural Language Inference,0,149,27,15,0,experiment : Natural Language Inference,0.5418181818181819,0.35064935064935066,0.5555555555555556
question-answering,7,The classifier with handcrafted features extracts a set of lexical features .,experiment,Natural Language Inference,0,150,28,16,0,experiment : Natural Language Inference,0.5454545454545454,0.36363636363636365,0.5925925925925926
question-answering,7,The next group of models are based on sentence encoding .,experiment,Natural Language Inference,0,151,29,17,0,experiment : Natural Language Inference,0.5490909090909091,0.37662337662337664,0.6296296296296297
question-answering,7,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output .",experiment,Natural Language Inference,0,152,30,18,0,experiment : Natural Language Inference,0.5527272727272727,0.38961038961038963,0.6666666666666666
question-answering,7,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,experiment,Natural Language Inference,0,153,31,19,0,experiment : Natural Language Inference,0.5563636363636364,0.4025974025974026,0.7037037037037037
question-answering,7,"However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",experiment,Natural Language Inference,0,154,32,20,0,experiment : Natural Language Inference,0.56,0.4155844155844156,0.7407407407407407
question-answering,7,NSE outperformed the previous sentence encoders on this task .,experiment,Natural Language Inference,0,155,33,21,0,experiment : Natural Language Inference,0.5636363636363636,0.42857142857142855,0.7777777777777778
question-answering,7,"The MMA - SNE further slightly improved the result , indicating that reading the premise memory is helpful while encoding the hypothesis .",experiment,Natural Language Inference,0,156,34,22,0,experiment : Natural Language Inference,0.5672727272727273,0.44155844155844154,0.8148148148148148
question-answering,7,The last set of methods designs inter-sentence relation with parameterized soft attention .,experiment,Natural Language Inference,0,157,35,23,0,experiment : Natural Language Inference,0.5709090909090909,0.45454545454545453,0.8518518518518519
question-answering,7,Our MMA - NSE attention model is similar to the LSTM attention model .,experiment,Natural Language Inference,1,158,36,24,0,experiment : Natural Language Inference,0.5745454545454546,0.4675324675324675,0.8888888888888888
question-answering,7,"Particularly , it attends over the premise encoder outputs {h p } T t= 1 in respect to the final hypothesis representation h h land constructs an attentively blended vector of the premise .",experiment,Natural Language Inference,0,159,37,25,0,experiment : Natural Language Inference,0.5781818181818181,0.4805194805194805,0.9259259259259259
question-answering,7,This model obtained 85.4 % accuracy score .,experiment,Natural Language Inference,1,160,38,26,0,experiment : Natural Language Inference,0.5818181818181818,0.4935064935064935,0.9629629629629629
question-answering,7,The best performing model for this task performs tree matching with attention mechanism and LSTM .,experiment,Natural Language Inference,0,161,39,27,0,experiment : Natural Language Inference,0.5854545454545454,0.5064935064935064,1.0
question-answering,7,Answer Sentence Selection,experiment,Answer Sentence Selection,1,162,40,1,0,experiment : Answer Sentence Selection,0.5890909090909091,0.5194805194805194,0.041666666666666664
question-answering,7,Answer sentence selection is an integral part of the open - domain question answering .,experiment,Answer Sentence Selection,0,163,41,2,0,experiment : Answer Sentence Selection,0.5927272727272728,0.5324675324675324,0.08333333333333333
question-answering,7,"For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences .",experiment,Answer Sentence Selection,0,164,42,3,0,experiment : Answer Sentence Selection,0.5963636363636363,0.5454545454545454,0.125
question-answering,7,We experiment on WikiQA dataset constructed from Wikipedia .,experiment,Answer Sentence Selection,0,165,43,4,0,experiment : Answer Sentence Selection,0.6,0.5584415584415584,0.16666666666666666
question-answering,7,"The dataset contains 20,360/2,733/6,165 QA pairs for train / dev / test sets .",experiment,Answer Sentence Selection,0,166,44,5,0,experiment : Answer Sentence Selection,0.6036363636363636,0.5714285714285714,0.20833333333333334
question-answering,7,"The MLP setup used in the language inference task is kept same , except that we now replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .",experiment,Answer Sentence Selection,0,167,45,6,0,experiment : Answer Sentence Selection,0.6072727272727273,0.5844155844155844,0.25
question-answering,7,where h q land ha l are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,experiment,Answer Sentence Selection,0,168,46,7,0,experiment : Answer Sentence Selection,0.610909090909091,0.5974025974025974,0.2916666666666667
question-answering,7,We trained the MMA - NSE attention model to minimize the sigmoid cross entropy loss .,experiment,Answer Sentence Selection,0,169,47,8,0,experiment : Answer Sentence Selection,0.6145454545454545,0.6103896103896104,0.3333333333333333
question-answering,7,MMA - NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories .,experiment,Answer Sentence Selection,0,170,48,9,0,experiment : Answer Sentence Selection,0.6181818181818182,0.6233766233766234,0.375
question-answering,7,"In our preliminary experiment , we found that the multiple memory access and the attention over answer encoder outputs {h a } T t= 1 are crucial to this problem .",experiment,Answer Sentence Selection,0,171,49,10,0,experiment : Answer Sentence Selection,0.6218181818181818,0.6363636363636364,0.4166666666666667
question-answering,7,"Following previous work , we adopt MAP and MRR as the evaluation metrics for this task .",experiment,Answer Sentence Selection,0,172,50,11,0,experiment : Answer Sentence Selection,0.6254545454545455,0.6493506493506493,0.4583333333333333
question-answering,7,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .",experiment,Answer Sentence Selection,1,173,51,12,0,experiment : Answer Sentence Selection,0.6290909090909091,0.6623376623376623,0.5
question-answering,7,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,experiment,Answer Sentence Selection,1,174,52,13,0,experiment : Answer Sentence Selection,0.6327272727272727,0.6753246753246753,0.5416666666666666
question-answering,7,The word embeddings are pre-trained 300 - D Glove 840B vectors .,experiment,Answer Sentence Selection,1,175,53,14,0,experiment : Answer Sentence Selection,0.6363636363636364,0.6883116883116883,0.5833333333333334
question-answering,7,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",experiment,Answer Sentence Selection,1,176,54,15,0,experiment : Answer Sentence Selection,0.64,0.7012987012987013,0.625
question-answering,7,presents the results of our model and the previous models for the task .,experiment,Answer Sentence Selection,0,177,55,16,0,experiment : Answer Sentence Selection,0.6436363636363637,0.7142857142857143,0.6666666666666666
question-answering,7,The classifier with handcrafted features is a SVM model trained with a set of features .,experiment,Answer Sentence Selection,0,178,56,17,0,experiment : Answer Sentence Selection,0.6472727272727272,0.7272727272727273,0.7083333333333334
question-answering,7,The Bigram - CNN model is a simple convolutional neural net .,experiment,Answer Sentence Selection,0,179,57,18,0,experiment : Answer Sentence Selection,0.6509090909090909,0.7402597402597403,0.75
question-answering,7,"While the LSTM and LSTM attention models outperform the previous best result by nearly 5 - 6 % by implementing deep LSTM with three hidden layers , NASM improves it further and sets a strong baseline by combining variational auto - encoder with the soft attention .",experiment,Answer Sentence Selection,0,180,58,19,0,experiment : Answer Sentence Selection,0.6545454545454545,0.7532467532467533,0.7916666666666666
question-answering,7,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,experiment,Answer Sentence Selection,1,181,59,20,0,experiment : Answer Sentence Selection,0.6581818181818182,0.7662337662337663,0.8333333333333334
question-answering,7,We used trec_eval script to calculate the evaluation metrics 7 Inclusion of simple word count feature improves the performance by around 0.15 - 0.3 across the board Model MAP MRR Classifier with features 0.5993 0.6068 Paragraph Vector 0.5110 0.5160,experiment,Answer Sentence Selection,0,182,60,21,0,experiment : Answer Sentence Selection,0.6618181818181819,0.7792207792207793,0.875
question-answering,7,Bigram- CNN 0.6190 0.6281 3 - layer LSTM 0.6552 0.6747 3 - layer LSTM attention 0.6639 0.6828 NASM 0.6705 0.6914 MMA - NSE attention 0.6811 0.6993 88.1 47.4 DRNN 86.6 49.8 2 - layer LSTM 86.3 46.0 Bi-LSTM 87.5 49.1 CT- LSTM 88.0 51.0 DMN 88.6 52.1 NSE 89.7 52.8 : Test accuracy for sentence classification .,experiment,Answer Sentence Selection,0,183,61,22,0,experiment : Answer Sentence Selection,0.6654545454545454,0.7922077922077922,0.9166666666666666
question-answering,7,Bin :,experiment,Answer Sentence Selection,0,184,62,23,0,experiment : Answer Sentence Selection,0.6690909090909091,0.8051948051948052,0.9583333333333334
question-answering,7,"Binary , FG : fine - grained 5 classes .",experiment,Answer Sentence Selection,0,185,63,24,0,experiment : Answer Sentence Selection,0.6727272727272727,0.8181818181818182,1.0
question-answering,7,Sentence Classification,experiment,Sentence Classification,1,186,64,1,0,experiment : Sentence Classification,0.6763636363636364,0.8311688311688312,0.07142857142857142
question-answering,7,We evaluated NSE on the Stanford Sentiment Treebank ( SST ) .,experiment,Sentence Classification,0,187,65,2,0,experiment : Sentence Classification,0.68,0.8441558441558441,0.14285714285714285
question-answering,7,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,experiment,Sentence Classification,0,188,66,3,0,experiment : Sentence Classification,0.6836363636363636,0.8571428571428571,0.21428571428571427
question-answering,7,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,experiment,Sentence Classification,0,189,67,4,0,experiment : Sentence Classification,0.6872727272727273,0.8701298701298701,0.2857142857142857
question-answering,7,The sentence representations were passed to a two - layer MLP for classification .,experiment,Sentence Classification,0,190,68,5,0,experiment : Sentence Classification,0.6909090909090909,0.8831168831168831,0.35714285714285715
question-answering,7,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,experiment,Sentence Classification,1,191,69,6,0,experiment : Sentence Classification,0.6945454545454546,0.8961038961038961,0.42857142857142855
question-answering,7,The second layer is a sof tmax layer .,experiment,Sentence Classification,1,192,70,7,0,experiment : Sentence Classification,0.6981818181818182,0.9090909090909091,0.5
question-answering,7,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,experiment,Sentence Classification,1,193,71,8,0,experiment : Sentence Classification,0.7018181818181818,0.922077922077922,0.5714285714285714
question-answering,7,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",experiment,Sentence Classification,1,194,72,9,0,experiment : Sentence Classification,0.7054545454545454,0.935064935064935,0.6428571428571429
question-answering,7,The write / read neural nets and the last linear layer were regularized by 50 % dropouts .,experiment,Sentence Classification,0,195,73,10,0,experiment : Sentence Classification,0.7090909090909091,0.948051948051948,0.7142857142857143
question-answering,7,compares the result of our model with the state - of - the - art methods on the two subtasks .,experiment,Sentence Classification,0,196,74,11,0,experiment : Sentence Classification,0.7127272727272728,0.961038961038961,0.7857142857142857
question-answering,7,Most best performing methods exploited the parse tree provided in the treebank on this task with the exception of the DMN .,experiment,Sentence Classification,0,197,75,12,0,experiment : Sentence Classification,0.7163636363636363,0.974025974025974,0.8571428571428571
question-answering,7,The Dynamic Memory Network ( DMN ) model is a memory - augmented network .,experiment,Sentence Classification,0,198,76,13,0,experiment : Sentence Classification,0.72,0.987012987012987,0.9285714285714286
question-answering,7,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,experiment,Sentence Classification,1,199,77,14,0,experiment : Sentence Classification,0.7236363636363636,1.0,1.0
question-answering,7,Document Sentiment Analysis,analysis,Document Sentiment Analysis,1,200,1,1,0,analysis : Document Sentiment Analysis,0.7272727272727273,0.015625,0.047619047619047616
question-answering,7,"We evaluated our models for document - level sentiment analysis on two publically available largescale datasets : the IMDB consisting of 335,018 movie reviews and 10 different classes and Yelp 13 consisting of 348,415 restaurant reviews and 5 different classes .",analysis,Document Sentiment Analysis,0,201,2,2,0,analysis : Document Sentiment Analysis,0.730909090909091,0.03125,0.09523809523809523
question-answering,7,Each document in the datasets is associated with human ratings and we used these ratings as gold labels for sentiment classification .,analysis,Document Sentiment Analysis,0,202,3,3,0,analysis : Document Sentiment Analysis,0.7345454545454545,0.046875,0.14285714285714285
question-answering,7,"Particularly , we used the pre-split datasets of .",analysis,Document Sentiment Analysis,0,203,4,4,0,analysis : Document Sentiment Analysis,0.7381818181818182,0.0625,0.19047619047619047
question-answering,7,We stack a NSE or LSTM on the top of another NSE for document modeling .,analysis,Document Sentiment Analysis,1,204,5,5,0,analysis : Document Sentiment Analysis,0.7418181818181818,0.078125,0.23809523809523808
question-answering,7,The first NSE encodes the sentences and the second NSE or LSTM takes sentence encoded outputs and constructs document representations .,analysis,Document Sentiment Analysis,0,205,6,6,0,analysis : Document Sentiment Analysis,0.7454545454545455,0.09375,0.2857142857142857
question-answering,7,The document representation is given to a output sof tmax layer .,analysis,Document Sentiment Analysis,0,206,7,7,0,analysis : Document Sentiment Analysis,0.7490909090909091,0.109375,0.3333333333333333
question-answering,7,The whole network is trained jointly by backpropagating the cross entropy loss .,analysis,Document Sentiment Analysis,1,207,8,8,0,analysis : Document Sentiment Analysis,0.7527272727272727,0.125,0.38095238095238093
question-answering,7,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,analysis,Document Sentiment Analysis,1,208,9,9,0,analysis : Document Sentiment Analysis,0.7563636363636363,0.140625,0.42857142857142855
question-answering,7,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",analysis,Document Sentiment Analysis,1,209,10,10,0,analysis : Document Sentiment Analysis,0.76,0.15625,0.47619047619047616
question-answering,7,The write / read neural nets and the document - level NSE / LSTM were regularized by 15 % dropouts and the softmax layer by 20 % dropouts .,analysis,Document Sentiment Analysis,0,210,11,11,0,analysis : Document Sentiment Analysis,0.7636363636363637,0.171875,0.5238095238095238
question-answering,7,"In order to speedup the training , we created document buckets by considering the number of sentences per document , i.e. , documents with the same number of sentences were put together in the same bucket .",analysis,Document Sentiment Analysis,0,211,12,12,0,analysis : Document Sentiment Analysis,0.7672727272727272,0.1875,0.5714285714285714
question-answering,7,The buckets were shuffled and updated per epoch .,analysis,Document Sentiment Analysis,0,212,13,13,0,analysis : Document Sentiment Analysis,0.7709090909090909,0.203125,0.6190476190476191
question-answering,7,"We did not use curriculum scheduling , although it is observed to help sequence training .",analysis,Document Sentiment Analysis,0,213,14,14,0,analysis : Document Sentiment Analysis,0.7745454545454545,0.21875,0.6666666666666666
question-answering,7,shows our results .,analysis,Document Sentiment Analysis,0,214,15,15,0,analysis : Document Sentiment Analysis,0.7781818181818182,0.234375,0.7142857142857143
question-answering,7,We report two performance metrics : accuracy and MSE .,analysis,Document Sentiment Analysis,0,215,16,16,0,analysis : Document Sentiment Analysis,0.7818181818181819,0.25,0.7619047619047619
question-answering,7,"The best results on the task were previously obtained by Conv - GRNN and LSTM - GRNN , which are also stacked models .",analysis,Document Sentiment Analysis,0,216,17,17,0,analysis : Document Sentiment Analysis,0.7854545454545454,0.265625,0.8095238095238095
question-answering,7,These models first learn the sentence representations with a CNN or LSTM and then combine them for document representation using a gated recurrent neural network ( GRNN : BLEU scores for English - German translation task .,analysis,Document Sentiment Analysis,0,217,18,18,0,analysis : Document Sentiment Analysis,0.7890909090909091,0.28125,0.8571428571428571
question-answering,7,Yelp 13 dataset has five classes to distinguish .,analysis,Document Sentiment Analysis,0,218,19,19,0,analysis : Document Sentiment Analysis,0.7927272727272727,0.296875,0.9047619047619048
question-answering,7,The stacked NSEs ( NSE - NSE ) performed slightly better than the NSE - LSTM on the IMDB dataset .,analysis,Document Sentiment Analysis,0,219,20,20,0,analysis : Document Sentiment Analysis,0.7963636363636364,0.3125,0.9523809523809523
question-answering,7,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,analysis,Document Sentiment Analysis,0,220,21,21,0,analysis : Document Sentiment Analysis,0.8,0.328125,1.0
question-answering,7,Machine Translation,analysis,Machine Translation,1,221,22,1,0,analysis : Machine Translation,0.8036363636363636,0.34375,0.041666666666666664
question-answering,7,"Lastly , we conducted an experiment on neural machine translation ( NMT ) .",analysis,Machine Translation,0,222,23,2,0,analysis : Machine Translation,0.8072727272727273,0.359375,0.08333333333333333
question-answering,7,The NMT problem is mostly defined within the encoder - decoder framework .,analysis,Machine Translation,0,223,24,3,0,analysis : Machine Translation,0.8109090909090909,0.375,0.125
question-answering,7,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation .,analysis,Machine Translation,0,224,25,4,0,analysis : Machine Translation,0.8145454545454546,0.390625,0.16666666666666666
question-answering,7,"For an efficient encoding , the attention - based NTM was introduced .",analysis,Machine Translation,0,225,26,5,0,analysis : Machine Translation,0.8181818181818182,0.40625,0.20833333333333334
question-answering,7,"For NTM , we implemented three different models .",analysis,Machine Translation,0,226,27,6,0,analysis : Machine Translation,0.8218181818181818,0.421875,0.25
question-answering,7,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,analysis,Machine Translation,0,227,28,7,0,analysis : Machine Translation,0.8254545454545454,0.4375,0.2916666666666667
question-answering,7,"This model ( LSTM - LSTM ) has two LSTM for the encoder / decoder and has the soft attention neural net , which attends over the source sentence and constructs a focused encoding vector for each target word .",analysis,Machine Translation,0,228,29,8,0,analysis : Machine Translation,0.8290909090909091,0.453125,0.3333333333333333
question-answering,7,The second model is an NSE - LSTM encoder - decoder which encodes the source sentence with NSE and generates the targets with the LSTM network by using the NSE output states and the attention network .,analysis,Machine Translation,0,229,30,9,0,analysis : Machine Translation,0.8327272727272728,0.46875,0.375
question-answering,7,"The last model is an NSE - NSE setup , where the encoding part is the same as the NSE - LSTM while the decoder NSE now uses the output state and has an access to the encoder memory , i.e. , the encoder and the decoder NSEs access a shared memory .",analysis,Machine Translation,0,230,31,10,0,analysis : Machine Translation,0.8363636363636363,0.484375,0.4166666666666667
question-answering,7,The memory is encoded by the first NSEs and then read / written by the decoder NSEs .,analysis,Machine Translation,0,231,32,11,0,analysis : Machine Translation,0.84,0.5,0.4583333333333333
question-answering,7,We used the English - German translation corpus from the IWSLT 2014 evaluation campaign .,analysis,Machine Translation,0,232,33,12,0,analysis : Machine Translation,0.8436363636363636,0.515625,0.5
question-answering,7,The corpus consists of sentence - aligned translation of TED talks .,analysis,Machine Translation,0,233,34,13,0,analysis : Machine Translation,0.8472727272727273,0.53125,0.5416666666666666
question-answering,7,The data was pre-processed and lowercased with the Moses toolkit .,analysis,Machine Translation,0,234,35,14,0,analysis : Machine Translation,0.850909090909091,0.546875,0.5833333333333334
question-answering,7,"We merged the dev2010 and dev2012 sets for development and the tst2010 , tst2011 and tst 2012 sets for test data :",analysis,Machine Translation,0,235,36,15,0,analysis : Machine Translation,0.8545454545454545,0.5625,0.625
question-answering,7,Word association or composition graphs produced by NSE memory access .,analysis,Machine Translation,0,236,37,16,0,analysis : Machine Translation,0.8581818181818182,0.578125,0.6666666666666666
question-answering,7,The directed arcs connect the words thatare composed via compose module .,analysis,Machine Translation,0,237,38,17,0,analysis : Machine Translation,0.8618181818181818,0.59375,0.7083333333333334
question-answering,7,The source nodes are input words and the destination nodes ( pointed by the arrows ) correspond to the accessed memory slots .,analysis,Machine Translation,0,238,39,18,0,analysis : Machine Translation,0.8654545454545455,0.609375,0.75
question-answering,7,S > denotes the beginning of sequence .,analysis,Machine Translation,0,239,40,19,0,analysis : Machine Translation,0.8690909090909091,0.625,0.7916666666666666
question-answering,7,the number of parameters of the models is roughly the equal .,analysis,Machine Translation,0,240,41,20,0,analysis : Machine Translation,0.8727272727272727,0.640625,0.8333333333333334
question-answering,7,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,analysis,Machine Translation,1,241,42,21,0,analysis : Machine Translation,0.8763636363636363,0.65625,0.875
question-answering,7,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",analysis,Machine Translation,1,242,43,22,0,analysis : Machine Translation,0.88,0.671875,0.9166666666666666
question-answering,7,We report BLEU score for each models .,analysis,Machine Translation,0,243,44,23,0,analysis : Machine Translation,0.8836363636363637,0.6875,0.9583333333333334
question-answering,7,11 5 Qualitative Analysis,analysis,Machine Translation,0,244,45,24,0,analysis : Machine Translation,0.8872727272727273,0.703125,1.0
question-answering,7,Memory Access and Compositionality,analysis,Memory Access and Compositionality,0,245,46,1,0,analysis : Memory Access and Compositionality,0.8909090909090909,0.71875,0.05263157894736842
question-answering,7,NSE is capabable of performing multiscale composition by retrieving associative slots for a particular input at a time step .,analysis,Memory Access and Compositionality,0,246,47,2,0,analysis : Memory Access and Compositionality,0.8945454545454545,0.734375,0.10526315789473684
question-answering,7,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,analysis,Memory Access and Compositionality,0,247,48,3,0,analysis : Memory Access and Compositionality,0.8981818181818182,0.75,0.15789473684210525
question-answering,7,shows the word association graphs for the two sentence picked from SNLI test set .,analysis,Memory Access and Compositionality,0,248,49,4,0,analysis : Memory Access and Compositionality,0.9018181818181819,0.765625,0.21052631578947367
question-answering,7,The association graph was constructed by inspecting the key vector z .,analysis,Memory Access and Compositionality,0,249,50,5,0,analysis : Memory Access and Compositionality,0.9054545454545454,0.78125,0.2631578947368421
question-answering,7,"For an input word , we connect it to the most active slot pointed by z 12 .",analysis,Memory Access and Compositionality,0,250,51,6,0,analysis : Memory Access and Compositionality,0.9090909090909091,0.796875,0.3157894736842105
question-answering,7,"Note the graph components clustered around the semantically rich words : "" sits "" , "" wall "" and "" autumn "" ( a ) and "" Three "" , "" puppies "" , "" tub "" and "" vet "" ( b ) .",analysis,Memory Access and Compositionality,0,251,52,7,0,analysis : Memory Access and Compositionality,0.9127272727272727,0.8125,0.3684210526315789
question-answering,7,The memory slots corresponding to words thatare semantically rich in the current context are the most frequently accessed .,analysis,Memory Access and Compositionality,0,252,53,8,0,analysis : Memory Access and Compositionality,0.9163636363636364,0.828125,0.42105263157894735
question-answering,7,"The graph is able to capture certain syntactic structures including phrases ( e.g. , "" hand built rock wall "" ) and modifier relations ( between "" sits "" and "" quietly "" and between "" tub "" and "" sprayed with water "" ) .",analysis,Memory Access and Compositionality,0,253,54,9,0,analysis : Memory Access and Compositionality,0.92,0.84375,0.47368421052631576
question-answering,7,Another interesting property is that the model tends to perform sensible compositions while processing the input sentence .,analysis,Memory Access and Compositionality,0,254,55,10,0,analysis : Memory Access and Compositionality,0.9236363636363636,0.859375,0.5263157894736842
question-answering,7,"For example , NSE retrieved the memory slot corresponding to "" wall "" or "" Three "" when reading the input "" rock "" or "" are "" .",analysis,Memory Access and Compositionality,0,255,56,11,0,analysis : Memory Access and Compositionality,0.9272727272727272,0.875,0.5789473684210527
question-answering,7,In Appendix,analysis,Memory Access and Compositionality,0,256,57,12,0,analysis : Memory Access and Compositionality,0.9309090909090909,0.890625,0.631578947368421
question-answering,7,", we show a step - by - step visualization of NSE memory states for the first sentence .",analysis,Memory Access and Compositionality,0,257,58,13,0,analysis : Memory Access and Compositionality,0.9345454545454546,0.90625,0.6842105263157895
question-answering,7,Note how the encoding memory is evolved overtime .,analysis,Memory Access and Compositionality,0,258,59,14,0,analysis : Memory Access and Compositionality,0.9381818181818182,0.921875,0.7368421052631579
question-answering,7,"In time step four ( t = 4 ) , the memory slot for "" quietly "" encodes information about "" quiet ( ly ) little child "" .",analysis,Memory Access and Compositionality,0,259,60,15,0,analysis : Memory Access and Compositionality,0.9418181818181818,0.9375,0.7894736842105263
question-answering,7,"When t = 6 , the model forms another composition involving "" quietly "" , "" quietly sits "" .",analysis,Memory Access and Compositionality,0,260,61,16,0,analysis : Memory Access and Compositionality,0.9454545454545454,0.953125,0.8421052631578947
question-answering,7,"In the last time step , we are able to find the most or the least frequently accessed slots in the memory .",analysis,Memory Access and Compositionality,0,261,62,17,0,analysis : Memory Access and Compositionality,0.9490909090909091,0.96875,0.8947368421052632
question-answering,7,The least accessed slots correspond to function words while the frequently accessed slots are content words and tend to carry out rich semantics and intrinsic compositions found in the input sentence .,analysis,Memory Access and Compositionality,0,262,63,18,0,analysis : Memory Access and Compositionality,0.9527272727272728,0.984375,0.9473684210526315
question-answering,7,Overall the model is less constrained and is able to compose multiword expressions .,analysis,Memory Access and Compositionality,0,263,64,19,0,analysis : Memory Access and Compositionality,0.9563636363636364,1.0,1.0
question-answering,7,Conclusion,conclusion,Conclusion,0,264,1,1,0,conclusion : Conclusion,0.96,0.08333333333333333,0.08333333333333333
question-answering,7,Our proposed memory augmented neural networks have achieved the state - of - the - art results when evaluated on five representative NLP tasks .,conclusion,Conclusion,0,265,2,2,0,conclusion : Conclusion,0.9636363636363636,0.16666666666666666,0.16666666666666666
question-answering,7,"NSE is capable of building an efficient architecture of the single , shared and multiple memory accesses for a specific NLP task .",conclusion,Conclusion,0,266,3,3,0,conclusion : Conclusion,0.9672727272727273,0.25,0.25
question-answering,7,"For example , for the NLI task NSE accesses premise encoded memory when processing hypothesis .",conclusion,Conclusion,0,267,4,4,0,conclusion : Conclusion,0.9709090909090909,0.3333333333333333,0.3333333333333333
question-answering,7,"For the QA task , NSE accesses answer encoded memory when reading question for QA .",conclusion,Conclusion,0,268,5,5,0,conclusion : Conclusion,0.9745454545454545,0.4166666666666667,0.4166666666666667
question-answering,7,"In machine translation , NSE shares a single encoded memory between encoder and decoder .",conclusion,Conclusion,0,269,6,6,0,conclusion : Conclusion,0.9781818181818182,0.5,0.5
question-answering,7,Such flexibility in the architectural choice of the NSE memory access allows for the robust models for a better performance .,conclusion,Conclusion,0,270,7,7,0,conclusion : Conclusion,0.9818181818181818,0.5833333333333334,0.5833333333333334
question-answering,7,The initial state of the NSE memory stores information about each word in the input sequence .,conclusion,Conclusion,0,271,8,8,0,conclusion : Conclusion,0.9854545454545455,0.6666666666666666,0.6666666666666666
question-answering,7,We in this paper used word embeddings to represent the words in the memory .,conclusion,Conclusion,0,272,9,9,0,conclusion : Conclusion,0.9890909090909091,0.75,0.75
question-answering,7,Different variations of word representations such as character - based models are left to be evaluated for memory initialization in the future .,conclusion,Conclusion,0,273,10,10,0,conclusion : Conclusion,0.9927272727272727,0.8333333333333334,0.8333333333333334
question-answering,7,We plan to extend NSE so that it learns to select and access a relevant subset from a memory set .,conclusion,Conclusion,0,274,11,11,0,conclusion : Conclusion,0.9963636363636363,0.9166666666666666,0.9166666666666666
question-answering,7,"One could also explore unsupervised variations of NSE , for example , to train them to produce encoding memory and representation vector of entire sentences or documents using either new or existing models such as the skip - gram model .",conclusion,Conclusion,0,275,12,12,0,conclusion : Conclusion,1.0,1.0,1.0
question-answering,8,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,title,title,1,2,1,1,0,title : title,0.008032128514056224,1.0,1.0
question-answering,8,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.012048192771084338,0.125,0.125
question-answering,8,Machine comprehension of text is an important problem in natural language processing .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.01606425702811245,0.25,0.25
question-answering,8,"recently released dataset , the Stanford Question Answering Dataset ( SQuAD ) , offers a large number of real questions and their answers created by humans through crowdsourcing .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.020080321285140562,0.375,0.375
question-answering,8,"SQuAD provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.024096385542168676,0.5,0.5
question-answering,8,We propose an end - to - end neural architecture for the task .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.028112449799196786,0.625,0.625
question-answering,8,"The architecture is based on match - LSTM , a model we proposed previously for textual entailment , and Pointer Net , a sequence - to - sequence model proposed by Vinyals et al. ( 2015 ) to constrain the output tokens to be from the input sequences .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.0321285140562249,0.75,0.75
question-answering,8,We propose two ways of using Pointer Net for our task .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03614457831325301,0.875,0.875
question-answering,8,Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. ( 2016 ) using logistic regression and manually crafted features .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.040160642570281124,1.0,1.0
question-answering,8,INTRODUCTION,introduction,introduction,0,11,1,1,0,introduction : introduction,0.04417670682730924,0.025,0.025
question-answering,8,Machine comprehension of text is one of the ultimate goals of natural language processing .,introduction,introduction,0,12,2,2,0,introduction : introduction,0.04819277108433735,0.05,0.05
question-answering,8,"While the ability of a machine to understand text can be assessed in many different ways , in recent years , several benchmark datasets have been created to focus on answering questions as away to evaluate machine comprehension .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.05220883534136546,0.075,0.075
question-answering,8,"In this setup , typically the machine is first presented with apiece of text such as a news article or a story .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05622489959839357,0.1,0.1
question-answering,8,The machine is then expected to answer one or multiple questions related to the text .,introduction,introduction,0,15,5,5,0,introduction : introduction,0.060240963855421686,0.125,0.125
question-answering,8,"In most of the benchmark datasets , a question can be treated as a multiple choice question , whose correct answer is to be chosen from a set of provided candidate answers .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.0642570281124498,0.15,0.15
question-answering,8,"Presumably , questions with more given candidate answers are more challenging .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.06827309236947791,0.175,0.175
question-answering,8,The Stanford Question Answering Dataset ( SQuAD ) introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text .,introduction,introduction,0,18,8,8,0,introduction : introduction,0.07228915662650602,0.2,0.2
question-answering,8,"Moreover , unlike some other datasets whose questions and answers were created automatically in Cloze style , the questions and answers in SQu AD were created by humans through crowdsourcing , which makes the dataset more realistic .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.07630522088353414,0.225,0.225
question-answering,8,"Given these advantages of the SQuAD dataset , in this paper , we focus on this new dataset to study machine comprehension of text .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.08032128514056225,0.25,0.25
question-answering,8,sample piece of text and three of its associated questions are shown in .,introduction,introduction,0,21,11,11,0,introduction : introduction,0.08433734939759036,0.275,0.275
question-answering,8,"Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering , including syntactic parsing , named entity recognition , question classification , semantic parsing , etc .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.08835341365461848,0.3,0.3
question-answering,8,"Recently , with the advances of applying neural network models in NLP , there has been much interest in building end - to - end neural architectures for various NLP tasks , including several pieces of work on machine comprehension .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.09236947791164658,0.325,0.325
question-answering,8,"However , given the properties of previous machine comprehension datasets , existing end - to - end neural architectures for the task either rely on the candidate answers or assume that the In 1870 , Tesla moved to Karlovac , to attend school at the Higher Real Gymnasium , where he was profoundly influenced by a math teacher Martin Sekuli ?.",introduction,introduction,0,24,14,14,0,introduction : introduction,0.0963855421686747,0.35,0.35
question-answering,8,"The classes were held in German , as it was a school within the Austro-Hungarian Military Frontier .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.10040160642570281,0.375,0.375
question-answering,8,"Tesla was able to perform integral calculus in his head , which prompted his teachers to believe that he was cheating .",introduction,introduction,0,26,16,16,0,introduction : introduction,0.10441767068273092,0.4,0.4
question-answering,8,"He finished a four - year term in three years , graduating in 1873 .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.10843373493975904,0.425,0.425
question-answering,8,1 .,introduction,introduction,0,28,18,18,0,introduction : introduction,0.11244979919678715,0.45,0.45
question-answering,8,In what language were the classes given ? German,introduction,introduction,0,29,19,19,0,introduction : introduction,0.11646586345381527,0.475,0.475
question-answering,8,In what language were the classes given ? German,introduction,introduction,0,30,20,20,0,introduction : introduction,0.12048192771084337,0.5,0.5
question-answering,8,. Who was Tesla 's main influence in Karlovac ? Martin Sekuli ? 3 . Why did Tesla go to Karlovac ? attend school at the Higher Real Gymnasium :,introduction,introduction,0,31,21,21,0,introduction : introduction,0.12449799196787148,0.525,0.525
question-answering,8,. Who was Tesla 's main influence in Karlovac ? Martin Sekuli ? 3 . Why did Tesla go to Karlovac ? attend school at the Higher Real Gymnasium :,introduction,introduction,0,32,22,22,0,introduction : introduction,0.1285140562248996,0.55,0.55
question-answering,8,. Who was Tesla 's main influence in Karlovac ? Martin Sekuli ? 3 . Why did Tesla go to Karlovac ? attend school at the Higher Real Gymnasium :,introduction,introduction,0,33,23,23,0,introduction : introduction,0.13253012048192772,0.575,0.575
question-answering,8,. Who was Tesla 's main influence in Karlovac ? Martin Sekuli ? 3 . Why did Tesla go to Karlovac ? attend school at the Higher Real Gymnasium :,introduction,introduction,0,34,24,24,0,introduction : introduction,0.13654618473895583,0.6,0.6
question-answering,8,"paragraph from Wikipedia and three associated questions together with their answers , taken from the SQuAD dataset .",introduction,introduction,0,35,25,25,0,introduction : introduction,0.14056224899598393,0.625,0.625
question-answering,8,The tokens in bold in the paragraph are our predicted answers while the texts next to the questions are the ground truth answers .,introduction,introduction,0,36,26,26,0,introduction : introduction,0.14457831325301204,0.65,0.65
question-answering,8,"answer is a single token , which make these methods unsuitable for the SQuAD dataset .",introduction,introduction,0,37,27,27,0,introduction : introduction,0.14859437751004015,0.675,0.675
question-answering,8,"In this paper , we propose a new end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .",introduction,introduction,1,38,28,28,0,introduction : introduction,0.15261044176706828,0.7,0.7
question-answering,8,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",introduction,introduction,1,39,29,29,0,introduction : introduction,0.1566265060240964,0.725,0.725
question-answering,8,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",introduction,introduction,1,40,30,30,0,introduction : introduction,0.1606425702811245,0.75,0.75
question-answering,8,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,introduction,introduction,1,41,31,31,0,introduction : introduction,0.1646586345381526,0.775,0.775
question-answering,8,We also further extend the boundary model with a search mechanism .,introduction,introduction,1,42,32,32,0,introduction : introduction,0.1686746987951807,0.8,0.8
question-answering,8,Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by .,introduction,introduction,0,43,33,33,0,introduction : introduction,0.17269076305220885,0.825,0.825
question-answering,8,"Moreover , using an ensemble of several of our models , we can achieve very competitive performance on SQuAD .",introduction,introduction,0,44,34,34,0,introduction : introduction,0.17670682730923695,0.85,0.85
question-answering,8,Our contributions can be summarized as follows :,introduction,introduction,0,45,35,35,0,introduction : introduction,0.18072289156626506,0.875,0.875
question-answering,8,"1 ) We propose two new end - to - end neural network models for machine comprehension , which combine match - LSTM and Ptr- Net to handle the special properties of the SQuAD dataset .",introduction,introduction,0,46,36,36,0,introduction : introduction,0.18473895582329317,0.9,0.9
question-answering,8,"2 ) We have achieved the performance of an exact match score of 67.9 % and an F1 score of 77.0 % on the unseen test dataset , which is much better than the featureengineered solution .",introduction,introduction,0,47,37,37,0,introduction : introduction,0.18875502008032127,0.925,0.925
question-answering,8,"Our performance is also close to the state of the art on SQuAD , which is 71.6 % in terms of exact match and 80.4 % in terms of F1 from Salesforce Research .",introduction,introduction,0,48,38,38,0,introduction : introduction,0.1927710843373494,0.95,0.95
question-answering,8,3 ) Our further analyses of the models reveal some useful insights for further improving the method .,introduction,introduction,0,49,39,39,0,introduction : introduction,0.19678714859437751,0.975,0.975
question-answering,8,"Beisdes , we also made our code available online 1 .",introduction,introduction,0,50,40,40,0,introduction : introduction,0.20080321285140562,1.0,1.0
question-answering,8,METHOD,method,METHOD,0,51,1,1,0,method : METHOD,0.20481927710843373,0.008771929824561403,0.25
question-answering,8,"In this section , we first briefly review match - LSTM and Pointer Net .",method,METHOD,0,52,2,2,0,method : METHOD,0.20883534136546184,0.017543859649122806,0.5
question-answering,8,These two pieces of existing work lay the foundation of our method .,method,METHOD,0,53,3,3,0,method : METHOD,0.21285140562248997,0.02631578947368421,0.75
question-answering,8,We then present our end - to - end neural architecture for machine comprehension .,method,METHOD,0,54,4,4,0,method : METHOD,0.21686746987951808,0.03508771929824561,1.0
question-answering,8,MATCH - LSTM,method,MATCH-LSTM,0,55,5,1,0,method : MATCH-LSTM,0.22088353413654618,0.043859649122807015,0.0625
question-answering,8,"na recent work on learning natural language inference , we proposed a match - LSTM model for predicting textual entailment .",method,MATCH-LSTM,0,56,6,2,0,method : MATCH-LSTM,0.2248995983935743,0.05263157894736842,0.125
question-answering,8,"In textual entailment , two sentences are given where one is a premise and the other is a hypothesis .",method,MATCH-LSTM,0,57,7,3,0,method : MATCH-LSTM,0.2289156626506024,0.06140350877192982,0.1875
question-answering,8,"To predict whether the premise entails the hypothesis , the match - LSTM model goes through the tokens of the hypothesis sequentially .",method,MATCH-LSTM,0,58,8,4,0,method : MATCH-LSTM,0.23293172690763053,0.07017543859649122,0.25
question-answering,8,"At each position of the hypothesis , attention mechanism is used to obtain a weighted vector representation of the premise .",method,MATCH-LSTM,0,59,9,5,0,method : MATCH-LSTM,0.23694779116465864,0.07894736842105263,0.3125
question-answering,8,"This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM , which we call the match - LSTM .",method,MATCH-LSTM,0,60,10,6,0,method : MATCH-LSTM,0.24096385542168675,0.08771929824561403,0.375
question-answering,8,The match - LSTM essentially sequentially aggregates the matching of the attention - weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction . :,method,MATCH-LSTM,0,61,11,7,0,method : MATCH-LSTM,0.24497991967871485,0.09649122807017543,0.4375
question-answering,8,An overview of our two models .,method,MATCH-LSTM,0,62,12,8,0,method : MATCH-LSTM,0.24899598393574296,0.10526315789473684,0.5
question-answering,8,"Both models consist of an LSTM preprocessing layer , a match - LSTM layer and an Answer Pointer layer .",method,MATCH-LSTM,0,63,13,9,0,method : MATCH-LSTM,0.25301204819277107,0.11403508771929824,0.5625
question-answering,8,"For each match - LSTM in a particular direction , h q i , which is defined as H q ? i , is computed using the ? in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",method,MATCH-LSTM,0,64,14,10,0,method : MATCH-LSTM,0.2570281124497992,0.12280701754385964,0.625
question-answering,8,"For each match - LSTM in a particular direction , h q i , which is defined as H q ? i , is computed using the ? in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",method,MATCH-LSTM,0,65,15,11,0,method : MATCH-LSTM,0.26104417670682734,0.13157894736842105,0.6875
question-answering,8,"For each match - LSTM in a particular direction , h q i , which is defined as H q ? i , is computed using the ? in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",method,MATCH-LSTM,0,66,16,12,0,method : MATCH-LSTM,0.26506024096385544,0.14035087719298245,0.75
question-answering,8,proposed a Pointer Network ( Ptr - Net ) model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence .,method,MATCH-LSTM,0,67,17,13,0,method : MATCH-LSTM,0.26907630522088355,0.14912280701754385,0.8125
question-answering,8,"Instead of picking an output token from a fixed vocabulary , Ptr - Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol .",method,MATCH-LSTM,0,68,18,14,0,method : MATCH-LSTM,0.27309236947791166,0.15789473684210525,0.875
question-answering,8,The pointer mechanism has inspired some recent work on language processing .,method,MATCH-LSTM,0,69,19,15,0,method : MATCH-LSTM,0.27710843373493976,0.16666666666666666,0.9375
question-answering,8,Here we adopt Ptr- Net in order to construct answers using tokens from the input text .,method,MATCH-LSTM,0,70,20,16,0,method : MATCH-LSTM,0.28112449799196787,0.17543859649122806,1.0
question-answering,8,POINTER NET,method,POINTER NET,0,71,21,1,0,method : POINTER NET,0.285140562248996,0.18421052631578946,1.0
question-answering,8,OUR METHOD,method,OUR METHOD,0,72,22,1,0,method : OUR METHOD,0.2891566265060241,0.19298245614035087,0.045454545454545456
question-answering,8,"Formally , the problem we are trying to solve can be formulated as follows .",method,OUR METHOD,0,73,23,2,0,method : OUR METHOD,0.2931726907630522,0.20175438596491227,0.09090909090909091
question-answering,8,"We are given apiece of text , which we refer to as a passage , and a question related to the passage .",method,OUR METHOD,0,74,24,3,0,method : OUR METHOD,0.2971887550200803,0.21052631578947367,0.13636363636363635
question-answering,8,"The passage is represented by matrix P ? R dP , where P is the length ( number of tokens ) of the passage and d is the dimensionality of word embeddings .",method,OUR METHOD,0,75,25,4,0,method : OUR METHOD,0.30120481927710846,0.21929824561403508,0.18181818181818182
question-answering,8,"The passage is represented by matrix P ? R dP , where P is the length ( number of tokens ) of the passage and d is the dimensionality of word embeddings .",method,OUR METHOD,0,76,26,5,0,method : OUR METHOD,0.30522088353413657,0.22807017543859648,0.22727272727272727
question-answering,8,"Similarly , the question is represented by matrix Q ? R d Q where Q is the length of the question .",method,OUR METHOD,0,77,27,6,0,method : OUR METHOD,0.3092369477911647,0.23684210526315788,0.2727272727272727
question-answering,8,"Similarly , the question is represented by matrix Q ? R d Q where Q is the length of the question .",method,OUR METHOD,0,78,28,7,0,method : OUR METHOD,0.3132530120481928,0.24561403508771928,0.3181818181818182
question-answering,8,Our goal is to identify a subsequence from the passage as the answer to the question .,method,OUR METHOD,0,79,29,8,0,method : OUR METHOD,0.3172690763052209,0.2543859649122807,0.36363636363636365
question-answering,8,"As pointed out earlier , since the output tokens are from the input , we would like to adopt the Pointer Net for this problem .",method,OUR METHOD,0,80,30,9,0,method : OUR METHOD,0.321285140562249,0.2631578947368421,0.4090909090909091
question-answering,8,"straightforward way of applying Ptr - Net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage , because Ptr - Net does not make the consecutivity assumption .",method,OUR METHOD,0,81,31,10,0,method : OUR METHOD,0.3253012048192771,0.2719298245614035,0.45454545454545453
question-answering,8,"Specifically , we represent the answer as a sequence of integers a = ( a 1 , a 2 , . . . ) , where each a i is an integer between 1 and P , indicating a certain position in the passage .",method,OUR METHOD,0,82,32,11,0,method : OUR METHOD,0.3293172690763052,0.2807017543859649,0.5
question-answering,8,"Alternatively , if we want to ensure consecutivity , that is , if we want to ensure that we indeed select a subsequence from the passage as an answer , we can use the Ptr-Net to predict only the start and the end of an answer .",method,OUR METHOD,0,83,33,12,0,method : OUR METHOD,0.3333333333333333,0.2894736842105263,0.5454545454545454
question-answering,8,"In this case , the Ptr - Net only needs to select two tokens from the input passage , and all the tokens between these two tokens in the passage are treated as the answer .",method,OUR METHOD,0,84,34,13,0,method : OUR METHOD,0.3373493975903614,0.2982456140350877,0.5909090909090909
question-answering,8,"Specifically , we can represent the answer to be predicted as two integers a = ( a s , a e ) , where a s an a e are integers between 1 and P .",method,OUR METHOD,0,85,35,14,0,method : OUR METHOD,0.3413654618473896,0.30701754385964913,0.6363636363636364
question-answering,8,We refer to the first setting above as a sequence model and the second setting above as a boundary model .,method,OUR METHOD,0,86,36,15,0,method : OUR METHOD,0.3453815261044177,0.3157894736842105,0.6818181818181818
question-answering,8,"For either model , we assume that a set of training examples in the form of triplets {( P n , Q n , an ) } N n=1 are given .",method,OUR METHOD,0,87,37,16,0,method : OUR METHOD,0.3493975903614458,0.32456140350877194,0.7272727272727273
question-answering,8,An overview of the two neural network models are shown in .,method,OUR METHOD,0,88,38,17,0,method : OUR METHOD,0.3534136546184739,0.3333333333333333,0.7727272727272727
question-answering,8,Both models consist of three layers :,method,OUR METHOD,0,89,39,18,0,method : OUR METHOD,0.357429718875502,0.34210526315789475,0.8181818181818182
question-answering,8,1 ) An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs .,method,OUR METHOD,0,90,40,19,0,method : OUR METHOD,0.3614457831325301,0.3508771929824561,0.8636363636363636
question-answering,8,2 ) A match - LSTM layer that tries to match the passage against the question .,method,OUR METHOD,0,91,41,20,0,method : OUR METHOD,0.3654618473895582,0.35964912280701755,0.9090909090909091
question-answering,8,3 ) An Answer Pointer ( Ans - Ptr ) layer that uses Ptr-Net to select a set of tokens from the passage as the answer .,method,OUR METHOD,0,92,42,21,0,method : OUR METHOD,0.36947791164658633,0.3684210526315789,0.9545454545454546
question-answering,8,The difference between the two models only lies in the third layer .,method,OUR METHOD,0,93,43,22,0,method : OUR METHOD,0.37349397590361444,0.37719298245614036,1.0
question-answering,8,LSTM,method,LSTM Preprocessing Layer,0,94,44,1,0,method : LSTM Preprocessing Layer,0.37751004016064255,0.38596491228070173,0.125
question-answering,8,Preprocessing Layer,method,LSTM Preprocessing Layer,0,95,45,2,0,method : LSTM Preprocessing Layer,0.3815261044176707,0.39473684210526316,0.25
question-answering,8,The purpose for the LSTM preprocessing layer is to incorporate contextual information into the representation of each token in the passage and the question .,method,LSTM Preprocessing Layer,0,96,46,3,0,method : LSTM Preprocessing Layer,0.3855421686746988,0.40350877192982454,0.375
question-answering,8,"We use a standard one - directional LSTM 2 to process the passage and the question separately , as shown below :",method,LSTM Preprocessing Layer,0,97,47,4,0,method : LSTM Preprocessing Layer,0.3895582329317269,0.41228070175438597,0.5
question-answering,8,"The resulting matrices H p ? R lP and H q ? R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",method,LSTM Preprocessing Layer,0,98,48,5,0,method : LSTM Preprocessing Layer,0.39357429718875503,0.42105263157894735,0.625
question-answering,8,"The resulting matrices H p ? R lP and H q ? R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",method,LSTM Preprocessing Layer,0,99,49,6,0,method : LSTM Preprocessing Layer,0.39759036144578314,0.4298245614035088,0.75
question-answering,8,"The resulting matrices H p ? R lP and H q ? R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",method,LSTM Preprocessing Layer,0,100,50,7,0,method : LSTM Preprocessing Layer,0.40160642570281124,0.43859649122807015,0.875
question-answering,8,"In other words , the i th column vector hp i ( or h q i ) in H p ( or H q ) represents the i th token in the passage ( or the question ) together with some contextual information from the left .",method,LSTM Preprocessing Layer,0,101,51,8,0,method : LSTM Preprocessing Layer,0.40562248995983935,0.4473684210526316,1.0
question-answering,8,Match- LSTM,method,Match-LSTM Layer,0,102,52,1,0,method : Match-LSTM Layer,0.40963855421686746,0.45614035087719296,1.0
question-answering,8,Layer,method,Answer Pointer Layer,0,103,53,1,0,method : Answer Pointer Layer,0.41365461847389556,0.4649122807017544,0.016129032258064516
question-answering,8,We apply the match - LSTM model proposed for textual entailment to our machine comprehension problem by treating the question as a premise and the passage as a hypothesis .,method,Answer Pointer Layer,0,104,54,2,0,method : Answer Pointer Layer,0.41767068273092367,0.47368421052631576,0.03225806451612903
question-answering,8,The match - LSTM sequentially goes through the passage .,method,Answer Pointer Layer,0,105,55,3,0,method : Answer Pointer Layer,0.42168674698795183,0.4824561403508772,0.04838709677419355
question-answering,8,"At position i of the passage , it first uses the standard word - by - word attention mechanism to obtain attention weight vector ? ? ? i ? R Q as follows :",method,Answer Pointer Layer,0,106,56,4,0,method : Answer Pointer Layer,0.42570281124497994,0.49122807017543857,0.06451612903225806
question-answering,8,"At position i of the passage , it first uses the standard word - by - word attention mechanism to obtain attention weight vector ? ? ? i ? R Q as follows :",method,Answer Pointer Layer,0,107,57,5,0,method : Answer Pointer Layer,0.42971887550200805,0.5,0.08064516129032258
question-answering,8,"At position i of the passage , it first uses the standard word - by - word attention mechanism to obtain attention weight vector ? ? ? i ? R Q as follows :",method,Answer Pointer Layer,0,108,58,6,0,method : Answer Pointer Layer,0.43373493975903615,0.5087719298245614,0.0967741935483871
question-answering,8,"where W q , W p , W r ? R ll , b p , w ? R land b ? R are parameters to be learned , ? ? hr i?1 ? R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",method,Answer Pointer Layer,0,109,59,7,0,method : Answer Pointer Layer,0.43775100401606426,0.5175438596491229,0.11290322580645161
question-answering,8,"where W q , W p , W r ? R ll , b p , w ? R land b ? R are parameters to be learned , ? ? hr i?1 ? R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",method,Answer Pointer Layer,0,110,60,8,0,method : Answer Pointer Layer,0.44176706827309237,0.5263157894736842,0.12903225806451613
question-answering,8,"where W q , W p , W r ? R ll , b p , w ? R land b ? R are parameters to be learned , ? ? hr i?1 ? R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",method,Answer Pointer Layer,0,111,61,9,0,method : Answer Pointer Layer,0.4457831325301205,0.5350877192982456,0.14516129032258066
question-answering,8,"where W q , W p , W r ? R ll , b p , w ? R land b ? R are parameters to be learned , ? ? hr i?1 ? R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",method,Answer Pointer Layer,0,112,62,10,0,method : Answer Pointer Layer,0.4497991967871486,0.543859649122807,0.16129032258064516
question-answering,8,"where W q , W p , W r ? R ll , b p , w ? R land b ? R are parameters to be learned , ? ? hr i?1 ? R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",method,Answer Pointer Layer,0,113,63,11,0,method : Answer Pointer Layer,0.4538152610441767,0.5526315789473685,0.1774193548387097
question-answering,8,"Essentially , the resulting attention weight ? ? ? i , j above indicates the degree of matching between the i th token in the passage with the j th token in the question .",method,Answer Pointer Layer,0,114,64,12,0,method : Answer Pointer Layer,0.4578313253012048,0.5614035087719298,0.1935483870967742
question-answering,8,"Essentially , the resulting attention weight ? ? ? i , j above indicates the degree of matching between the i th token in the passage with the j th token in the question .",method,Answer Pointer Layer,0,115,65,13,0,method : Answer Pointer Layer,0.46184738955823296,0.5701754385964912,0.20967741935483872
question-answering,8,"Next , we use the attention weight vector ? ? ? i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ? z i :",method,Answer Pointer Layer,0,116,66,14,0,method : Answer Pointer Layer,0.46586345381526106,0.5789473684210527,0.22580645161290322
question-answering,8,"Next , we use the attention weight vector ? ? ? i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ? z i :",method,Answer Pointer Layer,0,117,67,15,0,method : Answer Pointer Layer,0.46987951807228917,0.5877192982456141,0.24193548387096775
question-answering,8,"Next , we use the attention weight vector ? ? ? i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ? z i :",method,Answer Pointer Layer,0,118,68,16,0,method : Answer Pointer Layer,0.4738955823293173,0.5964912280701754,0.25806451612903225
question-answering,8,This vector ? ? z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,method,Answer Pointer Layer,0,119,69,17,0,method : Answer Pointer Layer,0.4779116465863454,0.6052631578947368,0.27419354838709675
question-answering,8,This vector ? ? z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,method,Answer Pointer Layer,0,120,70,18,0,method : Answer Pointer Layer,0.4819277108433735,0.6140350877192983,0.2903225806451613
question-answering,8,where ? ? hr i ? R l .,method,Answer Pointer Layer,0,121,71,19,0,method : Answer Pointer Layer,0.4859437751004016,0.6228070175438597,0.3064516129032258
question-answering,8,where ? ? hr i ? R l .,method,Answer Pointer Layer,0,122,72,20,0,method : Answer Pointer Layer,0.4899598393574297,0.631578947368421,0.3225806451612903
question-answering,8,where ? ? hr i ? R l .,method,Answer Pointer Layer,0,123,73,21,0,method : Answer Pointer Layer,0.4939759036144578,0.6403508771929824,0.3387096774193548
question-answering,8,We further build a similar match - LSTM in the reverse direction .,method,Answer Pointer Layer,0,124,74,22,0,method : Answer Pointer Layer,0.4979919678714859,0.6491228070175439,0.3548387096774194
question-answering,8,The purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage .,method,Answer Pointer Layer,0,125,75,23,0,method : Answer Pointer Layer,0.5020080321285141,0.6578947368421053,0.3709677419354839
question-answering,8,"To build this reverse match - LSTM , we first define",method,Answer Pointer Layer,0,126,76,24,0,method : Answer Pointer Layer,0.5060240963855421,0.6666666666666666,0.3870967741935484
question-answering,8,"Note that the parameters here ( W q , W p , W r , b p , wand b ) are the same as used in Eqn .",method,Answer Pointer Layer,0,127,77,25,0,method : Answer Pointer Layer,0.5100401606425703,0.6754385964912281,0.4032258064516129
question-answering,8,2 ) .,method,Answer Pointer Layer,0,128,78,26,0,method : Answer Pointer Layer,0.5140562248995983,0.6842105263157895,0.41935483870967744
question-answering,8,We then define ? ? z i in a similar way and finally define ? ? hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,method,Answer Pointer Layer,0,129,79,27,0,method : Answer Pointer Layer,0.5180722891566265,0.6929824561403509,0.43548387096774194
question-answering,8,We then define ? ? z i in a similar way and finally define ? ? hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,method,Answer Pointer Layer,0,130,80,28,0,method : Answer Pointer Layer,0.5220883534136547,0.7017543859649122,0.45161290322580644
question-answering,8,We then define ? ? z i in a similar way and finally define ? ? hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,method,Answer Pointer Layer,0,131,81,29,0,method : Answer Pointer Layer,0.5261044176706827,0.7105263157894737,0.46774193548387094
question-answering,8,We define H r ? R 2 lP as the concatenation of the two :,method,Answer Pointer Layer,0,132,82,30,0,method : Answer Pointer Layer,0.5301204819277109,0.7192982456140351,0.4838709677419355
question-answering,8,We define H r ? R 2 lP as the concatenation of the two :,method,Answer Pointer Layer,0,133,83,31,0,method : Answer Pointer Layer,0.5341365461847389,0.7280701754385965,0.5
question-answering,8,Answer Pointer Layer,method,Answer Pointer Layer,0,134,84,32,0,method : Answer Pointer Layer,0.5381526104417671,0.7368421052631579,0.5161290322580645
question-answering,8,"The top layer , the Answer Pointer ( Ans - Ptr ) layer , is motivated by the Pointer Net introduced by .",method,Answer Pointer Layer,0,135,85,33,0,method : Answer Pointer Layer,0.5421686746987951,0.7456140350877193,0.532258064516129
question-answering,8,This layer uses the sequence H r as input .,method,Answer Pointer Layer,0,136,86,34,0,method : Answer Pointer Layer,0.5461847389558233,0.7543859649122807,0.5483870967741935
question-answering,8,Recall that we have two different models :,method,Answer Pointer Layer,0,137,87,35,0,method : Answer Pointer Layer,0.5502008032128514,0.7631578947368421,0.5645161290322581
question-answering,8,The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage .,method,Answer Pointer Layer,0,138,88,36,0,method : Answer Pointer Layer,0.5542168674698795,0.7719298245614035,0.5806451612903226
question-answering,8,"The boundary model produces only the start token and the end token of the answer , and then all the tokens between these two in the original passage are considered to be the answer .",method,Answer Pointer Layer,0,139,89,37,0,method : Answer Pointer Layer,0.5582329317269076,0.7807017543859649,0.5967741935483871
question-answering,8,We now explain the two models separately .,method,Answer Pointer Layer,0,140,90,38,0,method : Answer Pointer Layer,0.5622489959839357,0.7894736842105263,0.6129032258064516
question-answering,8,The Sequence Model :,method,Answer Pointer Layer,0,141,91,39,0,method : Answer Pointer Layer,0.5662650602409639,0.7982456140350878,0.6290322580645161
question-answering,8,"Recall that in the sequence model , the answer is represented by a sequence of integers a = ( a 1 , a 2 , . . . ) indicating the positions of the selected tokens in the original passage .",method,Answer Pointer Layer,0,142,92,40,0,method : Answer Pointer Layer,0.570281124497992,0.8070175438596491,0.6451612903225806
question-answering,8,The Ans - Ptr layer models the generation of these integers in a sequential manner .,method,Answer Pointer Layer,0,143,93,41,0,method : Answer Pointer Layer,0.5742971887550201,0.8157894736842105,0.6612903225806451
question-answering,8,"Because the length of an answer is not fixed , in order to stop generating answer tokens at certain point , we allow each a k to take up an integer value between 1 and P + 1 , where P + 1 is a special value indicating the end of the answer .",method,Answer Pointer Layer,0,144,94,42,0,method : Answer Pointer Layer,0.5783132530120482,0.8245614035087719,0.6774193548387096
question-answering,8,"Once a k is set to be P + 1 , the generation of the answer stops .",method,Answer Pointer Layer,0,145,95,43,0,method : Answer Pointer Layer,0.5823293172690763,0.8333333333333334,0.6935483870967742
question-answering,8,"In order to generate the k th answer token indicated by a k , first , the attention mechanism is used again to obtain an attention weight vector ? k ? R ( P + 1 ) , where ? k , j ( 1 ? j ? P + 1 ) is the probability of selecting the j th token from the passage as the k th token in the answer , and ? k , ( P + 1 ) is the probability of stopping the answer generation at position k. ? k is modeled as follows :",method,Answer Pointer Layer,0,146,96,44,0,method : Answer Pointer Layer,0.5863453815261044,0.8421052631578947,0.7096774193548387
question-answering,8,"In order to generate the k th answer token indicated by a k , first , the attention mechanism is used again to obtain an attention weight vector ? k ? R ( P + 1 ) , where ? k , j ( 1 ? j ? P + 1 ) is the probability of selecting the j th token from the passage as the k th token in the answer , and ? k , ( P + 1 ) is the probability of stopping the answer generation at position k. ? k is modeled as follows :",method,Answer Pointer Layer,0,147,97,45,0,method : Answer Pointer Layer,0.5903614457831325,0.8508771929824561,0.7258064516129032
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,148,98,46,0,method : Answer Pointer Layer,0.5943775100401606,0.8596491228070176,0.7419354838709677
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,149,99,47,0,method : Answer Pointer Layer,0.5983935742971888,0.868421052631579,0.7580645161290323
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,150,100,48,0,method : Answer Pointer Layer,0.6024096385542169,0.8771929824561403,0.7741935483870968
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,151,101,49,0,method : Answer Pointer Layer,0.606425702811245,0.8859649122807017,0.7903225806451613
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,152,102,50,0,method : Answer Pointer Layer,0.6104417670682731,0.8947368421052632,0.8064516129032258
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,153,103,51,0,method : Answer Pointer Layer,0.6144578313253012,0.9035087719298246,0.8225806451612904
question-answering,8,"where H r ? R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ? R ll , b a , v ? R land c ? R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ? R l is the hidden vector at position k ? 1 of an answer LSTM as defined below :",method,Answer Pointer Layer,0,154,104,52,0,method : Answer Pointer Layer,0.6184738955823293,0.9122807017543859,0.8387096774193549
question-answering,8,"We can then model the probability of generating the answer sequence as p ( a| H r ) = k p ( a k | a 1 , a 2 , . . . , a k?1 , H r ) , and p ( a k = j|a 1 , a 2 , . . . , a k?1 , H r ) = ? k , j .",method,Answer Pointer Layer,0,155,105,53,0,method : Answer Pointer Layer,0.6224899598393574,0.9210526315789473,0.8548387096774194
question-answering,8,"We can then model the probability of generating the answer sequence as p ( a| H r ) = k p ( a k | a 1 , a 2 , . . . , a k?1 , H r ) , and p ( a k = j|a 1 , a 2 , . . . , a k?1 , H r ) = ? k , j .",method,Answer Pointer Layer,0,156,106,54,0,method : Answer Pointer Layer,0.6265060240963856,0.9298245614035088,0.8709677419354839
question-answering,8,"To train the model , we minimize the following loss function based on the training examples :",method,Answer Pointer Layer,0,157,107,55,0,method : Answer Pointer Layer,0.6305220883534136,0.9385964912280702,0.8870967741935484
question-answering,8,"log p ( a n | P n , Q n ) .",method,Answer Pointer Layer,0,158,108,56,0,method : Answer Pointer Layer,0.6345381526104418,0.9473684210526315,0.9032258064516129
question-answering,8,The Boundary Model :,method,Answer Pointer Layer,0,159,109,57,0,method : Answer Pointer Layer,0.6385542168674698,0.956140350877193,0.9193548387096774
question-answering,8,"The boundary model works in a way very similar to the sequence model above , except that instead of predicting a sequence of indices a 1 , a 2 , . . . , we only need to predict two indices a sand a e .",method,Answer Pointer Layer,0,160,110,58,0,method : Answer Pointer Layer,0.642570281124498,0.9649122807017544,0.9354838709677419
question-answering,8,"So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to H r , and the probability of generating an answer is simply modeled as p ( a| H r ) = p ( a s | H r ) p ( a e | a s , H r ) . Here "" Search "" refers to globally searching the spans with no more than 15 tokens , "" b "" refers to using bi-directional pre-processing LSTM , and "" en "" refers to ensemble method .",method,Answer Pointer Layer,0,161,111,59,0,method : Answer Pointer Layer,0.6465863453815262,0.9736842105263158,0.9516129032258065
question-answering,8,We further extend the boundary model by incorporating a search mechanism .,method,Answer Pointer Layer,0,162,112,60,0,method : Answer Pointer Layer,0.6506024096385542,0.9824561403508771,0.967741935483871
question-answering,8,"Specifically , during prediction , we try to limit the length of the span and globally search the span with the highest probability computed by p ( a s ) p ( a e ) .",method,Answer Pointer Layer,0,163,113,61,0,method : Answer Pointer Layer,0.6546184738955824,0.9912280701754386,0.9838709677419355
question-answering,8,"Besides , as the boundary has a sequence of fixed number of values , bi-directional Ans - Ptr can be simply combined to fine - tune the correct span .",method,Answer Pointer Layer,0,164,114,62,0,method : Answer Pointer Layer,0.6586345381526104,1.0,1.0
question-answering,8,EXPERIMENTS,experiment,EXPERIMENTS,0,165,1,1,0,experiment : EXPERIMENTS,0.6626506024096386,0.047619047619047616,0.5
question-answering,8,"In this section , we present our experiment results and perform some analyses to better understand how our models works .",experiment,EXPERIMENTS,0,166,2,2,0,experiment : EXPERIMENTS,0.6666666666666666,0.09523809523809523,1.0
question-answering,8,DATA,experiment,DATA,0,167,3,1,0,experiment : DATA,0.6706827309236948,0.14285714285714285,0.16666666666666666
question-answering,8,We use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .,experiment,DATA,0,168,4,2,0,experiment : DATA,0.6746987951807228,0.19047619047619047,0.3333333333333333
question-answering,8,Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics .,experiment,DATA,0,169,5,3,0,experiment : DATA,0.678714859437751,0.23809523809523808,0.5
question-answering,8,"Each passage is a single paragraph from a Wikipedia article , and each passage has around 5 questions associated with it .",experiment,DATA,0,170,6,4,0,experiment : DATA,0.6827309236947792,0.2857142857142857,0.6666666666666666
question-answering,8,"In total , there are 23,215 passages and 107,785 questions .",experiment,DATA,0,171,7,5,0,experiment : DATA,0.6867469879518072,0.3333333333333333,0.8333333333333334
question-answering,8,"The data has been split into a training set ( with 87,599 question - answer pairs ) , a development set ( with 10,570 questionanswer pairs ) and a hidden test set .",experiment,DATA,0,172,8,6,0,experiment : DATA,0.6907630522088354,0.38095238095238093,1.0
question-answering,8,EXPERIMENT SETTINGS,experiment,EXPERIMENT SETTINGS,0,173,9,1,0,experiment : EXPERIMENT SETTINGS,0.6947791164658634,0.42857142857142855,0.07692307692307693
question-answering,8,"We first tokenize all the passages , questions and answers .",experiment,EXPERIMENT SETTINGS,0,174,10,2,0,experiment : EXPERIMENT SETTINGS,0.6987951807228916,0.47619047619047616,0.15384615384615385
question-answering,8,The resulting vocabulary contains 117K unique words .,experiment,EXPERIMENT SETTINGS,0,175,11,3,0,experiment : EXPERIMENT SETTINGS,0.7028112449799196,0.5238095238095238,0.23076923076923078
question-answering,8,We use word embeddings from GloVe to initialize the model .,experiment,EXPERIMENT SETTINGS,1,176,12,4,0,experiment : EXPERIMENT SETTINGS,0.7068273092369478,0.5714285714285714,0.3076923076923077
question-answering,8,Words not found in Glo Ve are initialized as zero vectors .,experiment,EXPERIMENT SETTINGS,0,177,13,5,0,experiment : EXPERIMENT SETTINGS,0.7108433734939759,0.6190476190476191,0.38461538461538464
question-answering,8,The word embeddings are not updated during the training of the model .,experiment,EXPERIMENT SETTINGS,0,178,14,6,0,experiment : EXPERIMENT SETTINGS,0.714859437751004,0.6666666666666666,0.46153846153846156
question-answering,8,The dimensionality l of the hidden layers is set to be 150 or 300 .,experiment,EXPERIMENT SETTINGS,1,179,15,7,0,experiment : EXPERIMENT SETTINGS,0.7188755020080321,0.7142857142857143,0.5384615384615384
question-answering,8,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,experiment,EXPERIMENT SETTINGS,1,180,16,8,0,experiment : EXPERIMENT SETTINGS,0.7228915662650602,0.7619047619047619,0.6153846153846154
question-answering,8,Each update is computed through a minibatch of 30 instances .,experiment,EXPERIMENT SETTINGS,1,181,17,9,0,experiment : EXPERIMENT SETTINGS,0.7269076305220884,0.8095238095238095,0.6923076923076923
question-answering,8,We do not use L2-regularization .,experiment,EXPERIMENT SETTINGS,0,182,18,10,0,experiment : EXPERIMENT SETTINGS,0.7309236947791165,0.8571428571428571,0.7692307692307693
question-answering,8,"The performance is measured by two metrics : percentage of exact match with the ground truth answers , and word - level F 1 score when comparing the tokens in the predicted answers with the tokens in the ground truth answers .",experiment,EXPERIMENT SETTINGS,0,183,19,11,0,experiment : EXPERIMENT SETTINGS,0.7349397590361446,0.9047619047619048,0.8461538461538461
question-answering,8,Note that in the development set and the test set each question has around three ground truth answers .,experiment,EXPERIMENT SETTINGS,0,184,20,12,0,experiment : EXPERIMENT SETTINGS,0.7389558232931727,0.9523809523809523,0.9230769230769231
question-answering,8,F1 scores with the best matching answers are used to compute the average F1 score .,experiment,EXPERIMENT SETTINGS,0,185,21,13,0,experiment : EXPERIMENT SETTINGS,0.7429718875502008,1.0,1.0
question-answering,8,RESULTS,result,RESULTS,0,186,1,1,0,result : RESULTS,0.7469879518072289,0.025,0.058823529411764705
question-answering,8,The results of our models as well as the results of the baselines given by and are shown in .,result,RESULTS,0,187,2,2,0,result : RESULTS,0.751004016064257,0.05,0.11764705882352941
question-answering,8,We can see that both of our two models have clearly outper - :,result,RESULTS,0,188,3,3,0,result : RESULTS,0.7550200803212851,0.075,0.17647058823529413
question-answering,8,Visualization of the attention weights ? for three questions associated with the same passage .,result,RESULTS,0,189,4,4,0,result : RESULTS,0.7590361445783133,0.1,0.23529411764705882
question-answering,8,Visualization of the attention weights ? for three questions associated with the same passage .,result,RESULTS,0,190,5,5,0,result : RESULTS,0.7630522088353414,0.125,0.29411764705882354
question-answering,8,"outperformed the logistic regression model by , which relies on carefully designed features .",result,RESULTS,1,191,6,6,0,result : RESULTS,0.7670682730923695,0.15,0.35294117647058826
question-answering,8,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .",result,RESULTS,1,192,7,7,0,result : RESULTS,0.7710843373493976,0.175,0.4117647058823529
question-answering,8,"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .",result,RESULTS,0,193,8,8,0,result : RESULTS,0.7751004016064257,0.2,0.47058823529411764
question-answering,8,The improvement of our models over the logistic regression model shows that our end - to - end neural network models without much feature engineering are very effective on this task and this dataset .,result,RESULTS,0,194,9,9,0,result : RESULTS,0.7791164658634538,0.225,0.5294117647058824
question-answering,8,"Considering the effectiveness of boundary model , we further explore this model .",result,RESULTS,0,195,10,10,0,result : RESULTS,0.7831325301204819,0.25,0.5882352941176471
question-answering,8,"Observing that most of the answers are the spans with relatively small sizes , we simply limit the largest predicted span to have no more than 15 tokens and conducted experiment with span searching",result,RESULTS,0,196,11,11,0,result : RESULTS,0.7871485943775101,0.275,0.6470588235294118
question-answering,8,"This resulted in 1.5 % improvement in F1 on the development data and that outperformed the DCR model , which also introduced some language features such as POS and NE into their model .",result,RESULTS,0,197,12,12,0,result : RESULTS,0.7911646586345381,0.3,0.7058823529411765
question-answering,8,"Besides , we tried to increase the memory dimension l in the model or add bi-directional pre-processing LSTM or add bi-directional Ans - Ptr .",result,RESULTS,0,198,13,13,0,result : RESULTS,0.7951807228915663,0.325,0.7647058823529411
question-answering,8,The improvement on the development data using the first two methods is quite small .,result,RESULTS,0,199,14,14,0,result : RESULTS,0.7991967871485943,0.35,0.8235294117647058
question-answering,8,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .",result,RESULTS,1,200,15,15,0,result : RESULTS,0.8032128514056225,0.375,0.8823529411764706
question-answering,8,"Finally , we explore the ensemble method by simply computing the product of the boundary probabilities collected from 5 boundary models and then searching the most likely span with no more than 15 tokens .",result,RESULTS,0,201,16,16,0,result : RESULTS,0.8072289156626506,0.4,0.9411764705882353
question-answering,8,This ensemble method achieved the best performance as shown in the table .,result,RESULTS,0,202,17,17,0,result : RESULTS,0.8112449799196787,0.425,1.0
question-answering,8,FURTHER ANALYSES,result,FURTHER ANALYSES,0,203,18,1,0,result : FURTHER ANALYSES,0.8152610441767069,0.45,0.043478260869565216
question-answering,8,"To better understand the strengths and weaknesses of our models , we perform some further analyses of the results below .",result,FURTHER ANALYSES,0,204,19,2,0,result : FURTHER ANALYSES,0.8192771084337349,0.475,0.08695652173913043
question-answering,8,"First , we suspect that longer answers are harder to predict .",result,FURTHER ANALYSES,0,205,20,3,0,result : FURTHER ANALYSES,0.8232931726907631,0.5,0.13043478260869565
question-answering,8,"To verify this hypothesis , we analysed the performance in terms of both exact match and F 1 score with respect to the answer length on the development set .",result,FURTHER ANALYSES,0,206,21,4,0,result : FURTHER ANALYSES,0.8273092369477911,0.525,0.17391304347826086
question-answering,8,"For example , for questions whose answers contain more than 9 tokens , the F 1 score of the boundary model drops to around 55 % and the exact match score drops to only around 30 % , compared to the F 1 score and exact match score of close to 72 % and 67 % , respectively , for questions with single - token answers .",result,FURTHER ANALYSES,0,207,22,5,0,result : FURTHER ANALYSES,0.8313253012048193,0.55,0.21739130434782608
question-answering,8,And that supports our hypothesis .,result,FURTHER ANALYSES,0,208,23,6,0,result : FURTHER ANALYSES,0.8353413654618473,0.575,0.2608695652173913
question-answering,8,"Next , we analyze the performance of our models on different groups of questions .",result,FURTHER ANALYSES,0,209,24,7,0,result : FURTHER ANALYSES,0.8393574297188755,0.6,0.30434782608695654
question-answering,8,"We use a crude way to split the questions into different groups based on a set of question words we have defined , including "" what , "" "" how , "" "" who , "" "" when , "" "" which , "" "" where , "" and "" why . """,result,FURTHER ANALYSES,0,210,25,8,0,result : FURTHER ANALYSES,0.8433734939759037,0.625,0.34782608695652173
question-answering,8,These different question words roughly refer to questions with different types of answers .,result,FURTHER ANALYSES,0,211,26,9,0,result : FURTHER ANALYSES,0.8473895582329317,0.65,0.391304347826087
question-answering,8,"For example , "" when "" questions look for temporal expressions as answers , whereas "" where "" questions look for locations as answers .",result,FURTHER ANALYSES,0,212,27,10,0,result : FURTHER ANALYSES,0.8514056224899599,0.675,0.43478260869565216
question-answering,8,"According to the performance on the development data set , our models work the best for "" when "" questions .",result,FURTHER ANALYSES,0,213,28,11,0,result : FURTHER ANALYSES,0.8554216867469879,0.7,0.4782608695652174
question-answering,8,This maybe because in this dataset temporal expressions are relatively easier to recognize .,result,FURTHER ANALYSES,0,214,29,12,0,result : FURTHER ANALYSES,0.8594377510040161,0.725,0.5217391304347826
question-answering,8,"Other groups of questions whose answers are noun phrases , such as "" what "" questions , "" which "" questions and "" where "" questions , also get relatively better results .",result,FURTHER ANALYSES,0,215,30,13,0,result : FURTHER ANALYSES,0.8634538152610441,0.75,0.5652173913043478
question-answering,8,"On the other hand , "" why "" questions are the hardest to answer .",result,FURTHER ANALYSES,0,216,31,14,0,result : FURTHER ANALYSES,0.8674698795180723,0.775,0.6086956521739131
question-answering,8,"This is not surprising because the answers to "" why "" questions can be very diverse , and they are not restricted to any certain type of phrases .",result,FURTHER ANALYSES,0,217,32,15,0,result : FURTHER ANALYSES,0.8714859437751004,0.8,0.6521739130434783
question-answering,8,"Finally , we would like to check whether the attention mechanism used in the match - LSTM layer is effective in helping the model locate the answer .",result,FURTHER ANALYSES,0,218,33,16,0,result : FURTHER ANALYSES,0.8755020080321285,0.825,0.6956521739130435
question-answering,8,We show the attention weights ? in .,result,FURTHER ANALYSES,0,219,34,17,0,result : FURTHER ANALYSES,0.8795180722891566,0.85,0.7391304347826086
question-answering,8,We show the attention weights ? in .,result,FURTHER ANALYSES,0,220,35,18,0,result : FURTHER ANALYSES,0.8835341365461847,0.875,0.782608695652174
question-answering,8,In the figure the darker the color is the higher the weight is .,result,FURTHER ANALYSES,0,221,36,19,0,result : FURTHER ANALYSES,0.8875502008032129,0.9,0.8260869565217391
question-answering,8,We can see that some words have been well aligned based on the attention weights .,result,FURTHER ANALYSES,0,222,37,20,0,result : FURTHER ANALYSES,0.891566265060241,0.925,0.8695652173913043
question-answering,8,"For example , the word "" German "" in the passage is aligned well to the word "" language "" in the first question , and the model successfully predicts "" German "" as the answer to the question .",result,FURTHER ANALYSES,0,223,38,21,0,result : FURTHER ANALYSES,0.8955823293172691,0.95,0.9130434782608695
question-answering,8,"For the question word "" who "" in the second question , the word "" teacher "" actually receives relatively higher attention weight , and the model has predicted the phrase "" Martin Sekulic "" after that as the answer , which is correct .",result,FURTHER ANALYSES,0,224,39,22,0,result : FURTHER ANALYSES,0.8995983935742972,0.975,0.9565217391304348
question-answering,8,"For the last question that starts with "" why "" , the attention weights are more evenly distributed and it is not clear which words have been aligned to "" why "" .",result,FURTHER ANALYSES,0,225,40,23,0,result : FURTHER ANALYSES,0.9036144578313253,1.0,1.0
question-answering,8,RELATED WORK,related work,RELATED WORK,0,226,1,1,0,related work : RELATED WORK,0.9076305220883534,0.3333333333333333,0.3333333333333333
question-answering,8,"Machine comprehension of text has gained much attention in recent years , and increasingly researchers are building data - drive , end - to - end neural network models for the task .",related work,RELATED WORK,0,227,2,2,0,related work : RELATED WORK,0.9116465863453815,0.6666666666666666,0.6666666666666666
question-answering,8,We will first review the recently released datasets and then some end - to - end models on this task .,related work,RELATED WORK,0,228,3,3,0,related work : RELATED WORK,0.9156626506024096,1.0,1.0
question-answering,8,DATASETS,dataset,DATASETS,0,229,1,1,0,dataset : DATASETS,0.9196787148594378,0.2,0.2
question-answering,8,"number of datasets for studying machine comprehension were created in Cloze style by removing a single token from a sentence in the original corpus , and the task is to predict the missing word .",dataset,DATASETS,0,230,2,2,0,dataset : DATASETS,0.9236947791164659,0.4,0.4
question-answering,8,was also created by human annotators .,dataset,DATASETS,0,231,3,3,0,dataset : DATASETS,0.927710843373494,0.6,0.6
question-answering,8,"Different from the previous two , however , the SQuAD dataset does not provide candidate answers , and thus all possible subsequences from the given passage have to be considered as candidate answers .",dataset,DATASETS,0,232,4,4,0,dataset : DATASETS,0.9317269076305221,0.8,0.8
question-answering,8,"Besides the datasets above , there are also a few other datasets created for machine comprehension , such as WikiReading dataset and bAbI dataset , but they are quite different from the datasets above in nature .",dataset,DATASETS,0,233,5,5,0,dataset : DATASETS,0.9357429718875502,1.0,1.0
question-answering,8,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,234,1,1,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9397590361445783,0.16666666666666666,0.16666666666666666
question-answering,8,There have been a number of studies proposing end - to - end neural network models for machine comprehension .,system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,235,2,2,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9437751004016064,0.3333333333333333,0.3333333333333333
question-answering,8,common approach is to use recurrent neural networks ( RNNs ) to process the given text and the question in order to predictor generate the answers .,system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,236,3,3,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9477911646586346,0.5,0.5
question-answering,8,Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage .,system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,237,4,4,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9518072289156626,0.6666666666666666,0.6666666666666666
question-answering,8,"Given that answers often come from the given passage , Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers .",system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,238,5,5,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9558232931726908,0.8333333333333334,0.8333333333333334
question-answering,8,"Compared with existing work , we use match - LSTM to match a question and a given passage , and we use Pointer Network in a different way such that we can generate answers that contain multiple tokens from the given passage .",system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,239,6,6,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9598393574297188,1.0,1.0
question-answering,8,CONCLUSIONS,conclusion,CONCLUSIONS,0,240,1,1,0,conclusion : CONCLUSIONS,0.963855421686747,0.125,0.125
question-answering,8,"In this paper , We developed two models for the machine comprehension problem defined in the Stanford Question Answering ( SQuAD ) dataset , both making use of match - LSTM and Pointer Network .",conclusion,CONCLUSIONS,0,241,2,2,0,conclusion : CONCLUSIONS,0.9678714859437751,0.25,0.25
question-answering,8,"Experiments on the SQuAD dataset showed that our second model , the boundary model , could achieve an exact match score of 67.6 % and an F 1 score of 77 % on the test dataset , which is better than our sequence model and 's feature - engineered model .",conclusion,CONCLUSIONS,0,242,3,3,0,conclusion : CONCLUSIONS,0.9718875502008032,0.375,0.375
question-answering,8,"In the future , we plan to look further into the different types of questions and focus on those questions which currently have low performance , such as the "" why ' questions .",conclusion,CONCLUSIONS,0,243,4,4,0,conclusion : CONCLUSIONS,0.9759036144578314,0.5,0.5
question-answering,8,We also plan to test how our models could be applied to other machine comprehension datasets .,conclusion,CONCLUSIONS,0,244,5,5,0,conclusion : CONCLUSIONS,0.9799196787148594,0.625,0.625
question-answering,8,shows the numbers of answers with different lengths .,conclusion,CONCLUSIONS,0,245,6,6,0,conclusion : CONCLUSIONS,0.9839357429718876,0.75,0.75
question-answering,8,Bottom : Plot ( 3 ) shows the performance our the two models on different types of questions .,conclusion,CONCLUSIONS,0,246,7,7,0,conclusion : CONCLUSIONS,0.9879518072289156,0.875,0.875
question-answering,8,Plot ( 4 ) shows the numbers of different types of questions .,conclusion,CONCLUSIONS,0,247,8,8,0,conclusion : CONCLUSIONS,0.9919678714859438,1.0,1.0
question-answering,8,APPENDIX,APPENDIX,APPENDIX,0,248,1,1,0,APPENDIX : APPENDIX,0.9959839357429718,0.5,0.5
question-answering,8,"We show the performance breakdown by answer lengths and question types for our sequence model , boundary model and the ensemble model in .",APPENDIX,APPENDIX,0,249,2,2,0,APPENDIX : APPENDIX,1.0,1.0,1.0
question-answering,9,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,title,title,1,2,1,1,0,title : title,0.011235955056179775,1.0,1.0
question-answering,9,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.016853932584269662,0.07692307692307693,0.07692307692307693
question-answering,9,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.02247191011235955,0.15384615384615385,0.15384615384615385
question-answering,9,Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.028089887640449437,0.23076923076923078,0.23076923076923078
question-answering,9,"However , Rajpurkar et al . ( 2016 ) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.033707865168539325,0.3076923076923077,0.3076923076923077
question-answering,9,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03932584269662921,0.38461538461538464,0.38461538461538464
question-answering,9,We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.0449438202247191,0.46153846153846156,0.46153846153846156
question-answering,9,Our approach improves upon the best published results of Wang & Jiang ( 2016 ) by 5 % and decreases the error of Rajpurkar et al. 's baseline by > 50 %.,abstract,abstract,0,9,7,7,0,abstract : abstract,0.05056179775280899,0.5384615384615384,0.5384615384615384
question-answering,9,"Recently , Rajpurkar et al. ( 2016 ) released the less restricted SQUAD dataset 1 that does not place any constraints on the set of allowed answers , other than that they should be drawn from the evidence document .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.056179775280898875,0.6153846153846154,0.6153846153846154
question-answering,9,Rajpurkar et al. proposed a baseline system that chooses answers from the constituents identified by an existing syntactic parser .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.06179775280898876,0.6923076923076923,0.6923076923076923
question-answering,9,"This allows them to prune the O ( N 2 ) answer candidates in each document of length N , but it also effectively renders 20.7 % of all questions unanswerable .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.06741573033707865,0.7692307692307693,0.7692307692307693
question-answering,9,"Subsequent work by Wang & Jiang ( 2016 ) significantly improve upon this baseline by using an endto - end neural network architecture to identify answer spans by labeling either individual words , or the start and end of the answer span .",abstract,abstract,0,13,11,11,0,abstract : abstract,0.07303370786516854,0.8461538461538461,0.8461538461538461
question-answering,9,"Both of these methods do not make independence assumptions about substructures , but they are susceptible to search errors due to greedy training and decoding .",abstract,abstract,0,14,12,12,0,abstract : abstract,0.07865168539325842,0.9230769230769231,0.9230769230769231
question-answering,9,1,abstract,abstract,0,15,13,13,0,abstract : abstract,0.08426966292134831,1.0,1.0
question-answering,9,INTRODUCTION,introduction,introduction,0,16,1,1,0,introduction : introduction,0.0898876404494382,0.08333333333333333,0.08333333333333333
question-answering,9,primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,introduction,introduction,1,17,2,2,0,introduction : introduction,0.09550561797752809,0.16666666666666666,0.16666666666666666
question-answering,9,"The reading comprehension task is of practical interest - we want computers to be able to read the world 's text and then answer our questions - and , since we believe it requires deep language understanding , it has also become a flagship task in NLP research .",introduction,introduction,0,18,3,3,0,introduction : introduction,0.10112359550561797,0.25,0.25
question-answering,9,number of reading comprehension datasets have been developed that focus on answer selection from a small set of alternatives defined by annotators or existing NLP pipelines that can not be trained end - to - end .,introduction,introduction,0,19,4,4,0,introduction : introduction,0.10674157303370786,0.3333333333333333,0.3333333333333333
question-answering,9,"Subsequently , the models proposed for this task have tended to make use of the limited set of candidates , basing their predictions on mention - level attention weights , or centering classifiers , or network memories on candidate locations .",introduction,introduction,0,20,5,5,0,introduction : introduction,0.11235955056179775,0.4166666666666667,0.4166666666666667
question-answering,9,"In contrast , here we argue that it is beneficial to simplify the decoding procedure by enumerating all possible answer spans .",introduction,introduction,0,21,6,6,0,introduction : introduction,0.11797752808988764,0.5,0.5
question-answering,9,"By explicitly representing each answer span , our model can be globally normalized during training and decoded exactly during evaluation .",introduction,introduction,0,22,7,7,0,introduction : introduction,0.12359550561797752,0.5833333333333334,0.5833333333333334
question-answering,9,"naive approach to building the O ( N 2 ) spans of up to length N would require a network that is cubic in size with respect to the passage length , and such a network would be untrainable .",introduction,introduction,0,23,8,8,0,introduction : introduction,0.12921348314606743,0.6666666666666666,0.6666666666666666
question-answering,9,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .",introduction,introduction,1,24,9,9,0,introduction : introduction,0.1348314606741573,0.75,0.75
question-answering,9,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .",introduction,introduction,1,25,10,10,0,introduction : introduction,0.1404494382022472,0.8333333333333334,0.8333333333333334
question-answering,9,"In our experiments , we show an increase in performance over of 5 % in terms of exact match to a reference answer , and 3.6 % in terms of predicted answer F1 with respect to the reference .",introduction,introduction,0,26,11,11,0,introduction : introduction,0.14606741573033707,0.9166666666666666,0.9166666666666666
question-answering,9,"On both of these metrics , we close the gap between Rajpurkar et al. 's baseline and the human - performance upper-bound by > 50 %.",introduction,introduction,0,27,12,12,0,introduction : introduction,0.15168539325842698,1.0,1.0
question-answering,9,EXTRACTIVE QUESTION ANSWERING,system description,EXTRACTIVE QUESTION ANSWERING,0,28,1,1,0,system description : EXTRACTIVE QUESTION ANSWERING,0.15730337078651685,0.2,1.0
question-answering,9,TASK DEFINITION,system description,TASK DEFINITION,0,29,2,1,0,system description : TASK DEFINITION,0.16292134831460675,0.4,0.25
question-answering,9,"Extractive question answering systems take as input a question q = {q 0 , . . . , q n } and a passage of text p = {p 0 , . . . , pm } from which they predict a single answer span a = a start , a end , represented as a pair of indices into p.",system description,TASK DEFINITION,0,30,3,2,0,system description : TASK DEFINITION,0.16853932584269662,0.6,0.5
question-answering,9,"Machine learned extractive question answering systems , such as the one presented here , learn a predictor function f ( q , p ) ? a from a training dataset of q , p , a triples .",system description,TASK DEFINITION,0,31,4,3,0,system description : TASK DEFINITION,0.17415730337078653,0.8,0.75
question-answering,9,"Machine learned extractive question answering systems , such as the one presented here , learn a predictor function f ( q , p ) ? a from a training dataset of q , p , a triples .",system description,TASK DEFINITION,0,32,5,4,0,system description : TASK DEFINITION,0.1797752808988764,1.0,1.0
question-answering,9,RELATED WORK,related work,RELATED WORK,0,33,1,1,0,related work : RELATED WORK,0.1853932584269663,0.05263157894736842,0.05263157894736842
question-answering,9,"For the SQUAD dataset , the original paper from implemented a linear model with sparse features based on n-grams and part - of - speech tags present in the question and the candidate answer .",related work,RELATED WORK,0,34,2,2,0,related work : RELATED WORK,0.19101123595505617,0.10526315789473684,0.10526315789473684
question-answering,9,"Other than lexical features , they also used syntactic information in the form of dependency paths to extract more general features .",related work,RELATED WORK,0,35,3,3,0,related work : RELATED WORK,0.19662921348314608,0.15789473684210525,0.15789473684210525
question-answering,9,"They set a strong baseline for following work and also presented an in depth analysis , showing that lexical and syntactic features contribute most strongly to their model 's performance .",related work,RELATED WORK,0,36,4,4,0,related work : RELATED WORK,0.20224719101123595,0.21052631578947367,0.21052631578947367
question-answering,9,"Subsequent work by use an end - to - end neural network method that uses a Match - LSTM to model the question and the passage , and uses pointer networks to extract the answer span from the passage .",related work,RELATED WORK,0,37,5,5,0,related work : RELATED WORK,0.20786516853932585,0.2631578947368421,0.2631578947368421
question-answering,9,This model resorts to greedy decoding and falls short in terms of performance compared to our model ( see Section 5 for more detail ) .,related work,RELATED WORK,0,38,6,6,0,related work : RELATED WORK,0.21348314606741572,0.3157894736842105,0.3157894736842105
question-answering,9,"While we only compare to published baselines , there are other unpublished competitive systems on the SQUAD leaderboard , as listed in footnote",related work,RELATED WORK,0,39,7,7,0,related work : RELATED WORK,0.21910112359550563,0.3684210526315789,0.3684210526315789
question-answering,9,4 .,related work,RELATED WORK,0,40,8,8,0,related work : RELATED WORK,0.2247191011235955,0.42105263157894735,0.42105263157894735
question-answering,9,"task that is closely related to extractive question answering is the Cloze task , in which the goal is to predict a concealed span from a declarative sentence given a passage of supporting text .",related work,RELATED WORK,0,41,9,9,0,related work : RELATED WORK,0.2303370786516854,0.47368421052631576,0.47368421052631576
question-answering,9,presented a Cloze dataset in which the task is to predict the correct entity in an incomplete sentence given an abstractive summary of a news article .,related work,RELATED WORK,0,42,10,10,0,related work : RELATED WORK,0.23595505617977527,0.5263157894736842,0.5263157894736842
question-answering,9,Hermann et al . also present various neural architectures to solve the problem .,related work,RELATED WORK,0,43,11,11,0,related work : RELATED WORK,0.24157303370786518,0.5789473684210527,0.5789473684210527
question-answering,9,Hermann et al . also present various neural architectures to solve the problem .,related work,RELATED WORK,0,44,12,12,0,related work : RELATED WORK,0.24719101123595505,0.631578947368421,0.631578947368421
question-answering,9,"Although this dataset is large and varied in domain , recent analysis by shows that simple models can achieve close to the human upper bound .",related work,RELATED WORK,0,45,13,13,0,related work : RELATED WORK,0.25280898876404495,0.6842105263157895,0.6842105263157895
question-answering,9,"As noted by the authors of the SQUAD paper , the annotated answers in the SQUAD dataset are often spans that include non-entities and can be longer phrases , unlike the Cloze datasets , thus making the task more challenging .",related work,RELATED WORK,0,46,14,14,0,related work : RELATED WORK,0.25842696629213485,0.7368421052631579,0.7368421052631579
question-answering,9,"Another , more traditional line of work has focused on extractive question answering on sentences , where the task is to extract a sentence from a document , given a question .",related work,RELATED WORK,0,47,15,15,0,related work : RELATED WORK,0.2640449438202247,0.7894736842105263,0.7894736842105263
question-answering,9,"Relevant datasets include datasets from the annual TREC evaluations and WikiQA , where the latter dataset specifically focused on Wikipedia passages .",related work,RELATED WORK,0,48,16,16,0,related work : RELATED WORK,0.2696629213483146,0.8421052631578947,0.8421052631578947
question-answering,9,"There has been a line of interesting recent publications using neural architectures , focused on this variety of extractive question answering .",related work,RELATED WORK,0,49,17,17,0,related work : RELATED WORK,0.2752808988764045,0.8947368421052632,0.8947368421052632
question-answering,9,"These methods model the question and a candidate answer sentence , but do not focus on possible candidate answer spans that may contain the answer to the given question .",related work,RELATED WORK,0,50,18,18,0,related work : RELATED WORK,0.2808988764044944,0.9473684210526315,0.9473684210526315
question-answering,9,"In this work , we focus on the more challenging problem of extracting the precise answer span .",related work,RELATED WORK,0,51,19,19,0,related work : RELATED WORK,0.28651685393258425,1.0,1.0
question-answering,9,MODEL,model,MODEL,0,52,1,1,0,model : MODEL,0.29213483146067415,0.018867924528301886,0.08333333333333333
question-answering,9,"We propose a model architecture called RASOR 2 illustrated in , that explicitly computes embedding representations for candidate answer spans .",model,MODEL,0,53,2,2,0,model : MODEL,0.29775280898876405,0.03773584905660377,0.16666666666666666
question-answering,9,"In most structured prediction problems ( e.g. sequence labeling or parsing ) , the number of possible output structures is exponential in the input length , and computing representations for every candidate is prohibitively expensive .",model,MODEL,0,54,3,3,0,model : MODEL,0.30337078651685395,0.05660377358490566,0.25
question-answering,9,"However , we exploit the simplicity of our task , where we can trivially and tractably enumerate all candidates .",model,MODEL,0,55,4,4,0,model : MODEL,0.3089887640449438,0.07547169811320754,0.3333333333333333
question-answering,9,"This facilitates an expressive model that computes joint representations of every answer span , that can be globally normalized during learning .",model,MODEL,0,56,5,5,0,model : MODEL,0.3146067415730337,0.09433962264150944,0.4166666666666667
question-answering,9,"In order to compute these span representations , we must aggregate information from the passage and the question for every answer candidate .",model,MODEL,0,57,6,6,0,model : MODEL,0.3202247191011236,0.11320754716981132,0.5
question-answering,9,"For the example in , RASOR computes an embedding for the candidate answer spans : fixed to , fixed to the , to the , etc .",model,MODEL,0,58,7,7,0,model : MODEL,0.3258426966292135,0.1320754716981132,0.5833333333333334
question-answering,9,naive approach for these aggregations would require a network that is cubic in size with respect to the passage length .,model,MODEL,0,59,8,8,0,model : MODEL,0.33146067415730335,0.1509433962264151,0.6666666666666666
question-answering,9,"Instead , our model reduces this to a quadratic size by reusing recurrent computations for shared substructures ( i.e. common passage words ) from different spans .",model,MODEL,0,60,9,9,0,model : MODEL,0.33707865168539325,0.16981132075471697,0.75
question-answering,9,"Since the choice of answer span depends on the original question , we must incorporate this information into the computation of the span representation .",model,MODEL,0,61,10,10,0,model : MODEL,0.34269662921348315,0.18867924528301888,0.8333333333333334
question-answering,9,We model this by augmenting the passage word embeddings with additional embedding representations of the question .,model,MODEL,0,62,11,11,0,model : MODEL,0.34831460674157305,0.20754716981132076,0.9166666666666666
question-answering,9,"In this section , we motivate and describe the architecture for RASOR in a top - down manner .",model,MODEL,0,63,12,12,0,model : MODEL,0.3539325842696629,0.22641509433962265,1.0
question-answering,9,SCORING ANSWER SPANS,model,SCORING ANSWER SPANS,0,64,13,1,0,model : SCORING ANSWER SPANS,0.3595505617977528,0.24528301886792453,0.14285714285714285
question-answering,9,"The goal of our extractive question answering system is to predict the single best answer span among all candidates from the passage p , denoted as A ( p ) .",model,SCORING ANSWER SPANS,0,65,14,2,0,model : SCORING ANSWER SPANS,0.3651685393258427,0.2641509433962264,0.2857142857142857
question-answering,9,"Therefore , we define a probability distribution over all possible answer spans given the question q and passage p , and the predictor function finds the answer span with the maximum likelihood :",model,SCORING ANSWER SPANS,0,66,15,3,0,model : SCORING ANSWER SPANS,0.3707865168539326,0.2830188679245283,0.42857142857142855
question-answering,9,One might be tempted to introduce independence assumptions that would enable cheaper decoding .,model,SCORING ANSWER SPANS,0,67,16,4,0,model : SCORING ANSWER SPANS,0.37640449438202245,0.3018867924528302,0.5714285714285714
question-answering,9,"For example , this distribution can be modeled as ( 1 ) a product of conditionally independent distributions ( binary ) for every word or ( 2 ) a product of conditionally independent distributions ( over words ) for the start and end indices of the answer span .",model,SCORING ANSWER SPANS,0,68,17,5,0,model : SCORING ANSWER SPANS,0.38202247191011235,0.32075471698113206,0.7142857142857143
question-answering,9,"However , we show in Section 5.2 that such independence assumptions hurt the accuracy of the model , and instead we only assume a fixed - length representation ha of each candidate span that is scored and normalized with a softmax layer ( Span score and Softmax in ) :",model,SCORING ANSWER SPANS,0,69,18,6,0,model : SCORING ANSWER SPANS,0.38764044943820225,0.33962264150943394,0.8571428571428571
question-answering,9,where FFNN ( ) denotes a fully connected feed - forward neural network that provides a non-linear mapping of its input embedding .,model,SCORING ANSWER SPANS,0,70,19,7,0,model : SCORING ANSWER SPANS,0.39325842696629215,0.3584905660377358,1.0
question-answering,9,RASOR : RECURRENT SPAN REPRESENTATION,model,RASOR: RECURRENT SPAN REPRESENTATION,0,71,20,1,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.398876404494382,0.37735849056603776,0.1111111111111111
question-answering,9,"The previously defined probability distribution depends on the answer span representations , ha .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,72,21,2,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.4044943820224719,0.39622641509433965,0.2222222222222222
question-answering,9,"When computing ha , we assume access to representations of individual passage words that have been augmented with a representation of the question .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,73,22,3,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.4101123595505618,0.41509433962264153,0.3333333333333333
question-answering,9,"We denote these question - focused passage word embeddings as {p * 1 , . . . , p * m } and describe their creation in Section 3.3 .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,74,23,4,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.4157303370786517,0.4339622641509434,0.4444444444444444
question-answering,9,"In order to reuse computation for shared substructures , we use a bidirectional LSTM to encode the left and right context of every p * i ( Passage - level BiLSTM in ) .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,75,24,5,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.42134831460674155,0.4528301886792453,0.5555555555555556
question-answering,9,This allows us to simply concatenate the bidirectional LSTM ( BiLSTM ) outputs at the endpoints of a span to jointly encode its inside and outside information ( Span embedding in :,model,RASOR: RECURRENT SPAN REPRESENTATION,0,76,25,6,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.42696629213483145,0.4716981132075472,0.6666666666666666
question-answering,9,where BILSTM ( ) denotes a BiLSTM over it s input embedding sequence and p * i is the concatenation of forward and backward outputs at time - step i .,model,RASOR: RECURRENT SPAN REPRESENTATION,0,77,26,7,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.43258426966292135,0.49056603773584906,0.7777777777777778
question-answering,9,"While the visualization in shows a single layer BiLSTM for simplicity , we use a multi - layer BiLSTM in our experiments .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,78,27,8,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.43820224719101125,0.5094339622641509,0.8888888888888888
question-answering,9,"The concatenated output of each layer is used as input for the subsequent layer , allowing the upper layers to depend on the entire passage .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,79,28,9,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.4438202247191011,0.5283018867924528,1.0
question-answering,9,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,80,29,1,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.449438202247191,0.5471698113207547,0.045454545454545456
question-answering,9,"Computing the question - focused passage word embeddings {p * 1 , . . . , p * m } requires integrating question information into the passage .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,81,30,2,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.4550561797752809,0.5660377358490566,0.09090909090909091
question-answering,9,The architecture for this integration is flexible and likely depends on the nature of the dataset .,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,82,31,3,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.4606741573033708,0.5849056603773585,0.13636363636363635
question-answering,9,"For the SQUAD dataset , we find that both passage - aligned and passageindependent question representations are effective at incorporating this contextual information , and experiments will show that their benefits are complementary .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,83,32,4,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.46629213483146065,0.6037735849056604,0.18181818181818182
question-answering,9,"To incorporate these question representations , we simply concatenate them with the passage word embeddings ( Question - focused passage word embedding in ) .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,84,33,5,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.47191011235955055,0.6226415094339622,0.22727272727272727
question-answering,9,We use fixed pretrained embeddings to represent question and passage words .,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,85,34,6,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.47752808988764045,0.6415094339622641,0.2727272727272727
question-answering,9,"Therefore , in the following discussion , notation for the words are interchangeable with their embedding representations .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,86,35,7,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.48314606741573035,0.660377358490566,0.3181818181818182
question-answering,9,Question - independent passage word embedding,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,87,36,8,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.4887640449438202,0.6792452830188679,0.36363636363636365
question-answering,9,"The first component simply looks up the pretrained word embedding for the passage word , pi .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,88,37,9,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.4943820224719101,0.6981132075471698,0.4090909090909091
question-answering,9,Passage - aligned question representation,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,89,38,10,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5,0.7169811320754716,0.45454545454545453
question-answering,9,"In this dataset , the question - passage pairs often contain large lexical overlap or similarity near the correct answer span .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,90,39,11,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5056179775280899,0.7358490566037735,0.5
question-answering,9,"To encourage the model to exploit these similarities , we include a fixed - length representation of the question based on soft - alignments with the passage word .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,91,40,12,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5112359550561798,0.7547169811320755,0.5454545454545454
question-answering,9,"The alignments are computed via neural attention , and we use the variant proposed by , where attention scores are dot products between non-linear mappings of word embeddings .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,92,41,13,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5168539325842697,0.7735849056603774,0.5909090909090909
question-answering,9,align i = n j=1 a ij q j,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,93,42,14,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5224719101123596,0.7924528301886793,0.6363636363636364
question-answering,9,Passage - independent question representation,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,94,43,15,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5280898876404494,0.8113207547169812,0.6818181818181818
question-answering,9,We also include a representation of the question that does not depend on the passage and is shared for all passage words .,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,95,44,16,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5337078651685393,0.8301886792452831,0.7272727272727273
question-answering,9,"Similar to the previous question representation , an attention score is computed via a dot -product , except the question word is compared to a universal learned embedding rather any particular passage word .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,96,45,17,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5393258426966292,0.8490566037735849,0.7727272727272727
question-answering,9,"Additionally , we incorporate contextual information with a BiLSTM before aggregating the outputs using this attention mechanism .",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,97,46,18,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5449438202247191,0.8679245283018868,0.8181818181818182
question-answering,9,The goal is to generate a coarse - grained summary of the question that depends on word order .,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,98,47,19,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.550561797752809,0.8867924528301887,0.8636363636363636
question-answering,9,"Formally , the passage - independent question representation q indep is computed as follows :",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,99,48,20,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5561797752808989,0.9056603773584906,0.9090909090909091
question-answering,9,This representation is a bidirectional generalization of the question representation recently proposed by for a different question - answering task .,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,100,49,21,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5617977528089888,0.9245283018867925,0.9545454545454546
question-answering,9,"Given the above three components , the complete question - focused passage word embedding for pi is their concatenation :",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,101,50,22,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5674157303370787,0.9433962264150944,1.0
question-answering,9,LEARNING,model,LEARNING,0,102,51,1,0,model : LEARNING,0.5730337078651685,0.9622641509433962,0.3333333333333333
question-answering,9,"Given the above model specification , learning is straightforward .",model,LEARNING,0,103,52,2,0,model : LEARNING,0.5786516853932584,0.9811320754716981,0.6666666666666666
question-answering,9,We simply maximize the loglikelihood of the correct answer candidates and backpropagate the errors end - to - end .,model,LEARNING,0,104,53,3,0,model : LEARNING,0.5842696629213483,1.0,1.0
question-answering,9,EXPERIMENTAL SETUP,experiment,EXPERIMENTAL SETUP,0,105,1,1,0,experiment : EXPERIMENTAL SETUP,0.5898876404494382,0.1111111111111111,0.1111111111111111
question-answering,9,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,experiment,EXPERIMENTAL SETUP,1,106,2,2,0,experiment : EXPERIMENTAL SETUP,0.5955056179775281,0.2222222222222222,0.2222222222222222
question-answering,9,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,experiment,EXPERIMENTAL SETUP,0,107,3,3,0,experiment : EXPERIMENTAL SETUP,0.601123595505618,0.3333333333333333,0.3333333333333333
question-answering,9,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .",experiment,EXPERIMENTAL SETUP,1,108,4,4,0,experiment : EXPERIMENTAL SETUP,0.6067415730337079,0.4444444444444444,0.4444444444444444
question-answering,9,Hidden layers in the feed forward neural networks use rectified linear units .,experiment,EXPERIMENTAL SETUP,1,109,5,5,0,experiment : EXPERIMENTAL SETUP,0.6123595505617978,0.5555555555555556,0.5555555555555556
question-answering,9,Answer candidates are limited to spans with at most 30 words .,experiment,EXPERIMENTAL SETUP,0,110,6,6,0,experiment : EXPERIMENTAL SETUP,0.6179775280898876,0.6666666666666666,0.6666666666666666
question-answering,9,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .",experiment,EXPERIMENTAL SETUP,1,111,7,7,0,experiment : EXPERIMENTAL SETUP,0.6235955056179775,0.7777777777777778,0.7777777777777778
question-answering,9,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,experiment,EXPERIMENTAL SETUP,1,112,8,8,0,experiment : EXPERIMENTAL SETUP,0.6292134831460674,0.8888888888888888,0.8888888888888888
question-answering,9,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,experiment,EXPERIMENTAL SETUP,1,113,9,9,0,experiment : EXPERIMENTAL SETUP,0.6348314606741573,1.0,1.0
question-answering,9,RESULTS,result,RESULTS,0,114,1,1,0,result : RESULTS,0.6404494382022472,0.08333333333333333,0.3333333333333333
question-answering,9,"We train on the 80 k ( question , passage , answer span ) triples in the SQUAD training set and report results on the 10k examples in the SQUAD development and test sets .",result,RESULTS,0,115,2,2,0,result : RESULTS,0.6460674157303371,0.16666666666666666,0.6666666666666666
question-answering,9,"All results are calculated using the official SQUAD evaluation script , which reports exact answer match and F1 overlap of the unigrams between the predicted answer and the closest labeled answer from the 3 reference answers given in the SQUAD development set .",result,RESULTS,0,116,3,3,0,result : RESULTS,0.651685393258427,0.25,1.0
question-answering,9,COMPARISONS TO OTHER WORK,result,COMPARISONS TO OTHER WORK,0,117,4,1,0,result : COMPARISONS TO OTHER WORK,0.6573033707865169,0.3333333333333333,0.1111111111111111
question-answering,9,Our model with recurrent span representations ( RASOR ) is compared to all previously published systems,result,COMPARISONS TO OTHER WORK,0,118,5,2,0,result : COMPARISONS TO OTHER WORK,0.6629213483146067,0.4166666666666667,0.2222222222222222
question-answering,9,. published a logistic regression baseline as well as human performance on the SQUAD task .,result,COMPARISONS TO OTHER WORK,0,119,6,3,0,result : COMPARISONS TO OTHER WORK,0.6685393258426966,0.5,0.3333333333333333
question-answering,9,"The logistic regression baseline uses the output of an existing syntactic parser both as a constraint on the set of allowed answer spans , and as a method of creating sparse features for an answer -centric scoring model .",result,COMPARISONS TO OTHER WORK,0,120,7,4,0,result : COMPARISONS TO OTHER WORK,0.6741573033707865,0.5833333333333334,0.4444444444444444
question-answering,9,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",result,COMPARISONS TO OTHER WORK,1,121,8,5,0,result : COMPARISONS TO OTHER WORK,0.6797752808988764,0.6666666666666666,0.5555555555555556
question-answering,9,More closely related to RASOR is the boundary model with Match - LSTMs and Pointer Networks by .,result,COMPARISONS TO OTHER WORK,0,122,9,6,0,result : COMPARISONS TO OTHER WORK,0.6853932584269663,0.75,0.6666666666666666
question-answering,9,"Their model similarly uses recurrent networks to learn embeddings of each passage word in the context of the question , and it can also capture interactions between endpoints , since the end index probability distribution is conditioned on the start index .",result,COMPARISONS TO OTHER WORK,0,123,10,7,0,result : COMPARISONS TO OTHER WORK,0.6910112359550562,0.8333333333333334,0.7777777777777778
question-answering,9,"However , both training and evaluation are greedy , making their system susceptible to search errors when decoding .",result,COMPARISONS TO OTHER WORK,0,124,11,8,0,result : COMPARISONS TO OTHER WORK,0.6966292134831461,0.9166666666666666,0.8888888888888888
question-answering,9,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .",result,COMPARISONS TO OTHER WORK,1,125,12,9,0,result : COMPARISONS TO OTHER WORK,0.702247191011236,1.0,1.0
question-answering,9,MODEL VARIATIONS,model,MODEL VARIATIONS,0,126,1,1,0,model : MODEL VARIATIONS,0.7078651685393258,0.021739130434782608,0.1111111111111111
question-answering,9,We investigate two main questions in the following ablations and comparisons .,model,MODEL VARIATIONS,0,127,2,2,0,model : MODEL VARIATIONS,0.7134831460674157,0.043478260869565216,0.2222222222222222
question-answering,9,1 ) How important are the two methods of representing the question described in Section 3.3 ? ( 2 ) What is the impact of learning a loss function that accurately reflects the span prediction task ? Question representations shows the performance of RASOR when either of the two question representations described in Section 3.3 is removed .,model,MODEL VARIATIONS,0,128,3,3,0,model : MODEL VARIATIONS,0.7191011235955056,0.06521739130434782,0.3333333333333333
question-answering,9,1 ) How important are the two methods of representing the question described in Section 3.3 ? ( 2 ) What is the impact of learning a loss function that accurately reflects the span prediction task ? Question representations shows the performance of RASOR when either of the two question representations described in Section 3.3 is removed .,model,MODEL VARIATIONS,0,129,4,4,0,model : MODEL VARIATIONS,0.7247191011235955,0.08695652173913043,0.4444444444444444
question-answering,9,1 ) How important are the two methods of representing the question described in Section 3.3 ? ( 2 ) What is the impact of learning a loss function that accurately reflects the span prediction task ? Question representations shows the performance of RASOR when either of the two question representations described in Section 3.3 is removed .,model,MODEL VARIATIONS,0,130,5,5,0,model : MODEL VARIATIONS,0.7303370786516854,0.10869565217391304,0.5555555555555556
question-answering,9,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .",model,MODEL VARIATIONS,1,131,6,6,0,model : MODEL VARIATIONS,0.7359550561797753,0.13043478260869565,0.6666666666666666
question-answering,9,"If the question is only integrated through the inclusion of a passage - independent representation , performance drops drastically .",model,MODEL VARIATIONS,0,132,7,7,0,model : MODEL VARIATIONS,0.7415730337078652,0.15217391304347827,0.7777777777777778
question-answering,9,"The passage - independent question representation over the BiLSTM is less important , but it still accounts for over 3 % exact match and F 1 .",model,MODEL VARIATIONS,0,133,8,8,0,model : MODEL VARIATIONS,0.7471910112359551,0.17391304347826086,0.8888888888888888
question-answering,9,The input of both of these components is analyzed qualitatively in Section 6 .,model,MODEL VARIATIONS,0,134,9,9,0,model : MODEL VARIATIONS,0.7528089887640449,0.1956521739130435,1.0
question-answering,9,Question representation EM F1,model,Question representation EM F1,0,135,10,1,0,model : Question representation EM F1,0.7584269662921348,0.21739130434782608,0.3333333333333333
question-answering,9,Only passage - independent 48.7 56.6 Only passage - aligned 63.1 71.3 RASOR 66.4 74.9,model,Question representation EM F1,0,136,11,2,0,model : Question representation EM F1,0.7640449438202247,0.2391304347826087,0.6666666666666666
question-answering,9,a ) Ablation of question representations .,model,Question representation EM F1,0,137,12,3,0,model : Question representation EM F1,0.7696629213483146,0.2608695652173913,1.0
question-answering,9,Learning objective EM F1,model,Learning objective EM F1,0,138,13,1,0,model : Learning objective EM F1,0.7752808988764045,0.2826086956521739,0.029411764705882353
question-answering,9,Membership prediction 57.9 69.7 BIO sequence prediction 63.9 73.0 Endpoints prediction 65.3 75.1 Span prediction w/ log loss 65.2 73.6 ( b ) Comparisons for different learning objectives given the same passage - level BiLSTM .,model,Learning objective EM F1,0,139,14,2,0,model : Learning objective EM F1,0.7808988764044944,0.30434782608695654,0.058823529411764705
question-answering,9,Learning objectives,model,Learning objective EM F1,0,140,15,3,0,model : Learning objective EM F1,0.7865168539325843,0.32608695652173914,0.08823529411764706
question-answering,9,"Given a fixed architecture that is capable of encoding the input questionpassage pairs , there are many ways of setting up a learning objective to encourage the model to predict the correct span .",model,Learning objective EM F1,0,141,16,4,0,model : Learning objective EM F1,0.7921348314606742,0.34782608695652173,0.11764705882352941
question-answering,9,"In , we provide comparisons of some alternatives ( learned end - toend ) given only the passage - level BiLSTM from RASOR .",model,Learning objective EM F1,0,142,17,5,0,model : Learning objective EM F1,0.797752808988764,0.3695652173913043,0.14705882352941177
question-answering,9,"In order to provide clean comparisons , we restrict the alternatives to objectives thatare trained and evaluated with exact decoding .",model,Learning objective EM F1,0,143,18,6,0,model : Learning objective EM F1,0.8033707865168539,0.391304347826087,0.17647058823529413
question-answering,9,The simplest alternative is to consider this task as binary classification for every word ( Membership prediction in ) .,model,Learning objective EM F1,0,144,19,7,0,model : Learning objective EM F1,0.8089887640449438,0.41304347826086957,0.20588235294117646
question-answering,9,"In this baseline , we optimize the logistic loss for binary labels indicating whether passage words belong to the correct answer span .",model,Learning objective EM F1,0,145,20,8,0,model : Learning objective EM F1,0.8146067415730337,0.43478260869565216,0.23529411764705882
question-answering,9,"At prediction time , a valid span can be recovered in linear time by finding the maximum contiguous sum of scores .",model,Learning objective EM F1,0,146,21,9,0,model : Learning objective EM F1,0.8202247191011236,0.45652173913043476,0.2647058823529412
question-answering,9,proposed a sequence - labeling scheme that is similar to the above baseline ( BIO sequence prediction in ) .,model,Learning objective EM F1,0,147,22,10,0,model : Learning objective EM F1,0.8258426966292135,0.4782608695652174,0.29411764705882354
question-answering,9,We follow their proposed model and learn a conditional random field ( CRF ) layer after the passage - level BiLSTM to model transitions between the different labels .,model,Learning objective EM F1,0,148,23,11,0,model : Learning objective EM F1,0.8314606741573034,0.5,0.3235294117647059
question-answering,9,"At prediction time , a valid span can be recovered in linear time using Viterbi decoding , with hard transition constraints to enforce a single contiguous output .",model,Learning objective EM F1,0,149,24,12,0,model : Learning objective EM F1,0.8370786516853933,0.5217391304347826,0.35294117647058826
question-answering,9,We also consider a model that independently predicts the two endpoints of the answer span ( Endpoints prediction in ) .,model,Learning objective EM F1,0,150,25,13,0,model : Learning objective EM F1,0.8426966292134831,0.5434782608695652,0.38235294117647056
question-answering,9,This model uses the softmax loss over passage words during learning .,model,Learning objective EM F1,0,151,26,14,0,model : Learning objective EM F1,0.848314606741573,0.5652173913043478,0.4117647058823529
question-answering,9,"When decoding , we only need to enforce the constraint that the start index is no greater than the end index .",model,Learning objective EM F1,0,152,27,15,0,model : Learning objective EM F1,0.8539325842696629,0.5869565217391305,0.4411764705882353
question-answering,9,"Without the interactions between the endpoints , this can be computed in linear time .",model,Learning objective EM F1,0,153,28,16,0,model : Learning objective EM F1,0.8595505617977528,0.6086956521739131,0.47058823529411764
question-answering,9,Note that this model has the same expressivity as RASOR if the span - level FFNN were removed .,model,Learning objective EM F1,0,154,29,17,0,model : Learning objective EM F1,0.8651685393258427,0.6304347826086957,0.5
question-answering,9,"Lastly , we compare with a model using the same architecture as RASOR but is trained with a binary logistic loss rather than a softmax loss over spans ( Span prediction w/ logistic loss in ) .",model,Learning objective EM F1,0,155,30,18,0,model : Learning objective EM F1,0.8707865168539326,0.6521739130434783,0.5294117647058824
question-answering,9,The trend in shows that the model is better at leveraging the supervision as the learning objective more accurately reflects the fundamental task at hand : determining the best answer span .,model,Learning objective EM F1,0,156,31,19,0,model : Learning objective EM F1,0.8764044943820225,0.6739130434782609,0.5588235294117647
question-answering,9,"First , we observe general improvements when using labels that closely align with the task .",model,Learning objective EM F1,1,157,32,20,0,model : Learning objective EM F1,0.8820224719101124,0.6956521739130435,0.5882352941176471
question-answering,9,"For example , the labels for membership prediction simply happens to provide single contiguous spans in the supervision .",model,Learning objective EM F1,0,158,33,21,0,model : Learning objective EM F1,0.8876404494382022,0.717391304347826,0.6176470588235294
question-answering,9,The model must consider far more possible answers than it needs to ( the power set of all words ) .,model,Learning objective EM F1,0,159,34,22,0,model : Learning objective EM F1,0.8932584269662921,0.7391304347826086,0.6470588235294118
question-answering,9,The same problem holds for BIO sequence predictionthe model must do additional work to learn the semantics of the BIO tags .,model,Learning objective EM F1,0,160,35,23,0,model : Learning objective EM F1,0.898876404494382,0.7608695652173914,0.6764705882352942
question-answering,9,"On the other hand , in RASOR , the semantics of an answer span is naturally encoded by the set of labels .",model,Learning objective EM F1,0,161,36,24,0,model : Learning objective EM F1,0.9044943820224719,0.782608695652174,0.7058823529411765
question-answering,9,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .",model,Learning objective EM F1,1,162,37,25,0,model : Learning objective EM F1,0.9101123595505618,0.8043478260869565,0.7352941176470589
question-answering,9,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",model,Learning objective EM F1,1,163,38,26,0,model : Learning objective EM F1,0.9157303370786517,0.8260869565217391,0.7647058823529411
question-answering,9,"While this does not provide improvements for predicting the correct region of the answer ( captured by the F1 metric , which drops by 0.2 ) , it is more likely to predict a clean answer span that matches human judgment exactly ( captured by the exact - match metric ) .",model,Learning objective EM F1,0,164,39,27,0,model : Learning objective EM F1,0.9213483146067416,0.8478260869565217,0.7941176470588235
question-answering,9,shows how the performances of RASOR and the endpoint predictor introduced in Section 5.2 degrade as the lengths of their predictions increase .,model,Learning objective EM F1,0,165,40,28,0,model : Learning objective EM F1,0.9269662921348315,0.8695652173913043,0.8235294117647058
question-answering,9,It is clear that explicitly modeling interactions between end markers is increasingly important as the span grows in length .,model,Learning objective EM F1,0,166,41,29,0,model : Learning objective EM F1,0.9325842696629213,0.8913043478260869,0.8529411764705882
question-answering,9,"The passageindependent question representation pays most attention to the words that could attach to the answer in the passage ( "" brought "" , "" against "" ) or describe the answer category ( "" people "" ) .",model,Learning objective EM F1,0,167,42,30,0,model : Learning objective EM F1,0.9382022471910112,0.9130434782608695,0.8823529411764706
question-answering,9,"Meanwhile , the passage - aligned question representation pays attention to similar words .",model,Learning objective EM F1,0,168,43,31,0,model : Learning objective EM F1,0.9438202247191011,0.9347826086956522,0.9117647058823529
question-answering,9,"The top predictions for both examples are all valid syntactic constituents , and they all have the correct semantic category .",model,Learning objective EM F1,0,169,44,32,0,model : Learning objective EM F1,0.949438202247191,0.9565217391304348,0.9411764705882353
question-answering,9,"However , RASOR assigns almost as much probability mass to it 's incorrect third prediction "" British "" as it does to the top scoring correct prediction "" Egyptian "" .",model,Learning objective EM F1,0,170,45,33,0,model : Learning objective EM F1,0.9550561797752809,0.9782608695652174,0.9705882352941176
question-answering,9,"This showcases a common failure case for RASOR , where it can find an answer of the correct type close to a phrase that overlaps with the question - but it can not accurately represent the semantic dependency on that phrase .",model,Learning objective EM F1,0,171,46,34,0,model : Learning objective EM F1,0.9606741573033708,1.0,1.0
question-answering,9,ANALYSIS,analysis,ANALYSIS,0,172,1,1,0,analysis : ANALYSIS,0.9662921348314607,1.0,1.0
question-answering,9,CONCLUSION,conclusion,CONCLUSION,0,173,1,1,0,conclusion : CONCLUSION,0.9719101123595506,0.16666666666666666,0.16666666666666666
question-answering,9,We have shown a novel approach for perform extractive question answering on the SQUAD dataset by explicitly representing and scoring answer span candidates .,conclusion,CONCLUSION,0,174,2,2,0,conclusion : CONCLUSION,0.9775280898876404,0.3333333333333333,0.3333333333333333
question-answering,9,The core of our model relies on a recurrent network that enables shared computation for the shared substructure across span candidates .,conclusion,CONCLUSION,0,175,3,3,0,conclusion : CONCLUSION,0.9831460674157303,0.5,0.5
question-answering,9,"We explore different methods of encoding the passage and question , showing the benefits of including both passage - independent and passage - aligned question representations .",conclusion,CONCLUSION,0,176,4,4,0,conclusion : CONCLUSION,0.9887640449438202,0.6666666666666666,0.6666666666666666
question-answering,9,"While we show that this encoding method is beneficial for the task , this is orthogonal to the core contribution of efficiently computing span representation .",conclusion,CONCLUSION,0,177,5,5,0,conclusion : CONCLUSION,0.9943820224719101,0.8333333333333334,0.8333333333333334
question-answering,9,"In future work , we plan to explore alternate architectures that provide input to the recurrent span representations .",conclusion,CONCLUSION,0,178,6,6,0,conclusion : CONCLUSION,1.0,1.0,1.0
relation-classification,0,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,title,title,1,2,1,1,0,title : title,0.008849557522123894,1.0,1.0
relation-classification,0,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01327433628318584,0.125,0.125
relation-classification,0,We present a novel end - to - end neural model to extract entities and relations between them .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.017699115044247787,0.25,0.25
relation-classification,0,Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM - RNNs on bidirectional sequential LSTM - RNNs .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.022123893805309734,0.375,0.375
relation-classification,0,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,abstract,abstract,1,6,4,4,0,abstract : abstract,0.02654867256637168,0.5,0.5
relation-classification,0,We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.030973451327433628,0.625,0.625
relation-classification,0,"Our model improves over the stateof - the - art feature - based model on end -toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1score on ACE2005 and ACE2004 , respectively .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.035398230088495575,0.75,0.75
relation-classification,0,We also show that our LSTM - RNN based model compares favorably to the state - of - the - art CNN based model ( in F1-score ) on nominal relation classification ( Sem Eval - 2010 Task 8 ) .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03982300884955752,0.875,0.875
relation-classification,0,"Finally , we present an extensive ablation analysis of several model components .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.04424778761061947,1.0,1.0
relation-classification,0,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.048672566371681415,0.041666666666666664,0.041666666666666664
relation-classification,0,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,introduction,introduction,1,12,2,2,0,introduction : introduction,0.05309734513274336,0.08333333333333333,0.08333333333333333
relation-classification,0,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .",introduction,introduction,1,13,3,3,0,introduction : introduction,0.05752212389380531,0.125,0.125
relation-classification,0,"For instance , to learn that Toefting and Bolton have an Organization - Affiliation ( ORG - AFF ) relation in the sentence Toefting transferred to Bolton , the entity information that Toefting and Bolton are Person and Organization entities is important .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.061946902654867256,0.16666666666666666,0.16666666666666666
relation-classification,0,"Extraction of these entities is in turn encouraged by the presence of the context words transferred to , which indicate an employment relation .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.06637168141592921,0.20833333333333334,0.20833333333333334
relation-classification,0,Previous joint models have employed feature - based structured learning .,introduction,introduction,0,16,6,6,0,introduction : introduction,0.07079646017699115,0.25,0.25
relation-classification,0,An alternative approach to this end - to - end relation extraction task is to employ automatic feature learning via neural network ( NN ) based models .,introduction,introduction,0,17,7,7,0,introduction : introduction,0.0752212389380531,0.2916666666666667,0.2916666666666667
relation-classification,0,There are two ways to represent relations between entities using neural networks : recurrent / recursive neural networks ( RNNs ) and convolutional neural networks ( CNNs ) .,introduction,introduction,0,18,8,8,0,introduction : introduction,0.07964601769911504,0.3333333333333333,0.3333333333333333
relation-classification,0,"Among these , RNNs can directly represent essential linguistic structures , i.e. , word sequences and constituent / dependency trees .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.084070796460177,0.375,0.375
relation-classification,0,"Despite this representation ability , for relation classification tasks , the previously reported performance using long short - term memory ( LSTM ) based RNNs is worse than one using CNNs .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.08849557522123894,0.4166666666666667,0.4166666666666667
relation-classification,0,"These previous LSTM - based systems mostly include limited linguistic structures and neural architectures , and do not model entities and relations jointly .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.09292035398230089,0.4583333333333333,0.4583333333333333
relation-classification,0,We are able to achieve improvements over state - of - the - art models via endto - end modeling of entities and relations based on richer LSTM - RNN architectures that incorporate complementary linguistic structures .,introduction,introduction,0,22,12,12,0,introduction : introduction,0.09734513274336283,0.5,0.5
relation-classification,0,Word sequence and tree structure are known to be complementary information for extracting relations .,introduction,introduction,0,23,13,13,0,introduction : introduction,0.10176991150442478,0.5416666666666666,0.5416666666666666
relation-classification,0,"For instance , dependencies between words are not enough to predict that source and U.S. have an ORG - AFF relation in the sentence "" This is ... "" , one U.S. source said , and the context word said is required for this prediction .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.10619469026548672,0.5833333333333334,0.5833333333333334
relation-classification,0,"Many traditional , feature - based relation classification models extract features from both sequences and parse trees .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.11061946902654868,0.625,0.625
relation-classification,0,"However , previous RNNbased models focus on only one of these linguistic structures .",introduction,introduction,0,26,16,16,0,introduction : introduction,0.11504424778761062,0.6666666666666666,0.6666666666666666
relation-classification,0,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,introduction,introduction,1,27,17,17,0,introduction : introduction,0.11946902654867257,0.7083333333333334,0.7083333333333334
relation-classification,0,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,introduction,introduction,1,28,18,18,0,introduction : introduction,0.12389380530973451,0.75,0.75
relation-classification,0,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .",introduction,introduction,1,29,19,19,0,introduction : introduction,0.12831858407079647,0.7916666666666666,0.7916666666666666
relation-classification,0,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .",introduction,introduction,1,30,20,20,0,introduction : introduction,0.13274336283185842,0.8333333333333334,0.8333333333333334
relation-classification,0,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .",introduction,introduction,1,31,21,21,0,introduction : introduction,0.13716814159292035,0.875,0.875
relation-classification,0,"On end - to - end relation extraction , we improve over the state - of - the - art feature - based model , with 12.1 % ( ACE2005 ) and 5.7 % ( ACE2004 ) relative error reductions in F1-score .",introduction,introduction,0,32,22,22,0,introduction : introduction,0.1415929203539823,0.9166666666666666,0.9166666666666666
relation-classification,0,"On nominal relation classification ( Sem Eval - 2010 Task 8 ) , our model compares favorably to the state - of - the - art CNNbased model in F1-score .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.14601769911504425,0.9583333333333334,0.9583333333333334
relation-classification,0,"Finally , we also ablate and compare our various model components , which leads to some key findings ( both positive and negative ) about the contribution and effectiveness of different RNN structures , input dependency relation structures , different parsing models , external resources , and joint learning settings .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.1504424778761062,1.0,1.0
relation-classification,0,Related Work,related work,Related Work,0,35,1,1,0,related work : Related Work,0.15486725663716813,0.07142857142857142,0.07142857142857142
relation-classification,0,"LSTM - RNNs have been widely used for sequential labeling , such as clause identification , phonetic labeling , and NER .",related work,Related Work,0,36,2,2,0,related work : Related Work,0.1592920353982301,0.14285714285714285,0.14285714285714285
relation-classification,0,"showed that building a conditional random field ( CRF ) layer on top of bidirectional LSTM - RNNs performs comparably to the state - of - the - art methods in the partof - speech ( POS ) tagging , chunking , and NER .",related work,Related Work,0,37,3,3,0,related work : Related Work,0.16371681415929204,0.21428571428571427,0.21428571428571427
relation-classification,0,"For relation classification , in addition to traditional feature / kernel - based approaches , several neural models have been proposed in the , including embedding - based models , , and RNN - based models .",related work,Related Work,0,38,4,4,0,related work : Related Work,0.168141592920354,0.2857142857142857,0.2857142857142857
relation-classification,0,"Recently , and showed that the shortest dependency paths between relation arguments , which were used in feature / kernel - based systems , are also useful in NN - based models .",related work,Related Work,0,39,5,5,0,related work : Related Work,0.17256637168141592,0.35714285714285715,0.35714285714285715
relation-classification,0,"also showed that LSTM - RNNs are useful for relation classification , but the performance was worse than CNN - based models .",related work,Related Work,0,40,6,6,0,related work : Related Work,0.17699115044247787,0.42857142857142855,0.42857142857142855
relation-classification,0,"compared separate sequence - based and tree - structured LSTM - RNNs on relation classification , using basic RNN model structures .",related work,Related Work,0,41,7,7,0,related work : Related Work,0.18141592920353983,0.5,0.5
relation-classification,0,"Research on tree - structured LSTM - RNNs fixes the direction of information propagation from bottom to top , and also can not handle an arbitrary number of typed children as in a typed dependency tree .",related work,Related Work,0,42,8,8,0,related work : Related Work,0.18584070796460178,0.5714285714285714,0.5714285714285714
relation-classification,0,"Furthermore , no RNNbased relation classification model simultaneously uses word sequence and dependency tree information .",related work,Related Work,0,43,9,9,0,related work : Related Work,0.1902654867256637,0.6428571428571429,0.6428571428571429
relation-classification,0,"We propose several such novel model structures and training settings , investigating the simultaneous use of bidirectional sequential and bidirectional tree - structured LSTM - RNNs to jointly capture linear and dependency context for end - toend extraction of relations between entities .",related work,Related Work,0,44,10,10,0,related work : Related Work,0.19469026548672566,0.7142857142857143,0.7142857142857143
relation-classification,0,"As for end - to - end ( joint ) extraction of relations between entities , all existing models are featurebased systems ( and no NN - based model has been proposed ) .",related work,Related Work,0,45,11,11,0,related work : Related Work,0.19911504424778761,0.7857142857142857,0.7857142857142857
relation-classification,0,"Such models include structured prediction , integer linear programming , card - pyramid parsing ( Kate and Mooney , 2010 ) , and global probabilistic graphical models .",related work,Related Work,0,46,12,12,0,related work : Related Work,0.20353982300884957,0.8571428571428571,0.8571428571428571
relation-classification,0,"Among these , structured prediction methods are state - of - the - art on several corpora .",related work,Related Work,0,47,13,13,0,related work : Related Work,0.2079646017699115,0.9285714285714286,0.9285714285714286
relation-classification,0,"We present an improved , NN - based alternative for the end - to - end relation extraction .",related work,Related Work,0,48,14,14,0,related work : Related Work,0.21238938053097345,1.0,1.0
relation-classification,0,Model,model,Model,0,49,1,1,0,model : Model,0.2168141592920354,0.012987012987012988,0.14285714285714285
relation-classification,0,"We design our model with LSTM - RNNs that represent both word sequences and dependency tree structures , and perform end - to - end extraction of relations between entities on top of these RNNs.",model,Model,0,50,2,2,0,model : Model,0.22123893805309736,0.025974025974025976,0.2857142857142857
relation-classification,0,illustrates the overview of the model .,model,Model,0,51,3,3,0,model : Model,0.22566371681415928,0.03896103896103896,0.42857142857142855
relation-classification,0,"The model mainly consists of three representation layers : a word embeddings layer ( embedding layer ) , a word sequence based LSTM - RNN layer ( sequence layer ) , and finally a dependency subtree based LSTM - RNN layer ( dependency layer ) .",model,Model,0,52,4,4,0,model : Model,0.23008849557522124,0.05194805194805195,0.5714285714285714
relation-classification,0,"During decoding , we build greedy , left - to - right entity detection on the sequence layer and realize relation classification on the dependency layers , where each subtree based LSTM - RNN corresponds to a relation candidate between two detected entities .",model,Model,0,53,5,5,0,model : Model,0.2345132743362832,0.06493506493506493,0.7142857142857143
relation-classification,0,"After decoding the entire model structure , we update the parameters simultaneously via backpropagation through time ( BPTT ) .",model,Model,0,54,6,6,0,model : Model,0.23893805309734514,0.07792207792207792,0.8571428571428571
relation-classification,0,"The dependency layers are stacked on the sequence layer , so the embedding and sequence layers are shared by both entity detection and relation classification , and the shared parameters are affected by both entity and relation labels .",model,Model,0,55,7,7,0,model : Model,0.24336283185840707,0.09090909090909091,1.0
relation-classification,0,Embedding Layer,model,Embedding Layer,0,56,8,1,0,model : Embedding Layer,0.24778761061946902,0.1038961038961039,0.3333333333333333
relation-classification,0,The embedding layer handles embedding representations .,model,Embedding Layer,0,57,9,2,0,model : Embedding Layer,0.252212389380531,0.11688311688311688,0.6666666666666666
relation-classification,0,"w , n p , n d and n e - dimensional vectors v , v ( p ) , v and v ( e ) are embedded to words , part - of - speech ( POS ) tags , dependency types , and entity labels , respectively .",model,Embedding Layer,0,58,10,3,0,model : Embedding Layer,0.25663716814159293,0.12987012987012986,1.0
relation-classification,0,Sequence Layer,model,Sequence Layer,0,59,11,1,0,model : Sequence Layer,0.2610619469026549,0.14285714285714285,0.1
relation-classification,0,The sequence layer represents words in a linear sequence using the representations from the embedding layer .,model,Sequence Layer,0,60,12,2,0,model : Sequence Layer,0.26548672566371684,0.15584415584415584,0.2
relation-classification,0,"This layer represents sentential context information and maintains entities , as shown in bottom - left part of .",model,Sequence Layer,0,61,13,3,0,model : Sequence Layer,0.26991150442477874,0.16883116883116883,0.3
relation-classification,0,We represent the word sequence in a sentence with bidirectional LSTM - RNNs .,model,Sequence Layer,0,62,14,4,0,model : Sequence Layer,0.2743362831858407,0.18181818181818182,0.4
relation-classification,0,"The LSTM unit at t-th word consists of a collection of n ls - dimensional vectors : an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht .",model,Sequence Layer,0,63,15,5,0,model : Sequence Layer,0.27876106194690264,0.19480519480519481,0.5
relation-classification,0,"The unit receives an n-dimensional input vector x t , the previous hidden state h t?1 , and the memory cell c t?1 , and calculates the new vectors using the following equations :",model,Sequence Layer,0,64,16,6,0,model : Sequence Layer,0.2831858407079646,0.2077922077922078,0.6
relation-classification,0,1 ),model,Sequence Layer,0,65,17,7,0,model : Sequence Layer,0.28761061946902655,0.22077922077922077,0.7
relation-classification,0,"where ? denotes the logistic function , denotes element - wise multiplication , W and U are weight matrices , and bare bias vectors .",model,Sequence Layer,0,66,18,8,0,model : Sequence Layer,0.2920353982300885,0.23376623376623376,0.8
relation-classification,0,"where ? denotes the logistic function , denotes element - wise multiplication , W and U are weight matrices , and bare bias vectors .",model,Sequence Layer,0,67,19,9,0,model : Sequence Layer,0.29646017699115046,0.24675324675324675,0.9
relation-classification,0,The LSTM unit at t-th word receives the concatenation of word and POS embeddings as its input vector :,model,Sequence Layer,0,68,20,10,0,model : Sequence Layer,0.3008849557522124,0.2597402597402597,1.0
relation-classification,0,.,model,Entity Detection,0,69,21,1,0,model : Entity Detection,0.3053097345132743,0.2727272727272727,0.07142857142857142
relation-classification,0,"We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ? ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ? ht , and pass it to the subsequent layers .",model,Entity Detection,0,70,22,2,0,model : Entity Detection,0.30973451327433627,0.2857142857142857,0.14285714285714285
relation-classification,0,"We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ? ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ? ht , and pass it to the subsequent layers .",model,Entity Detection,0,71,23,3,0,model : Entity Detection,0.3141592920353982,0.2987012987012987,0.21428571428571427
relation-classification,0,"We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ? ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ? ht , and pass it to the subsequent layers .",model,Entity Detection,0,72,24,4,0,model : Entity Detection,0.3185840707964602,0.3116883116883117,0.2857142857142857
relation-classification,0,Entity Detection,model,Entity Detection,0,73,25,5,0,model : Entity Detection,0.3230088495575221,0.3246753246753247,0.35714285714285715
relation-classification,0,We treat entity detection as a sequence labeling task .,model,Entity Detection,0,74,26,6,0,model : Entity Detection,0.3274336283185841,0.33766233766233766,0.42857142857142855
relation-classification,0,"We assign an entity tag to each word using a commonly used encoding scheme BILOU ( Begin , Inside , Last , Outside , Unit ) ( Ratinov and , where each entity tag represents the entity type and the position of a word in the entity .",model,Entity Detection,0,75,27,7,0,model : Entity Detection,0.33185840707964603,0.35064935064935066,0.5
relation-classification,0,"For example , in , we assign B - PER and L - PER ( which denote the beginning and last words of a person entity type , respectively ) to each word in Sidney Yates to represent this phrase as a PER ( person ) entity type .",model,Entity Detection,0,76,28,8,0,model : Entity Detection,0.336283185840708,0.36363636363636365,0.5714285714285714
relation-classification,0,We perform entity detection on top of the sequence layer .,model,Entity Detection,0,77,29,9,0,model : Entity Detection,0.3407079646017699,0.37662337662337664,0.6428571428571429
relation-classification,0,We employ a two - layered NN with an n he - dimensional hidden layer h ( e ) and a softmax output layer for entity detection .,model,Entity Detection,0,78,30,10,0,model : Entity Detection,0.34513274336283184,0.38961038961038963,0.7142857142857143
relation-classification,0,"Here , Ware weight matrices and bare bias vectors .",model,Entity Detection,0,79,31,11,0,model : Entity Detection,0.3495575221238938,0.4025974025974026,0.7857142857142857
relation-classification,0,"We assign entity labels to words in a greedy , left - to - right manner .",model,Entity Detection,0,80,32,12,0,model : Entity Detection,0.35398230088495575,0.4155844155844156,0.8571428571428571
relation-classification,0,"During this decoding , we use the predicted label of a word to predict the label of the next word so as to take label dependencies into account .",model,Entity Detection,0,81,33,13,0,model : Entity Detection,0.3584070796460177,0.42857142857142855,0.9285714285714286
relation-classification,0,The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word ) .,model,Entity Detection,0,82,34,14,0,model : Entity Detection,0.36283185840707965,0.44155844155844154,1.0
relation-classification,0,Dependency Layer,model,Dependency Layer,0,83,35,1,0,model : Dependency Layer,0.3672566371681416,0.45454545454545453,0.05
relation-classification,0,"The dependency layer represents a relation between a pair of two target words ( corresponding to a relation candidate in relation classification ) in the dependency tree , and is in charge of relationspecific representations , as is shown in top - right part of .",model,Dependency Layer,0,84,36,2,0,model : Dependency Layer,0.37168141592920356,0.4675324675324675,0.1
relation-classification,0,"This layer mainly focuses on the shortest path between a pair of target words in the dependency tree ( i.e. , the path between the least common node and the two target words ) since these paths are shown to be effective in relation classification .",model,Dependency Layer,0,85,37,3,0,model : Dependency Layer,0.37610619469026546,0.4805194805194805,0.15
relation-classification,0,"For example , we show the shortest path between Yates and Chicago in the bottom of , and this path well captures the key phrase of their relation , i.e. , born in .",model,Dependency Layer,0,86,38,4,0,model : Dependency Layer,0.3805309734513274,0.4935064935064935,0.2
relation-classification,0,"We employ bidirectional tree - structured LSTM - RNNs ( i.e. , bottom - up and top - down ) to represent a relation candidate by capturing the dependency structure around the target word pair .",model,Dependency Layer,0,87,39,5,0,model : Dependency Layer,0.38495575221238937,0.5064935064935064,0.25
relation-classification,0,This bidirectional structure propagates to each node not only the information from the leaves but also information from the root .,model,Dependency Layer,0,88,40,6,0,model : Dependency Layer,0.3893805309734513,0.5194805194805194,0.3
relation-classification,0,"This is especially important for relation classification , which makes use of argument nodes near the bottom of the tree , and our top - down LSTM - RNN sends information from the top of the tree to such near - leaf nodes ( unlike in standard bottom - up LSTM - RNNs ) .",model,Dependency Layer,0,89,41,7,0,model : Dependency Layer,0.3938053097345133,0.5324675324675324,0.35
relation-classification,0,Note that the two variants of tree - structured LSTM - RNNs by are notable to represent our target structures which have a variable number of typed children : the Child - Sum Tree - LSTM does not deal with types and the N - ary Tree assumes a fixed number of children .,model,Dependency Layer,0,90,42,8,0,model : Dependency Layer,0.39823008849557523,0.5454545454545454,0.4
relation-classification,0,We thus propose a new variant of tree - structured LSTM - RNN that shares weight matrices U s for same - type children and also allows variable number of children .,model,Dependency Layer,0,91,43,9,0,model : Dependency Layer,0.4026548672566372,0.5584415584415584,0.45
relation-classification,0,"For this variant , we calculate n lt - dimensional vectors in the LSTM unit at t-th node with C ( t ) children using following equations :",model,Dependency Layer,0,92,44,10,0,model : Dependency Layer,0.40707964601769914,0.5714285714285714,0.5
relation-classification,0,where m ( ) is a type mapping function .,model,Dependency Layer,0,93,45,11,0,model : Dependency Layer,0.41150442477876104,0.5844155844155844,0.55
relation-classification,0,"To investigate appropriate structures to represent relations between two target word pairs , we experiment with three structure options .",model,Dependency Layer,0,94,46,12,0,model : Dependency Layer,0.415929203539823,0.5974025974025974,0.6
relation-classification,0,"We primarily employ the shortest path structure ( SP - Tree ) , which captures the core dependency path between a target word pair and is widely used in relation classification models , e.g. , .",model,Dependency Layer,0,95,47,13,0,model : Dependency Layer,0.42035398230088494,0.6103896103896104,0.65
relation-classification,0,We also try two other dependency structures : SubTree and Full - Tree .,model,Dependency Layer,0,96,48,14,0,model : Dependency Layer,0.4247787610619469,0.6233766233766234,0.7
relation-classification,0,SubTree is the subtree under the lowest common ancestor of the target word pair .,model,Dependency Layer,0,97,49,15,0,model : Dependency Layer,0.42920353982300885,0.6363636363636364,0.75
relation-classification,0,This provides additional modifier information to the path and the word pair in SPTree .,model,Dependency Layer,0,98,50,16,0,model : Dependency Layer,0.4336283185840708,0.6493506493506493,0.8
relation-classification,0,FullTree is the full dependency tree .,model,Dependency Layer,0,99,51,17,0,model : Dependency Layer,0.43805309734513276,0.6623376623376623,0.85
relation-classification,0,This captures context from the entire sentence .,model,Dependency Layer,0,100,52,18,0,model : Dependency Layer,0.4424778761061947,0.6753246753246753,0.9
relation-classification,0,"While we use one node type for SPTree , we define two node types for SubTree and FullTree , i.e. , one for nodes on shortest paths and one for all other nodes .",model,Dependency Layer,0,101,53,19,0,model : Dependency Layer,0.4469026548672566,0.6883116883116883,0.95
relation-classification,0,We use the type mapping function m ( ) to distinguish these two nodes types .,model,Dependency Layer,0,102,54,20,0,model : Dependency Layer,0.45132743362831856,0.7012987012987013,1.0
relation-classification,0,Stacking Sequence and Dependency Layers,model,Stacking Sequence and Dependency Layers,0,103,55,1,0,model : Stacking Sequence and Dependency Layers,0.4557522123893805,0.7142857142857143,0.25
relation-classification,0,We stack the dependency layers ( corresponding to relation candidates ) on top of the sequence layer to incorporate both word sequence and dependency tree structure information into the output .,model,Stacking Sequence and Dependency Layers,0,104,56,2,0,model : Stacking Sequence and Dependency Layers,0.46017699115044247,0.7272727272727273,0.5
relation-classification,0,The dependency - layer LSTM unit at the t - th word receives as input,model,Stacking Sequence and Dependency Layers,0,105,57,3,0,model : Stacking Sequence and Dependency Layers,0.4646017699115044,0.7402597402597403,0.75
relation-classification,0,", the concatenation of its corresponding hidden state vectors st in the sequence layer , dependency type embedding v",model,Stacking Sequence and Dependency Layers,0,106,58,4,0,model : Stacking Sequence and Dependency Layers,0.4690265486725664,0.7532467532467533,1.0
relation-classification,0,Relation Classification,model,Relation Classification,0,107,59,1,0,model : Relation Classification,0.47345132743362833,0.7662337662337663,0.05263157894736842
relation-classification,0,"We incrementally build relation candidates using all possible combinations of the last words of detected entities , i.e. , words with L or U labels in the BILOU scheme , during decoding .",model,Relation Classification,0,108,60,2,0,model : Relation Classification,0.4778761061946903,0.7792207792207793,0.10526315789473684
relation-classification,0,"For instance , in , we build a relation candidate using Yates with an L - PER label and Chicago with an U - LOC label .",model,Relation Classification,0,109,61,3,0,model : Relation Classification,0.4823008849557522,0.7922077922077922,0.15789473684210525
relation-classification,0,"For each relation candidate , we realize the dependency layer d p ( described above ) corresponding to the path between the word pair pin the relation candidate , and the NN receives a relation candidate vector constructed from the output of the dependency tree layer , and predicts its relation label .",model,Relation Classification,0,110,62,4,0,model : Relation Classification,0.48672566371681414,0.8051948051948052,0.21052631578947367
relation-classification,0,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation .,model,Relation Classification,0,111,63,5,0,model : Relation Classification,0.4911504424778761,0.8181818181818182,0.2631578947368421
relation-classification,0,"We represent relation labels by type and direction , except for negative relations that have no direction .",model,Relation Classification,0,112,64,6,0,model : Relation Classification,0.49557522123893805,0.8311688311688312,0.3157894736842105
relation-classification,0,"The relation candidate vector is constructed as the concatenation d p = [?h p A ; ?h p 1 ; ?h p 2 ] , where ?h p",model,Relation Classification,0,113,65,7,0,model : Relation Classification,0.5,0.8441558441558441,0.3684210526315789
relation-classification,0,is the hidden state vector of the top LSTM,model,Relation Classification,0,114,66,8,0,model : Relation Classification,0.504424778761062,0.8571428571428571,0.42105263157894735
relation-classification,0,We use the dependency to the parent since the number of children varies .,model,Relation Classification,0,115,67,9,0,model : Relation Classification,0.5088495575221239,0.8701298701298701,0.47368421052631576
relation-classification,0,"Dependency types can also be incorporated into m ( ) , but this did not help in initial experiments .",model,Relation Classification,0,116,68,10,0,model : Relation Classification,0.5132743362831859,0.8831168831168831,0.5263157894736842
relation-classification,0,"unit in the bottom - up LSTM - RNN ( representing the lowest common ancestor of the target word pair p ) , and ?h p 1 , ?h p 2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top - down LSTM - RNN .",model,Relation Classification,0,117,69,11,0,model : Relation Classification,0.5176991150442478,0.8961038961038961,0.5789473684210527
relation-classification,0,All the corresponding arrows are shown in .,model,Relation Classification,0,118,70,12,0,model : Relation Classification,0.5221238938053098,0.9090909090909091,0.631578947368421
relation-classification,0,"Similarly to the entity detection , we employ a two - layered NN with an n hr -dimensional hidden layer h ( r ) and a softmax output layer ( with weight matrices W , bias vectors b ) .",model,Relation Classification,0,119,71,13,0,model : Relation Classification,0.5265486725663717,0.922077922077922,0.6842105263157895
relation-classification,0,"We construct the input d p for relation classification from tree - structured LSTM - RNNs stacked on sequential LSTM - RNNs , so the contribution of sequence layer to the input is indirect .",model,Relation Classification,0,120,72,14,0,model : Relation Classification,0.5309734513274337,0.935064935064935,0.7368421052631579
relation-classification,0,"Furthermore , our model uses words for representing entities , so it can not fully use the entity information .",model,Relation Classification,0,121,73,15,0,model : Relation Classification,0.5353982300884956,0.948051948051948,0.7894736842105263
relation-classification,0,"To alleviate these problems , we directly concatenate the average of hidden state vectors for each entity from the sequence layer to the input d p to relation classification , i.e. , d p =",model,Relation Classification,0,122,74,16,0,model : Relation Classification,0.5398230088495575,0.961038961038961,0.8421052631578947
relation-classification,0,where I p 1 and I p 2 represent sets of word indices in the first and second entities .,model,Relation Classification,0,123,75,17,0,model : Relation Classification,0.5442477876106194,0.974025974025974,0.8947368421052632
relation-classification,0,"Also , we assign two labels to each word pair in prediction since we consider both left - to - right and right - to - left directions .",model,Relation Classification,0,124,76,18,0,model : Relation Classification,0.5486725663716814,0.987012987012987,0.9473684210526315
relation-classification,0,"When the predicted labels are inconsistent , we select the positive and more confident label , similar to .",model,Relation Classification,0,125,77,19,0,model : Relation Classification,0.5530973451327433,1.0,1.0
relation-classification,0,Training,training,Training,0,126,1,1,0,training : Training,0.5575221238938053,0.125,0.125
relation-classification,0,"We update the model parameters including weights , biases , and embeddings by BPTT and Adam ( Kingma and Ba , 2015 ) with gradient clipping , parameter averaging , and L2-regularization ( we regularize weights W and U , not the bias terms b ) .",training,Training,0,127,2,2,0,training : Training,0.5619469026548672,0.25,0.25
relation-classification,0,We also apply dropout to the embedding layer and to the final hidden layers for entity detection and relation classification .,training,Training,0,128,3,3,0,training : Training,0.5663716814159292,0.375,0.375
relation-classification,0,"We employ two enhancements , scheduled sampling and entity pretraining , to alleviate the problem of unreliable prediction of entities in the early stage of training , and to encourage building positive relation instances from the detected entities .",training,Training,0,129,4,4,0,training : Training,0.5707964601769911,0.5,0.5
relation-classification,0,"In scheduled sampling , we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal .",training,Training,0,130,5,5,0,training : Training,0.5752212389380531,0.625,0.625
relation-classification,0,"As for i , we choose the inverse sigmoid decay i = k / ( k + exp ( i / k ) ) , where k( ? 1 ) is a hyper - parameter that adjusts how often we use the gold labels as prediction .",training,Training,0,131,6,6,0,training : Training,0.5796460176991151,0.75,0.75
relation-classification,0,"As for i , we choose the inverse sigmoid decay i = k / ( k + exp ( i / k ) ) , where k( ? 1 ) is a hyper - parameter that adjusts how often we use the gold labels as prediction .",training,Training,0,132,7,7,0,training : Training,0.584070796460177,0.875,0.875
relation-classification,0,"Entity pretraining is inspired by , and we pretrain the entity detection model using the training data before training the entire model parameters .",training,Training,0,133,8,8,0,training : Training,0.588495575221239,1.0,1.0
relation-classification,0,Results and Discussion,result,Results and Discussion,0,134,1,1,0,result : Results and Discussion,0.5929203539823009,0.05263157894736842,1.0
relation-classification,0,Data and Task Settings,result,Data and Task Settings,0,135,2,1,0,result : Data and Task Settings,0.5973451327433629,0.10526315789473684,0.05555555555555555
relation-classification,0,"We evaluate on three datasets : ACE05 and ACE04 for end - to - end relation extraction , and SemEval - 2010 Task 8 for relation classification .",result,Data and Task Settings,0,136,3,2,0,result : Data and Task Settings,0.6017699115044248,0.15789473684210525,0.1111111111111111
relation-classification,0,"We use the first two datasets as our primary target , and use the last one to thoroughly analyze and ablate the relation classification part of our model .",result,Data and Task Settings,0,137,4,3,0,result : Data and Task Settings,0.6061946902654868,0.21052631578947367,0.16666666666666666
relation-classification,0,ACE05 defines 7 coarse - grained entity types and 6 coarse - grained relation types between entities .,result,Data and Task Settings,0,138,5,4,0,result : Data and Task Settings,0.6106194690265486,0.2631578947368421,0.2222222222222222
relation-classification,0,"We use the same data splits , preprocessing , and task settings as .",result,Data and Task Settings,0,139,6,5,0,result : Data and Task Settings,0.6150442477876106,0.3157894736842105,0.2777777777777778
relation-classification,0,We report the primary micro F1 -scores as well as micro precision and recall on both entity and relation extraction to better explain model performance .,result,Data and Task Settings,0,140,7,6,0,result : Data and Task Settings,0.6194690265486725,0.3684210526315789,0.3333333333333333
relation-classification,0,We treat an entity as correct when it s type and the region of its head are correct .,result,Data and Task Settings,0,141,8,7,0,result : Data and Task Settings,0.6238938053097345,0.42105263157894735,0.3888888888888889
relation-classification,0,We treat a relation as correct when it s type and argument entities are correct ; we thus treat all non-negative relations on wrong entities as false positives .,result,Data and Task Settings,0,142,9,8,0,result : Data and Task Settings,0.6283185840707964,0.47368421052631576,0.4444444444444444
relation-classification,0,"ACE04 defines the same 7 coarse - grained entity types as ACE05 , but defines 7 coarse - grained relation types .",result,Data and Task Settings,0,143,10,9,0,result : Data and Task Settings,0.6327433628318584,0.5263157894736842,0.5
relation-classification,0,"We follow the cross-validation setting of Chan and and , and the preprocessing and evaluation metrics of ACE05 .",result,Data and Task Settings,0,144,11,10,0,result : Data and Task Settings,0.6371681415929203,0.5789473684210527,0.5555555555555556
relation-classification,0,SemEval-2010,result,Data and Task Settings,0,145,12,11,0,result : Data and Task Settings,0.6415929203539823,0.631578947368421,0.6111111111111112
relation-classification,0,Task 8 defines 9 relation types between nominals and a tenth type,result,Data and Task Settings,0,146,13,12,0,result : Data and Task Settings,0.6460176991150443,0.6842105263157895,0.6666666666666666
relation-classification,0,Other when two nouns have none of these relations .,result,Data and Task Settings,0,147,14,13,0,result : Data and Task Settings,0.6504424778761062,0.7368421052631579,0.7222222222222222
relation-classification,0,"We treat this Other type as a negative relation type , and no direction is considered .",result,Data and Task Settings,0,148,15,14,0,result : Data and Task Settings,0.6548672566371682,0.7894736842105263,0.7777777777777778
relation-classification,0,"The dataset consists of 8,000 training and 2,717 test sentences , and each sentence is annotated with a relation between two given nominals .",result,Data and Task Settings,0,149,16,15,0,result : Data and Task Settings,0.6592920353982301,0.8421052631578947,0.8333333333333334
relation-classification,0,We randomly selected 800 sentences from the training set as our development set .,result,Data and Task Settings,0,150,17,16,0,result : Data and Task Settings,0.6637168141592921,0.8947368421052632,0.8888888888888888
relation-classification,0,"We followed the official task setting , and report the official macro -averaged F1 - score ( Macro - F1 ) on the 9 relation types .",result,Data and Task Settings,0,151,18,17,0,result : Data and Task Settings,0.668141592920354,0.9473684210526315,0.9444444444444444
relation-classification,0,"For more details of the data and task settings , please refer to the supplementary material .",result,Data and Task Settings,0,152,19,18,0,result : Data and Task Settings,0.672566371681416,1.0,1.0
relation-classification,0,Experimental Settings,experiment,Experimental Settings,0,153,1,1,0,experiment : Experimental Settings,0.6769911504424779,0.023255813953488372,0.09090909090909091
relation-classification,0,We implemented our model using the cnn library .,experiment,Experimental Settings,1,154,2,2,0,experiment : Experimental Settings,0.6814159292035398,0.046511627906976744,0.18181818181818182
relation-classification,0,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .",experiment,Experimental Settings,1,155,3,3,0,experiment : Experimental Settings,0.6858407079646017,0.06976744186046512,0.2727272727272727
relation-classification,0,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .",experiment,Experimental Settings,1,156,4,4,0,experiment : Experimental Settings,0.6902654867256637,0.09302325581395349,0.36363636363636365
relation-classification,0,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,experiment,Experimental Settings,1,157,5,5,0,experiment : Experimental Settings,0.6946902654867256,0.11627906976744186,0.45454545454545453
relation-classification,0,We tuned hyper - parameters using development sets for ACE05 and SemEval - 2010 Task 8 to achieve high primary ( Micro - and Macro - ) F1-scores .,experiment,Experimental Settings,0,158,6,6,0,experiment : Experimental Settings,0.6991150442477876,0.13953488372093023,0.5454545454545454
relation-classification,0,"For ACE04 , we directly employed the best parameters for ACE05 .",experiment,Experimental Settings,0,159,7,7,0,experiment : Experimental Settings,0.7035398230088495,0.16279069767441862,0.6363636363636364
relation-classification,0,The hyperparameter settings are shown in the supplementary material .,experiment,Experimental Settings,0,160,8,8,0,experiment : Experimental Settings,0.7079646017699115,0.18604651162790697,0.7272727272727273
relation-classification,0,For SemEval-2010,experiment,Experimental Settings,0,161,9,9,0,experiment : Experimental Settings,0.7123893805309734,0.20930232558139536,0.8181818181818182
relation-classification,0,"Task 8 , we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types .",experiment,Experimental Settings,0,162,10,10,0,experiment : Experimental Settings,0.7168141592920354,0.23255813953488372,0.9090909090909091
relation-classification,0,Our statistical significance results are based on the Approximate Randomization ( AR ) test .,experiment,Experimental Settings,0,163,11,11,0,experiment : Experimental Settings,0.7212389380530974,0.2558139534883721,1.0
relation-classification,0,End - to - end Relation Extraction Results,experiment,End-to-end Relation Extraction Results,0,164,12,1,0,experiment : End-to-end Relation Extraction Results,0.7256637168141593,0.27906976744186046,0.03125
relation-classification,0,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .",experiment,End-to-end Relation Extraction Results,1,165,13,2,0,experiment : End-to-end Relation Extraction Results,0.7300884955752213,0.3023255813953488,0.0625
relation-classification,0,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .",experiment,End-to-end Relation Extraction Results,1,166,14,3,0,experiment : End-to-end Relation Extraction Results,0.7345132743362832,0.32558139534883723,0.09375
relation-classification,0,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .",experiment,End-to-end Relation Extraction Results,1,167,15,4,0,experiment : End-to-end Relation Extraction Results,0.7389380530973452,0.3488372093023256,0.125
relation-classification,0,"This is reasonable because the model can only create relation instances when both of the entities are found and , without these enhancements , it may get too late to find some relations .",experiment,End-to-end Relation Extraction Results,0,168,16,5,0,experiment : End-to-end Relation Extraction Results,0.7433628318584071,0.37209302325581395,0.15625
relation-classification,0,Removing label embeddings did not affect 6 https://github.com/clab/cnn,experiment,End-to-end Relation Extraction Results,0,169,17,6,0,experiment : End-to-end Relation Extraction Results,0.7477876106194691,0.3953488372093023,0.1875
relation-classification,0,http://nlp.stanford.edu/software/,experiment,End-to-end Relation Extraction Results,0,170,18,7,0,experiment : End-to-end Relation Extraction Results,0.7522123893805309,0.4186046511627907,0.21875
relation-classification,0,stanford-corenlp-full-2015-04-20.zip,experiment,End-to-end Relation Extraction Results,0,171,19,8,0,experiment : End-to-end Relation Extraction Results,0.7566371681415929,0.4418604651162791,0.25
relation-classification,0,https://dumps.wikimedia.org/enwiki/ 20150901/,experiment,End-to-end Relation Extraction Results,0,172,20,9,0,experiment : End-to-end Relation Extraction Results,0.7610619469026548,0.46511627906976744,0.28125
relation-classification,0,9,experiment,End-to-end Relation Extraction Results,0,173,21,10,0,experiment : End-to-end Relation Extraction Results,0.7654867256637168,0.4883720930232558,0.3125
relation-classification,0,"We did not tune the precision - recall trade - offs , but doing so can specifically improve precision further .",experiment,End-to-end Relation Extraction Results,0,174,22,11,0,experiment : End-to-end Relation Extraction Results,0.7699115044247787,0.5116279069767442,0.34375
relation-classification,0,"Other work on ACE is not comparable or performs worse than the model by the entity detection performance , but this degraded the recall in relation classification .",experiment,End-to-end Relation Extraction Results,0,175,23,12,0,experiment : End-to-end Relation Extraction Results,0.7743362831858407,0.5348837209302325,0.375
relation-classification,0,This indicates that entity label information is helpful in detecting relations .,experiment,End-to-end Relation Extraction Results,0,176,24,13,0,experiment : End-to-end Relation Extraction Results,0.7787610619469026,0.5581395348837209,0.40625
relation-classification,0,"We also show the performance without sharing parameters , i.e. , embedding and sequence layers , for detecting entities and relations ( ? Shared parameters ) ; we first train the entity detection model , detect entities with the model , and build a separate relation extraction model using the detected entities , i.e. , without entity detection .",experiment,End-to-end Relation Extraction Results,0,177,25,14,0,experiment : End-to-end Relation Extraction Results,0.7831858407079646,0.5813953488372093,0.4375
relation-classification,0,This setting can be regarded as a pipeline model since two separate models are trained sequentially .,experiment,End-to-end Relation Extraction Results,0,178,26,15,0,experiment : End-to-end Relation Extraction Results,0.7876106194690266,0.6046511627906976,0.46875
relation-classification,0,"Without the shared parameters , both the performance in entity detection and relation classification drops slightly , although the differences are not significant .",experiment,End-to-end Relation Extraction Results,0,179,27,16,0,experiment : End-to-end Relation Extraction Results,0.7920353982300885,0.627906976744186,0.5
relation-classification,0,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .",experiment,End-to-end Relation Extraction Results,1,180,28,17,0,experiment : End-to-end Relation Extraction Results,0.7964601769911505,0.6511627906976745,0.53125
relation-classification,0,"Next , we show the performance with different LSTM - RNN structures in .",experiment,End-to-end Relation Extraction Results,0,181,29,18,0,experiment : End-to-end Relation Extraction Results,0.8008849557522124,0.6744186046511628,0.5625
relation-classification,0,"We first compare the three input dependency structures ( SPTree , SubTree , FullTree ) for tree - structured LSTM - RNNs .",experiment,End-to-end Relation Extraction Results,0,182,30,19,0,experiment : End-to-end Relation Extraction Results,0.8053097345132744,0.6976744186046512,0.59375
relation-classification,0,"Performances on these three structures are almost same when we distinguish the nodes in the shortest paths from other nodes , but when we do not distinguish them ( - SP ) , the information outside of the shortest path , i.e. , FullTree ( - SP ) , significantly hurts performance ( p < 0.05 ) .",experiment,End-to-end Relation Extraction Results,0,183,31,20,0,experiment : End-to-end Relation Extraction Results,0.8097345132743363,0.7209302325581395,0.625
relation-classification,0,We then compare our tree - structured LSTM - RNN ( SPTree ) with the Child - Sum treestructured LSTM - RNN on the shortest path of .,experiment,End-to-end Relation Extraction Results,0,184,32,21,0,experiment : End-to-end Relation Extraction Results,0.8141592920353983,0.7441860465116279,0.65625
relation-classification,0,"Child - Sum performs worse than our SPTree model , but not with as big of a decrease as above .",experiment,End-to-end Relation Extraction Results,0,185,33,22,0,experiment : End-to-end Relation Extraction Results,0.8185840707964602,0.7674418604651163,0.6875
relation-classification,0,This maybe because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child .,experiment,End-to-end Relation Extraction Results,0,186,34,23,0,experiment : End-to-end Relation Extraction Results,0.8230088495575221,0.7906976744186046,0.71875
relation-classification,0,We finally show results with two counterparts of sequence - based LSTM - RNNs using the shortest path ( last two rows in ) .,experiment,End-to-end Relation Extraction Results,0,187,35,24,0,experiment : End-to-end Relation Extraction Results,0.827433628318584,0.813953488372093,0.75
relation-classification,0,SPSeq is a bidirectional LSTM - RNN on the shortest path .,experiment,End-to-end Relation Extraction Results,0,188,36,25,0,experiment : End-to-end Relation Extraction Results,0.831858407079646,0.8372093023255814,0.78125
relation-classification,0,The LSTM unit receives input from the sequence layer concatenated with embeddings for the surrounding dependency types and directions .,experiment,End-to-end Relation Extraction Results,0,189,37,26,0,experiment : End-to-end Relation Extraction Results,0.8362831858407079,0.8604651162790697,0.8125
relation-classification,0,We concatenate the outputs of the two RNNs for the relation candidate .,experiment,End-to-end Relation Extraction Results,0,190,38,27,0,experiment : End-to-end Relation Extraction Results,0.8407079646017699,0.8837209302325582,0.84375
relation-classification,0,SPX u is our adaptation of the shortest path LSTM - RNN proposed by to match our sequence - layer based model .,experiment,End-to-end Relation Extraction Results,0,191,39,28,0,experiment : End-to-end Relation Extraction Results,0.8451327433628318,0.9069767441860465,0.875
relation-classification,0,11 This has two LSTM - RNNs for the left and right subpaths of the shortest path .,experiment,End-to-end Relation Extraction Results,0,192,40,29,0,experiment : End-to-end Relation Extraction Results,0.8495575221238938,0.9302325581395349,0.90625
relation-classification,0,"We first calculate the max pooling of the LSTM units for each of these two RNNs , and then concatenate the outputs of the pooling for the relation candidate .",experiment,End-to-end Relation Extraction Results,0,193,41,30,0,experiment : End-to-end Relation Extraction Results,0.8539823008849557,0.9534883720930233,0.9375
relation-classification,0,The comparison with these sequence - based LSTM - RNNs indicates that a tree - structured LSTM - RNN is comparable to sequence - based ones in representing shortest paths .,experiment,End-to-end Relation Extraction Results,0,194,42,31,0,experiment : End-to-end Relation Extraction Results,0.8584070796460177,0.9767441860465116,0.96875
relation-classification,0,"Overall , the performance comparison of the LSTM - RNN structures in show that for end - to - end relation extraction , selecting the appropriate tree structure representation of the input ( i.e. , the shortest path ) is more important than the choice of the LSTM - RNN structure on that input ( i.e. , sequential versus tree - based ) .",experiment,End-to-end Relation Extraction Results,0,195,43,32,0,experiment : End-to-end Relation Extraction Results,0.8628318584070797,1.0,1.0
relation-classification,0,Relation Classification Analysis Results,analysis,Relation Classification Analysis Results,0,196,1,1,0,analysis : Relation Classification Analysis Results,0.8672566371681416,0.041666666666666664,0.3333333333333333
relation-classification,0,"To thoroughly analyze the relation classification part alone , e.g. , comparing different LSTM structures , architecture components such as hidden layers and input information , and classification task settings , we use the SemEval - 2010 Task 8 .",analysis,Relation Classification Analysis Results,0,197,2,2,0,analysis : Relation Classification Analysis Results,0.8716814159292036,0.08333333333333333,0.6666666666666666
relation-classification,0,"This dataset , often used to evaluate NN models for relation classification , annotates only relation - related nominals ( unlike ACE datasets ) , so we can focus cleanly on the relation classification part .",analysis,Relation Classification Analysis Results,0,198,3,3,0,analysis : Relation Classification Analysis Results,0.8761061946902655,0.125,1.0
relation-classification,0,Settings,analysis,Settings,0,199,4,1,0,analysis : Settings,0.8805309734513275,0.16666666666666666,0.047619047619047616
relation-classification,0,Macro - F1 No External Knowledge Resources Our Model ( SPTree ) 0.844 dos 0.841 0.840 + Word,analysis,Settings,0,200,5,2,0,analysis : Settings,0.8849557522123894,0.20833333333333334,0.09523809523809523
relation-classification,0,Net Our Model ( SPTree + WordNet ) 0.855 0.856 0.837 We first report official test set results in Table 4 .,analysis,Settings,0,201,6,3,0,analysis : Settings,0.8893805309734514,0.25,0.14285714285714285
relation-classification,0,"Our novel LSTM - RNN model is comparable to both the state - of - the - art CNN - based models on this task with or without external sources , i.e. , WordNet , unlike the previous best LSTM - RNN model .",analysis,Settings,0,202,7,4,0,analysis : Settings,0.8938053097345132,0.2916666666666667,0.19047619047619047
relation-classification,0,"Next , we compare different LSTM - RNN structures in .",analysis,Settings,0,203,8,5,0,analysis : Settings,0.8982300884955752,0.3333333333333333,0.23809523809523808
relation-classification,0,"As for the three input dependency structures ( SPTree , SubTree , FullTree ) , Full",analysis,Settings,0,204,9,6,0,analysis : Settings,0.9026548672566371,0.375,0.2857142857142857
relation-classification,0,"Tree performs significantly worse than other structures regardless of whether or not we distinguish the nodes in the shortest paths from the other nodes , which hints that the information outside of the shortest path significantly hurts the performance ( p < 0.05 ) .",analysis,Settings,0,205,10,7,0,analysis : Settings,0.9070796460176991,0.4166666666666667,0.3333333333333333
relation-classification,0,We also compare our treestructured LSTM - RNN ( SPTree ) with sequencebased LSTM - RNNs ( SPSeq and SPXu ) and treestructured LSTM - RNNs ( Child - Sum ) .,analysis,Settings,0,206,11,8,0,analysis : Settings,0.911504424778761,0.4583333333333333,0.38095238095238093
relation-classification,0,"All these LSTM - RNNs perform slightly worse than our SP - 12 When incorporating WordNet information into our model , we prepared embeddings for WordNet hypernyms extracted by SuperSenseTagger and concatenated the embeddings to the input vector ( the concatenation of word and POS embeddings ) of the sequence LSTM .",analysis,Settings,0,207,12,9,0,analysis : Settings,0.915929203539823,0.5,0.42857142857142855
relation-classification,0,We tuned the dimension of the WordNet embeddings and set it to 15 using the development dataset .,analysis,Settings,0,208,13,10,0,analysis : Settings,0.9203539823008849,0.5416666666666666,0.47619047619047616
relation-classification,0,"produces different results on FullTree as compared to the results on ACE05 in , the trend still holds that selecting the appropriate tree structure representation of the input is more important than the choice of the LSTM - RNN structure on that input .",analysis,Settings,0,209,14,11,0,analysis : Settings,0.9247787610619469,0.5833333333333334,0.5238095238095238
relation-classification,0,"Finally , summarizes the contribution of several model components and training settings on SemEval relation classification .",analysis,Settings,0,210,15,12,0,analysis : Settings,0.9292035398230089,0.625,0.5714285714285714
relation-classification,0,"We first remove the hidden layer by directly connecting the LSTM - RNN layers to the softmax layers , and found that this slightly degraded performance , but the difference was small .",analysis,Settings,0,211,16,13,0,analysis : Settings,0.9336283185840708,0.6666666666666666,0.6190476190476191
relation-classification,0,We then skip the sequence layer and directly use the word and POS embeddings for the dependency layer .,analysis,Settings,0,212,17,14,0,analysis : Settings,0.9380530973451328,0.7083333333333334,0.6666666666666666
relation-classification,0,"Removing the sequence layer 13 or entity - related information from the sequence layer ( ? Pair ) slightly degraded performance , and , on removing both , the performance dropped significantly ( p < 0.05 ) .",analysis,Settings,0,213,18,15,0,analysis : Settings,0.9424778761061947,0.75,0.7142857142857143
relation-classification,0,This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task .,analysis,Settings,0,214,19,16,0,analysis : Settings,0.9469026548672567,0.7916666666666666,0.7619047619047619
relation-classification,0,"When we replace the Stanford neural dependency parser with the Stanford lexicalized PCFG parser ( Stanford PCFG ) , the performance slightly dropped , but the difference was small .",analysis,Settings,0,215,20,17,0,analysis : Settings,0.9513274336283186,0.8333333333333334,0.8095238095238095
relation-classification,0,This indicates that the selection of parsing models is not critical .,analysis,Settings,0,216,21,18,0,analysis : Settings,0.9557522123893806,0.875,0.8571428571428571
relation-classification,0,"We also included WordNet , and this slightly improved the performance ( + WordNet ) , but the difference was small .",analysis,Settings,0,217,22,19,0,analysis : Settings,0.9601769911504425,0.9166666666666666,0.9047619047619048
relation-classification,0,"Lastly , for the generation of relation candidates , generating only leftto - right candidates slightly degraded the perfor- mance , but the difference was small and hence the creation of right - to - left candidates was not critical .",analysis,Settings,0,218,23,20,0,analysis : Settings,0.9646017699115044,0.9583333333333334,0.9523809523809523
relation-classification,0,"Treating the inverse relation candidate as a negative instance ( Negative sampling ) also performed comparably to other generation methods in our model , which showed a significance improvement over generating only left - to - right candidates ) .",analysis,Settings,0,219,24,21,0,analysis : Settings,0.9690265486725663,1.0,1.0
relation-classification,0,Conclusion,conclusion,Conclusion,0,220,1,1,0,conclusion : Conclusion,0.9734513274336283,0.14285714285714285,0.14285714285714285
relation-classification,0,We presented a novel end - to - end relation extraction model that represents both word sequence and dependency tree structures by using bidirectional sequential and bidirectional tree - structured LSTM - RNNs .,conclusion,Conclusion,0,221,2,2,0,conclusion : Conclusion,0.9778761061946902,0.2857142857142857,0.2857142857142857
relation-classification,0,"This allowed us to represent both entities and relations in a single model , achieving gains over the state - of - the - art , feature - based system on end - to - end relation extraction ( ACE04 and ACE05 ) , and showing favorably comparable performance to recent state - of - the - art CNNbased models on nominal relation classification ( Sem Eval - 2010 Task 8 ) .",conclusion,Conclusion,0,222,3,3,0,conclusion : Conclusion,0.9823008849557522,0.42857142857142855,0.42857142857142855
relation-classification,0,Our evaluation and ablation led to three key findings .,conclusion,Conclusion,0,223,4,4,0,conclusion : Conclusion,0.9867256637168141,0.5714285714285714,0.5714285714285714
relation-classification,0,"First , the use of both word sequence and dependency tree structures is effective .",conclusion,Conclusion,0,224,5,5,0,conclusion : Conclusion,0.9911504424778761,0.7142857142857143,0.7142857142857143
relation-classification,0,"Second , training with the shared parameters improves relation extraction accuracy , especially when employed with entity pretraining , scheduled sampling , and label embeddings .",conclusion,Conclusion,0,225,6,6,0,conclusion : Conclusion,0.995575221238938,0.8571428571428571,0.8571428571428571
relation-classification,0,"Finally , the shortest path , which has been widely used in relation classification , is also appropriate for representing tree structures in neural LSTM models .",conclusion,Conclusion,0,226,7,7,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,1,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,title,title,1,2,1,1,0,title : title,0.008130081300813009,1.0,1.0
relation-classification,1,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.012195121951219513,0.16666666666666666,0.16666666666666666
relation-classification,1,Joint extraction of entities and relations is an important task in information extraction .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.016260162601626018,0.3333333333333333,0.3333333333333333
relation-classification,1,"To tackle this problem , we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02032520325203252,0.5,0.5
relation-classification,1,"Then , based on our tagging scheme , we study different end - toend models to extract entities and their relations directly , without identifying entities and relations separately .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.024390243902439025,0.6666666666666666,0.6666666666666666
relation-classification,1,We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.028455284552845527,0.8333333333333334,0.8333333333333334
relation-classification,1,"What 's more , the end - to - end model proposed in this paper , achieves the best results on the public dataset .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.032520325203252036,1.0,1.0
relation-classification,1,Introduction,introduction,introduction,0,9,1,1,0,introduction : introduction,0.036585365853658534,0.024390243902439025,0.024390243902439025
relation-classification,1,"Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text , as shows .",introduction,introduction,1,10,2,2,0,introduction : introduction,0.04065040650406504,0.04878048780487805,0.04878048780487805
relation-classification,1,"Different from open information extraction ( Open IE ) ) whose relation words are extracted from the given sentence , in this task , relation words are extracted from a predefined relation set which may not appear in the given sentence .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.044715447154471545,0.07317073170731707,0.07317073170731707
relation-classification,1,It is an important issue in knowledge extraction and automatic construction of knowledge base .,introduction,introduction,0,12,4,4,0,introduction : introduction,0.04878048780487805,0.0975609756097561,0.0975609756097561
relation-classification,1,"Traditional methods handle this task in a pipelined manner , i.e. , extracting the entities first and then recognizing their relations .",introduction,introduction,0,13,5,5,0,introduction : introduction,0.052845528455284556,0.12195121951219512,0.12195121951219512
relation-classification,1,"This separated framework makes the task easy to deal with , and each component can be more flexible .",introduction,introduction,0,14,6,6,0,introduction : introduction,0.056910569105691054,0.14634146341463414,0.14634146341463414
relation-classification,1,But it neglects the relevance between these two sub - tasks and each subtask is an independent model .,introduction,introduction,0,15,7,7,0,introduction : introduction,0.06097560975609756,0.17073170731707318,0.17073170731707318
relation-classification,1,The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery .,introduction,introduction,0,16,8,8,0,introduction : introduction,0.06504065040650407,0.1951219512195122,0.1951219512195122
relation-classification,1,"Different from the pipelined methods , joint learning framework is to extract entities together with relations using a single model .",introduction,introduction,1,17,9,9,0,introduction : introduction,0.06910569105691057,0.21951219512195122,0.21951219512195122
relation-classification,1,"It can effectively integrate the information of entities and relations , and it has been shown to achieve better results in this task .",introduction,introduction,0,18,10,10,0,introduction : introduction,0.07317073170731707,0.24390243902439024,0.24390243902439024
relation-classification,1,"However , most existing joint methods are feature - based structured systems .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.07723577235772358,0.2682926829268293,0.2682926829268293
relation-classification,1,"They need complicated feature engineering and heavily rely on the other NLP toolkits , which might also lead to error propagation .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.08130081300813008,0.2926829268292683,0.2926829268292683
relation-classification,1,"In order to reduce the manual work in feature extraction , recently , presents a neural network - based method for the end - to - end entities and relations extraction .",introduction,introduction,0,21,13,13,0,introduction : introduction,0.08536585365853659,0.3170731707317073,0.3170731707317073
relation-classification,1,"Although the joint models can represent both entities and relations with shared parameters in a single model , they also extract the entities and relations separately and produce redundant information .",introduction,introduction,0,22,14,14,0,introduction : introduction,0.08943089430894309,0.34146341463414637,0.34146341463414637
relation-classification,1,"For instance , the sentence in contains three entities : "" United States "" , "" Trump "" and "" Apple Inc "" .",introduction,introduction,0,23,15,15,0,introduction : introduction,0.09349593495934959,0.36585365853658536,0.36585365853658536
relation-classification,1,"But only "" United States "" and "" Trump "" hold a fix relation "" Country - President "" .",introduction,introduction,0,24,16,16,0,introduction : introduction,0.0975609756097561,0.3902439024390244,0.3902439024390244
relation-classification,1,"Entity "" Apple Inc "" has no obvious relationship with the other entities in this sentence .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.1016260162601626,0.4146341463414634,0.4146341463414634
relation-classification,1,"Hence , the extracted result from this sentence is { United States e 1 , Country - President r , Trump e 2 } , which called triplet here .",introduction,introduction,0,26,18,18,0,introduction : introduction,0.10569105691056911,0.43902439024390244,0.43902439024390244
relation-classification,1,"In this paper , we focus on the extraction of triplets thatare composed of two entities and one relation between these two entities .",introduction,introduction,0,27,19,19,0,introduction : introduction,0.10975609756097561,0.4634146341463415,0.4634146341463415
relation-classification,1,"Therefore , we can model the triplets directly , rather than extracting the entities and relations separately .",introduction,introduction,0,28,20,20,0,introduction : introduction,0.11382113821138211,0.4878048780487805,0.4878048780487805
relation-classification,1,"Based on the motivations , we propose a tagging scheme accompanied with the end - to - end model to settle this problem .",introduction,introduction,0,29,21,21,0,introduction : introduction,0.11788617886178862,0.5121951219512195,0.5121951219512195
relation-classification,1,We design a kind of novel tags which contain the information of entities and the relationships they hold .,introduction,introduction,0,30,22,22,0,introduction : introduction,0.12195121951219512,0.5365853658536586,0.5365853658536586
relation-classification,1,"Based on this tagging scheme , the joint extraction of entities and relations can be transformed into a tagging problem .",introduction,introduction,0,31,23,23,0,introduction : introduction,0.12601626016260162,0.5609756097560976,0.5609756097560976
relation-classification,1,"In this way , we can also easily use neural networks to model the task without complicated feature engineering .",introduction,introduction,0,32,24,24,0,introduction : introduction,0.13008130081300814,0.5853658536585366,0.5853658536585366
relation-classification,1,"Recently , end - to - end models based on LSTM have been successfully applied to various tagging tasks :",introduction,introduction,0,33,25,25,0,introduction : introduction,0.13414634146341464,0.6097560975609756,0.6097560975609756
relation-classification,1,"Named Entity Recognition , CCG",introduction,introduction,0,34,26,26,0,introduction : introduction,0.13821138211382114,0.6341463414634146,0.6341463414634146
relation-classification,1,"Supertagging , Chunking et al . LSTM is capable of learning long - term dependencies , which is beneficial to sequence modeling tasks .",introduction,introduction,0,35,27,27,0,introduction : introduction,0.14227642276422764,0.6585365853658537,0.6585365853658537
relation-classification,1,"Supertagging , Chunking et al . LSTM is capable of learning long - term dependencies , which is beneficial to sequence modeling tasks .",introduction,introduction,0,36,28,28,0,introduction : introduction,0.14634146341463414,0.6829268292682927,0.6829268292682927
relation-classification,1,"Therefore , based on our tagging scheme , we investigate different kinds of LSTM - based end - to - end models to jointly extract the entities and relations .",introduction,introduction,0,37,29,29,0,introduction : introduction,0.15040650406504066,0.7073170731707317,0.7073170731707317
relation-classification,1,We also modify the decoding method by adding a biased loss to make it more suitable for our special tags .,introduction,introduction,0,38,30,30,0,introduction : introduction,0.15447154471544716,0.7317073170731707,0.7317073170731707
relation-classification,1,The method we proposed is a supervised learning algorithm .,introduction,introduction,0,39,31,31,0,introduction : introduction,0.15853658536585366,0.7560975609756098,0.7560975609756098
relation-classification,1,"In reality , however , the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone .",introduction,introduction,0,40,32,32,0,introduction : introduction,0.16260162601626016,0.7804878048780488,0.7804878048780488
relation-classification,1,"Therefore , we conduct experiments on a public dataset 1 which is produced by distant supervision method to validate our approach .",introduction,introduction,0,41,33,33,0,introduction : introduction,0.16666666666666666,0.8048780487804879,0.8048780487804879
relation-classification,1,The experimental results show that our tagging scheme is effective in this task .,introduction,introduction,0,42,34,34,0,introduction : introduction,0.17073170731707318,0.8292682926829268,0.8292682926829268
relation-classification,1,"In addition , our end - to - end model can achieve the best results on the public dataset .",introduction,introduction,0,43,35,35,0,introduction : introduction,0.17479674796747968,0.8536585365853658,0.8536585365853658
relation-classification,1,"The major contributions of this paper are : ( 1 ) A novel tagging scheme is proposed to jointly extract entities and relations , which can easily transform the extraction problem into a tagging task .",introduction,introduction,0,44,36,36,0,introduction : introduction,0.17886178861788618,0.8780487804878049,0.8780487804878049
relation-classification,1,"2 ) Based on our tagging scheme , we study different kinds of end - to - end models to settle the problem .",introduction,introduction,0,45,37,37,0,introduction : introduction,0.18292682926829268,0.9024390243902439,0.9024390243902439
relation-classification,1,The tagging - based methods are better than most of the existing pipelined and joint learning methods .,introduction,introduction,0,46,38,38,0,introduction : introduction,0.18699186991869918,0.926829268292683,0.926829268292683
relation-classification,1,"3 ) Furthermore , we also develop an end - to - 1 https://github.com/shanzhenren/Co",introduction,introduction,0,47,39,39,0,introduction : introduction,0.1910569105691057,0.9512195121951219,0.9512195121951219
relation-classification,1,Type end model with biased loss function to suit for the novel tags .,introduction,introduction,0,48,40,40,0,introduction : introduction,0.1951219512195122,0.975609756097561,0.975609756097561
relation-classification,1,It can enhance the association between related entities .,introduction,introduction,0,49,41,41,0,introduction : introduction,0.1991869918699187,1.0,1.0
relation-classification,1,Related Works,related work,Related Works,0,50,1,1,0,related work : Related Works,0.2032520325203252,0.0625,0.0625
relation-classification,1,"Entities and relations extraction is an important step to construct a knowledge base , which can be benefit for many NLP tasks .",related work,Related Works,0,51,2,2,0,related work : Related Works,0.2073170731707317,0.125,0.125
relation-classification,1,Two main frameworks have been widely used to solve the problem of extracting entity and their relationships .,related work,Related Works,0,52,3,3,0,related work : Related Works,0.21138211382113822,0.1875,0.1875
relation-classification,1,One is the pipelined method and the other is the joint learning method .,related work,Related Works,0,53,4,4,0,related work : Related Works,0.21544715447154472,0.25,0.25
relation-classification,1,"The pipelined method treats this task as two separated tasks , i.e. , named entity recognition ( NER ) and relation classification ( RC ) .",related work,Related Works,0,54,5,5,0,related work : Related Works,0.21951219512195122,0.3125,0.3125
relation-classification,1,"Classical NER models are linear statistical models , such as Hidden Markov Models ( HMM ) and Conditional Random Fields ( CRF ) .",related work,Related Works,0,55,6,6,0,related work : Related Works,0.22357723577235772,0.375,0.375
relation-classification,1,"Recently , several neural network architectures have been successfully applied to NER , which is regarded as a sequential token tagging task .",related work,Related Works,0,56,7,7,0,related work : Related Works,0.22764227642276422,0.4375,0.4375
relation-classification,1,Existing methods for relation classification can also be divided into handcrafted feature based methods and neural network based methods .,related work,Related Works,0,57,8,8,0,related work : Related Works,0.23170731707317074,0.5,0.5
relation-classification,1,While joint models extract entities and relations using a single model .,related work,Related Works,0,58,9,9,0,related work : Related Works,0.23577235772357724,0.5625,0.5625
relation-classification,1,Most of the joint methods are feature - based structured systems .,related work,Related Works,0,59,10,10,0,related work : Related Works,0.23983739837398374,0.625,0.625
relation-classification,1,"Recently , uses a LSTMbased model to extract entities and relations , which can reduce the manual work .",related work,Related Works,0,60,11,11,0,related work : Related Works,0.24390243902439024,0.6875,0.6875
relation-classification,1,"Different from the above methods , the method proposed in this paper is based on a special tagging manner , so that we can easily use end - toend model to extract results without NER and RC .",related work,Related Works,0,61,12,12,0,related work : Related Works,0.24796747967479674,0.75,0.75
relation-classification,1,end - to - end method is to map the input sentence into meaningful vectors and then back to produce a sequence .,related work,Related Works,0,62,13,13,0,related work : Related Works,0.25203252032520324,0.8125,0.8125
relation-classification,1,It is widely used in machine translation and sequence tagging tasks .,related work,Related Works,0,63,14,14,0,related work : Related Works,0.25609756097560976,0.875,0.875
relation-classification,1,"Most methods apply bidirectional LSTM to encode the input sentences , but the decoding methods are always different .",related work,Related Works,0,64,15,15,0,related work : Related Works,0.2601626016260163,0.9375,0.9375
relation-classification,1,"For examples , use a CRF layers to decode the tag sequence , while apply LSTM layer to produce the tag sequence .",related work,Related Works,0,65,16,16,0,related work : Related Works,0.26422764227642276,1.0,1.0
relation-classification,1,Method,method,Method,0,66,1,1,0,method : Method,0.2682926829268293,0.014285714285714285,0.06666666666666667
relation-classification,1,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,method,Method,1,67,2,2,0,method : Method,0.27235772357723576,0.02857142857142857,0.13333333333333333
relation-classification,1,"In this section , we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method .",method,Method,0,68,3,3,0,method : Method,0.2764227642276423,0.04285714285714286,0.2
relation-classification,1,Then we detail the model we used to extract results .,method,Method,0,69,4,4,0,method : Method,0.2804878048780488,0.05714285714285714,0.26666666666666666
relation-classification,1,is an example of how the results are tagged .,method,Method,0,70,5,5,0,method : Method,0.2845528455284553,0.07142857142857142,0.3333333333333333
relation-classification,1,Each word is assigned a label that contributes to extract the results .,method,Method,0,71,6,6,0,method : Method,0.2886178861788618,0.08571428571428572,0.4
relation-classification,1,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .",method,Method,1,72,7,7,0,method : Method,0.2926829268292683,0.1,0.4666666666666667
relation-classification,1,"In addition to "" O "" , the other tags consist of three parts : the word position in the entity , the relation type , and the relation role .",method,Method,0,73,8,8,0,method : Method,0.2967479674796748,0.11428571428571428,0.5333333333333333
relation-classification,1,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .",method,Method,1,74,9,9,0,method : Method,0.3008130081300813,0.12857142857142856,0.6
relation-classification,1,"The relation type information is obtained from a predefined set of relations and the relation role information is represented by the numbers "" 1 "" and "" 2 "" .",method,Method,0,75,10,10,0,method : Method,0.3048780487804878,0.14285714285714285,0.6666666666666666
relation-classification,1,"An extracted result is represented by a triplet : ( Entity 1 , Relation T ype , Entity 2 ) .",method,Method,0,76,11,11,0,method : Method,0.3089430894308943,0.15714285714285714,0.7333333333333333
relation-classification,1,"1 "" means that the word belongs to the first entity in the triplet , while "" 2 "" belongs to second entity that behind the relation type .",method,Method,0,77,12,12,0,method : Method,0.3130081300813008,0.17142857142857143,0.8
relation-classification,1,"Thus , the total number of tags is N t = 2 * 4 * | R | + 1 , where | R | is the size of the predefined relation set .",method,Method,0,78,13,13,0,method : Method,0.3170731707317073,0.18571428571428572,0.8666666666666667
relation-classification,1,is an example illustrating our tagging method .,method,Method,0,79,14,14,0,method : Method,0.32113821138211385,0.2,0.9333333333333333
relation-classification,1,"The input sentence contains two triplets : { United States , Country - President , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } , where "" Country - President "" and "" Company - Founder "" are the predefined relation types .",method,Method,0,80,15,15,0,method : Method,0.3252032520325203,0.21428571428571427,1.0
relation-classification,1,The Tagging Scheme,method,The Tagging Scheme,0,81,16,1,0,method : The Tagging Scheme,0.32926829268292684,0.22857142857142856,0.16666666666666666
relation-classification,1,"The words "" United "" , "" States "" , "" Trump "" , "" Apple "" , "" Inc "" , "" Steven "" , "" Paul "" and "" Jobs "" are all related to the final extracted results .",method,The Tagging Scheme,0,82,17,2,0,method : The Tagging Scheme,0.3333333333333333,0.24285714285714285,0.3333333333333333
relation-classification,1,Thus they are tagged based on our special tags .,method,The Tagging Scheme,0,83,18,3,0,method : The Tagging Scheme,0.33739837398373984,0.2571428571428571,0.5
relation-classification,1,"For example , the word of "" United "" is the first word of entity "" United States "" and is related to the relation "" Country - President "" , so its tag is "" B - CP - 1 "" .",method,The Tagging Scheme,0,84,19,4,0,method : The Tagging Scheme,0.34146341463414637,0.2714285714285714,0.6666666666666666
relation-classification,1,"The other entity "" Trump "" , which is corresponding to "" United States "" , is labeled as "" S - CP - 2 "" .",method,The Tagging Scheme,0,85,20,5,0,method : The Tagging Scheme,0.34552845528455284,0.2857142857142857,0.8333333333333334
relation-classification,1,"Besides , the other words irrelevant to the final result are labeled as "" O "" .",method,The Tagging Scheme,0,86,21,6,0,method : The Tagging Scheme,0.34959349593495936,0.3,1.0
relation-classification,1,From Tag Sequence To Extracted Results,method,From Tag Sequence To Extracted Results,0,87,22,1,0,method : From Tag Sequence To Extracted Results,0.35365853658536583,0.3142857142857143,0.1
relation-classification,1,"From the tag sequence in , we know that "" Trump "" and "" United States "" share the same relation type "" Country - President "" , "" Apple Inc "" and "" Steven Paul Jobs "" share the same relation type "" Company - Founder "" .",method,From Tag Sequence To Extracted Results,0,88,23,2,0,method : From Tag Sequence To Extracted Results,0.35772357723577236,0.32857142857142857,0.2
relation-classification,1,We combine entities with the same relation type into a triplet to get the final result .,method,From Tag Sequence To Extracted Results,0,89,24,3,0,method : From Tag Sequence To Extracted Results,0.3617886178861789,0.34285714285714286,0.3
relation-classification,1,"Accordingly , "" Trump "" and "" United States "" can be combined into a triplet whose relation type is "" Country - President "" .",method,From Tag Sequence To Extracted Results,0,90,25,4,0,method : From Tag Sequence To Extracted Results,0.36585365853658536,0.35714285714285715,0.4
relation-classification,1,"Because , the relation role of "" Trump "" is "" 2 "" and "" United States "" is "" 1 "" , the final result is { United States , Country - President , Trump } .",method,From Tag Sequence To Extracted Results,0,91,26,5,0,method : From Tag Sequence To Extracted Results,0.3699186991869919,0.37142857142857144,0.5
relation-classification,1,"The same applies to { Apple Inc , Company - Founder , Steven Paul Jobs } .",method,From Tag Sequence To Extracted Results,0,92,27,6,0,method : From Tag Sequence To Extracted Results,0.37398373983739835,0.38571428571428573,0.6
relation-classification,1,"Besides , if a sentence contains two or more triplets with the same relation type , we combine every two entities into a triplet based on the nearest principle .",method,From Tag Sequence To Extracted Results,0,93,28,7,0,method : From Tag Sequence To Extracted Results,0.3780487804878049,0.4,0.7
relation-classification,1,"For example , if the relation type "" Country - President "" in is "" Company - Founder "" , then there will be four entities in the given sentence with the same relation type .",method,From Tag Sequence To Extracted Results,0,94,29,8,0,method : From Tag Sequence To Extracted Results,0.3821138211382114,0.4142857142857143,0.8
relation-classification,1,"United States "" is closest to entity "" Trump "" and the "" Apple Inc "" is closest to "" Jobs "" , so the results will be { United States , Company - Founder , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } .",method,From Tag Sequence To Extracted Results,0,95,30,9,0,method : From Tag Sequence To Extracted Results,0.3861788617886179,0.42857142857142855,0.9
relation-classification,1,"In this paper , we only consider the situation where an entity belongs to a triplet , and we leave identification of overlapping relations for future work .",method,From Tag Sequence To Extracted Results,0,96,31,10,0,method : From Tag Sequence To Extracted Results,0.3902439024390244,0.44285714285714284,1.0
relation-classification,1,The End - to - end Model,method,The End-to-end Model,1,97,32,1,0,method : The End-to-end Model,0.3943089430894309,0.45714285714285713,0.02564102564102564
relation-classification,1,"In recent years , end - to - end model based on neural network is been widely used in sequence tagging task .",method,The End-to-end Model,0,98,33,2,0,method : The End-to-end Model,0.3983739837398374,0.4714285714285714,0.05128205128205128
relation-classification,1,"In this paper , we investigate an end - toend model to produce the tags sequence as shows .",method,The End-to-end Model,1,99,34,3,0,method : The End-to-end Model,0.4024390243902439,0.4857142857142857,0.07692307692307693
relation-classification,1,It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .,method,The End-to-end Model,1,100,35,4,0,method : The End-to-end Model,0.4065040650406504,0.5,0.10256410256410256
relation-classification,1,The biased loss can enhance the relevance of entity tags .,method,The End-to-end Model,1,101,36,5,0,method : The End-to-end Model,0.4105691056910569,0.5142857142857142,0.1282051282051282
relation-classification,1,The Bi - LSTM Encoding Layer .,method,The End-to-end Model,0,102,37,6,0,method : The End-to-end Model,0.4146341463414634,0.5285714285714286,0.15384615384615385
relation-classification,1,"In sequence tagging problems , the Bi - LSTM encoding layer has been shown the effectiveness to capture the semantic information of each word .",method,The End-to-end Model,0,103,38,7,0,method : The End-to-end Model,0.4186991869918699,0.5428571428571428,0.1794871794871795
relation-classification,1,"It contains forward lstm layer , backward lstm layer and the concatenate layer .",method,The End-to-end Model,0,104,39,8,0,method : The End-to-end Model,0.42276422764227645,0.5571428571428572,0.20512820512820512
relation-classification,1,The word embedding layer converts the word with 1 - hot representation to an embedding vector .,method,The End-to-end Model,0,105,40,9,0,method : The End-to-end Model,0.4268292682926829,0.5714285714285714,0.23076923076923078
relation-classification,1,"Hence , a sequence of words can be represented as : Gold standard annotation for an example sentence based on our tagging scheme , where "" CP "" is short for "" Country - President "" and "" CF "" is short for "" Company - Founder "" .",method,The End-to-end Model,0,106,41,10,0,method : The End-to-end Model,0.43089430894308944,0.5857142857142857,0.2564102564102564
relation-classification,1,corresponding to the t - th word in the sentence and n is the length of the given sentence .,method,The End-to-end Model,0,107,42,11,0,method : The End-to-end Model,0.4349593495934959,0.6,0.28205128205128205
relation-classification,1,"After word embedding layer , there are two parallel LSTM layers : forward LSTM layer and backward LSTM layer .",method,The End-to-end Model,0,108,43,12,0,method : The End-to-end Model,0.43902439024390244,0.6142857142857143,0.3076923076923077
relation-classification,1,"The LSTM architecture consists of a set of recurrently connected subnets , known as memory blocks .",method,The End-to-end Model,0,109,44,13,0,method : The End-to-end Model,0.44308943089430897,0.6285714285714286,0.3333333333333333
relation-classification,1,Each time - step is a LSTM memory block .,method,The End-to-end Model,0,110,45,14,0,method : The End-to-end Model,0.44715447154471544,0.6428571428571429,0.358974358974359
relation-classification,1,"The LSTM memory block in Bi - LSTM encoding layer is used to compute current hidden vector ht based on the previous hidden vector h t?1 , the previous cell vector c t?1 and the current input word embedding wt .",method,The End-to-end Model,0,111,46,15,0,method : The End-to-end Model,0.45121951219512196,0.6571428571428571,0.38461538461538464
relation-classification,1,"It s structure diagram is shown in , and detail operations are defined as follows :",method,The End-to-end Model,0,112,47,16,0,method : The End-to-end Model,0.45528455284552843,0.6714285714285714,0.41025641025641024
relation-classification,1,"where i , f and o are the input gate , forget gate and output gate respectively , b is the bias term , c is the cell memory , and W ( . ) are the parameters .",method,The End-to-end Model,0,113,48,17,0,method : The End-to-end Model,0.45934959349593496,0.6857142857142857,0.4358974358974359
relation-classification,1,"For each word wt , the forward LSTM layer will encode wt by considering the contextual information from word w 1 tow t , which is marked as ? ? ht .",method,The End-to-end Model,0,114,49,18,0,method : The End-to-end Model,0.4634146341463415,0.7,0.46153846153846156
relation-classification,1,"In the similar way , the backward LSTM layer will encode wt based on the contextual information from w n tow t , which is marked as ? ? ht .",method,The End-to-end Model,0,115,50,19,0,method : The End-to-end Model,0.46747967479674796,0.7142857142857143,0.48717948717948717
relation-classification,1,"Finally , we concatenate ? ? ht and ? ? ht to represent word t 's encoding information , denoted as",method,The End-to-end Model,0,116,51,20,0,method : The End-to-end Model,0.4715447154471545,0.7285714285714285,0.5128205128205128
relation-classification,1,"Finally , we concatenate ? ? ht and ? ? ht to represent word t 's encoding information , denoted as",method,The End-to-end Model,0,117,52,21,0,method : The End-to-end Model,0.47560975609756095,0.7428571428571429,0.5384615384615384
relation-classification,1,"Finally , we concatenate ? ? ht and ? ? ht to represent word t 's encoding information , denoted as",method,The End-to-end Model,0,118,53,22,0,method : The End-to-end Model,0.4796747967479675,0.7571428571428571,0.5641025641025641
relation-classification,1,The LSTM Decoding Layer .,method,The End-to-end Model,0,119,54,23,0,method : The End-to-end Model,0.483739837398374,0.7714285714285715,0.5897435897435898
relation-classification,1,We also adopt a LSTM structure to produce the tag sequence .,method,The End-to-end Model,0,120,55,24,0,method : The End-to-end Model,0.4878048780487805,0.7857142857142857,0.6153846153846154
relation-classification,1,"When detecting the tag of word wt , the inputs of decoding layer are : ht obtained from Bi - LSTM encoding layer , former predicted tag embedding T t?1 , former cell value c ( 2 ) t? 1 , and the former hidden vector in decoding layer h ( 2 ) t?1 .",method,The End-to-end Model,0,121,56,25,0,method : The End-to-end Model,0.491869918699187,0.8,0.6410256410256411
relation-classification,1,"The structure diagram of the memory block in LSTM dis shown in , and detail operations are defined as follows :",method,The End-to-end Model,0,122,57,26,0,method : The End-to-end Model,0.4959349593495935,0.8142857142857143,0.6666666666666666
relation-classification,1,The final softmax layer computes normalized entity tag probabilities based on the tag predicted vector T t :,method,The End-to-end Model,0,123,58,27,0,method : The End-to-end Model,0.5,0.8285714285714286,0.6923076923076923
relation-classification,1,"where W y is the softmax matrix , N t is the total number of tags .",method,The End-to-end Model,0,124,59,28,0,method : The End-to-end Model,0.5040650406504065,0.8428571428571429,0.717948717948718
relation-classification,1,"Because T is similar to tag embedding and LSTM is capable of learning long - term dependencies , the decoding manner can model tag interactions .",method,The End-to-end Model,0,125,60,29,0,method : The End-to-end Model,0.508130081300813,0.8571428571428571,0.7435897435897436
relation-classification,1,The Bias Objective Function .,method,The End-to-end Model,0,126,61,30,0,method : The End-to-end Model,0.5121951219512195,0.8714285714285714,0.7692307692307693
relation-classification,1,We train our model to maximize the log-likelihood of the data and the optimization method we used is RM - Sprop proposed by Hinton in .,method,The End-to-end Model,0,127,62,31,0,method : The End-to-end Model,0.516260162601626,0.8857142857142857,0.7948717948717948
relation-classification,1,The objective function can be defined as :,method,The End-to-end Model,0,128,63,32,0,method : The End-to-end Model,0.5203252032520326,0.9,0.8205128205128205
relation-classification,1,"where | D| is the size of training set , L j is the length of sentence x j , y ( j ) t is the label of word tin sentence x j and p ( j ) t is the normalized probabilities of tags which defined in Formula 15 .",method,The End-to-end Model,0,129,64,33,0,method : The End-to-end Model,0.524390243902439,0.9142857142857143,0.8461538461538461
relation-classification,1,"Besides , I ( O ) is a switching function to distinguish the loss of tag ' O ' and relational tags that can indicate the results .",method,The End-to-end Model,0,130,65,34,0,method : The End-to-end Model,0.5284552845528455,0.9285714285714286,0.8717948717948718
relation-classification,1,It is defined as follows :,method,The End-to-end Model,0,131,66,35,0,method : The End-to-end Model,0.532520325203252,0.9428571428571428,0.8974358974358975
relation-classification,1,is the bias weight .,method,The End-to-end Model,0,132,67,36,0,method : The End-to-end Model,0.5365853658536586,0.9571428571428572,0.9230769230769231
relation-classification,1,is the bias weight .,method,The End-to-end Model,0,133,68,37,0,method : The End-to-end Model,0.540650406504065,0.9714285714285714,0.9487179487179487
relation-classification,1,"The larger ? is , the greater influence of relational tags on the model .",method,The End-to-end Model,0,134,69,38,0,method : The End-to-end Model,0.5447154471544715,0.9857142857142858,0.9743589743589743
relation-classification,1,"The larger ? is , the greater influence of relational tags on the model .",method,The End-to-end Model,0,135,70,39,0,method : The End-to-end Model,0.5487804878048781,1.0,1.0
relation-classification,1,Experiments,experiment,Experiments,0,136,1,1,0,experiment : Experiments,0.5528455284552846,0.5,1.0
relation-classification,1,Experimental setting,experiment,Experimental setting,0,137,2,1,0,experiment : Experimental setting,0.556910569105691,1.0,1.0
relation-classification,1,Dataset,dataset,dataset,0,138,1,1,0,dataset : dataset,0.5609756097560976,0.16666666666666666,0.16666666666666666
relation-classification,1,"To evaluate the performance of our methods , we use the public dataset NYT 2 which is produced by distant supervision method .",dataset,dataset,0,139,2,2,0,dataset : dataset,0.5650406504065041,0.3333333333333333,0.3333333333333333
relation-classification,1,large amount of training data can be obtained by means of distant supervision methods without manually labeling .,dataset,dataset,0,140,3,3,0,dataset : dataset,0.5691056910569106,0.5,0.5
relation-classification,1,While the test set is manually labeled to ensure its quality .,dataset,dataset,0,141,4,4,0,dataset : dataset,0.573170731707317,0.6666666666666666,0.6666666666666666
relation-classification,1,"In total , the training data contains 353 k triplets , and the test set contains 3 , 880 triplets .",dataset,dataset,0,142,5,5,0,dataset : dataset,0.5772357723577236,0.8333333333333334,0.8333333333333334
relation-classification,1,"Besides , the size of relation set is 24 .",dataset,dataset,0,143,6,6,0,dataset : dataset,0.5813008130081301,1.0,1.0
relation-classification,1,Evaluation,evaluation,evaluation,0,144,1,1,0,evaluation : evaluation,0.5853658536585366,0.125,0.125
relation-classification,1,"We adopt standard Precision ( Prec ) , Recall ( Rec ) and F 1 score to evaluate the results .",evaluation,evaluation,0,145,2,2,0,evaluation : evaluation,0.5894308943089431,0.25,0.25
relation-classification,1,"Different from classical methods , our method can extract triplets without knowing the information of entity types .",evaluation,evaluation,0,146,3,3,0,evaluation : evaluation,0.5934959349593496,0.375,0.375
relation-classification,1,"In other words , we did not use the label of entity types to train the model , therefore we do not need to consider the entity types in the evaluation .",evaluation,evaluation,0,147,4,4,0,evaluation : evaluation,0.5975609756097561,0.5,0.5
relation-classification,1,triplet is regarded as correct when its relation type and the head offsets of two corresponding entities are both correct .,evaluation,evaluation,0,148,5,5,0,evaluation : evaluation,0.6016260162601627,0.625,0.625
relation-classification,1,"Besides , the groundtruth relation mentions are given and "" None "" label is excluded as did .",evaluation,evaluation,0,149,6,6,0,evaluation : evaluation,0.6056910569105691,0.75,0.75
relation-classification,1,We create a validation set by randomly sampling 10 % data from test set and use the remaining data as evaluation based on ) 's suggestion .,evaluation,evaluation,0,150,7,7,0,evaluation : evaluation,0.6097560975609756,0.875,0.875
relation-classification,1,We run 10 times for each experiment then report the average results and their standard deviation as shows .,evaluation,evaluation,0,151,8,8,0,evaluation : evaluation,0.6138211382113821,1.0,1.0
relation-classification,1,Hyperparameters,hyperparameters,hyperparameters,0,152,1,1,0,hyperparameters : hyperparameters,0.6178861788617886,0.0625,0.0625
relation-classification,1,Our model consists of a Bi - LSTM encoding layer and a LSTM decoding layer with bias objective function .,hyperparameters,hyperparameters,0,153,2,2,0,hyperparameters : hyperparameters,0.6219512195121951,0.125,0.125
relation-classification,1,The word embeddings used in the encoding part are initialed by running word2vec 3 on NYT training corpus .,hyperparameters,hyperparameters,1,154,3,3,0,hyperparameters : hyperparameters,0.6260162601626016,0.1875,0.1875
relation-classification,1,The dimension of the word embeddings is d = 300 .,hyperparameters,hyperparameters,1,155,4,4,0,hyperparameters : hyperparameters,0.6300813008130082,0.25,0.25
relation-classification,1,We regularize our network using dropout on embedding layer and the dropout ratio is 0.5 .,hyperparameters,hyperparameters,1,156,5,5,0,hyperparameters : hyperparameters,0.6341463414634146,0.3125,0.3125
relation-classification,1,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,hyperparameters,hyperparameters,1,157,6,6,0,hyperparameters : hyperparameters,0.6382113821138211,0.375,0.375
relation-classification,1,The bias parameter ? corresponding to the results in is 10 .,hyperparameters,hyperparameters,1,158,7,7,0,hyperparameters : hyperparameters,0.6422764227642277,0.4375,0.4375
relation-classification,1,The bias parameter ? corresponding to the results in is 10 .,hyperparameters,hyperparameters,1,159,8,8,0,hyperparameters : hyperparameters,0.6463414634146342,0.5,0.5
relation-classification,1,2,hyperparameters,hyperparameters,0,160,9,9,0,hyperparameters : hyperparameters,0.6504065040650406,0.5625,0.5625
relation-classification,1,The dataset can be downloaded at : https://github.com/shanzhenren/CoType.,hyperparameters,hyperparameters,0,161,10,10,0,hyperparameters : hyperparameters,0.6544715447154471,0.625,0.625
relation-classification,1,There are three data sets in the public resource and we only use the NYT dataset .,hyperparameters,hyperparameters,0,162,11,11,0,hyperparameters : hyperparameters,0.6585365853658537,0.6875,0.6875
relation-classification,1,Because more than 50 % of the data in BioInfer has overlapping relations which is beyond the scope of this paper .,hyperparameters,hyperparameters,0,163,12,12,0,hyperparameters : hyperparameters,0.6626016260162602,0.75,0.75
relation-classification,1,"As for dataset Wiki - KBP , the number of relation type in the test set is more than that of the train set , which is also not suitable for a supervised training method .",hyperparameters,hyperparameters,0,164,13,13,0,hyperparameters : hyperparameters,0.6666666666666666,0.8125,0.8125
relation-classification,1,Details of the data can be found in is the pipelined methods and the second part ( row 4 to 6 ) is the jointly extracting methods .,hyperparameters,hyperparameters,0,165,14,14,0,hyperparameters : hyperparameters,0.6707317073170732,0.875,0.875
relation-classification,1,Our tagging methods are shown in part three ( row 7 to 9 ) .,hyperparameters,hyperparameters,0,166,15,15,0,hyperparameters : hyperparameters,0.6747967479674797,0.9375,0.9375
relation-classification,1,"In this part , we not only report the results of precision , recall and F1 , we also compute their standard deviation .",hyperparameters,hyperparameters,0,167,16,16,0,hyperparameters : hyperparameters,0.6788617886178862,1.0,1.0
relation-classification,1,Baselines,baseline,baseline,0,168,1,1,0,baseline : baseline,0.6829268292682927,0.08333333333333333,0.08333333333333333
relation-classification,1,"We compare our method with several classical triplet extraction methods , which can be divided into the following categories : the pipelined methods , the jointly extracting methods and the end - to - end methods based our tagging scheme .",baseline,baseline,1,169,2,2,0,baseline : baseline,0.6869918699186992,0.16666666666666666,0.16666666666666666
relation-classification,1,"For the pipelined methods , we follow ) 's settings :",baseline,baseline,1,170,3,3,0,baseline : baseline,0.6910569105691057,0.25,0.25
relation-classification,1,The NER results are obtained by CoType then several classical relation classification methods are applied to detect the relations .,baseline,baseline,0,171,4,4,0,baseline : baseline,0.6951219512195121,0.3333333333333333,0.3333333333333333
relation-classification,1,These methods are :,baseline,baseline,0,172,5,5,0,baseline : baseline,0.6991869918699187,0.4166666666666667,0.4166666666666667
relation-classification,1,"1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ; ( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .",baseline,baseline,1,173,6,6,0,baseline : baseline,0.7032520325203252,0.5,0.5
relation-classification,1,"1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ; ( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .",baseline,baseline,1,174,7,7,0,baseline : baseline,0.7073170731707317,0.5833333333333334,0.5833333333333334
relation-classification,1,"The jointly extracting methods used in this paper are listed as follows : ( 4 ) DS - Joint ) is a supervised method , which jointly extracts entities and relations using structured perceptron on human - annotated dataset ; ( 5 ) MultiR is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data ; ( 6 ) CoType ) is a domain independent framework by jointly embedding entity mentions , relation mentions , text features and type labels into meaningful representations .",baseline,baseline,1,175,8,8,0,baseline : baseline,0.7113821138211383,0.6666666666666666,0.6666666666666666
relation-classification,1,"In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .",baseline,baseline,1,176,9,9,0,baseline : baseline,0.7154471544715447,0.75,0.75
relation-classification,1,LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .,baseline,baseline,1,177,10,10,0,baseline : baseline,0.7195121951219512,0.8333333333333334,0.8333333333333334
relation-classification,1,"Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .",baseline,baseline,1,178,11,11,0,baseline : baseline,0.7235772357723578,0.9166666666666666,0.9166666666666666
relation-classification,1,They are used for the first time to jointly extract entities and relations based on our tagging scheme .,baseline,baseline,0,179,12,12,0,baseline : baseline,0.7276422764227642,1.0,1.0
relation-classification,1,Experimental Results,experiment,Experimental Results,0,180,1,1,0,experiment : Experimental Results,0.7317073170731707,0.05555555555555555,0.05555555555555555
relation-classification,1,We report the results of different methods as shown in .,experiment,Experimental Results,0,181,2,2,0,experiment : Experimental Results,0.7357723577235772,0.1111111111111111,0.1111111111111111
relation-classification,1,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .",experiment,Experimental Results,1,182,3,3,0,experiment : Experimental Results,0.7398373983739838,0.16666666666666666,0.16666666666666666
relation-classification,1,It shows the effectiveness of our proposed method .,experiment,Experimental Results,0,183,4,4,0,experiment : Experimental Results,0.7439024390243902,0.2222222222222222,0.2222222222222222
relation-classification,1,"Furthermore , from , we also can see that the jointly extracting methods are better than pipelined methods , and the tagging methods are better than most of the jointly extracting methods .",experiment,Experimental Results,0,184,5,5,0,experiment : Experimental Results,0.7479674796747967,0.2777777777777778,0.2777777777777778
relation-classification,1,It also validates the validity of our tagging scheme for the task of jointly extracting entities and relations .,experiment,Experimental Results,0,185,6,6,0,experiment : Experimental Results,0.7520325203252033,0.3333333333333333,0.3333333333333333
relation-classification,1,"When compared with the traditional methods , the precisions of the end - to - end models are significantly improved .",experiment,Experimental Results,1,186,7,7,0,experiment : Experimental Results,0.7560975609756098,0.3888888888888889,0.3888888888888889
relation-classification,1,But only LSTM - LSTM - Bias can be better to balance the precision and recall .,experiment,Experimental Results,0,187,8,8,0,experiment : Experimental Results,0.7601626016260162,0.4444444444444444,0.4444444444444444
relation-classification,1,The reason maybe that these end - to - end models all use a Bi - LSTM encoding input sentence and different neural networks to decode the results .,experiment,Experimental Results,0,188,9,9,0,experiment : Experimental Results,0.7642276422764228,0.5,0.5
relation-classification,1,The methods based on neural networks can well fit the data .,experiment,Experimental Results,0,189,10,10,0,experiment : Experimental Results,0.7682926829268293,0.5555555555555556,0.5555555555555556
relation-classification,1,"Therefore , they can learn the common features of the training set well and may lead to the lower expansibility .",experiment,Experimental Results,0,190,11,11,0,experiment : Experimental Results,0.7723577235772358,0.6111111111111112,0.6111111111111112
relation-classification,1,We also find that the LSTM - LSTM model is better than LSTM - CRF model based on our tagging scheme .,experiment,Experimental Results,1,191,12,12,0,experiment : Experimental Results,0.7764227642276422,0.6666666666666666,0.6666666666666666
relation-classification,1,"Because , LSTM is capable of learning long - term dependencies and CRF is good at capturing the joint probability of the entire sequence of labels .",experiment,Experimental Results,0,192,13,13,0,experiment : Experimental Results,0.7804878048780488,0.7222222222222222,0.7222222222222222
relation-classification,1,The related tags may have along distance from each other .,experiment,Experimental Results,0,193,14,14,0,experiment : Experimental Results,0.7845528455284553,0.7777777777777778,0.7777777777777778
relation-classification,1,"Hence , LSTM decoding manner is a little better than CRF .",experiment,Experimental Results,0,194,15,15,0,experiment : Experimental Results,0.7886178861788617,0.8333333333333334,0.8333333333333334
relation-classification,1,"Hence , LSTM decoding manner is a little better than CRF .",experiment,Experimental Results,0,195,16,16,0,experiment : Experimental Results,0.7926829268292683,0.8888888888888888,0.8888888888888888
relation-classification,1,LSTM - LSTM - Bias adds a bias weight to enhance the effect of entity tags and weaken the effect of invalid tag .,experiment,Experimental Results,0,196,17,17,0,experiment : Experimental Results,0.7967479674796748,0.9444444444444444,0.9444444444444444
relation-classification,1,"Therefore , in this tagging scheme , our method can be better than the common LSTM - decoding methods .",experiment,Experimental Results,0,197,18,18,0,experiment : Experimental Results,0.8008130081300813,1.0,1.0
relation-classification,1,Analysis and Discussion,analysis,Analysis and Discussion,0,198,1,1,0,analysis : Analysis and Discussion,0.8048780487804879,0.023809523809523808,1.0
relation-classification,1,Error Analysis,analysis,Error Analysis,0,199,2,1,0,analysis : Error Analysis,0.8089430894308943,0.047619047619047616,0.07142857142857142
relation-classification,1,"In this paper , we focus on extracting triplets composed of two entities and a relation .",analysis,Error Analysis,0,200,3,2,0,analysis : Error Analysis,0.8130081300813008,0.07142857142857142,0.14285714285714285
relation-classification,1,has shown the predict results of the task .,analysis,Error Analysis,0,201,4,3,0,analysis : Error Analysis,0.8170731707317073,0.09523809523809523,0.21428571428571427
relation-classification,1,It treats an triplet is correct only when the relation type and the head offsets of two corresponding entities are both correct .,analysis,Error Analysis,0,202,5,4,0,analysis : Error Analysis,0.8211382113821138,0.11904761904761904,0.2857142857142857
relation-classification,1,"In order to find out the factors that affect the results of end - to - end models , we analyze the performance on predicting each element in the triplet as shows .",analysis,Error Analysis,0,203,6,5,0,analysis : Error Analysis,0.8252032520325203,0.14285714285714285,0.35714285714285715
relation-classification,1,"E1 and E2 represent the performance on predicting each entity , respectively .",analysis,Error Analysis,0,204,7,6,0,analysis : Error Analysis,0.8292682926829268,0.16666666666666666,0.42857142857142855
relation-classification,1,"If the head offset of the first entity is correct , then the instance of E1 is correct , the same to E2 .",analysis,Error Analysis,0,205,8,7,0,analysis : Error Analysis,0.8333333333333334,0.19047619047619047,0.5
relation-classification,1,"Regardless of relation type , if the head offsets of two corresponding entities are both correct , the instance of ( E1 , E2 ) is correct .",analysis,Error Analysis,0,206,9,8,0,analysis : Error Analysis,0.8373983739837398,0.21428571428571427,0.5714285714285714
relation-classification,1,"As shown in , ( E1 , E2 ) has higher precision when compared with E1 and E2 .",analysis,Error Analysis,0,207,10,9,0,analysis : Error Analysis,0.8414634146341463,0.23809523809523808,0.6428571428571429
relation-classification,1,But its recall result is lower than E1 and E2 .,analysis,Error Analysis,0,208,11,10,0,analysis : Error Analysis,0.8455284552845529,0.2619047619047619,0.7142857142857143
relation-classification,1,It means that some of the predicted entities do not form a pair .,analysis,Error Analysis,0,209,12,11,0,analysis : Error Analysis,0.8495934959349594,0.2857142857142857,0.7857142857142857
relation-classification,1,"They only obtain E1 and do not find its corresponding E2 , or obtain E2 and do not find its corresponding E1 .",analysis,Error Analysis,0,210,13,12,0,analysis : Error Analysis,0.8536585365853658,0.30952380952380953,0.8571428571428571
relation-classification,1,"Thus it leads to the prediction of more single E and less ( E1 , E2 ) pairs .",analysis,Error Analysis,0,211,14,13,0,analysis : Error Analysis,0.8577235772357723,0.3333333333333333,0.9285714285714286
relation-classification,1,"Therefore , entity pair ( E1 , E2 ) has higher precision and lower recall than single E. Besides , the predicted results of ( E1 , E2 ) in have about 3 % improvement when compared predicted results in Table 1 , which means that 3 % of the test data is pre-dicted to be wrong because the relation type is predicted to be wrong .",analysis,Error Analysis,0,212,15,14,0,analysis : Error Analysis,0.8617886178861789,0.35714285714285715,1.0
relation-classification,1,Analysis of Biased Loss,analysis,Analysis of Biased Loss,0,213,16,1,0,analysis : Analysis of Biased Loss,0.8658536585365854,0.38095238095238093,0.16666666666666666
relation-classification,1,"Different from LSTM - CRF and LSTM - LSTM , our approach is biased towards relational labels to enhance links between entities .",analysis,Analysis of Biased Loss,0,214,17,2,0,analysis : Analysis of Biased Loss,0.8699186991869918,0.40476190476190477,0.3333333333333333
relation-classification,1,"In order to further analyze the effect of the bias objective function , we visualize the ratio of predicted single entities for each end - to - end method as .",analysis,Analysis of Biased Loss,0,215,18,3,0,analysis : Analysis of Biased Loss,0.8739837398373984,0.42857142857142855,0.5
relation-classification,1,The single entities refer to those who can not find their corresponding entities .,analysis,Analysis of Biased Loss,0,216,19,4,0,analysis : Analysis of Biased Loss,0.8780487804878049,0.4523809523809524,0.6666666666666666
relation-classification,1,"shows whether it is E1 or E2 , our method can get a relatively low ratio on the single entities .",analysis,Analysis of Biased Loss,0,217,20,5,0,analysis : Analysis of Biased Loss,0.8821138211382114,0.47619047619047616,0.8333333333333334
relation-classification,1,It means that our method can effectively associate two entities when compared LSTM - CRF and LSTM - LSTM which pay little attention to the relational tags .,analysis,Analysis of Biased Loss,0,218,21,6,0,analysis : Analysis of Biased Loss,0.8861788617886179,0.5,1.0
relation-classification,1,Single E1,analysis,Single E1,0,219,22,1,0,analysis : Single E1,0.8902439024390244,0.5238095238095238,0.14285714285714285
relation-classification,1,"Single Besides , we also change the Bias Parameter ? from 1 to 20 , and the predicted results are shown in .",analysis,Single E1,0,220,23,2,0,analysis : Single E1,0.8943089430894309,0.5476190476190477,0.2857142857142857
relation-classification,1,"Single Besides , we also change the Bias Parameter ? from 1 to 20 , and the predicted results are shown in .",analysis,Single E1,0,221,24,3,0,analysis : Single E1,0.8983739837398373,0.5714285714285714,0.42857142857142855
relation-classification,1,"If ? is too large , it will affect the accuracy of prediction and if ? is too small , the recall will decline .",analysis,Single E1,0,222,25,4,0,analysis : Single E1,0.9024390243902439,0.5952380952380952,0.5714285714285714
relation-classification,1,"If ? is too large , it will affect the accuracy of prediction and if ? is too small , the recall will decline .",analysis,Single E1,0,223,26,5,0,analysis : Single E1,0.9065040650406504,0.6190476190476191,0.7142857142857143
relation-classification,1,"If ? is too large , it will affect the accuracy of prediction and if ? is too small , the recall will decline .",analysis,Single E1,0,224,27,6,0,analysis : Single E1,0.9105691056910569,0.6428571428571429,0.8571428571428571
relation-classification,1,"When ? = 10 , LSTM - LSTM - Bias can balance the precision and recall , and can achieve the best F 1 scores .",analysis,Single E1,0,225,28,7,0,analysis : Single E1,0.9146341463414634,0.6666666666666666,1.0
relation-classification,1,Case Study,analysis,Case Study,0,226,29,1,0,analysis : Case Study,0.9186991869918699,0.6904761904761905,0.07142857142857142
relation-classification,1,"In this section , we observe the prediction results of end - to - end methods , and then select several representative examples to illustrate the advantages and dis advantages of the methods as shows .",analysis,Case Study,0,227,30,2,0,analysis : Case Study,0.9227642276422764,0.7142857142857143,0.14285714285714285
relation-classification,1,"Each example contains three row , the first row is the gold standard , the second and the third rows are the extracted results of model LSTM - LSTM and LSTM - LSTM - Bias respectively .",analysis,Case Study,0,228,31,3,0,analysis : Case Study,0.926829268292683,0.7380952380952381,0.21428571428571427
relation-classification,1,"S1 represents the situation that the distance between the two interrelated entities is faraway from each other , which is more difficult to detect their relationships .",analysis,Case Study,0,229,32,4,0,analysis : Case Study,0.9308943089430894,0.7619047619047619,0.2857142857142857
relation-classification,1,"When compared with LSTM - LSTM , LSTM - LSTM - Bias uses a bias objective function which enhance the relevance between entities .",analysis,Case Study,0,230,33,5,0,analysis : Case Study,0.9349593495934959,0.7857142857142857,0.35714285714285715
relation-classification,1,"Therefore , in this example , LSTM - LSTM - Bias can extract two related entities , while LSTM - LSTM can only extract one entity of "" Florida "" and can not detect entity "" Panama City Beach "" .",analysis,Case Study,0,231,34,6,0,analysis : Case Study,0.9390243902439024,0.8095238095238095,0.42857142857142855
relation-classification,1,S2 is a negative example that shows these methods may mistakenly predict one of the entity .,analysis,Case Study,0,232,35,7,0,analysis : Case Study,0.943089430894309,0.8333333333333334,0.5
relation-classification,1,There are no indicative words between entities Nuremberg and Germany .,analysis,Case Study,0,233,36,8,0,analysis : Case Study,0.9471544715447154,0.8571428571428571,0.5714285714285714
relation-classification,1,"Besides , the patten "" a * of * "" between Germany and M iddle Ages maybe easy to mislead the models that there exists a relation of "" Contains "" between them .",analysis,Case Study,0,234,37,9,0,analysis : Case Study,0.9512195121951219,0.8809523809523809,0.6428571428571429
relation-classification,1,The problem can be solved by adding some samples of this kind of expression patterns to the training data .,analysis,Case Study,0,235,38,10,0,analysis : Case Study,0.9552845528455285,0.9047619047619048,0.7142857142857143
relation-classification,1,"S3 is a case that models can predict the entities ' head offset right , but the relational role is wrong .",analysis,Case Study,0,236,39,11,0,analysis : Case Study,0.959349593495935,0.9285714285714286,0.7857142857142857
relation-classification,1,"LSTM - LSTM treats both "" Stephen A. Schwarzman "" and "" Blackstone Group "" as entity E1 , and can not find its corresponding E2 .",analysis,Case Study,0,237,40,12,0,analysis : Case Study,0.9634146341463414,0.9523809523809523,0.8571428571428571
relation-classification,1,"Although , LSTM - LSMT - Bias can find the entities pair ( E1 , E2 ) , it reverses the roles of "" Stephen A. Schwarzman "" and "" Blackstone Group "" .",analysis,Case Study,0,238,41,13,0,analysis : Case Study,0.967479674796748,0.9761904761904762,0.9285714285714286
relation-classification,1,"It shows that LSTM - LSTM - Bias is able to better on pre-dicting entities pair , but it remains to be improved in distinguishing the relationship between the two entities .",analysis,Case Study,0,239,42,14,0,analysis : Case Study,0.9715447154471545,1.0,1.0
relation-classification,1,Conclusion,conclusion,Conclusion,0,240,1,1,0,conclusion : Conclusion,0.975609756097561,0.14285714285714285,0.14285714285714285
relation-classification,1,"In this paper , we propose a novel tagging scheme and investigate the end - to - end models to jointly extract entities and relations .",conclusion,Conclusion,0,241,2,2,0,conclusion : Conclusion,0.9796747967479674,0.2857142857142857,0.2857142857142857
relation-classification,1,The experimental results show the effectiveness of our proposed method .,conclusion,Conclusion,0,242,3,3,0,conclusion : Conclusion,0.983739837398374,0.42857142857142855,0.42857142857142855
relation-classification,1,But it still has shortcoming on the identification of the overlapping relations .,conclusion,Conclusion,0,243,4,4,0,conclusion : Conclusion,0.9878048780487805,0.5714285714285714,0.5714285714285714
relation-classification,1,"In the future work , we will replace the softmax function in the output layer with multiple classifier , so that a word can has multiple tags .",conclusion,Conclusion,0,244,5,5,0,conclusion : Conclusion,0.991869918699187,0.7142857142857143,0.7142857142857143
relation-classification,1,"In this way , a word can appear in multiple triplet results , which can solve the problem of overlapping relations .",conclusion,Conclusion,0,245,6,6,0,conclusion : Conclusion,0.9959349593495935,0.8571428571428571,0.8571428571428571
relation-classification,1,"Although , our model can enhance the effect of entity tags , the association between two corresponding entities still requires refinement in next works .",conclusion,Conclusion,0,246,7,7,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,2,Joint entity recognition and relation extraction as a multi-head selection problem,title,title,1,2,1,1,0,title : title,0.006779661016949152,1.0,1.0
relation-classification,2,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.010169491525423728,0.125,0.125
relation-classification,2,State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.013559322033898305,0.25,0.25
relation-classification,2,"Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01694915254237288,0.375,0.375
relation-classification,2,"However , these features are not always accurate for various languages and contexts .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.020338983050847456,0.5,0.5
relation-classification,2,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.023728813559322035,0.625,0.625
relation-classification,2,"Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02711864406779661,0.75,0.75
relation-classification,2,"We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.030508474576271188,0.875,0.875
relation-classification,2,"Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.03389830508474576,1.0,1.0
relation-classification,2,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.03728813559322034,0.03125,0.03125
relation-classification,2,The goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts .,introduction,introduction,0,12,2,2,0,introduction : introduction,0.04067796610169491,0.0625,0.0625
relation-classification,2,It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.04406779661016949,0.09375,0.09375
relation-classification,2,"The problem is traditionally approached as two separate subtasks , namely ( i ) named entity recognition ( NER ) and ( ii ) relation extraction ( RE ) , in a pipeline setting .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.04745762711864407,0.125,0.125
relation-classification,2,"The main limitations of the pipeline models are : ( i ) error propagation between the components ( i.e. , NER and RE ) and ( ii ) possible useful information from the one task is not exploited by the other ( e.g. , identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities , i.e. , PER , ORG and vice versa ) .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.05084745762711865,0.15625,0.15625
relation-classification,2,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",introduction,introduction,1,16,6,6,0,introduction : introduction,0.05423728813559322,0.1875,0.1875
relation-classification,2,The previous joint models heavily rely on hand - crafted features .,introduction,introduction,0,17,7,7,0,introduction : introduction,0.0576271186440678,0.21875,0.21875
relation-classification,2,"Recent advances in neural networks alleviate the issue of manual feature engineering , but some of them still depend on NLP tools ( e.g. , POS taggers , dependency parsers ) .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.061016949152542375,0.25,0.25
relation-classification,2,propose a Recurrent Neural Network ( RNN ) - based joint model that uses a bidirectional sequential LSTM ( Long Short Term Memory ) to model the entities and a tree - LSTM that takes into account dependency tree information to model the relations between the entities .,introduction,introduction,0,19,9,9,0,introduction : introduction,0.06440677966101695,0.28125,0.28125
relation-classification,2,The dependency information is extracted using an external dependency parser .,introduction,introduction,0,20,10,10,0,introduction : introduction,0.06779661016949153,0.3125,0.3125
relation-classification,2,"Similarly , in the work of for entity and relation extraction from biomedical text , a model which also uses tree - LSTMs is applied to extract dependency information .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.0711864406779661,0.34375,0.34375
relation-classification,2,"propose a method that relies on RNNs but uses a lot of hand - crafted features and additional NLP tools to extract features such as POS - tags , etc. replicate the context around the entities with Convolutional Neural Networks ( CNNs ) .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.07457627118644068,0.375,0.375
relation-classification,2,"Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.07796610169491526,0.40625,0.40625
relation-classification,2,This means that relations of other pairs of entities in the same sentence - which could be helpful in deciding on the relation type for a particular pair - are not taken into account .,introduction,introduction,0,24,14,14,0,introduction : introduction,0.08135593220338982,0.4375,0.4375
relation-classification,2,"propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.0847457627118644,0.46875,0.46875
relation-classification,2,introduce a quadratic scoring layer to model the two tasks simultaneously .,introduction,introduction,0,26,16,16,0,introduction : introduction,0.08813559322033898,0.5,0.5
relation-classification,2,"The limitation of this approach is that only a single relation can be assigned to a token , while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.09152542372881356,0.53125,0.53125
relation-classification,2,"In this work , we focus on a new general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .",introduction,introduction,1,28,18,18,0,introduction : introduction,0.09491525423728814,0.5625,0.5625
relation-classification,2,"Our model achieves state - of - the - art performance in a number of different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) without relying on any manually engineered features nor additional NLP tools .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.09830508474576272,0.59375,0.59375
relation-classification,2,"In summary , our proposed model ( which will be detailed next in Section 3 ) solves several shortcomings that we identified in related works ( Section 2 ) for joint entity recognition and relation extraction : ( i ) our model does not rely on external NLP tools nor hand - crafted features , ( ii ) entities and relations within the same text fragment ( typically a sentence ) are extracted simultaneously , where ( iii ) an entity can be involved in multiple relations at once .",introduction,introduction,0,30,20,20,0,introduction : introduction,0.1016949152542373,0.625,0.625
relation-classification,2,"Specifically , the model of depends on dependency parsers , which perform particularly well on specific languages ( i.e. , English ) and contexts ( i.e. , news ) .",introduction,introduction,0,31,21,21,0,introduction : introduction,0.10508474576271186,0.65625,0.65625
relation-classification,2,"Yet , our ambition is to develop a model that generalizes well in various setups , therefore using only automatically extracted features thatare learned during training .",introduction,introduction,0,32,22,22,0,introduction : introduction,0.10847457627118644,0.6875,0.6875
relation-classification,2,"For instance , and use exactly the same model in different contexts , i.e. , news ( ACE04 ) and biomedical data ( ADE ) , respectively .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.11186440677966102,0.71875,0.71875
relation-classification,2,"Comparing our results to the ADE dataset , we obtain a 1.8 % improvement on the NER task and ? 3 % on the RE task .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.1152542372881356,0.75,0.75
relation-classification,2,"On the other hand , our model performs within a reasonable margin ( ? 0.6 % in the NER task and ? 1 % on the RE task ) on the ACE04 dataset without the use of pre-calculated features .",introduction,introduction,0,35,25,25,0,introduction : introduction,0.11864406779661017,0.78125,0.78125
relation-classification,2,This shows that the model of strongly relies on the features extracted by the dependency parsers and can not generalize well into different contexts where dependency parser features are weak .,introduction,introduction,0,36,26,26,0,introduction : introduction,0.12203389830508475,0.8125,0.8125
relation-classification,2,"Comparing to , we train our model by modeling all the entities and the relations of the sentence at once .",introduction,introduction,0,37,27,27,0,introduction : introduction,0.12542372881355932,0.84375,0.84375
relation-classification,2,This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time .,introduction,introduction,0,38,28,28,0,introduction : introduction,0.1288135593220339,0.875,0.875
relation-classification,2,"Finally , we solve the underlying problem of the models proposed by and , who essentially assume classes ( i.e. , relations ) to be mutually exclusive : we solve this by phrasing the relation extraction component as a multi-label prediction problem .",introduction,introduction,0,39,29,29,0,introduction : introduction,0.13220338983050847,0.90625,0.90625
relation-classification,2,"To demonstrate the effectiveness of the proposed method , we conduct the largest experimental evaluation to date ( to the best of our knowledge ) in jointly performing both entity recognition and relation extraction ( see Section 4 and Section 5 ) , using different datasets from various domains ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",introduction,introduction,0,40,30,30,0,introduction : introduction,0.13559322033898305,0.9375,0.9375
relation-classification,2,"Specifically , we apply our method to four datasets , namely ACE04 ( news ) , Adverse Drug Events ( ADE ) , Dutch Real Estate Classifieds ( DREC ) and CoNLL'04 ( news ) .",introduction,introduction,0,41,31,31,0,introduction : introduction,0.13898305084745763,0.96875,0.96875
relation-classification,2,"Our method outperforms all state - of - the - art methods that do not rely on any additional features or tools , while performance is very close ( or even better in the biomedical dataset ) compared to methods that do exploit hand - engineered features or NLP tools .",introduction,introduction,0,42,32,32,0,introduction : introduction,0.1423728813559322,1.0,1.0
relation-classification,2,Related work,related work,Related work,0,43,1,1,0,related work : Related work,0.14576271186440679,0.2,0.2
relation-classification,2,The tasks of entity recognition and relation extraction can be applied either one by one in a pipeline setting or in a joint model .,related work,Related work,0,44,2,2,0,related work : Related work,0.14915254237288136,0.4,0.4
relation-classification,2,"In this section , we present related work for each task ( i.e. , named entity recognition and relation extraction ) as well as prior work into joint entity and relation extraction .",related work,Related work,0,45,3,3,0,related work : Related work,0.15254237288135594,0.6,0.6
relation-classification,2,1,related work,Related work,0,46,4,4,0,related work : Related work,0.15593220338983052,0.8,0.8
relation-classification,2,"Note that another difference is that we use a CRF layer for the NER part , while Katiyar & Cardie ( 2017 ) uses a softmax and uses a quadratic scoring layer ; see further , when we discuss performance comparison results in Section 5 .",related work,Related work,0,47,5,5,0,related work : Related work,0.15932203389830507,1.0,1.0
relation-classification,2,Named entity recognition,system description,Named entity recognition,0,48,1,1,0,system description : Named entity recognition,0.16271186440677965,0.017241379310344827,0.029411764705882353
relation-classification,2,"We formulate the entity identification task as a sequence labeling problem , similar to previous work on joint learning models and named entity recognition using the BIO ( Beginning , Inside , Outside ) encoding scheme .",system description,Named entity recognition,0,49,2,2,0,system description : Named entity recognition,0.16610169491525423,0.034482758620689655,0.058823529411764705
relation-classification,2,Each entity consists of multiple sequential tokens within the sentence and we should assign a tag for every token in the sentence .,system description,Named entity recognition,0,50,3,3,0,system description : Named entity recognition,0.1694915254237288,0.05172413793103448,0.08823529411764706
relation-classification,2,"That way we are able to identify the entity arguments ( start and end position ) and its type ( e.g. , ORG ) .",system description,Named entity recognition,0,51,4,4,0,system description : Named entity recognition,0.17288135593220338,0.06896551724137931,0.11764705882352941
relation-classification,2,"To do so , we assign the B - type ( beginning ) to the first token of the entity , the I - type ( inside ) to every other token within the entity and the O tag ( outside ) if a token is not part of an entity .",system description,Named entity recognition,0,52,5,5,0,system description : Named entity recognition,0.17627118644067796,0.08620689655172414,0.14705882352941177
relation-classification,2,shows an example of the BIO encoding tags assigned to the tokens of the sentence .,system description,Named entity recognition,0,53,6,6,0,system description : Named entity recognition,0.17966101694915254,0.10344827586206896,0.17647058823529413
relation-classification,2,"In the CRF layer , one can observe that we assign the B - ORG and I - ORG tags to indicate the beginning and the inside tokens of the entity "" Disease Control Center "" , respectively .",system description,Named entity recognition,0,54,7,7,0,system description : Named entity recognition,0.18305084745762712,0.1206896551724138,0.20588235294117646
relation-classification,2,"On top of the BiLSTM layer , we employ either a softmax or a CRF layer to calculate the most probable entity tag for each token .",system description,Named entity recognition,0,55,8,8,0,system description : Named entity recognition,0.1864406779661017,0.13793103448275862,0.23529411764705882
relation-classification,2,We calculate the score of each token w i for each entity tag :,system description,Named entity recognition,0,56,9,9,0,system description : Named entity recognition,0.18983050847457628,0.15517241379310345,0.2647058823529412
relation-classification,2,"where the superscript ( e ) is used for the notation of the NER task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) , with d as the hidden size of the LSTM , p the number of NER tags ( e.g. , B - ORG ) and",system description,Named entity recognition,0,57,10,10,0,system description : Named entity recognition,0.19322033898305085,0.1724137931034483,0.29411764705882354
relation-classification,2,"where the superscript ( e ) is used for the notation of the NER task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) , with d as the hidden size of the LSTM , p the number of NER tags ( e.g. , B - ORG ) and",system description,Named entity recognition,0,58,11,11,0,system description : Named entity recognition,0.19661016949152543,0.1896551724137931,0.3235294117647059
relation-classification,2,the layer width .,system description,Named entity recognition,0,59,12,12,0,system description : Named entity recognition,0.2,0.20689655172413793,0.35294117647058826
relation-classification,2,We calculate the probabilities of all the candidate tags for a give n,system description,Named entity recognition,0,60,13,13,0,system description : Named entity recognition,0.2033898305084746,0.22413793103448276,0.38235294117647056
relation-classification,2,"we employ the softmax approach only for the entity classification ( EC ) task ( which is similar to NER ) where we need to predict only the entity types ( e.g. , PER ) for each token assuming boundaries are given .",system description,Named entity recognition,0,61,14,14,0,system description : Named entity recognition,0.20677966101694914,0.2413793103448276,0.4117647058823529
relation-classification,2,The CRF approach is used for the NER task which includes both entity type and boundaries recognition .,system description,Named entity recognition,0,62,15,15,0,system description : Named entity recognition,0.21016949152542372,0.25862068965517243,0.4411764705882353
relation-classification,2,"In the softmax approach , we assign entity types to tokens in a greedy way at prediction time ( i.e. , the selected tag is just the highest scoring tag over all possible set of tags ) .",system description,Named entity recognition,0,63,16,16,0,system description : Named entity recognition,0.2135593220338983,0.27586206896551724,0.47058823529411764
relation-classification,2,"Although assuming an independent tag distribution is beneficial for entity classification tasks ( e.g. , POS tagging ) , this is not the case when there are strong de-pendencies between the tags .",system description,Named entity recognition,0,64,17,17,0,system description : Named entity recognition,0.21694915254237288,0.29310344827586204,0.5
relation-classification,2,"Specifically , in NER , the BIO tagging scheme forces several restrictions ( e.g. , B - LOC can not be followed by I - PER ) .",system description,Named entity recognition,0,65,18,18,0,system description : Named entity recognition,0.22033898305084745,0.3103448275862069,0.5294117647058824
relation-classification,2,"The softmax method allows local decisions ( i.e. , for the tag of each token w i ) even though the BiLSTM captures information about the neighboring words .",system description,Named entity recognition,0,66,19,19,0,system description : Named entity recognition,0.22372881355932203,0.3275862068965517,0.5588235294117647
relation-classification,2,"Still , the neighboring tags are not taken into account for the tag decision of a specific token .",system description,Named entity recognition,0,67,20,20,0,system description : Named entity recognition,0.2271186440677966,0.3448275862068966,0.5882352941176471
relation-classification,2,"For example , in the entity "" John Smith "" , tagging "" Smith "" as PER is useful for deciding that "" John "" is B - PER .",system description,Named entity recognition,0,68,21,21,0,system description : Named entity recognition,0.2305084745762712,0.3620689655172414,0.6176470588235294
relation-classification,2,"To this end , for NER , we use a linear - chain CRF , similar to Lample et al . where an improvement of ? 1 % F 1 NER points is reported when using CRF .",system description,Named entity recognition,0,69,22,22,0,system description : Named entity recognition,0.23389830508474577,0.3793103448275862,0.6470588235294118
relation-classification,2,"To this end , for NER , we use a linear - chain CRF , similar to Lample et al . where an improvement of ? 1 % F 1 NER points is reported when using CRF .",system description,Named entity recognition,0,70,23,23,0,system description : Named entity recognition,0.23728813559322035,0.39655172413793105,0.6764705882352942
relation-classification,2,"In our case , with the use of CRF we also report a ? 1 % over all performance improvement as observed in",system description,Named entity recognition,0,71,24,24,0,system description : Named entity recognition,0.24067796610169492,0.41379310344827586,0.7058823529411765
relation-classification,2,"is the score of the predicted tag for token w i , T is a square transition matrix in which each entry represents transition scores from one tag to another .",system description,Named entity recognition,0,72,25,25,0,system description : Named entity recognition,0.2440677966101695,0.43103448275862066,0.7352941176470589
relation-classification,2,? R ( p+2 ) ( p + 2 ) because y,system description,Named entity recognition,0,73,26,26,0,system description : Named entity recognition,0.24745762711864408,0.4482758620689655,0.7647058823529411
relation-classification,2,? R ( p+2 ) ( p + 2 ) because y,system description,Named entity recognition,0,74,27,27,0,system description : Named entity recognition,0.25084745762711863,0.46551724137931033,0.7941176470588235
relation-classification,2,We apply Viterbi to obtain the tag sequence ? ( e ) with the highest score .,system description,Named entity recognition,0,75,28,28,0,system description : Named entity recognition,0.2542372881355932,0.4827586206896552,0.8235294117647058
relation-classification,2,We train both the softmax ( for the EC task ) and the CRF layer ( for NER ) by minimizing the cross - entropy loss L NER .,system description,Named entity recognition,0,76,29,29,0,system description : Named entity recognition,0.2576271186440678,0.5,0.8529411764705882
relation-classification,2,"We also use the entity tags as input to our relation extraction layer by learning label embeddings , motivated by where an improvement of 2 % F 1 is reported ( with the use of label embeddings ) .",system description,Named entity recognition,0,77,30,30,0,system description : Named entity recognition,0.26101694915254237,0.5172413793103449,0.8823529411764706
relation-classification,2,"In our case , label embeddings lead to an increase of 1 % F 1 score as reported in ( see Section 5.2 ) .",system description,Named entity recognition,0,78,31,31,0,system description : Named entity recognition,0.26440677966101694,0.5344827586206896,0.9117647058823529
relation-classification,2,"The input to the next layer is twofold : the output states of the LSTM and the learned label embedding representation , encoding the intuition that knowledge of named enti-ties can be useful for relation extraction .",system description,Named entity recognition,0,79,32,32,0,system description : Named entity recognition,0.2677966101694915,0.5517241379310345,0.9411764705882353
relation-classification,2,"During training , we use the gold entity tags , while at prediction time we use the predicted entity tags as input to the next layer .",system description,Named entity recognition,0,80,33,33,0,system description : Named entity recognition,0.2711864406779661,0.5689655172413793,0.9705882352941176
relation-classification,2,The input to the next layer is the concatenation of the hidden LSTM state hi with the label embedding g i for token w i :,system description,Named entity recognition,0,81,34,34,0,system description : Named entity recognition,0.2745762711864407,0.5862068965517241,1.0
relation-classification,2,Relation extraction,system description,Relation extraction,0,82,35,1,0,system description : Relation extraction,0.27796610169491526,0.603448275862069,0.14285714285714285
relation-classification,2,We consider relation extraction as the second task of our joint model .,system description,Relation extraction,0,83,36,2,0,system description : Relation extraction,0.28135593220338984,0.6206896551724138,0.2857142857142857
relation-classification,2,The main approaches for relation extraction rely either on hand - crafted features or neural networks .,system description,Relation extraction,0,84,37,3,0,system description : Relation extraction,0.2847457627118644,0.6379310344827587,0.42857142857142855
relation-classification,2,"Feature - based methods focus on obtaining effective hand - crafted features , for instance defining kernel functions and designing lexical , syntactic , semantic features , etc . .",system description,Relation extraction,0,85,38,4,0,system description : Relation extraction,0.288135593220339,0.6551724137931034,0.5714285714285714
relation-classification,2,Neural network models have been proposed to overcome the issue of manually designing hand - crafted features leading to improved performance .,system description,Relation extraction,0,86,39,5,0,system description : Relation extraction,0.29152542372881357,0.6724137931034483,0.7142857142857143
relation-classification,2,CNN - and models have been introduced to automatically extract lexical and sentence level features leading to a deeper language understanding .,system description,Relation extraction,0,87,40,6,0,system description : Relation extraction,0.29491525423728815,0.6896551724137931,0.8571428571428571
relation-classification,2,combine CNNs and RNNs using an ensemble scheme to achieve state - of - the - art results .,system description,Relation extraction,0,88,41,7,0,system description : Relation extraction,0.2983050847457627,0.7068965517241379,1.0
relation-classification,2,Joint entity and relation extraction,system description,Joint entity and relation extraction,0,89,42,1,0,system description : Joint entity and relation extraction,0.3016949152542373,0.7241379310344828,0.058823529411764705
relation-classification,2,Entity and relation extraction includes the task of ( i ) identifying the entities ( described in Section 2.1 ) and ( ii ) extracting the relations among them ( described in Section 2.2 ) .,system description,Joint entity and relation extraction,0,90,43,2,0,system description : Joint entity and relation extraction,0.3050847457627119,0.7413793103448276,0.11764705882352941
relation-classification,2,Feature - based joint models have been proposed to simultaneously solve the entity recognition and relation extraction ( RE ) subtasks .,system description,Joint entity and relation extraction,0,91,44,3,0,system description : Joint entity and relation extraction,0.30847457627118646,0.7586206896551724,0.17647058823529413
relation-classification,2,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features and thus ( i ) require additional effort for the data preprocessing , ( ii ) perform poorly in different application and language settings where the NLP tools are not reliable , and ( iii ) increase the computational complexity .",system description,Joint entity and relation extraction,0,92,45,4,0,system description : Joint entity and relation extraction,0.31186440677966104,0.7758620689655172,0.23529411764705882
relation-classification,2,"In this paper , we introduce a joint neural network model to overcome the aforementioned issues and to automatically perform end - to - end relation extraction without the need of any manual feature engineering or the use of additional NLP components .",system description,Joint entity and relation extraction,0,93,46,5,0,system description : Joint entity and relation extraction,0.3152542372881356,0.7931034482758621,0.29411764705882354
relation-classification,2,Neural network approaches have been considered to address the problem in a joint setting ( end - to - end relation extraction ) and typically include the use of RNNs and CNNs .,system description,Joint entity and relation extraction,0,94,47,6,0,system description : Joint entity and relation extraction,0.31864406779661014,0.8103448275862069,0.35294117647058826
relation-classification,2,"Specifically , propose the use of bidirectional tree - structured RNNs to capture dependency tree information ( where parse trees are extracted using state - of - the - art dependency parsers ) which has been proven beneficial for relation extraction .",system description,Joint entity and relation extraction,0,95,48,7,0,system description : Joint entity and relation extraction,0.3220338983050847,0.8275862068965517,0.4117647058823529
relation-classification,2,"apply the work of to biomedical text , reporting state - of - the - art performance for two biomedical datasets .",system description,Joint entity and relation extraction,0,96,49,8,0,system description : Joint entity and relation extraction,0.3254237288135593,0.8448275862068966,0.47058823529411764
relation-classification,2,propose the use of a lot of hand - crafted features along with RNNs .,system description,Joint entity and relation extraction,0,97,50,9,0,system description : Joint entity and relation extraction,0.3288135593220339,0.8620689655172413,0.5294117647058824
relation-classification,2,"solve the entity classification task ( which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted ) and relation extraction problems using an approximation of a global normalization objective ( i.e. , CRF ) : they replicate the context of the sentence ( left and right part of the entities ) to feed one entity pair at a time to a CNN for relation extraction .",system description,Joint entity and relation extraction,0,98,51,10,0,system description : Joint entity and relation extraction,0.33220338983050846,0.8793103448275862,0.5882352941176471
relation-classification,2,"Thus , they do not simultaneously infer other potential entities and relations within the same sentence .",system description,Joint entity and relation extraction,0,99,52,11,0,system description : Joint entity and relation extraction,0.33559322033898303,0.896551724137931,0.6470588235294118
relation-classification,2,"and The input of our model is the words of the sentence which are then represented as word vectors ( i.e. , embeddings ) .",system description,Joint entity and relation extraction,0,100,53,12,0,system description : Joint entity and relation extraction,0.3389830508474576,0.9137931034482759,0.7058823529411765
relation-classification,2,The BiLSTM layer extracts a more complex representation for each word .,system description,Joint entity and relation extraction,0,101,54,13,0,system description : Joint entity and relation extraction,0.3423728813559322,0.9310344827586207,0.7647058823529411
relation-classification,2,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,system description,Joint entity and relation extraction,0,102,55,14,0,system description : Joint entity and relation extraction,0.34576271186440677,0.9482758620689655,0.8235294117647058
relation-classification,2,"The outputs for each token ( e.g. , Smith ) are : ( i ) an entity recognition label ( e.g. , I - PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",system description,Joint entity and relation extraction,0,103,56,15,0,system description : Joint entity and relation extraction,0.34915254237288135,0.9655172413793104,0.8823529411764706
relation-classification,2,"tional complexity described by , by dividing the loss functions into a NER and a relation extraction component .",system description,Joint entity and relation extraction,0,104,57,16,0,system description : Joint entity and relation extraction,0.3525423728813559,0.9827586206896551,0.9411764705882353
relation-classification,2,"Moreover , we are able to handle multiple relations instead of just predicting single ones , as was described for the application of structured real estate advertisements of .",system description,Joint entity and relation extraction,0,105,58,17,0,system description : Joint entity and relation extraction,0.3559322033898305,1.0,1.0
relation-classification,2,Joint model,model,Joint model,0,106,1,1,0,model : Joint model,0.3593220338983051,0.015384615384615385,0.07692307692307693
relation-classification,2,"In this section , we present our multi-head joint model illustrated in .",model,Joint model,1,107,2,2,0,model : Joint model,0.36271186440677966,0.03076923076923077,0.15384615384615385
relation-classification,2,"The model is able to simultaneously identify the entities ( i.e. , types and boundaries ) and all the possible relations between them at once .",model,Joint model,0,108,3,3,0,model : Joint model,0.36610169491525424,0.046153846153846156,0.23076923076923078
relation-classification,2,We formulate the problem as a multi-head selection problem extending previous work as described in Section 2.3 .,model,Joint model,0,109,4,4,0,model : Joint model,0.3694915254237288,0.06153846153846154,0.3076923076923077
relation-classification,2,"By multi-head , we mean that any particular entity maybe the CoNLL04 dataset is presented .",model,Joint model,0,110,5,5,0,model : Joint model,0.3728813559322034,0.07692307692307693,0.38461538461538464
relation-classification,2,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .",model,Joint model,1,111,6,6,0,model : Joint model,0.376271186440678,0.09230769230769231,0.46153846153846156
relation-classification,2,The BiLSTM layer is able to extract a more complex representation for each word that incorporates the context via the RNN structure .,model,Joint model,0,112,7,7,0,model : Joint model,0.37966101694915255,0.1076923076923077,0.5384615384615384
relation-classification,2,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,model,Joint model,0,113,8,8,0,model : Joint model,0.38305084745762713,0.12307692307692308,0.6153846153846154
relation-classification,2,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",model,Joint model,1,114,9,9,0,model : Joint model,0.3864406779661017,0.13846153846153847,0.6923076923076923
relation-classification,2,"Since we assume token - based encoding , we consider only the last token of the entity as head of another token , eliminating redundant relations .",model,Joint model,0,115,10,10,0,model : Joint model,0.3898305084745763,0.15384615384615385,0.7692307692307693
relation-classification,2,"For instance , there is a Works for relation between entities "" John Smith "" and "" Disease Control Center "" .",model,Joint model,0,116,11,11,0,model : Joint model,0.39322033898305087,0.16923076923076924,0.8461538461538461
relation-classification,2,"Instead of connecting all tokens of the entities , we connect only "" Smith "" with "" Center "" .",model,Joint model,0,117,12,12,0,model : Joint model,0.39661016949152544,0.18461538461538463,0.9230769230769231
relation-classification,2,"Also , for the case of no relation , we introduce the "" N "" label and we predict the token itself as the head .",model,Joint model,0,118,13,13,0,model : Joint model,0.4,0.2,1.0
relation-classification,2,Embedding layer,model,Embedding layer,0,119,14,1,0,model : Embedding layer,0.4033898305084746,0.2153846153846154,0.08333333333333333
relation-classification,2,"Given a sentence w = w 1 , ... , w n as a sequence of tokens , the word embedding layer is responsible to map each token to a word vector ( w word2vec ) .",model,Embedding layer,0,120,15,2,0,model : Embedding layer,0.4067796610169492,0.23076923076923078,0.16666666666666666
relation-classification,2,We use pre-trained word embeddings using the Skip - Gram word2vec model .,model,Embedding layer,0,121,16,3,0,model : Embedding layer,0.4101694915254237,0.24615384615384617,0.25
relation-classification,2,"In this work , we also use character embeddings since they are commonly applied to neural NER .",model,Embedding layer,0,122,17,4,0,model : Embedding layer,0.4135593220338983,0.26153846153846155,0.3333333333333333
relation-classification,2,This type of embeddings is able to capture morphological features such as prefixes and suffixes .,model,Embedding layer,0,123,18,5,0,model : Embedding layer,0.41694915254237286,0.27692307692307694,0.4166666666666667
relation-classification,2,"For instance , in the Adverse Drug Events ( ADE ) dataset , the suffix "" toxicity "" can specify an adverse drug event entity such as "" neurotoxicity "" or "" hepatotoxicity "" and thus it is very informative .",model,Embedding layer,0,124,19,6,0,model : Embedding layer,0.42033898305084744,0.2923076923076923,0.5
relation-classification,2,"Another example might be the Dutch suffix "" kamer "" ( "" room "" in English ) in the Dutch Real Estate Classifieds ( DREC ) dataset which is used to specify the space entities "" badkamer "" ( "" bathroom "" in English ) and "" slaapkamer "" ( "" bedroom "" in English ) .",model,Embedding layer,0,125,20,7,0,model : Embedding layer,0.423728813559322,0.3076923076923077,0.5833333333333334
relation-classification,2,"Character - level embeddings are learned during training , similar to Ma & Hovy and .",model,Embedding layer,0,126,21,8,0,model : Embedding layer,0.4271186440677966,0.3230769230769231,0.6666666666666666
relation-classification,2,"In the work of Lample et al. , character embeddings lead to a performance improvement of up to 3 % in terms of NER F 1 score .",model,Embedding layer,0,127,22,9,0,model : Embedding layer,0.43050847457627117,0.3384615384615385,0.75
relation-classification,2,"In our work , by incorporating character embeddings , we report in an increase of ? 2 %",model,Embedding layer,0,128,23,10,0,model : Embedding layer,0.43389830508474575,0.35384615384615387,0.8333333333333334
relation-classification,2,over all F 1 scoring points .,model,Embedding layer,0,129,24,11,0,model : Embedding layer,0.43728813559322033,0.36923076923076925,0.9166666666666666
relation-classification,2,"For more details , see Section 5.2 .",model,Embedding layer,0,130,25,12,0,model : Embedding layer,0.4406779661016949,0.38461538461538464,1.0
relation-classification,2,Bidirectional LSTM encoding layer,model,Bidirectional LSTM encoding layer,0,131,26,1,0,model : Bidirectional LSTM encoding layer,0.4440677966101695,0.4,0.16666666666666666
relation-classification,2,RNNs are commonly used in modeling sequential data and have been successfully applied in various NLP tasks .,model,Bidirectional LSTM encoding layer,0,132,27,2,0,model : Bidirectional LSTM encoding layer,0.44745762711864406,0.4153846153846154,0.3333333333333333
relation-classification,2,"In this work , we use multi - layer LSTMs , a specific kind of RNNs which are able to capture long term dependencies well .",model,Bidirectional LSTM encoding layer,0,133,28,3,0,model : Bidirectional LSTM encoding layer,0.45084745762711864,0.4307692307692308,0.5
relation-classification,2,We employ a BiLSTM which is able to encode information from left to right ( past to future ) and right to left ( future to past ) .,model,Bidirectional LSTM encoding layer,0,134,29,4,0,model : Bidirectional LSTM encoding layer,0.4542372881355932,0.4461538461538462,0.6666666666666666
relation-classification,2,"This way , we can combine bidirectional information for each word by concatenating the forward ( hi ) and the backward ( hi ) output at timestep i .",model,Bidirectional LSTM encoding layer,0,135,30,5,0,model : Bidirectional LSTM encoding layer,0.4576271186440678,0.46153846153846156,0.8333333333333334
relation-classification,2,The BiLSTM output at timestep i can be written as :,model,Bidirectional LSTM encoding layer,0,136,31,6,0,model : Bidirectional LSTM encoding layer,0.4610169491525424,0.47692307692307695,1.0
relation-classification,2,Relation extraction as multi-head selection,model,Relation extraction as multi-head selection,0,137,32,1,0,model : Relation extraction as multi-head selection,0.46440677966101696,0.49230769230769234,0.041666666666666664
relation-classification,2,"In this subsection , we describe the relation extraction task , formulated as a multi-head selection problem .",model,Relation extraction as multi-head selection,0,138,33,2,0,model : Relation extraction as multi-head selection,0.46779661016949153,0.5076923076923077,0.08333333333333333
relation-classification,2,"In the general formulation of our method , each token w i can have multiple heads ( i.e. , multiple relations with other tokens ) .",model,Relation extraction as multi-head selection,0,139,34,3,0,model : Relation extraction as multi-head selection,0.4711864406779661,0.5230769230769231,0.125
relation-classification,2,"We predict the tuple (? i ,? i ) where ? i is the vector of heads and ? i is the vector of the corresponding relations for each token w i .",model,Relation extraction as multi-head selection,0,140,35,4,0,model : Relation extraction as multi-head selection,0.4745762711864407,0.5384615384615384,0.16666666666666666
relation-classification,2,"We predict the tuple (? i ,? i ) where ? i is the vector of heads and ? i is the vector of the corresponding relations for each token w i .",model,Relation extraction as multi-head selection,0,141,36,5,0,model : Relation extraction as multi-head selection,0.47796610169491527,0.5538461538461539,0.20833333333333334
relation-classification,2,"We predict the tuple (? i ,? i ) where ? i is the vector of heads and ? i is the vector of the corresponding relations for each token w i .",model,Relation extraction as multi-head selection,0,142,37,6,0,model : Relation extraction as multi-head selection,0.48135593220338985,0.5692307692307692,0.25
relation-classification,2,"This is different for the previous standard head selection for dependency parsing method since ( i ) it is extended to predict multiple heads and ( ii ) the decisions for the heads and the relations are jointly taken ( i.e. , instead of first predicting the heads and then in a next step the relations by using an additional classifier ) .",model,Relation extraction as multi-head selection,0,143,38,7,0,model : Relation extraction as multi-head selection,0.4847457627118644,0.5846153846153846,0.2916666666666667
relation-classification,2,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ? { 0 , ... , n} the vector of the most probable heads ? i ? wand the vector of the most probable corresponding relation labelsr i ? R .",model,Relation extraction as multi-head selection,0,144,39,8,0,model : Relation extraction as multi-head selection,0.488135593220339,0.6,0.3333333333333333
relation-classification,2,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ? { 0 , ... , n} the vector of the most probable heads ? i ? wand the vector of the most probable corresponding relation labelsr i ? R .",model,Relation extraction as multi-head selection,0,145,40,9,0,model : Relation extraction as multi-head selection,0.4915254237288136,0.6153846153846154,0.375
relation-classification,2,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ? { 0 , ... , n} the vector of the most probable heads ? i ? wand the vector of the most probable corresponding relation labelsr i ? R .",model,Relation extraction as multi-head selection,0,146,41,10,0,model : Relation extraction as multi-head selection,0.49491525423728816,0.6307692307692307,0.4166666666666667
relation-classification,2,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ? { 0 , ... , n} the vector of the most probable heads ? i ? wand the vector of the most probable corresponding relation labelsr i ? R .",model,Relation extraction as multi-head selection,0,147,42,11,0,model : Relation extraction as multi-head selection,0.49830508474576274,0.6461538461538462,0.4583333333333333
relation-classification,2,We calculate the score between tokens w i and w j given a label r k as follows :,model,Relation extraction as multi-head selection,0,148,43,12,0,model : Relation extraction as multi-head selection,0.5016949152542373,0.6615384615384615,0.5
relation-classification,2,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) , is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",model,Relation extraction as multi-head selection,0,149,44,13,0,model : Relation extraction as multi-head selection,0.5050847457627119,0.676923076923077,0.5416666666666666
relation-classification,2,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) , is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",model,Relation extraction as multi-head selection,0,150,45,14,0,model : Relation extraction as multi-head selection,0.5084745762711864,0.6923076923076923,0.5833333333333334
relation-classification,2,We define,model,Relation extraction as multi-head selection,0,151,46,15,0,model : Relation extraction as multi-head selection,0.511864406779661,0.7076923076923077,0.625
relation-classification,2,"to be the probability of token w j to be selected as the head of token w i with the relation label r k between them , where ?( . ) stands for the sigmoid function .",model,Relation extraction as multi-head selection,0,152,47,16,0,model : Relation extraction as multi-head selection,0.5152542372881356,0.7230769230769231,0.6666666666666666
relation-classification,2,We minimize the cross - entropy loss L rel during training :,model,Relation extraction as multi-head selection,0,153,48,17,0,model : Relation extraction as multi-head selection,0.5186440677966102,0.7384615384615385,0.7083333333333334
relation-classification,2,where y i ? wand r i ? R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,model,Relation extraction as multi-head selection,0,154,49,18,0,model : Relation extraction as multi-head selection,0.5220338983050847,0.7538461538461538,0.75
relation-classification,2,where y i ? wand r i ? R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,model,Relation extraction as multi-head selection,0,155,50,19,0,model : Relation extraction as multi-head selection,0.5254237288135594,0.7692307692307693,0.7916666666666666
relation-classification,2,where y i ? wand r i ? R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,model,Relation extraction as multi-head selection,0,156,51,20,0,model : Relation extraction as multi-head selection,0.5288135593220339,0.7846153846153846,0.8333333333333334
relation-classification,2,"After training , we keep the combination of heads ? i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq..",model,Relation extraction as multi-head selection,0,157,52,21,0,model : Relation extraction as multi-head selection,0.5322033898305085,0.8,0.875
relation-classification,2,"After training , we keep the combination of heads ? i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq..",model,Relation extraction as multi-head selection,0,158,53,22,0,model : Relation extraction as multi-head selection,0.535593220338983,0.8153846153846154,0.9166666666666666
relation-classification,2,"Unlike previous work on joint models ( Katiyar & Cardie , 2017 ) , we are able to predict multiple relations considering the classes as independent and not mutually exclusive ( the probabilities do not necessarily sum to 1 for different classes ) .",model,Relation extraction as multi-head selection,0,159,54,23,0,model : Relation extraction as multi-head selection,0.5389830508474577,0.8307692307692308,0.9583333333333334
relation-classification,2,"For the joint entity and relation extraction task , we calculate the final objective as L NER + L rel .",model,Relation extraction as multi-head selection,0,160,55,24,0,model : Relation extraction as multi-head selection,0.5423728813559322,0.8461538461538461,1.0
relation-classification,2,Edmonds ' algorithm,model,Edmonds' algorithm,0,161,56,1,0,model : Edmonds' algorithm,0.5457627118644067,0.8615384615384616,0.1
relation-classification,2,Our model is able to simultaneously extract entity mentions and the relations between them .,model,Edmonds' algorithm,0,162,57,2,0,model : Edmonds' algorithm,0.5491525423728814,0.8769230769230769,0.2
relation-classification,2,"To demonstrate the effectiveness and the general purpose nature of our model , we also test it on the recently introduced Dutch real estate classifieds ( DREC ) dataset where the entities need to form a tree structure .",model,Edmonds' algorithm,0,163,58,3,0,model : Edmonds' algorithm,0.5525423728813559,0.8923076923076924,0.3
relation-classification,2,"By using thresholded inference , a tree structure of relations is not guaranteed .",model,Edmonds' algorithm,0,164,59,4,0,model : Edmonds' algorithm,0.5559322033898305,0.9076923076923077,0.4
relation-classification,2,Thus we should enforce tree structure constraints to our model .,model,Edmonds' algorithm,0,165,60,5,0,model : Edmonds' algorithm,0.559322033898305,0.9230769230769231,0.5
relation-classification,2,"To this end , we post-process the output of our system with Edmonds ' maximum spanning tree algorithm for directed graphs .",model,Edmonds' algorithm,0,166,61,6,0,model : Edmonds' algorithm,0.5627118644067797,0.9384615384615385,0.6
relation-classification,2,"fully connected directed graph G = ( V , E ) is constructed , where the vertices",model,Edmonds' algorithm,0,167,62,7,0,model : Edmonds' algorithm,0.5661016949152542,0.9538461538461539,0.7
relation-classification,2,"fully connected directed graph G = ( V , E ) is constructed , where the vertices",model,Edmonds' algorithm,0,168,63,8,0,model : Edmonds' algorithm,0.5694915254237288,0.9692307692307692,0.8
relation-classification,2,represent the last tokens of the identified entities ( as predicted by NER ) and the edges E represent the highest scoring relations with their scores as weights .,model,Edmonds' algorithm,0,169,64,9,0,model : Edmonds' algorithm,0.5728813559322034,0.9846153846153847,0.9
relation-classification,2,Edmonds ' algorithm is applied in cases a tree is not already formed by thresholded inference .,model,Edmonds' algorithm,0,170,65,10,0,model : Edmonds' algorithm,0.576271186440678,1.0,1.0
relation-classification,2,Experimental setup,experiment,Experimental setup,0,171,1,1,0,experiment : Experimental setup,0.5796610169491525,1.0,1.0
relation-classification,2,Datasets and evaluation metrics,dataset,Datasets and evaluation metrics,0,172,1,1,0,dataset : Datasets and evaluation metrics,0.5830508474576271,0.03225806451612903,0.03225806451612903
relation-classification,2,"We conduct experiments on four datasets : ( i ) Automatic Content Extraction , ACE04 ( Dod - We follow the cross -validation setting of Li & Ji and .",dataset,Datasets and evaluation metrics,0,173,2,2,0,dataset : Datasets and evaluation metrics,0.5864406779661017,0.06451612903225806,0.06451612903225806
relation-classification,2,We removed DISC and did 5 - fold cross -validation on the bnews and nwire subsets ( 348 documents ) .,dataset,Datasets and evaluation metrics,0,174,3,3,0,dataset : Datasets and evaluation metrics,0.5898305084745763,0.0967741935483871,0.0967741935483871
relation-classification,2,We obtained the preprocessing script from Miwa 's github codebase .,dataset,Datasets and evaluation metrics,0,175,4,4,0,dataset : Datasets and evaluation metrics,0.5932203389830508,0.12903225806451613,0.12903225806451613
relation-classification,2,"We measure the performance of our system using micro F 1 scores , Precision and Recall on both entities and relations .",dataset,Datasets and evaluation metrics,0,176,5,5,0,dataset : Datasets and evaluation metrics,0.5966101694915255,0.16129032258064516,0.16129032258064516
relation-classification,2,We treat an entity as correct when the entity type and the region of its head are correct .,dataset,Datasets and evaluation metrics,0,177,6,6,0,dataset : Datasets and evaluation metrics,0.6,0.1935483870967742,0.1935483870967742
relation-classification,2,"We treat a relation as correct when it s type and argument entities are correct , similar to and .",dataset,Datasets and evaluation metrics,0,178,7,7,0,dataset : Datasets and evaluation metrics,0.6033898305084746,0.22580645161290322,0.22580645161290322
relation-classification,2,We refer to this type of evaluation as strict .,dataset,Datasets and evaluation metrics,0,179,8,8,0,dataset : Datasets and evaluation metrics,0.6067796610169491,0.25806451612903225,0.25806451612903225
relation-classification,2,"We select the best hyperparameter values on a randomly selected validation set for each fold , selected from the training set ( 15 % of the data ) since there are no official train and validation splits in the work of .",dataset,Datasets and evaluation metrics,0,180,9,9,0,dataset : Datasets and evaluation metrics,0.6101694915254238,0.2903225806451613,0.2903225806451613
relation-classification,2,"CoNLL04 : There are four entity types in the dataset ( Location , Organization , Person , and Other ) and five relation types ( Kill , Live in , Located in , OrgBased in and Work for ) .",dataset,Datasets and evaluation metrics,0,181,10,10,0,dataset : Datasets and evaluation metrics,0.6135593220338983,0.3225806451612903,0.3225806451612903
relation-classification,2,"CoNLL04 : There are four entity types in the dataset ( Location , Organization , Person , and Other ) and five relation types ( Kill , Live in , Located in , OrgBased in and Work for ) .",dataset,Datasets and evaluation metrics,0,182,11,11,0,dataset : Datasets and evaluation metrics,0.6169491525423729,0.3548387096774194,0.3548387096774194
relation-classification,2,We use the splits defined by Gupta et al. and .,dataset,Datasets and evaluation metrics,0,183,12,12,0,dataset : Datasets and evaluation metrics,0.6203389830508474,0.3870967741935484,0.3870967741935484
relation-classification,2,"The dataset consists of 910 training instances , 243 for validation and 288 for testing .",dataset,Datasets and evaluation metrics,0,184,13,13,0,dataset : Datasets and evaluation metrics,0.6237288135593221,0.41935483870967744,0.41935483870967744
relation-classification,2,We measure the performance by computing the F 1 score on the test set .,dataset,Datasets and evaluation metrics,0,185,14,14,0,dataset : Datasets and evaluation metrics,0.6271186440677966,0.45161290322580644,0.45161290322580644
relation-classification,2,We adopt two evaluation settings to compare to previous work .,dataset,Datasets and evaluation metrics,0,186,15,15,0,dataset : Datasets and evaluation metrics,0.6305084745762712,0.4838709677419355,0.4838709677419355
relation-classification,2,"Specifically , we perform an EC task assuming the entity boundaries are given similar to Gupta et al. and .",dataset,Datasets and evaluation metrics,0,187,16,16,0,dataset : Datasets and evaluation metrics,0.6338983050847458,0.5161290322580645,0.5161290322580645
relation-classification,2,"To obtain comparable results , we omit the entity class "" Other "" when computing the EC score .",dataset,Datasets and evaluation metrics,0,188,17,17,0,dataset : Datasets and evaluation metrics,0.6372881355932203,0.5483870967741935,0.5483870967741935
relation-classification,2,We score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .,dataset,Datasets and evaluation metrics,0,189,18,18,0,dataset : Datasets and evaluation metrics,0.6406779661016949,0.5806451612903226,0.5806451612903226
relation-classification,2,We report macro-average F 1 scores for EC and RE to obtain comparable results to previous studies .,dataset,Datasets and evaluation metrics,0,190,19,19,0,dataset : Datasets and evaluation metrics,0.6440677966101694,0.6129032258064516,0.6129032258064516
relation-classification,2,"Moreover , we perform actual NER evaluation instead of just EC , reporting results using the strict evaluation metric .",dataset,Datasets and evaluation metrics,0,191,20,20,0,dataset : Datasets and evaluation metrics,0.6474576271186441,0.6451612903225806,0.6451612903225806
relation-classification,2,We measure the performance by computing the F 1 score on the test set .,dataset,Datasets and evaluation metrics,0,192,21,21,0,dataset : Datasets and evaluation metrics,0.6508474576271186,0.6774193548387096,0.6774193548387096
relation-classification,2,"To compare our results with previous work , we use the boundaries evaluation setting .",dataset,Datasets and evaluation metrics,0,193,22,22,0,dataset : Datasets and evaluation metrics,0.6542372881355932,0.7096774193548387,0.7096774193548387
relation-classification,2,"In this setting , we count an entity as correct if the boundaries of the entity are correct .",dataset,Datasets and evaluation metrics,0,194,23,23,0,dataset : Datasets and evaluation metrics,0.6576271186440678,0.7419354838709677,0.7419354838709677
relation-classification,2,relation is correct when the relation is correct and the argument entities are both correct .,dataset,Datasets and evaluation metrics,0,195,24,24,0,dataset : Datasets and evaluation metrics,0.6610169491525424,0.7741935483870968,0.7741935483870968
relation-classification,2,"Also , we report results using the strict evaluation for future reference .",dataset,Datasets and evaluation metrics,0,196,25,25,0,dataset : Datasets and evaluation metrics,0.6644067796610169,0.8064516129032258,0.8064516129032258
relation-classification,2,ADE : There are two types of entities ( drugs and diseases ) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease ( adverse drug events ) .,dataset,Datasets and evaluation metrics,0,197,26,26,0,dataset : Datasets and evaluation metrics,0.6677966101694915,0.8387096774193549,0.8387096774193549
relation-classification,2,"There are 6,821 sentences in total and similar to previous work , we remove ? 130 relations with overlapping entities ( e.g. , "" lithium "" is a drug which is related to "" lithium intoxication "" ) .",dataset,Datasets and evaluation metrics,0,198,27,27,0,dataset : Datasets and evaluation metrics,0.6711864406779661,0.8709677419354839,0.8709677419354839
relation-classification,2,"Since there are no official sets , we evaluate our model using 10 - fold cross- validation where 10 % of the data was used as validation and 10 % for test set similar to .",dataset,Datasets and evaluation metrics,0,199,28,28,0,dataset : Datasets and evaluation metrics,0.6745762711864407,0.9032258064516129,0.9032258064516129
relation-classification,2,The final results are displayed in F 1 metric as a macro -average across the folds .,dataset,Datasets and evaluation metrics,0,200,29,29,0,dataset : Datasets and evaluation metrics,0.6779661016949152,0.9354838709677419,0.9354838709677419
relation-classification,2,"The dataset consists of 10,652 entities and 6,682 relations .",dataset,Datasets and evaluation metrics,0,201,30,30,0,dataset : Datasets and evaluation metrics,0.6813559322033899,0.967741935483871,0.967741935483871
relation-classification,2,We report results similar to previous work on this dataset using the strict evaluation metric .,dataset,Datasets and evaluation metrics,0,202,31,31,0,dataset : Datasets and evaluation metrics,0.6847457627118644,1.0,1.0
relation-classification,2,Word embeddings,system description,Word embeddings,0,203,1,1,0,system description : Word embeddings,0.688135593220339,0.25,0.25
relation-classification,2,"We use pre-trained word2vec embeddings used in previous work , so as to retain the same inputs for our model and to obtain comparable results thatare not affected by the input embeddings .",system description,Word embeddings,0,204,2,2,0,system description : Word embeddings,0.6915254237288135,0.5,0.5
relation-classification,2,"Specifically , we use the 200 - dimensional word embeddings used in the work of for the ACE04 dataset 6 trained on Wikipedia .",system description,Word embeddings,0,205,3,3,0,system description : Word embeddings,0.6949152542372882,0.75,0.75
relation-classification,2,We obtained the 50 - dimensional word embeddings used by,system description,Word embeddings,0,206,4,4,0,system description : Word embeddings,0.6983050847457627,1.0,1.0
relation-classification,2,Hyperparameters and implementation details,implementation,Hyperparameters and implementation details,0,207,1,1,0,implementation : Hyperparameters and implementation details,0.7016949152542373,0.07142857142857142,0.07142857142857142
relation-classification,2,We have developed our joint model by using Python with the TensorFlow machine learning library .,implementation,Hyperparameters and implementation details,1,208,2,2,0,implementation : Hyperparameters and implementation details,0.7050847457627119,0.14285714285714285,0.14285714285714285
relation-classification,2,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .",implementation,Hyperparameters and implementation details,1,209,3,3,0,implementation : Hyperparameters and implementation details,0.7084745762711865,0.21428571428571427,0.21428571428571427
relation-classification,2,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,implementation,Hyperparameters and implementation details,1,210,4,4,0,implementation : Hyperparameters and implementation details,0.711864406779661,0.2857142857142857,0.2857142857142857
relation-classification,2,We use dropout to regularize our network .,implementation,Hyperparameters and implementation details,1,211,5,5,0,implementation : Hyperparameters and implementation details,0.7152542372881356,0.35714285714285715,0.35714285714285715
relation-classification,2,Dropout is applied in the input embeddings and in between the hidden layers for both tasks .,implementation,Hyperparameters and implementation details,0,212,6,6,0,implementation : Hyperparameters and implementation details,0.7186440677966102,0.42857142857142855,0.42857142857142855
relation-classification,2,Different dropout rates have been applied but the best dropout values ( 0.2 to 0.4 ) for each dataset have been used .,implementation,Hyperparameters and implementation details,0,213,7,7,0,implementation : Hyperparameters and implementation details,0.7220338983050848,0.5,0.5
relation-classification,2,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,implementation,Hyperparameters and implementation details,1,214,8,8,0,implementation : Hyperparameters and implementation details,0.7254237288135593,0.5714285714285714,0.5714285714285714
relation-classification,2,We also fixed our label embeddings to be of size b = 25 for all the datasets except for CoNLL04 where the label embeddings were not beneficial and thus were not used .,implementation,Hyperparameters and implementation details,0,215,9,9,0,implementation : Hyperparameters and implementation details,0.7288135593220338,0.6428571428571429,0.6428571428571429
relation-classification,2,We experimented with tanh and relu activation functions ( recall that this is the function f ( ) from the model description relu activation only in the ACE04 and tanh in all other datasets .,implementation,Hyperparameters and implementation details,0,216,10,10,0,implementation : Hyperparameters and implementation details,0.7322033898305085,0.7142857142857143,0.7142857142857143
relation-classification,2,We employ the technique of early stopping based on the validation set .,implementation,Hyperparameters and implementation details,1,217,11,11,0,implementation : Hyperparameters and implementation details,0.735593220338983,0.7857142857142857,0.7857142857142857
relation-classification,2,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .",implementation,Hyperparameters and implementation details,1,218,12,12,0,implementation : Hyperparameters and implementation details,0.7389830508474576,0.8571428571428571,0.8571428571428571
relation-classification,2,We select the best epoch according to the results in the validation set .,implementation,Hyperparameters and implementation details,0,219,13,13,0,implementation : Hyperparameters and implementation details,0.7423728813559322,0.9285714285714286,0.9285714285714286
relation-classification,2,For more details about the effect of each hyperparameter to the model performance see the Appendix .,implementation,Hyperparameters and implementation details,0,220,14,14,0,implementation : Hyperparameters and implementation details,0.7457627118644068,1.0,1.0
relation-classification,2,Results and discussion,result,Results and discussion,0,221,1,1,0,result : Results and discussion,0.7491525423728813,0.02564102564102564,1.0
relation-classification,2,Results,result,Results,0,222,2,1,0,result : Results,0.752542372881356,0.05128205128205128,0.02631578947368421
relation-classification,2,"In , we present the results of our analysis .",result,Results,0,223,3,2,0,result : Results,0.7559322033898305,0.07692307692307693,0.05263157894736842
relation-classification,2,The first column indicates the considered dataset .,result,Results,0,224,4,3,0,result : Results,0.7593220338983051,0.10256410256410256,0.07894736842105263
relation-classification,2,"In the second column , we denote the model which is applied ( i.e. , previous work and the proposed models ) .",result,Results,0,225,5,4,0,result : Results,0.7627118644067796,0.1282051282051282,0.10526315789473684
relation-classification,2,The proposed models are the following :,result,Results,0,226,6,5,0,result : Results,0.7661016949152543,0.15384615384615385,0.13157894736842105
relation-classification,2,"i ) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss for multiple head prediction , ( ii ) multi-head +",result,Results,0,227,7,6,0,result : Results,0.7694915254237288,0.1794871794871795,0.15789473684210525
relation-classification,2,"is the proposed model with addition of Edmonds ' algorithm to guarantee a tree - structured output for the DREC dataset , ( iii ) single - head is the proposed method but it predicts only one head per token using a softmax loss instead of a sigmoid , and ( iv ) multi-head EC is the proposed method with a softmax to predict the entity classes assuming that the boundaries are given , and the sigmoid loss for multiple head selection . ( iii ) Relaxed : we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .",result,Results,0,228,8,7,0,result : Results,0.7728813559322034,0.20512820512820512,0.18421052631578946
relation-classification,2,"In the next three columns , we present the results for the entity identification task ( Precision , Recall , F 1 ) and then ( in the subsequent three columns ) the results of the relation extraction task ( Precision , Recall , F 1 ) .",result,Results,0,229,9,8,0,result : Results,0.7762711864406779,0.23076923076923078,0.21052631578947367
relation-classification,2,"Finally , in the last column , we report an additional F 1 measure which is the average F 1 performance of the two subtasks .",result,Results,0,230,10,9,0,result : Results,0.7796610169491526,0.2564102564102564,0.23684210526315788
relation-classification,2,"We mark with bold font in , the class probabilities do not necessarily sum up to one since the classes are considered independent .",result,Results,0,231,11,10,0,result : Results,0.7830508474576271,0.28205128205128205,0.2631578947368421
relation-classification,2,"Moreover , we use a CRF - layer to model the NER task to capture dependencies between sequential tokens .",result,Results,0,232,12,11,0,result : Results,0.7864406779661017,0.3076923076923077,0.2894736842105263
relation-classification,2,"Finally , we obtain more effective word representations by using character - level embeddings .",result,Results,0,233,13,12,0,result : Results,0.7898305084745763,0.3333333333333333,0.3157894736842105
relation-classification,2,"On the other hand , our model performs within a reasonable margin ( ? 0.5 % for the NER task and ? 1 %",result,Results,0,234,14,13,0,result : Results,0.7932203389830509,0.358974358974359,0.34210526315789475
relation-classification,2,"for the RE task ) compared to For the CoNLL04 dataset , there are two different evaluation settings , namely relaxed and strict .",result,Results,0,235,15,14,0,result : Results,0.7966101694915254,0.38461538461538464,0.3684210526315789
relation-classification,2,"In the relaxed setting , we perform an EC task instead of NER assuming that the boundaries of the entities are given .",result,Results,0,236,16,15,0,result : Results,0.8,0.41025641025641024,0.39473684210526316
relation-classification,2,We adopt this setting to produce comparable results with previous studies ) .,result,Results,0,237,17,16,0,result : Results,0.8033898305084746,0.4358974358974359,0.42105263157894735
relation-classification,2,"Similar to , we present results of single models and no ensembles .",result,Results,0,238,18,17,0,result : Results,0.8067796610169492,0.46153846153846156,0.4473684210526316
relation-classification,2,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,result,Results,1,239,19,18,0,result : Results,0.8101694915254237,0.48717948717948717,0.47368421052631576
relation-classification,2,"Unlike these previous studies that consider pairs of entities to obtain the entity types and the corresponding relations , we model the whole sentence at once .",result,Results,0,240,20,19,0,result : Results,0.8135593220338984,0.5128205128205128,0.5
relation-classification,2,"That way , our method is able to directly infer all entities and relations of a sentence and benefit from their possible interactions that can not be modeled when training is performed for each entity pair individually , one at a time .",result,Results,0,241,21,20,0,result : Results,0.8169491525423729,0.5384615384615384,0.5263157894736842
relation-classification,2,"In the same setting , we also report the results of Gupta et al. in which they use multiple complicated hand - crafted features coming from NLP tools .",result,Results,0,242,22,21,0,result : Results,0.8203389830508474,0.5641025641025641,0.5526315789473685
relation-classification,2,Our model performs slightly better for the EC task and within a margin of 1 % in terms of over all F 1 score .,result,Results,0,243,23,22,0,result : Results,0.823728813559322,0.5897435897435898,0.5789473684210527
relation-classification,2,The difference in the over all performance is due to the fact that our model uses only automatically generated features .,result,Results,0,244,24,23,0,result : Results,0.8271186440677966,0.6153846153846154,0.6052631578947368
relation-classification,2,"We also report re-sults on the same dataset conducting NER ( i.e. , predicting entity types and boundaries ) and evaluating using the strict evaluation measure , similar to .",result,Results,0,245,25,24,0,result : Results,0.8305084745762712,0.6410256410256411,0.631578947368421
relation-classification,2,Our results are not directly comparable to the work of because we use the splits provided by .,result,Results,0,246,26,25,0,result : Results,0.8338983050847457,0.6666666666666666,0.6578947368421053
relation-classification,2,"However , in this setting we present the results from as reference .",result,Results,0,247,27,26,0,result : Results,0.8372881355932204,0.6923076923076923,0.6842105263157895
relation-classification,2,"We report an improvement of ? 2 % over all F 1 score , which suggests that our neural model is able to extract more informative representations compared to feature - based approaches .",result,Results,0,248,28,27,0,result : Results,0.8406779661016949,0.717948717948718,0.7105263157894737
relation-classification,2,"We also report results for the DREC dataset , with two different evaluation settings .",result,Results,1,249,29,28,0,result : Results,0.8440677966101695,0.7435897435897436,0.7368421052631579
relation-classification,2,"Specifically , we use the boundaries and the strict settings .",result,Results,1,250,30,29,0,result : Results,0.847457627118644,0.7692307692307693,0.7631578947368421
relation-classification,2,"We transform the previous results from to the boundaries setting to make them comparable to our model since in their work , they report token - based F 1 score , which is not a common evaluation metric in relation extraction problems .",result,Results,0,251,31,30,0,result : Results,0.8508474576271187,0.7948717948717948,0.7894736842105263
relation-classification,2,"Also , in their work , they focus on identifying only the boundaries of the entities and not the types ( e.g. , Floor , Space ) .",result,Results,0,252,32,31,0,result : Results,0.8542372881355932,0.8205128205128205,0.8157894736842105
relation-classification,2,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .",result,Results,1,253,33,32,0,result : Results,0.8576271186440678,0.8461538461538461,0.8421052631578947
relation-classification,2,"This is due to the fact that their quadratic scoring layer is beneficial for the RE task , yet complicates NER , which is usually modeled as a sequence labeling task .",result,Results,0,254,34,33,0,result : Results,0.8610169491525423,0.8717948717948718,0.868421052631579
relation-classification,2,"Moreover , we report results using the strict evaluation which is used in most related works .",result,Results,0,255,35,34,0,result : Results,0.864406779661017,0.8974358974358975,0.8947368421052632
relation-classification,2,"Using the prior knowledge that each entity has only one head , we can simplify our model and predict only one head each time ( i.e. , using a softmax loss ) .",result,Results,0,256,36,35,0,result : Results,0.8677966101694915,0.9230769230769231,0.9210526315789473
relation-classification,2,The difference between the single and the multi-head models is marginal ( < 0.1 % for both tasks ) .,result,Results,0,257,37,36,0,result : Results,0.8711864406779661,0.9487179487179487,0.9473684210526315
relation-classification,2,"This shows that our model ( multi-head ) can adapt to various environments , even if the setting is single head ( in terms of the application , and thus also in both training and test data ) .",result,Results,0,258,38,37,0,result : Results,0.8745762711864407,0.9743589743589743,0.9736842105263158
relation-classification,2,"Finally , we compare our model with previous work",result,Results,0,259,39,38,0,result : Results,0.8779661016949153,1.0,1.0
relation-classification,2,Analysis of feature contribution,analysis,Analysis of feature contribution,0,260,1,1,0,analysis : Analysis of feature contribution,0.8813559322033898,0.1111111111111111,0.1111111111111111
relation-classification,2,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,analysis,Analysis of feature contribution,1,261,2,2,0,analysis : Analysis of feature contribution,0.8847457627118644,0.2222222222222222,0.2222222222222222
relation-classification,2,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,analysis,Analysis of feature contribution,1,262,3,3,0,analysis : Analysis of feature contribution,0.888135593220339,0.3333333333333333,0.3333333333333333
relation-classification,2,"This shows that the NER labels , as expected , provide meaningful information for the RE component .",analysis,Analysis of feature contribution,0,263,4,4,0,analysis : Analysis of feature contribution,0.8915254237288136,0.4444444444444444,0.4444444444444444
relation-classification,2,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,analysis,Analysis of feature contribution,1,264,5,5,0,analysis : Analysis of feature contribution,0.8949152542372881,0.5555555555555556,0.5555555555555556
relation-classification,2,"This illustrates that composing words by the representation of characters is effective , and our method benefits from additional information such as capital letters , suffixes and prefixes within the token ( i.e. , its character sequences ) .",analysis,Analysis of feature contribution,0,265,6,6,0,analysis : Analysis of feature contribution,0.8983050847457628,0.6666666666666666,0.6666666666666666
relation-classification,2,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .",analysis,Analysis of feature contribution,1,266,7,7,0,analysis : Analysis of feature contribution,0.9016949152542373,0.7777777777777778,0.7777777777777778
relation-classification,2,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .",analysis,Analysis of feature contribution,1,267,8,8,0,analysis : Analysis of feature contribution,0.9050847457627119,0.8888888888888888,0.8888888888888888
relation-classification,2,"This happens because the CRF loss is able to capture the strong tag dependencies ( e.g. , I - LOC can not follow B - PER ) thatare present in the dataset instead of just assuming that the tag decision for each token is independent from tag decisions of neighboring tokens .",analysis,Analysis of feature contribution,0,268,9,9,0,analysis : Analysis of feature contribution,0.9084745762711864,1.0,1.0
relation-classification,2,Conclusion,conclusion,Conclusion,0,269,1,1,0,conclusion : Conclusion,0.911864406779661,0.037037037037037035,0.037037037037037035
relation-classification,2,"In this work , we present a joint neural model to simultaneously extract entities and relations from textual data .",conclusion,Conclusion,0,270,2,2,0,conclusion : Conclusion,0.9152542372881356,0.07407407407407407,0.07407407407407407
relation-classification,2,Our model comprises a CRF layer for the entity recognition task and a sigmoid layer for the relation extraction task .,conclusion,Conclusion,0,271,3,3,0,conclusion : Conclusion,0.9186440677966101,0.1111111111111111,0.1111111111111111
relation-classification,2,"Specifically , we model the relation extraction task as a multi-head selection problem since one entity can have multiple relations .",conclusion,Conclusion,0,272,4,4,0,conclusion : Conclusion,0.9220338983050848,0.14814814814814814,0.14814814814814814
relation-classification,2,"Previous models on this task rely heavily on external NLP tools ( i.e. , POS taggers , dependency parsers ) .",conclusion,Conclusion,0,273,5,5,0,conclusion : Conclusion,0.9254237288135593,0.18518518518518517,0.18518518518518517
relation-classification,2,"Thus , the performance of these models is affected by the accuracy of the extracted features .",conclusion,Conclusion,0,274,6,6,0,conclusion : Conclusion,0.9288135593220339,0.2222222222222222,0.2222222222222222
relation-classification,2,"Unlike previous studies , our model produces automatically generated features rather than relying on hand - crafted ones , or existing NLP tools .",conclusion,Conclusion,0,275,7,7,0,conclusion : Conclusion,0.9322033898305084,0.25925925925925924,0.25925925925925924
relation-classification,2,"Given its independence from such NLP or other feature generating tools , our approach can be easily adopted for any language and context .",conclusion,Conclusion,0,276,8,8,0,conclusion : Conclusion,0.9355932203389831,0.2962962962962963,0.2962962962962963
relation-classification,2,We demonstrate the effectiveness of our approach by conducting a large scale experimental study .,conclusion,Conclusion,0,277,9,9,0,conclusion : Conclusion,0.9389830508474576,0.3333333333333333,0.3333333333333333
relation-classification,2,Our model is able to outperform neural methods that automatically generate features while the results are marginally similar ( or sometimes better ) compared to feature - based neural network approaches .,conclusion,Conclusion,0,278,10,10,0,conclusion : Conclusion,0.9423728813559322,0.37037037037037035,0.37037037037037035
relation-classification,2,"As future work , we aim to explore the effectiveness of entity pre-training for the entity recognition module .",conclusion,Conclusion,0,279,11,11,0,conclusion : Conclusion,0.9457627118644067,0.4074074074074074,0.4074074074074074
relation-classification,2,This approach has been proven beneficial in the work of Miwa & Bansal ( 2016 ) for both the entity and the relation extraction modules .,conclusion,Conclusion,0,280,12,12,0,conclusion : Conclusion,0.9491525423728814,0.4444444444444444,0.4444444444444444
relation-classification,2,"In addition , we are planning to explore away to reduce the calculations in the quadratic relation scoring layer .",conclusion,Conclusion,0,281,13,13,0,conclusion : Conclusion,0.9525423728813559,0.48148148148148145,0.48148148148148145
relation-classification,2,"In addition , we are planning to explore away to reduce the calculations in the quadratic relation scoring layer .",conclusion,Conclusion,0,282,14,14,0,conclusion : Conclusion,0.9559322033898305,0.5185185185185185,0.5185185185185185
relation-classification,2,"For instance , a straightforward way to do so is to use in the sigmoid layer only the tokens that have been identified as entities .",conclusion,Conclusion,0,283,15,15,0,conclusion : Conclusion,0.9593220338983051,0.5555555555555556,0.5555555555555556
relation-classification,2,"Gupta , P. , . optimize only over the NER task ) , ( ii ) explore several hyperparameters of the network ( e.g. , dropout , LSTM size , character embeddings size ) , and ( iii ) report F 1 score using different word embeddings compared to the embeddings used in previous works .",conclusion,Conclusion,0,284,16,16,0,conclusion : Conclusion,0.9627118644067797,0.5925925925925926,0.5925925925925926
relation-classification,2,"In of the main paper , we focused on comparing our model against other joint models thatare able to solve the two tasks ( i.e. , NER and relation extraction ) simultaneously , mainly demonstrating superiority of phrasing the relation extraction as a multi-head selection problem ( enabling the extraction of multiple relations at once ) .",conclusion,Conclusion,0,285,17,17,0,conclusion : Conclusion,0.9661016949152542,0.6296296296296297,0.6296296296296297
relation-classification,2,"Here , in and vice versa ) .",conclusion,Conclusion,0,286,18,18,0,conclusion : Conclusion,0.9694915254237289,0.6666666666666666,0.6666666666666666
relation-classification,2,"Note that improving NER in isolation was not the objective of our multi-head model , but we rather aimed to compare our model against other joint models that solve the task of entity recognition and relation identification simultaneously .",conclusion,Conclusion,0,287,19,19,0,conclusion : Conclusion,0.9728813559322034,0.7037037037037037,0.7037037037037037
relation-classification,2,We thus did not envision to claim or achieve state - of - the - art performance in each of the individual building blocks of our joint model .,conclusion,Conclusion,0,288,20,20,0,conclusion : Conclusion,0.976271186440678,0.7407407407407407,0.7407407407407407
relation-classification,2,"and A4 show the performance of our model on the test set for different values of the embedding dropout , LSTM layer dropout and the LSTM output dropout hyperparameters , respectively .",conclusion,Conclusion,0,289,21,21,0,conclusion : Conclusion,0.9796610169491525,0.7777777777777778,0.7777777777777778
relation-classification,2,"Note that the hyperparameter values used for the results in Section 5 were obtained by tuning over the development set , and these are indicated in boldface in the tables below .",conclusion,Conclusion,0,290,22,22,0,conclusion : Conclusion,0.9830508474576272,0.8148148148148148,0.8148148148148148
relation-classification,2,We vary one hyperparameter at a time in order to assess the effect of a particular hyperparameter .,conclusion,Conclusion,0,291,23,23,0,conclusion : Conclusion,0.9864406779661017,0.8518518518518519,0.8518518518518519
relation-classification,2,"The main outcomes from these tables are twofold : ( i ) low dropout values ( e.g. , 0 , 0.1 ) lead to a performance decrease in the over all F 1 score ( see where a ? 3 % F 1 decrease is reported on the ACE04 dataset ) and ( ii ) average dropout values ( i.e. , 0.2 - 0.4 ) lead to consistently similar results .",conclusion,Conclusion,0,292,24,24,0,conclusion : Conclusion,0.9898305084745763,0.8888888888888888,0.8888888888888888
relation-classification,2,"In In the main results ( see Section 5 ) , to guarantee a fair comparison to previous work and to obtain comparable results thatare not affected by the input embeddings , we use embeddings used also in prior studies .",conclusion,Conclusion,0,293,25,25,0,conclusion : Conclusion,0.9932203389830508,0.9259259259259259,0.9259259259259259
relation-classification,2,"To assess the performance of our system to input variations , we also report results using different word embeddings ( see ) ( i.e. , ; Li et al. ) on the ACE04 dataset .",conclusion,Conclusion,0,294,26,26,0,conclusion : Conclusion,0.9966101694915255,0.9629629629629629,0.9629629629629629
relation-classification,2,"Our results showcase that our model , even when using different word embeddings , is still performing better compared to other works that , like ours , do not rely on additional NLP tools .",conclusion,Conclusion,0,295,27,27,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,3,Adversarial training for multi-context joint entity and relation extraction,title,title,1,2,1,1,0,title : title,0.014598540145985401,1.0,1.0
relation-classification,3,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.021897810218978103,0.25,0.25
relation-classification,3,Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.029197080291970802,0.5,0.5
relation-classification,3,We show how to use AT for the tasks of entity recognition and relation extraction .,abstract,abstract,1,5,3,3,0,abstract : abstract,0.0364963503649635,0.75,0.75
relation-classification,3,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.043795620437956206,1.0,1.0
relation-classification,3,Introduction,introduction,introduction,0,7,1,1,0,introduction : introduction,0.051094890510948905,0.09090909090909091,0.09090909090909091
relation-classification,3,"Many neural network methods have recently been exploited in various natural language processing ( NLP ) tasks , such as parsing , POS tagging , relation extraction , translation , and joint tasks .",introduction,introduction,0,8,2,2,0,introduction : introduction,0.058394160583941604,0.18181818181818182,0.18181818181818182
relation-classification,3,"However , observed that intentional small scale perturbations ( i.e. , adversarial examples ) to the input of such models may lead to incorrect decisions ( with high confidence ) .",introduction,introduction,0,9,3,3,0,introduction : introduction,0.06569343065693431,0.2727272727272727,0.2727272727272727
relation-classification,3,proposed adversarial training ( AT ) ( for image recognition ) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model .,introduction,introduction,0,10,4,4,0,introduction : introduction,0.072992700729927,0.36363636363636365,0.36363636363636365
relation-classification,3,"Although AT has recently been applied in NLP tasks ( e.g. , text classification ) , this paper - to the best of our knowledge - is the first attempt investigating regularization effects of AT in a joint setting for two related tasks .",introduction,introduction,0,11,5,5,0,introduction : introduction,0.08029197080291971,0.45454545454545453,0.45454545454545453
relation-classification,3,We start from a baseline joint model that performs the tasks of named entity recognition and relation extraction at once .,introduction,introduction,0,12,6,6,0,introduction : introduction,0.08759124087591241,0.5454545454545454,0.5454545454545454
relation-classification,3,"Previously proposed models ( summarized in Section 2 ) exhibit several issues that the neural network - based baseline approach ( detailed in Section 3.1 ) overcomes : ( i ) our model uses automatically extracted features without the need of external parsers nor manually extracted features ( see ; ; ) , ( ii ) all entities and the corresponding relations within the sentence are extracted at once , instead of examining one pair of entities at a time ( see ) , and ( iii ) we model relation extraction in a multi-label setting , allowing multiple relations per entity ( see ; ) .",introduction,introduction,0,13,7,7,0,introduction : introduction,0.0948905109489051,0.6363636363636364,0.6363636363636364
relation-classification,3,The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task ( Section 3.2 ) .,introduction,introduction,0,14,8,8,0,introduction : introduction,0.10218978102189781,0.7272727272727273,0.7272727272727273
relation-classification,3,"To evaluate the proposed AT method , we perform a large scale experimental study in this joint task ( see Section 4 ) , using datasets from different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",introduction,introduction,0,15,9,9,0,introduction : introduction,0.10948905109489052,0.8181818181818182,0.8181818181818182
relation-classification,3,"We use a strong baseline that outperforms all previous models that rely on automatically extracted features , achieving state - of - the - art performance ( Section 5 ) .",introduction,introduction,0,16,10,10,0,introduction : introduction,0.11678832116788321,0.9090909090909091,0.9090909090909091
relation-classification,3,"Compared to the baseline model , applying AT during training leads to a consistent additional increase in joint extraction effectiveness .",introduction,introduction,0,17,11,11,0,introduction : introduction,0.12408759124087591,1.0,1.0
relation-classification,3,Related work,related work,Related work,0,18,1,1,0,related work : Related work,0.13138686131386862,0.0625,0.0625
relation-classification,3,Joint entity and relation extraction :,related work,Related work,0,19,2,2,0,related work : Related work,0.1386861313868613,0.125,0.125
relation-classification,3,Joint models thatare based on manually extracted features have been proposed for performing both the named entity recognition ( NER ) and relation extraction subtasks at once .,related work,Related work,0,20,3,3,0,related work : Related work,0.145985401459854,0.1875,0.1875
relation-classification,3,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features leading to additional complexity .",related work,Related work,0,21,4,4,0,related work : Related work,0.15328467153284672,0.25,0.25
relation-classification,3,Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs .,related work,Related work,0,22,5,5,0,related work : Related work,0.16058394160583941,0.3125,0.3125
relation-classification,3,"Specifically , as well as apply bidirectional tree - structured RNNs for different contexts ( i.e. , news , biomedical ) to capture syntactic information ( using external dependency parsers ) .",related work,Related work,0,23,6,6,0,related work : Related work,0.1678832116788321,0.375,0.375
relation-classification,3,propose the use of various manually extracted features along with RNNs .,related work,Related work,0,24,7,7,0,related work : Related work,0.17518248175182483,0.4375,0.4375
relation-classification,3,"solve the simpler problem of entity classification ( EC , assuming entity boundaries are given ) , instead of NER , and they replicate the context around the entities , feeding entity pairs to the relation extraction layer .",related work,Related work,0,25,8,8,0,related work : Related work,0.18248175182481752,0.5,0.5
relation-classification,3,investigate RNNs with attention without taking into account that relation labels are not mutually exclusive .,related work,Related work,0,26,9,9,0,related work : Related work,0.1897810218978102,0.5625,0.5625
relation-classification,3,"Finally , use LSTMs in a joint model for extracting just one relation at a time , but increase the complexity of the NER part .",related work,Related work,0,27,10,10,0,related work : Related work,0.19708029197080293,0.625,0.625
relation-classification,3,Our baseline model enables simultaneous extraction of multiple relations from the same input .,related work,Related work,0,28,11,11,0,related work : Related work,0.20437956204379562,0.6875,0.6875
relation-classification,3,"Then , we further extend this strong baseline using adversarial training .",related work,Related work,0,29,12,12,0,related work : Related work,0.2116788321167883,0.75,0.75
relation-classification,3,Adversarial training ( AT ) has been proposed to make classifiers more robust to input perturbations in the context of image recognition .,related work,Related work,0,30,13,13,0,related work : Related work,0.21897810218978103,0.8125,0.8125
relation-classification,3,"In the context of NLP , several variants have been proposed for different tasks such as text classification , relation extraction and POS tagging .",related work,Related work,0,31,14,14,0,related work : Related work,0.22627737226277372,0.875,0.875
relation-classification,3,AT is considered as a regularization method .,related work,Related work,0,32,15,15,0,related work : Related work,0.23357664233576642,0.9375,0.9375
relation-classification,3,"Unlike other regularization methods ( i.e. , dropout , word dropout ) that introduce random noise , AT generates perturbations thatare variations of examples easily misclassified by the model .",related work,Related work,0,33,16,16,0,related work : Related work,0.24087591240875914,1.0,1.0
relation-classification,3,Model,model,Model,0,34,1,1,0,model : Model,0.24817518248175183,0.021739130434782608,1.0
relation-classification,3,Joint learning as head selection,model,Joint learning as head selection,0,35,2,1,0,model : Joint learning as head selection,0.25547445255474455,0.043478260869565216,0.03125
relation-classification,3,"The baseline model , described in detail in , is illustrated in .",model,Joint learning as head selection,1,36,3,2,0,model : Joint learning as head selection,0.26277372262773724,0.06521739130434782,0.0625
relation-classification,3,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,model,Joint learning as head selection,1,37,4,3,0,model : Joint learning as head selection,0.27007299270072993,0.08695652173913043,0.09375
relation-classification,3,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",model,Joint learning as head selection,1,38,5,4,0,model : Joint learning as head selection,0.2773722627737226,0.10869565217391304,0.125
relation-classification,3,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",model,Joint learning as head selection,1,39,6,5,0,model : Joint learning as head selection,0.2846715328467153,0.13043478260869565,0.15625
relation-classification,3,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,model,Joint learning as head selection,1,40,7,6,0,model : Joint learning as head selection,0.291970802919708,0.15217391304347827,0.1875
relation-classification,3,We also use pre-trained word embeddings .,model,Joint learning as head selection,1,41,8,7,0,model : Joint learning as head selection,0.29927007299270075,0.17391304347826086,0.21875
relation-classification,3,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .",model,Joint learning as head selection,1,42,9,8,0,model : Joint learning as head selection,0.30656934306569344,0.1956521739130435,0.25
relation-classification,3,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .",model,Joint learning as head selection,1,43,10,9,0,model : Joint learning as head selection,0.31386861313868614,0.21739130434782608,0.28125
relation-classification,3,"In , the B - PER tag is assigned to the beginning token of a ' person ' ( PER ) entity .",model,Joint learning as head selection,0,44,11,10,0,model : Joint learning as head selection,0.32116788321167883,0.2391304347826087,0.3125
relation-classification,3,"For the prediction of the entity tags , we use : ( i ) a softmax approach for the entity classification ( EC ) task ( assuming entity boundaries given ) or ( ii ) a CRF approach where we identify both the type and the boundaries for each entity .",model,Joint learning as head selection,0,45,12,11,0,model : Joint learning as head selection,0.3284671532846715,0.2608695652173913,0.34375
relation-classification,3,"During decoding , in the softmax setting , we greedily detect the entity types of the tokens ( i.e. , independent prediction ) .",model,Joint learning as head selection,0,46,13,12,0,model : Joint learning as head selection,0.3357664233576642,0.2826086956521739,0.375
relation-classification,3,"Although independent distribution of types is reasonable for EC tasks , this is not the case when there are strong correlations between neighboring tags .",model,Joint learning as head selection,0,47,14,13,0,model : Joint learning as head selection,0.34306569343065696,0.30434782608695654,0.40625
relation-classification,3,"For instance , the BIO encoding scheme imposes several constraints in the NER task ( e.g. , the B - PER and I - LOC tags can not be sequential ) .",model,Joint learning as head selection,0,48,15,14,0,model : Joint learning as head selection,0.35036496350364965,0.32608695652173914,0.4375
relation-classification,3,"Motivated by this intuition , we use a linear - chain CRF for the NER task .",model,Joint learning as head selection,0,49,16,15,0,model : Joint learning as head selection,0.35766423357664234,0.34782608695652173,0.46875
relation-classification,3,"For decoding , in the CRF setting , we use the Viterbi algorithm .",model,Joint learning as head selection,0,50,17,16,0,model : Joint learning as head selection,0.36496350364963503,0.3695652173913043,0.5
relation-classification,3,"During training , for both EC ( softmax ) and NER tasks ( CRF ) , we minimize the cross - entropy loss L NER .",model,Joint learning as head selection,0,51,18,17,0,model : Joint learning as head selection,0.3722627737226277,0.391304347826087,0.53125
relation-classification,3,"The entity tags are later fed into the relation extraction layer as label embeddings ( see ) , assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities .",model,Joint learning as head selection,0,52,19,18,0,model : Joint learning as head selection,0.3795620437956204,0.41304347826086957,0.5625
relation-classification,3,We model the relation extraction task as a multi-label head selection problem .,model,Joint learning as head selection,1,53,20,19,0,model : Joint learning as head selection,0.38686131386861317,0.43478260869565216,0.59375
relation-classification,3,"In our model , each word w i can be involved in multiple relations with other words .",model,Joint learning as head selection,0,54,21,20,0,model : Joint learning as head selection,0.39416058394160586,0.45652173913043476,0.625
relation-classification,3,"For instance , in the example illustrated in , "" Smith "" could be involved not only in a Lives in relation with the token "" California "" ( head ) but also in other relations simultaneously ( e.g. , Works for , Born In with some corresponding tokens ) .",model,Joint learning as head selection,0,55,22,21,0,model : Joint learning as head selection,0.40145985401459855,0.4782608695652174,0.65625
relation-classification,3,"The goal of the task is to predict for each word w i , a vector of heads ? i and the vector of corresponding relationsr i .",model,Joint learning as head selection,0,56,23,22,0,model : Joint learning as head selection,0.40875912408759124,0.5,0.6875
relation-classification,3,"The goal of the task is to predict for each word w i , a vector of heads ? i and the vector of corresponding relationsr i .",model,Joint learning as head selection,0,57,24,23,0,model : Joint learning as head selection,0.41605839416058393,0.5217391304347826,0.71875
relation-classification,3,"We compute the score s ( w j , w i , r k ) of word w j to be the head of w i given a relation label r k using a single layer neural network .",model,Joint learning as head selection,0,58,25,24,0,model : Joint learning as head selection,0.4233576642335766,0.5434782608695652,0.75
relation-classification,3,"The corresponding probability is defined as : P ( w j , r k | w i ; ? ) = ? ( s ( w j , w i , r k ) ) , where ?( . ) is the sigmoid function .",model,Joint learning as head selection,0,59,26,25,0,model : Joint learning as head selection,0.4306569343065693,0.5652173913043478,0.78125
relation-classification,3,"During training , we minimize the cross - entropy loss L rel as :",model,Joint learning as head selection,0,60,27,26,0,model : Joint learning as head selection,0.43795620437956206,0.5869565217391305,0.8125
relation-classification,3,where m is the number of associated heads ( and thus relations ) per word w i .,model,Joint learning as head selection,0,61,28,27,0,model : Joint learning as head selection,0.44525547445255476,0.6086956521739131,0.84375
relation-classification,3,"During decoding , the most probable heads and relations are selected using threshold - based prediction .",model,Joint learning as head selection,0,62,29,28,0,model : Joint learning as head selection,0.45255474452554745,0.6304347826086957,0.875
relation-classification,3,The final objective for the joint task is computed as L JOINT ( w ; ? ) = L NER + L rel where ? is a set of parameters .,model,Joint learning as head selection,0,63,30,29,0,model : Joint learning as head selection,0.45985401459854014,0.6521739130434783,0.90625
relation-classification,3,The final objective for the joint task is computed as L JOINT ( w ; ? ) = L NER + L rel where ? is a set of parameters .,model,Joint learning as head selection,0,64,31,30,0,model : Joint learning as head selection,0.46715328467153283,0.6739130434782609,0.9375
relation-classification,3,"In the case of multi-token entities , only the last token of the entity can serve as head of another token , to eliminate redundant relations .",model,Joint learning as head selection,0,65,32,31,0,model : Joint learning as head selection,0.4744525547445255,0.6956521739130435,0.96875
relation-classification,3,"If an entity is not involved in any relation , we predict the auxiliary "" N "" relation label and the token itself as head .",model,Joint learning as head selection,0,66,33,32,0,model : Joint learning as head selection,0.48175182481751827,0.717391304347826,1.0
relation-classification,3,Adversarial training ( AT ),model,Adversarial training (AT),1,67,34,1,0,model : Adversarial training (AT),0.48905109489051096,0.7391304347826086,0.07692307692307693
relation-classification,3,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,model,Adversarial training (AT),1,68,35,2,0,model : Adversarial training (AT),0.49635036496350365,0.7608695652173914,0.15384615384615385
relation-classification,3,"Specifically , we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation .",model,Adversarial training (AT),0,69,36,3,0,model : Adversarial training (AT),0.5036496350364964,0.782608695652174,0.23076923076923078
relation-classification,3,This is similar to the concept introduced by to improve the robustness of image recognition classifiers .,model,Adversarial training (AT),0,70,37,4,0,model : Adversarial training (AT),0.5109489051094891,0.8043478260869565,0.3076923076923077
relation-classification,3,We generate an adversarial example by adding the worst - case perturbation ? adv to the original embedding w that maximizes the loss function :,model,Adversarial training (AT),0,71,38,5,0,model : Adversarial training (AT),0.5182481751824818,0.8260869565217391,0.38461538461538464
relation-classification,3,We generate an adversarial example by adding the worst - case perturbation ? adv to the original embedding w that maximizes the loss function :,model,Adversarial training (AT),0,72,39,6,0,model : Adversarial training (AT),0.5255474452554745,0.8478260869565217,0.46153846153846156
relation-classification,3,where ? is a copy of the current model parameters .,model,Adversarial training (AT),0,73,40,7,0,model : Adversarial training (AT),0.5328467153284672,0.8695652173913043,0.5384615384615384
relation-classification,3,where ? is a copy of the current model parameters .,model,Adversarial training (AT),0,74,41,8,0,model : Adversarial training (AT),0.5401459854014599,0.8913043478260869,0.6153846153846154
relation-classification,3,Since Eq.,model,Adversarial training (AT),0,75,42,9,0,model : Adversarial training (AT),0.5474452554744526,0.9130434782608695,0.6923076923076923
relation-classification,3,"2 ) is intractable in neural networks , we use the approximation proposed in defined as : ? adv = g/ g , with g = ? w L JOINT ( w ; ? ) , where is a small bounded norm treated as a hyperparameter .",model,Adversarial training (AT),0,76,43,10,0,model : Adversarial training (AT),0.5547445255474452,0.9347826086956522,0.7692307692307693
relation-classification,3,"Similar to , we set to be ? ? D ( where Dis the dimension of the embeddings ) .",model,Adversarial training (AT),0,77,44,11,0,model : Adversarial training (AT),0.5620437956204379,0.9565217391304348,0.8461538461538461
relation-classification,3,"Similar to , we set to be ? ? D ( where Dis the dimension of the embeddings ) .",model,Adversarial training (AT),0,78,45,12,0,model : Adversarial training (AT),0.5693430656934306,0.9782608695652174,0.9230769230769231
relation-classification,3,"We train on the mixture of original and adversarial examples , so the final loss is computed as : L JOINT ( w ; ? ) + L JOINT ( w + ? adv ;? ) .",model,Adversarial training (AT),0,79,46,13,0,model : Adversarial training (AT),0.5766423357664233,1.0,1.0
relation-classification,3,Experimental setup,experiment,Experimental setup,0,80,1,1,0,experiment : Experimental setup,0.583941605839416,0.02127659574468085,0.02127659574468085
relation-classification,3,"We evaluate our models on four datasets , using the code as available from our github codebase .",experiment,Experimental setup,1,81,2,2,0,experiment : Experimental setup,0.5912408759124088,0.0425531914893617,0.0425531914893617
relation-classification,3,1,experiment,Experimental setup,0,82,3,3,0,experiment : Experimental setup,0.5985401459854015,0.06382978723404255,0.06382978723404255
relation-classification,3,"Specifically , we follow the 5 - fold crossvalidation defined by for the ACE04 dataset .",experiment,Experimental setup,0,83,4,4,0,experiment : Experimental setup,0.6058394160583942,0.0851063829787234,0.0851063829787234
relation-classification,3,"For the CoNLL04 ) EC task ( assuming boundaries are given ) , we use the same splits as in ; .",experiment,Experimental setup,0,84,5,5,0,experiment : Experimental setup,0.6131386861313869,0.10638297872340426,0.10638297872340426
relation-classification,3,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,experiment,Experimental setup,1,85,6,6,0,experiment : Experimental setup,0.6204379562043796,0.1276595744680851,0.1276595744680851
relation-classification,3,"For the Dutch Real Estate Classifieds , DREC ( Bekoulis et al. , 2017 ) dataset , we use train - test splits as in .",experiment,Experimental setup,0,86,7,7,0,experiment : Experimental setup,0.6277372262773723,0.14893617021276595,0.14893617021276595
relation-classification,3,"For the Adverse Drug Events , ADE , we perform 10 - fold cross -validation similar to .",experiment,Experimental setup,0,87,8,8,0,experiment : Experimental setup,0.635036496350365,0.1702127659574468,0.1702127659574468
relation-classification,3,"To obtain comparable results thatare not affected by the input embeddings , we use the embeddings of the previous works .",experiment,Experimental setup,0,88,9,9,0,experiment : Experimental setup,0.6423357664233577,0.19148936170212766,0.19148936170212766
relation-classification,3,We employ early stopping in all of the experiments .,experiment,Experimental setup,1,89,10,10,0,experiment : Experimental setup,0.6496350364963503,0.2127659574468085,0.2127659574468085
relation-classification,3,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .",experiment,Experimental setup,1,90,11,11,0,experiment : Experimental setup,0.656934306569343,0.23404255319148937,0.23404255319148937
relation-classification,3,"The scaling parameter ? is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",experiment,Experimental setup,0,91,12,12,0,experiment : Experimental setup,0.6642335766423357,0.2553191489361702,0.2553191489361702
relation-classification,3,"The scaling parameter ? is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",experiment,Experimental setup,0,92,13,13,0,experiment : Experimental setup,0.6715328467153284,0.2765957446808511,0.2765957446808511
relation-classification,3,"Larger values of ? ( i.e. , larger perturbations ) lead to consistent performance decrease in our early experiments .",experiment,Experimental setup,0,93,14,14,0,experiment : Experimental setup,0.6788321167883211,0.2978723404255319,0.2978723404255319
relation-classification,3,This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .,experiment,Experimental setup,0,94,15,15,0,experiment : Experimental setup,0.6861313868613139,0.3191489361702128,0.3191489361702128
relation-classification,3,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .",experiment,Experimental setup,1,95,16,16,0,experiment : Experimental setup,0.6934306569343066,0.3404255319148936,0.3404255319148936
relation-classification,3,"The proposed models are : ( i ) baseline , ( ii ) baseline EC ( predicts only entity classes ) and ( iii ) baseline ( EC ) + AT ( regularized by AT ) .",experiment,Experimental setup,0,96,17,17,0,experiment : Experimental setup,0.7007299270072993,0.3617021276595745,0.3617021276595745
relation-classification,3,The and symbols indicate whether the models rely on external NLP tools .,experiment,Experimental setup,0,97,18,18,0,experiment : Experimental setup,0.708029197080292,0.3829787234042553,0.3829787234042553
relation-classification,3,"We include different evaluation types ( S , Rand B ) .",experiment,Experimental setup,0,98,19,19,0,experiment : Experimental setup,0.7153284671532847,0.40425531914893614,0.40425531914893614
relation-classification,3,"boundaries are known ( CoNLL04 ) , to compare to previous works .",experiment,Experimental setup,0,99,20,20,0,experiment : Experimental setup,0.7226277372262774,0.425531914893617,0.425531914893617
relation-classification,3,"In all cases , a relation is considered as correct when both the relation type and the argument entities are correct .",experiment,Experimental setup,0,100,21,21,0,experiment : Experimental setup,0.7299270072992701,0.44680851063829785,0.44680851063829785
relation-classification,3,shows our experimental results .,experiment,Experimental setup,0,101,22,22,0,experiment : Experimental setup,0.7372262773722628,0.46808510638297873,0.46808510638297873
relation-classification,3,The name of the dataset is presented in the first column while the models are listed in the second column .,experiment,Experimental setup,0,102,23,23,0,experiment : Experimental setup,0.7445255474452555,0.48936170212765956,0.48936170212765956
relation-classification,3,"The proposed models are the following : ( i ) baseline : the baseline model shown in with the CRF layer and the sigmoid loss , ( ii ) baseline EC : the proposed model with the softmax layer for EC , ( iii ) baseline ( EC ) + AT : the baseline regularized using AT .",experiment,Experimental setup,0,103,24,24,0,experiment : Experimental setup,0.7518248175182481,0.5106382978723404,0.5106382978723404
relation-classification,3,The final three columns present the F 1 results for the two subtasks and their average performance .,experiment,Experimental setup,0,104,25,25,0,experiment : Experimental setup,0.7591240875912408,0.5319148936170213,0.5319148936170213
relation-classification,3,Bold values indicate the best results among models that use only automatically extracted features .,experiment,Experimental setup,0,105,26,26,0,experiment : Experimental setup,0.7664233576642335,0.5531914893617021,0.5531914893617021
relation-classification,3,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .",experiment,Experimental setup,1,106,27,27,0,experiment : Experimental setup,0.7737226277372263,0.574468085106383,0.574468085106383
relation-classification,3,"This improvement can be explained by the use of : ( i ) multi-label head selection , ( ii ) CRF - layer and ( iii ) character level embeddings .",experiment,Experimental setup,0,107,28,28,0,experiment : Experimental setup,0.781021897810219,0.5957446808510638,0.5957446808510638
relation-classification,3,"Compared to , who rely on NLP tools , the baseline performs within a reasonable margin ( less than 1 % ) on the joint task .",experiment,Experimental setup,0,108,29,29,0,experiment : Experimental setup,0.7883211678832117,0.6170212765957447,0.6170212765957447
relation-classification,3,"On the other hand , use the same model for the ADE biomedical dataset , where we report a 2.5 % over all improvement .",experiment,Experimental setup,0,109,30,30,0,experiment : Experimental setup,0.7956204379562044,0.6382978723404256,0.6382978723404256
relation-classification,3,This indicates that NLP tools are not always accurate for various contexts .,experiment,Experimental setup,0,110,31,31,0,experiment : Experimental setup,0.8029197080291971,0.6595744680851063,0.6595744680851063
relation-classification,3,"For the CoNLL04 dataset , we use two evaluation settings .",experiment,Experimental setup,1,111,32,32,0,experiment : Experimental setup,0.8102189781021898,0.6808510638297872,0.6808510638297872
relation-classification,3,We use the relaxed evaluation similar to ; on the EC task .,experiment,Experimental setup,0,112,33,33,0,experiment : Experimental setup,0.8175182481751825,0.7021276595744681,0.7021276595744681
relation-classification,3,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .",experiment,Experimental setup,1,113,34,34,0,experiment : Experimental setup,0.8248175182481752,0.723404255319149,0.723404255319149
relation-classification,3,"Moreover , compared to the model of that relies on complex features , the baseline model performs within a margin of 1 % in terms of over all F 1 score .",experiment,Experimental setup,0,114,35,35,0,experiment : Experimental setup,0.8321167883211679,0.7446808510638298,0.7446808510638298
relation-classification,3,"We also report NER results on the same dataset and improve over all F 1 score with ? 1 % compared to , indicating that our automatically extracted features are more informative than the hand - crafted ones .",experiment,Experimental setup,0,115,36,36,0,experiment : Experimental setup,0.8394160583941606,0.7659574468085106,0.7659574468085106
relation-classification,3,These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model .,experiment,Experimental setup,0,116,37,37,0,experiment : Experimental setup,0.8467153284671532,0.7872340425531915,0.7872340425531915
relation-classification,3,"For the DREC dataset , we use two evaluation methods .",experiment,Experimental setup,1,117,38,38,0,experiment : Experimental setup,0.8540145985401459,0.8085106382978723,0.8085106382978723
relation-classification,3,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .",experiment,Experimental setup,1,118,39,39,0,experiment : Experimental setup,0.8613138686131386,0.8297872340425532,0.8297872340425532
relation-classification,3,and show the effectiveness of the adversarial training on top of the baseline model .,experiment,Experimental setup,1,119,40,40,0,experiment : Experimental setup,0.8686131386861314,0.851063829787234,0.851063829787234
relation-classification,3,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .",experiment,Experimental setup,1,120,41,41,0,experiment : Experimental setup,0.8759124087591241,0.8723404255319149,0.8723404255319149
relation-classification,3,"Moreover , as seen in , the performance of the models using AT is closer to maximum even from the early training epochs .",experiment,Experimental setup,0,121,42,42,0,experiment : Experimental setup,0.8832116788321168,0.8936170212765957,0.8936170212765957
relation-classification,3,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the over all F 1 performance ( 0.4 % ) .",experiment,Experimental setup,1,122,43,43,0,experiment : Experimental setup,0.8905109489051095,0.9148936170212766,0.9148936170212766
relation-classification,3,"For CoNLL04 , we note an improvement in the over all F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .",experiment,Experimental setup,1,123,44,44,0,experiment : Experimental setup,0.8978102189781022,0.9361702127659575,0.9361702127659575
relation-classification,3,"For the DREC dataset , in both settings , there is an over all improvement of ? 1 % .",experiment,Experimental setup,1,124,45,45,0,experiment : Experimental setup,0.9051094890510949,0.9574468085106383,0.9574468085106383
relation-classification,3,"shows that from the first epochs , the model obtains its maximum performance on the DREC validation set .",experiment,Experimental setup,0,125,46,46,0,experiment : Experimental setup,0.9124087591240876,0.9787234042553191,0.9787234042553191
relation-classification,3,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .",experiment,Experimental setup,1,126,47,47,0,experiment : Experimental setup,0.9197080291970803,1.0,1.0
relation-classification,3,Results,result,Results,0,127,1,1,0,result : Results,0.927007299270073,0.14285714285714285,0.14285714285714285
relation-classification,3,"Our results demonstrate that AT outperforms the neural baseline model consistently , considering our experiments across multiple and more diverse datasets than typical related works .",result,Results,0,128,2,2,0,result : Results,0.9343065693430657,0.2857142857142857,0.2857142857142857
relation-classification,3,The im - provement of AT over our baseline ( depending on the dataset ) ranges from ? 0.4 % to ? 0.9 % in terms of over all F 1 score .,result,Results,0,129,3,3,0,result : Results,0.9416058394160584,0.42857142857142855,0.42857142857142855
relation-classification,3,"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component , which is in accordance with the recent advances in NER using neural networks that report similarly small gains ( e.g. , the performance improvement in and on the CoNLL - 2003 test set is 0.01 % and 0.17 % F 1 percentage points , while in the work of , a 0.07 % F 1 improvement on CoNLL - 2000 using AT for NER is reported ) .",result,Results,0,130,4,4,0,result : Results,0.948905109489051,0.5714285714285714,0.5714285714285714
relation-classification,3,"However , the relation extraction performance increases by ? 1 % F 1 scoring points , except for the ACE04 dataset .",result,Results,0,131,5,5,0,result : Results,0.9562043795620438,0.7142857142857143,0.7142857142857143
relation-classification,3,"Further , as seen in , the improvement for CoNLL04 is particularly small on the evaluation set .",result,Results,0,132,6,6,0,result : Results,0.9635036496350365,0.8571428571428571,0.8571428571428571
relation-classification,3,"This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models , but this needs further investigation in future work .",result,Results,0,133,7,7,0,result : Results,0.9708029197080292,1.0,1.0
relation-classification,3,Conclusion,conclusion,Conclusion,0,134,1,1,0,conclusion : Conclusion,0.9781021897810219,0.25,0.25
relation-classification,3,We proposed to use adversarial training ( AT ) for the joint task of entity recognition and relation extraction .,conclusion,Conclusion,0,135,2,2,0,conclusion : Conclusion,0.9854014598540146,0.5,0.5
relation-classification,3,"The contribution of this study is twofold : ( i ) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model , with ( ii ) a large scale experimental evaluation .",conclusion,Conclusion,0,136,3,3,0,conclusion : Conclusion,0.9927007299270073,0.75,0.75
relation-classification,3,"Experiments show that AT improves the results for each task separately , as well as the over all performance of the baseline joint model , while reaching high performance already during the first epochs of the training procedure .",conclusion,Conclusion,0,137,4,4,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,4,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,title,title,1,2,1,1,0,title : title,0.007633587786259542,1.0,1.0
relation-classification,4,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011450381679389313,0.1111111111111111,0.1111111111111111
relation-classification,4,Dependency trees help relation extraction models capture long - range relations between words .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.015267175572519083,0.2222222222222222,0.2222222222222222
relation-classification,4,"However , existing dependency - based models either neglect crucial information ( e.g. , negation ) by pruning the dependency trees too aggressively , or are computationally inefficient because it is difficult to parallelize over different tree structures .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.019083969465648856,0.3333333333333333,0.3333333333333333
relation-classification,4,"We propose an extension of graph convolutional networks that is tailored for relation extraction , which pools information over arbitrary dependency structures efficiently in parallel .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.022900763358778626,0.4444444444444444,0.4444444444444444
relation-classification,4,"To incorporate relevant information while maximally removing irrelevant content , we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.026717557251908396,0.5555555555555556,0.5555555555555556
relation-classification,4,"The resulting model achieves state - of - the - art performance on the large - scale TACRED dataset , outperforming existing sequence and dependency - based neural models .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.030534351145038167,0.6666666666666666,0.6666666666666666
relation-classification,4,"We also show through detailed analysis that this model has complementary strengths to sequence models , and combining them further improves the state of the art .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.03435114503816794,0.7777777777777778,0.7777777777777778
relation-classification,4,Equal contribution .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.03816793893129771,0.8888888888888888,0.8888888888888888
relation-classification,4,The order of authorship was decided by a tossed coin .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.04198473282442748,1.0,1.0
relation-classification,4,Introduction,introduction,introduction,0,12,1,1,0,introduction : introduction,0.04580152671755725,0.041666666666666664,0.041666666666666664
relation-classification,4,"Relation extraction involves discerning whether a relation exists between two entities in a sentence ( often termed subject and object , respectively ) .",introduction,introduction,0,13,2,2,0,introduction : introduction,0.04961832061068702,0.08333333333333333,0.08333333333333333
relation-classification,4,"Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale , such as question answering , knowledge base population , and biomedical knowledge discovery .",introduction,introduction,0,14,3,3,0,introduction : introduction,0.05343511450381679,0.125,0.125
relation-classification,4,"Models making use of dependency parses of the input sentences , or dependency - based models , : An example modified from the TAC KBP challenge corpus .",introduction,introduction,0,15,4,4,0,introduction : introduction,0.05725190839694656,0.16666666666666666,0.16666666666666666
relation-classification,4,"subtree of the original UD dependency tree between the subject ( "" he "" ) and object ( "" Mike Cane "" ) is also shown , where the shortest dependency path between the entities is highlighted in bold .",introduction,introduction,0,16,5,5,0,introduction : introduction,0.061068702290076333,0.20833333333333334,0.20833333333333334
relation-classification,4,"Note that negation ( "" not "" ) is off the dependency path .",introduction,introduction,0,17,6,6,0,introduction : introduction,0.0648854961832061,0.25,0.25
relation-classification,4,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",introduction,introduction,1,18,7,7,0,introduction : introduction,0.06870229007633588,0.2916666666666667,0.2916666666666667
relation-classification,4,Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,introduction,introduction,0,19,8,8,0,introduction : introduction,0.07251908396946564,0.3333333333333333,0.3333333333333333
relation-classification,4,"However , these models face the challenge of sparse feature spaces and are brittle to lexical variations .",introduction,introduction,0,20,9,9,0,introduction : introduction,0.07633587786259542,0.375,0.375
relation-classification,4,More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees .,introduction,introduction,0,21,10,10,0,introduction : introduction,0.08015267175572519,0.4166666666666667,0.4166666666666667
relation-classification,4,One common approach to leverage dependency information is to perform bottom - up or top - down computation along the parse tree or the subtree below the lowest common ancestor ( LCA ) of the entities .,introduction,introduction,0,22,11,11,0,introduction : introduction,0.08396946564885496,0.4583333333333333,0.4583333333333333
relation-classification,4,"Another popular approach , inspired by , is to reduce the parse tree to the shortest dependency path between the entities .",introduction,introduction,0,23,12,12,0,introduction : introduction,0.08778625954198473,0.5,0.5
relation-classification,4,"However , these models suffer from several drawbacks .",introduction,introduction,0,24,13,13,0,introduction : introduction,0.0916030534351145,0.5416666666666666,0.5416666666666666
relation-classification,4,"Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient , because aligning trees for efficient batch training is usually nontrivial .",introduction,introduction,0,25,14,14,0,introduction : introduction,0.09541984732824428,0.5833333333333334,0.5833333333333334
relation-classification,4,"Models based on the shortest dependency path between the subject and object are computationally more efficient , but this simplifying assumption has major limitations as well .",introduction,introduction,0,26,15,15,0,introduction : introduction,0.09923664122137404,0.625,0.625
relation-classification,4,"shows a real - world example where crucial information ( i.e. , negation ) would be excluded when the model is restricted to only considering the dependency path .",introduction,introduction,0,27,16,16,0,introduction : introduction,0.10305343511450382,0.6666666666666666,0.6666666666666666
relation-classification,4,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .",introduction,introduction,0,28,17,17,0,introduction : introduction,0.10687022900763359,0.7083333333333334,0.7083333333333334
relation-classification,4,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",introduction,introduction,1,29,18,18,0,introduction : introduction,0.11068702290076336,0.75,0.75
relation-classification,4,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",introduction,introduction,1,30,19,19,0,introduction : introduction,0.11450381679389313,0.7916666666666666,0.7916666666666666
relation-classification,4,"We test our model on the popular SemEval 2010 Task 8 dataset and the more recent , larger TAC - RED dataset .",introduction,introduction,0,31,20,20,0,introduction : introduction,0.1183206106870229,0.8333333333333334,0.8333333333333334
relation-classification,4,"On both datasets , our model not only outperforms existing dependency - based neural models by a significant margin when combined with the new pruning technique , but also achieves a 10 - 100x speedup over existing tree - based models .",introduction,introduction,0,32,21,21,0,introduction : introduction,0.12213740458015267,0.875,0.875
relation-classification,4,"On TACRED , our model further achieves the state - of - the - art performance , surpassing a competitive neural sequence model baseline .",introduction,introduction,0,33,22,22,0,introduction : introduction,0.12595419847328243,0.9166666666666666,0.9166666666666666
relation-classification,4,"This model also exhibits complementary strengths to sequence models on TACRED , and combining these two model types through simple prediction interpolation further improves the state of the art .",introduction,introduction,0,34,23,23,0,introduction : introduction,0.1297709923664122,0.9583333333333334,0.9583333333333334
relation-classification,4,"To recap , our main contributions are : ( i ) we propose a neural model for relation extraction based on graph convolutional networks , which allows it to efficiently pool information over arbitrary dependency structures ; ( ii ) we present a new pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness ; ( iii ) we present detailed analysis on the model and the pruning technique , and show that dependency - based models have complementary strengths with sequence models .",introduction,introduction,0,35,24,24,0,introduction : introduction,0.13358778625954199,1.0,1.0
relation-classification,4,Models,model,Models,0,36,1,1,0,model : Models,0.13740458015267176,0.014925373134328358,0.5
relation-classification,4,"In this section , we first describe graph convolutional networks ( GCNs ) over dependency tree structures , and then we introduce an architecture that uses GCNs at its core for relation extraction .",model,Models,0,37,2,2,0,model : Models,0.14122137404580154,0.029850746268656716,1.0
relation-classification,4,Graph Convolutional Networks over Dependency Trees,model,Graph Convolutional Networks over Dependency Trees,0,38,3,1,0,model : Graph Convolutional Networks over Dependency Trees,0.1450381679389313,0.04477611940298507,0.043478260869565216
relation-classification,4,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs .,model,Graph Convolutional Networks over Dependency Trees,0,39,4,2,0,model : Graph Convolutional Networks over Dependency Trees,0.14885496183206107,0.05970149253731343,0.08695652173913043
relation-classification,4,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",model,Graph Convolutional Networks over Dependency Trees,0,40,5,3,0,model : Graph Convolutional Networks over Dependency Trees,0.15267175572519084,0.07462686567164178,0.13043478260869565
relation-classification,4,"In an L-layer GCN , if we denote by h ( l?1 ) i the input vector and h ( l ) i the output vector of node i at the l - th layer , a graph convolution operation can be written as",model,Graph Convolutional Networks over Dependency Trees,0,41,6,4,0,model : Graph Convolutional Networks over Dependency Trees,0.15648854961832062,0.08955223880597014,0.17391304347826086
relation-classification,4,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ? a nonlinear function ( e.g. , ReLU ) .",model,Graph Convolutional Networks over Dependency Trees,0,42,7,5,0,model : Graph Convolutional Networks over Dependency Trees,0.16030534351145037,0.1044776119402985,0.21739130434782608
relation-classification,4,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ? a nonlinear function ( e.g. , ReLU ) .",model,Graph Convolutional Networks over Dependency Trees,0,43,8,6,0,model : Graph Convolutional Networks over Dependency Trees,0.16412213740458015,0.11940298507462686,0.2608695652173913
relation-classification,4,"Intuitively , during each graph convolution , each node gathers and summarizes information from its neighboring nodes in the graph .",model,Graph Convolutional Networks over Dependency Trees,0,44,9,7,0,model : Graph Convolutional Networks over Dependency Trees,0.16793893129770993,0.13432835820895522,0.30434782608695654
relation-classification,4,"We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A , where A ij = 1 if there is a dependency edge between tokens i and j .",model,Graph Convolutional Networks over Dependency Trees,0,45,10,8,0,model : Graph Convolutional Networks over Dependency Trees,0.1717557251908397,0.14925373134328357,0.34782608695652173
relation-classification,4,"However , naively applying the graph convolution operation in Equation ( 1 ) could lead to node representations with drastically different magnitudes , since the degree of a token varies a lot .",model,Graph Convolutional Networks over Dependency Trees,0,46,11,9,0,model : Graph Convolutional Networks over Dependency Trees,0.17557251908396945,0.16417910447761194,0.391304347826087
relation-classification,4,This could bias our sentence representation towards favoring high - degree nodes regardless of the information carried in the node ( see details in Section 2.2 ) .,model,Graph Convolutional Networks over Dependency Trees,0,47,12,10,0,model : Graph Convolutional Networks over Dependency Trees,0.17938931297709923,0.1791044776119403,0.43478260869565216
relation-classification,4,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .",model,Graph Convolutional Networks over Dependency Trees,0,48,13,11,0,model : Graph Convolutional Networks over Dependency Trees,0.183206106870229,0.19402985074626866,0.4782608695652174
relation-classification,4,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",model,Graph Convolutional Networks over Dependency Trees,0,49,14,12,0,model : Graph Convolutional Networks over Dependency Trees,0.18702290076335878,0.208955223880597,0.5217391304347826
relation-classification,4,"where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",model,Graph Convolutional Networks over Dependency Trees,0,50,15,13,0,model : Graph Convolutional Networks over Dependency Trees,0.19083969465648856,0.22388059701492538,0.5652173913043478
relation-classification,4,Relation extraction with a graph convolutional network .,model,Graph Convolutional Networks over Dependency Trees,0,51,16,14,0,model : Graph Convolutional Networks over Dependency Trees,0.1946564885496183,0.23880597014925373,0.6086956521739131
relation-classification,4,"The left side shows the over all architecture , while on the right side , we only show the detailed graph convolution computation for the word "" relative "" for clarity .",model,Graph Convolutional Networks over Dependency Trees,0,52,17,15,0,model : Graph Convolutional Networks over Dependency Trees,0.1984732824427481,0.2537313432835821,0.6521739130434783
relation-classification,4,full unlabeled dependency parse of the sentence is also provided for reference .,model,Graph Convolutional Networks over Dependency Trees,0,53,18,16,0,model : Graph Convolutional Networks over Dependency Trees,0.20229007633587787,0.26865671641791045,0.6956521739130435
relation-classification,4,Stacking this operation over,model,Graph Convolutional Networks over Dependency Trees,0,54,19,17,0,model : Graph Convolutional Networks over Dependency Trees,0.20610687022900764,0.2835820895522388,0.7391304347826086
relation-classification,4,"layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .",model,Graph Convolutional Networks over Dependency Trees,0,55,20,18,0,model : Graph Convolutional Networks over Dependency Trees,0.2099236641221374,0.29850746268656714,0.782608695652174
relation-classification,4,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,model,Graph Convolutional Networks over Dependency Trees,0,56,21,19,0,model : Graph Convolutional Networks over Dependency Trees,0.21374045801526717,0.31343283582089554,0.8260869565217391
relation-classification,4,"We also experimented with : ( 1 ) using different transformation matrices W for topdown , bottom - up , and self - loop edges ; and ( 2 ) adding dependency relation - specific parameters for edge - wise gating , similar to .",model,Graph Convolutional Networks over Dependency Trees,0,57,22,20,0,model : Graph Convolutional Networks over Dependency Trees,0.21755725190839695,0.3283582089552239,0.8695652173913043
relation-classification,4,"We found that modeling directions does not lead to improvement , 1 and adding edgewise gating further hurts performance .",model,Graph Convolutional Networks over Dependency Trees,0,58,23,21,0,model : Graph Convolutional Networks over Dependency Trees,0.22137404580152673,0.34328358208955223,0.9130434782608695
relation-classification,4,"We hypothesize that this is because the presented GCN model is usually already able to capture dependency edge patterns that are informative for classifying relations , and modeling edge directions and types does not offer additional discriminative power to the network before it leads to overfitting .",model,Graph Convolutional Networks over Dependency Trees,0,59,24,22,0,model : Graph Convolutional Networks over Dependency Trees,0.22519083969465647,0.3582089552238806,0.9565217391304348
relation-classification,4,"For example , the relations entailed by "" A 's son , B "" and "" B 's son , A "" can be readily distinguished with "" 's "" attached to different entities , even when edge directionality is not considered .",model,Graph Convolutional Networks over Dependency Trees,0,60,25,23,0,model : Graph Convolutional Networks over Dependency Trees,0.22900763358778625,0.373134328358209,1.0
relation-classification,4,Encoding Relations with GCN,model,Encoding Relations with GCN,0,61,26,1,0,model : Encoding Relations with GCN,0.23282442748091603,0.3880597014925373,0.25
relation-classification,4,We now formally define the task of relation extraction .,model,Encoding Relations with GCN,0,62,27,2,0,model : Encoding Relations with GCN,0.2366412213740458,0.40298507462686567,0.5
relation-classification,4,"Let X = [ x 1 , ... , x n ] denote a sentence , where xi is the i th token .",model,Encoding Relations with GCN,0,63,28,3,0,model : Encoding Relations with GCN,0.24045801526717558,0.417910447761194,0.75
relation-classification,4,subject entity and an object entity are identified and correspond to two spans in the sentence :,model,Encoding Relations with GCN,0,64,29,4,0,model : Encoding Relations with GCN,0.24427480916030533,0.43283582089552236,1.0
relation-classification,4,.,model,Contextualized GCN,0,65,30,1,0,model : Contextualized GCN,0.2480916030534351,0.44776119402985076,0.041666666666666664
relation-classification,4,"Given X , X s , and X o , the goal of relation extraction is to predict a relation r ? R ( a predefined relation set ) that holds between the entities or "" no relation "" otherwise .",model,Contextualized GCN,0,66,31,2,0,model : Contextualized GCN,0.25190839694656486,0.4626865671641791,0.08333333333333333
relation-classification,4,"Given X , X s , and X o , the goal of relation extraction is to predict a relation r ? R ( a predefined relation set ) that holds between the entities or "" no relation "" otherwise .",model,Contextualized GCN,0,67,32,3,0,model : Contextualized GCN,0.25572519083969464,0.47761194029850745,0.125
relation-classification,4,"After applying an L-layer GCN over word vectors , we obtain hidden representations of each token that are directly influenced by its neighbors no more than L edges apart in the dependency tree .",model,Contextualized GCN,0,68,33,4,0,model : Contextualized GCN,0.2595419847328244,0.4925373134328358,0.16666666666666666
relation-classification,4,"To make use of these word representations for relation extraction , we first obtain a sentence representation as follows ( see also left ) :",model,Contextualized GCN,0,69,34,5,0,model : Contextualized GCN,0.2633587786259542,0.5074626865671642,0.20833333333333334
relation-classification,4,"where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ? Rd is a max pooling function that maps from n output vectors to the sentence vector .",model,Contextualized GCN,0,70,35,6,0,model : Contextualized GCN,0.26717557251908397,0.5223880597014925,0.25
relation-classification,4,"where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ? Rd is a max pooling function that maps from n output vectors to the sentence vector .",model,Contextualized GCN,0,71,36,7,0,model : Contextualized GCN,0.27099236641221375,0.5373134328358209,0.2916666666666667
relation-classification,4,We also observe that information close to entity tokens in the dependency tree is often central to relation classification .,model,Contextualized GCN,0,72,37,8,0,model : Contextualized GCN,0.2748091603053435,0.5522388059701493,0.3333333333333333
relation-classification,4,"Therefore , we also obtain a subject representation h s from h ( L ) as follows",model,Contextualized GCN,0,73,38,9,0,model : Contextualized GCN,0.2786259541984733,0.5671641791044776,0.375
relation-classification,4,as well as an object representation ho similarly .,model,Contextualized GCN,0,74,39,10,0,model : Contextualized GCN,0.2824427480916031,0.582089552238806,0.4166666666666667
relation-classification,4,"Inspired by recent work on relational learning between entities , we obtain the final representation used for classification by concatenating the sentence and the entity representations , and feeding them through a feed - forward neural network ( FFNN ) :",model,Contextualized GCN,0,75,40,11,0,model : Contextualized GCN,0.2862595419847328,0.5970149253731343,0.4583333333333333
relation-classification,4,This h final representation is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over relations .,model,Contextualized GCN,0,76,41,12,0,model : Contextualized GCN,0.2900763358778626,0.6119402985074627,0.5
relation-classification,4,Contextualized GCN,model,Contextualized GCN,0,77,42,13,0,model : Contextualized GCN,0.29389312977099236,0.6268656716417911,0.5416666666666666
relation-classification,4,"The network architecture introduced so far learns effective representations for relation extraction , but it also leaves a few issues inadequately addressed .",model,Contextualized GCN,0,78,43,14,0,model : Contextualized GCN,0.29770992366412213,0.6417910447761194,0.5833333333333334
relation-classification,4,"First , the input word vectors do not contain contextual information about word order or dis ambiguation .",model,Contextualized GCN,0,79,44,15,0,model : Contextualized GCN,0.3015267175572519,0.6567164179104478,0.625
relation-classification,4,"Second , the GCN highly depends on a correct parse tree to extract crucial information from the sentence ( especially when pruning is performed ) , while existing parsing algorithms produce imperfect trees in many cases .",model,Contextualized GCN,0,80,45,16,0,model : Contextualized GCN,0.3053435114503817,0.6716417910447762,0.6666666666666666
relation-classification,4,"To resolve these issues , we further apply a Contextualized GCN ( C - GCN ) model , where the input word vectors are first fed into a bi-directional long short - term memory ( LSTM ) network to generate contextualized representations , which are then used ash ( 0 ) in the original model .",model,Contextualized GCN,0,81,46,17,0,model : Contextualized GCN,0.30916030534351147,0.6865671641791045,0.7083333333333334
relation-classification,4,This BiL - STM contextualization layer is trained jointly with the rest of the network .,model,Contextualized GCN,0,82,47,18,0,model : Contextualized GCN,0.31297709923664124,0.7014925373134329,0.75
relation-classification,4,We show empirically in Section 5 that this augmentation substantially improves the performance over the original model .,model,Contextualized GCN,0,83,48,19,0,model : Contextualized GCN,0.31679389312977096,0.7164179104477612,0.7916666666666666
relation-classification,4,"We note that this relation extraction model is conceptually similar to graph kernel - based models , in that it aims to utilize local dependency tree patterns to inform relation classification .",model,Contextualized GCN,0,84,49,20,0,model : Contextualized GCN,0.32061068702290074,0.7313432835820896,0.8333333333333334
relation-classification,4,"Our model also incorporates crucial off - path information , which greatly improves its robustness compared to shortest dependency pathbased approaches .",model,Contextualized GCN,0,85,50,21,0,model : Contextualized GCN,0.3244274809160305,0.746268656716418,0.875
relation-classification,4,"Compared to tree - structured models ( e.g. , Tree - LSTM",model,Contextualized GCN,0,86,51,22,0,model : Contextualized GCN,0.3282442748091603,0.7611940298507462,0.9166666666666666
relation-classification,4,"Tai et al. , 2015 ) ) , it not only is able to capture more global information through the use of pooling functions , but also achieves substantial speedup by not requiring recursive operations that are difficult to parallelize .",model,Contextualized GCN,0,87,52,23,0,model : Contextualized GCN,0.3320610687022901,0.7761194029850746,0.9583333333333334
relation-classification,4,"For example , we observe that on a Titan Xp GPU , training a Tree - LSTM model over a minibatch of 50 examples takes 6.54 seconds on average , while training the original GCN model takes only 0.07 seconds , and the C - GCN model 0.08 seconds .",model,Contextualized GCN,0,88,53,24,0,model : Contextualized GCN,0.33587786259541985,0.7910447761194029,1.0
relation-classification,4,Incorporating Off - path Information with Path - centric Pruning,model,Incorporating Off-path Information with Path-centric Pruning,0,89,54,1,0,model : Incorporating Off-path Information with Path-centric Pruning,0.33969465648854963,0.8059701492537313,0.07142857142857142
relation-classification,4,"Dependency trees provide rich structures that one can exploit in relation extraction , but most of the information pertinent to relations is usually contained within the subtree rooted at the lowest common ancestor ( LCA ) of the two entities .",model,Incorporating Off-path Information with Path-centric Pruning,0,90,55,2,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3435114503816794,0.8208955223880597,0.14285714285714285
relation-classification,4,Previous studies have shown that removing tokens outside this scope helps relation extraction by eliminating irrelevant information from the sentence .,model,Incorporating Off-path Information with Path-centric Pruning,0,91,56,3,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3473282442748092,0.835820895522388,0.21428571428571427
relation-classification,4,It is therefore desirable to combine our GCN models with tree pruning strategies to further improve performance .,model,Incorporating Off-path Information with Path-centric Pruning,0,92,57,4,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3511450381679389,0.8507462686567164,0.2857142857142857
relation-classification,4,"However , pruning too aggressively ( e.g. , keeping only the dependency path ) could lead to loss of crucial information and conversely hurt robustness .",model,Incorporating Off-path Information with Path-centric Pruning,0,93,58,5,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3549618320610687,0.8656716417910447,0.35714285714285715
relation-classification,4,"For instance , the negation in is neglected when a model is restricted to only looking at the dependency path between the entities .",model,Incorporating Off-path Information with Path-centric Pruning,0,94,59,6,0,model : Incorporating Off-path Information with Path-centric Pruning,0.35877862595419846,0.8805970149253731,0.42857142857142855
relation-classification,4,"Similarly , in the sentence "" She was diagnosed with cancer last year , and succumbed this June "" , the dependency path She?diagnosed ? cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction dependency to succumbed is also present .",model,Incorporating Off-path Information with Path-centric Pruning,0,95,60,7,0,model : Incorporating Off-path Information with Path-centric Pruning,0.36259541984732824,0.8955223880597015,0.5
relation-classification,4,"Similarly , in the sentence "" She was diagnosed with cancer last year , and succumbed this June "" , the dependency path She?diagnosed ? cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction dependency to succumbed is also present .",model,Incorporating Off-path Information with Path-centric Pruning,0,96,61,8,0,model : Incorporating Off-path Information with Path-centric Pruning,0.366412213740458,0.9104477611940298,0.5714285714285714
relation-classification,4,"Motivated by these observations , we propose path - centric pruning , a novel technique to incorporate information off the dependency path .",model,Incorporating Off-path Information with Path-centric Pruning,0,97,62,9,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3702290076335878,0.9253731343283582,0.6428571428571429
relation-classification,4,This is achieved by including tokens that are up to distance K away from the dependency path in the LCA subtree .,model,Incorporating Off-path Information with Path-centric Pruning,0,98,63,10,0,model : Incorporating Off-path Information with Path-centric Pruning,0.37404580152671757,0.9402985074626866,0.7142857142857143
relation-classification,4,"= 0 , corresponds to pruning the tree down to the path , K = 1 keeps all nodes that are directly attached to the path , and K = ? retains the entire LCA subtree .",model,Incorporating Off-path Information with Path-centric Pruning,0,99,64,11,0,model : Incorporating Off-path Information with Path-centric Pruning,0.37786259541984735,0.9552238805970149,0.7857142857142857
relation-classification,4,"= 0 , corresponds to pruning the tree down to the path , K = 1 keeps all nodes that are directly attached to the path , and K = ? retains the entire LCA subtree .",model,Incorporating Off-path Information with Path-centric Pruning,0,100,65,12,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3816793893129771,0.9701492537313433,0.8571428571428571
relation-classification,4,"We combine this pruning strategy with our GCN model , by directly feeding the pruned trees into the graph convolutional layers .",model,Incorporating Off-path Information with Path-centric Pruning,0,101,66,13,0,model : Incorporating Off-path Information with Path-centric Pruning,0.38549618320610685,0.9850746268656716,0.9285714285714286
relation-classification,4,"We show that pruning with K = 1 achieves the best balance between including relevant information ( e.g. , negation and conjunction ) and keeping irrelevant content out of the resulting pruned tree as much as possible .",model,Incorporating Off-path Information with Path-centric Pruning,0,102,67,14,0,model : Incorporating Off-path Information with Path-centric Pruning,0.3893129770992366,1.0,1.0
relation-classification,4,Related Work,related work,Related Work,0,103,1,1,0,related work : Related Work,0.3931297709923664,0.05555555555555555,0.05555555555555555
relation-classification,4,"At the core of fully - supervised and distantlysupervised relation extraction approaches are statistical classifiers , many of which find syntactic information beneficial .",related work,Related Work,0,104,2,2,0,related work : Related Work,0.3969465648854962,0.1111111111111111,0.1111111111111111
relation-classification,4,"For example , explored adding syntactic features to a statistical classifier and found them to be useful when sentences are long .",related work,Related Work,0,105,3,3,0,related work : Related Work,0.40076335877862596,0.16666666666666666,0.16666666666666666
relation-classification,4,"Various kernel - based approaches also leverage syntactic information to measure similarity between training and test examples to predict the relation , finding that tree - based kernels and dependency path - based kernels ( Bunescu and Mooney , 2005 ) are effective for this task .",related work,Related Work,0,106,4,4,0,related work : Related Work,0.40458015267175573,0.2222222222222222,0.2222222222222222
relation-classification,4,Recent studies have found neural models effective in relation extraction .,related work,Related Work,0,107,5,5,0,related work : Related Work,0.4083969465648855,0.2777777777777778,0.2777777777777778
relation-classification,4,first applied a one - dimensional convolutional neural network ( CNN ) with manual features to encode relations .,related work,Related Work,0,108,6,6,0,related work : Related Work,0.4122137404580153,0.3333333333333333,0.3333333333333333
relation-classification,4,showed that combining a CNN with a recurrent neural network ( RNN ) through a voting scheme can further improve performance .,related work,Related Work,0,109,7,7,0,related work : Related Work,0.41603053435114506,0.3888888888888889,0.3888888888888889
relation-classification,4,and proposed to use attention mechanisms over RNN and CNN architectures for this task .,related work,Related Work,0,110,8,8,0,related work : Related Work,0.4198473282442748,0.4444444444444444,0.4444444444444444
relation-classification,4,"Apart from neural models over word sequences , incorporating dependency trees into neural models has also been shown to improve relation extraction performance by capturing long - distance relations .",related work,Related Work,0,111,9,9,0,related work : Related Work,0.42366412213740456,0.5,0.5
relation-classification,4,generalized the idea of dependency path kernels by applying a LSTM network over the shortest dependency path between entities .,related work,Related Work,0,112,10,10,0,related work : Related Work,0.42748091603053434,0.5555555555555556,0.5555555555555556
relation-classification,4,Liu et al. first applied a recursive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path .,related work,Related Work,0,113,11,11,0,related work : Related Work,0.4312977099236641,0.6111111111111112,0.6111111111111112
relation-classification,4,"Miwa and Bansal ( 2016 ) applied a Tree - LSTM , a generalized form of LSTM over dependency trees , in a joint entity and relation extraction setting .",related work,Related Work,0,114,12,12,0,related work : Related Work,0.4351145038167939,0.6666666666666666,0.6666666666666666
relation-classification,4,They found it to be most effective when applied to the subtree rooted at the LCA of the two entities .,related work,Related Work,0,115,13,13,0,related work : Related Work,0.4389312977099237,0.7222222222222222,0.7222222222222222
relation-classification,4,"More recently , and have shown that relatively simple neural models ( CNN and augmented LSTM , respectively ) can achieve comparable or superior performance to dependency - based models when trained on larger datasets .",related work,Related Work,0,116,14,14,0,related work : Related Work,0.44274809160305345,0.7777777777777778,0.7777777777777778
relation-classification,4,"In this paper , we study dependency - based models in depth and show that with a properly designed architecture , they can outperform and have complementary advantages to sequence models , even in a large - scale setting .",related work,Related Work,0,117,15,15,0,related work : Related Work,0.44656488549618323,0.8333333333333334,0.8333333333333334
relation-classification,4,"Finally , we note that a technique similar to pathcentric pruning has been applied to reduce the space of possible arguments in semantic role labeling .",related work,Related Work,0,118,16,16,0,related work : Related Work,0.45038167938931295,0.8888888888888888,0.8888888888888888
relation-classification,4,"The authors showed pruning words too faraway from the path between the predicate and the root to be beneficial , but reported the best pruning distance to be 10 , which almost always retains the entire tree .",related work,Related Work,0,119,17,17,0,related work : Related Work,0.4541984732824427,0.9444444444444444,0.9444444444444444
relation-classification,4,"Our method differs in that it is applied to the shortest dependency path between entities , and we show that in our technique the best pruning distance is 1 for several dependency - based relation extraction models .",related work,Related Work,0,120,18,18,0,related work : Related Work,0.4580152671755725,1.0,1.0
relation-classification,4,Experiments,experiment,Experiments,0,121,1,1,0,experiment : Experiments,0.4618320610687023,1.0,1.0
relation-classification,4,Baseline Models,model,Baseline Models,0,122,1,1,0,model : Baseline Models,0.46564885496183206,0.07142857142857142,0.07142857142857142
relation-classification,4,We compare our models with several competitive dependency - based and neural sequence models .,model,Baseline Models,0,123,2,2,0,model : Baseline Models,0.46946564885496184,0.14285714285714285,0.14285714285714285
relation-classification,4,Dependency - based models .,model,Baseline Models,1,124,3,3,0,model : Baseline Models,0.4732824427480916,0.21428571428571427,0.21428571428571427
relation-classification,4,In our main experiments we compare with three types of dependency - based models .,model,Baseline Models,0,125,4,4,0,model : Baseline Models,0.4770992366412214,0.2857142857142857,0.2857142857142857
relation-classification,4,1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,model,Baseline Models,1,126,5,5,0,model : Baseline Models,0.48091603053435117,0.35714285714285715,0.35714285714285715
relation-classification,4,"2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .",model,Baseline Models,1,127,6,6,0,model : Baseline Models,0.4847328244274809,0.42857142857142855,0.42857142857142855
relation-classification,4,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",model,Baseline Models,1,128,7,7,0,model : Baseline Models,0.48854961832061067,0.5,0.5
relation-classification,4,"We investigate the child - sum variant of Tree - LSTM , and apply it to the dependency tree ( or part of it ) .",model,Baseline Models,0,129,8,8,0,model : Baseline Models,0.49236641221374045,0.5714285714285714,0.5714285714285714
relation-classification,4,"In practice , we find that modifying this model by concatenating dependency label embeddings to the input of forget gates improves its performance on relation extraction , and therefore use this variant in our experiments .",model,Baseline Models,0,130,9,9,0,model : Baseline Models,0.4961832061068702,0.6428571428571429,0.6428571428571429
relation-classification,4,"Earlier , our group compared and with sequence models , and we report these results ; for ( 3 ) we report results with our own implementation .",model,Baseline Models,0,131,10,10,0,model : Baseline Models,0.5,0.7142857142857143,0.7142857142857143
relation-classification,4,Neural sequence model .,model,Baseline Models,1,132,11,11,0,model : Baseline Models,0.5038167938931297,0.7857142857142857,0.7857142857142857
relation-classification,4,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .",model,Baseline Models,1,133,12,12,0,model : Baseline Models,0.5076335877862596,0.8571428571428571,0.8571428571428571
relation-classification,4,"We compare with this strong baseline , and use its open implementation in further analysis .",model,Baseline Models,0,134,13,13,0,model : Baseline Models,0.5114503816793893,0.9285714285714286,0.9285714285714286
relation-classification,4,3,model,Baseline Models,0,135,14,14,0,model : Baseline Models,0.5152671755725191,1.0,1.0
relation-classification,4,Experimental Setup,experiment,Experimental Setup,0,136,1,1,0,experiment : Experimental Setup,0.5190839694656488,0.023809523809523808,0.07692307692307693
relation-classification,4,We conduct experiments on two relation extraction datasets :,experiment,Experimental Setup,0,137,2,2,0,experiment : Experimental Setup,0.5229007633587787,0.047619047619047616,0.15384615384615385
relation-classification,4,"1 ) TACRED : Introduced in , TACRED contains over 106 k mention pairs drawn from the yearly TAC KBP 4 challenge .",experiment,Experimental Setup,0,138,3,3,0,experiment : Experimental Setup,0.5267175572519084,0.07142857142857142,0.23076923076923078
relation-classification,4,It represents 41 relation types and a special no relation class when the mention pair does not have a relation between them within these categories .,experiment,Experimental Setup,0,139,4,4,0,experiment : Experimental Setup,0.5305343511450382,0.09523809523809523,0.3076923076923077
relation-classification,4,"Mentions in TACRED are typed , with subjects categorized into person and organization , and objects into 16 fine - grained types ( e.g. , date and location ) .",experiment,Experimental Setup,0,140,5,5,0,experiment : Experimental Setup,0.5343511450381679,0.11904761904761904,0.38461538461538464
relation-classification,4,We report micro-averaged F 1 scores on this dataset as is conventional .,experiment,Experimental Setup,0,141,6,6,0,experiment : Experimental Setup,0.5381679389312977,0.14285714285714285,0.46153846153846156
relation-classification,4,"For fair comparisons on the TACRED dataset , we follow the evaluation protocol used in by selecting the model with the median dev F 1 from 5 independent runs and reporting its test F 1 .",experiment,Experimental Setup,0,142,7,7,0,experiment : Experimental Setup,0.5419847328244275,0.16666666666666666,0.5384615384615384
relation-classification,4,"We also use the same "" entity mask "" strategy where we replace each subject ( and object similarly ) entity with a special SUBJ - < NER > token .",experiment,Experimental Setup,0,143,8,8,0,experiment : Experimental Setup,0.5458015267175572,0.19047619047619047,0.6153846153846154
relation-classification,4,"For all models , we also adopt the "" multichannel "" strategy by concatenating the input word embeddings with POS and NER embeddings .",experiment,Experimental Setup,0,144,9,9,0,experiment : Experimental Setup,0.549618320610687,0.21428571428571427,0.6923076923076923
relation-classification,4,"Traditionally , evaluation on SemEval is conducted without entity mentions masked .",experiment,Experimental Setup,0,145,10,10,0,experiment : Experimental Setup,0.5534351145038168,0.23809523809523808,0.7692307692307693
relation-classification,4,"However , as we will discuss in Section 6.4 , this method encourages models to overfit to these mentions and fails to test their actual ability to generalize .",experiment,Experimental Setup,0,146,11,11,0,experiment : Experimental Setup,0.5572519083969466,0.2619047619047619,0.8461538461538461
relation-classification,4,"We therefore report results with two evaluation protocols : ( 1 ) with- mention , where mentions are kept for comparison with previous work ; and ( 2 ) maskmention , where they are masked to test the generalization of our model in a more realistic setting .",experiment,Experimental Setup,0,147,12,12,0,experiment : Experimental Setup,0.5610687022900763,0.2857142857142857,0.9230769230769231
relation-classification,4,"Due to space limitations , we report model training details in the supplementary material .",experiment,Experimental Setup,0,148,13,13,0,experiment : Experimental Setup,0.5648854961832062,0.30952380952380953,1.0
relation-classification,4,Results on the TACRED Dataset,experiment,Results on the TACRED Dataset,1,149,14,1,0,experiment : Results on the TACRED Dataset,0.5687022900763359,0.3333333333333333,0.07142857142857142
relation-classification,4,We present our main results on the TACRED test set in .,experiment,Results on the TACRED Dataset,0,150,15,2,0,experiment : Results on the TACRED Dataset,0.5725190839694656,0.35714285714285715,0.14285714285714285
relation-classification,4,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,experiment,Results on the TACRED Dataset,1,151,16,3,0,experiment : Results on the TACRED Dataset,0.5763358778625954,0.38095238095238093,0.21428571428571427
relation-classification,4,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .",experiment,Results on the TACRED Dataset,1,152,17,4,0,experiment : Results on the TACRED Dataset,0.5801526717557252,0.40476190476190477,0.2857142857142857
relation-classification,4,"In addition , we find our model improves upon other dependencybased models in both precision and recall .",experiment,Results on the TACRED Dataset,1,153,18,5,0,experiment : Results on the TACRED Dataset,0.583969465648855,0.42857142857142855,0.35714285714285715
relation-classification,4,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .",experiment,Results on the TACRED Dataset,1,154,19,6,0,experiment : Results on the TACRED Dataset,0.5877862595419847,0.4523809523809524,0.42857142857142855
relation-classification,4,We hypothesize that this is because the C - GCN is more robust to parse errors by capturing local word patterns ( see also Section 6.2 ) .,experiment,Results on the TACRED Dataset,0,155,20,7,0,experiment : Results on the TACRED Dataset,0.5916030534351145,0.47619047619047616,0.5
relation-classification,4,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .",experiment,Results on the TACRED Dataset,1,156,21,8,0,experiment : Results on the TACRED Dataset,0.5954198473282443,0.5,0.5714285714285714
relation-classification,4,"To leverage this result , we experiment with a simple interpolation strategy to combine these models .",experiment,Results on the TACRED Dataset,0,157,22,9,0,experiment : Results on the TACRED Dataset,0.5992366412213741,0.5238095238095238,0.6428571428571429
relation-classification,4,"Given the output probabilities PG ( r|x ) from a GCN model and PS ( r|x ) from the sequence model for any relation r , we calculate the interpolated probability as",experiment,Results on the TACRED Dataset,0,158,23,10,0,experiment : Results on the TACRED Dataset,0.6030534351145038,0.5476190476190477,0.7142857142857143
relation-classification,4,"where ? ? [ 0 , 1 ] is chosen on the dev set and set to 0.6 .",experiment,Results on the TACRED Dataset,0,159,24,11,0,experiment : Results on the TACRED Dataset,0.6068702290076335,0.5714285714285714,0.7857142857142857
relation-classification,4,"where ? ? [ 0 , 1 ] is chosen on the dev set and set to 0.6 .",experiment,Results on the TACRED Dataset,0,160,25,12,0,experiment : Results on the TACRED Dataset,0.6106870229007634,0.5952380952380952,0.8571428571428571
relation-classification,4,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .",experiment,Results on the TACRED Dataset,0,161,26,13,0,experiment : Results on the TACRED Dataset,0.6145038167938931,0.6190476190476191,0.9285714285714286
relation-classification,4,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,experiment,Results on the TACRED Dataset,0,162,27,14,0,experiment : Results on the TACRED Dataset,0.6183206106870229,0.6428571428571429,1.0
relation-classification,4,Results on the SemEval Dataset,experiment,Results on the SemEval Dataset,1,163,28,1,0,experiment : Results on the SemEval Dataset,0.6221374045801527,0.6666666666666666,0.2
relation-classification,4,"To study the generalizability of our proposed model , we also trained and evaluated our best C - GCN model on the SemEval test set ) .",experiment,Results on the SemEval Dataset,0,164,29,2,0,experiment : Results on the SemEval Dataset,0.6259541984732825,0.6904761904761905,0.4
relation-classification,4,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .",experiment,Results on the SemEval Dataset,1,165,30,3,0,experiment : Results on the SemEval Dataset,0.6297709923664122,0.7142857142857143,0.6
relation-classification,4,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .",experiment,Results on the SemEval Dataset,1,166,31,4,0,experiment : Results on the SemEval Dataset,0.6335877862595419,0.7380952380952381,0.8
relation-classification,4,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .",experiment,Results on the SemEval Dataset,1,167,32,5,0,experiment : Results on the SemEval Dataset,0.6374045801526718,0.7619047619047619,1.0
relation-classification,4,Effect of Path - centric Pruning,experiment,Effect of Path-centric Pruning,1,168,33,1,0,experiment : Effect of Path-centric Pruning,0.6412213740458015,0.7857142857142857,0.1111111111111111
relation-classification,4,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .",experiment,Effect of Path-centric Pruning,1,169,34,2,0,experiment : Effect of Path-centric Pruning,0.6450381679389313,0.8095238095238095,0.2222222222222222
relation-classification,4,"We experimented with K ? { 0 , 1 , 2 , ?} on the TACRED dev set , and also include results when the full tree is used .",experiment,Effect of Path-centric Pruning,0,170,35,3,0,experiment : Effect of Path-centric Pruning,0.648854961832061,0.8333333333333334,0.3333333333333333
relation-classification,4,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .",experiment,Effect of Path-centric Pruning,1,171,36,4,0,experiment : Effect of Path-centric Pruning,0.6526717557251909,0.8571428571428571,0.4444444444444444
relation-classification,4,This confirms our hypothesis in Section 3 that incorporating off - path information is crucial to relation extraction .,experiment,Effect of Path-centric Pruning,0,172,37,5,0,experiment : Effect of Path-centric Pruning,0.6564885496183206,0.8809523809523809,0.5555555555555556
relation-classification,4,Miwa and Bansal ( 2016 ) reported that a Tree - LSTM achieves similar performance when the dependency path and the LCA subtree are used respectively .,experiment,Effect of Path-centric Pruning,0,173,38,6,0,experiment : Effect of Path-centric Pruning,0.6603053435114504,0.9047619047619048,0.6666666666666666
relation-classification,4,"Our experiments confirm this , and further show that the result can be improved by path - centric pruning with K = 1 .",experiment,Effect of Path-centric Pruning,0,174,39,7,0,experiment : Effect of Path-centric Pruning,0.6641221374045801,0.9285714285714286,0.7777777777777778
relation-classification,4,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .",experiment,Effect of Path-centric Pruning,1,175,40,8,0,experiment : Effect of Path-centric Pruning,0.6679389312977099,0.9523809523809523,0.8888888888888888
relation-classification,4,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .",experiment,Effect of Path-centric Pruning,1,176,41,9,0,experiment : Effect of Path-centric Pruning,0.6717557251908397,0.9761904761904762,1.0
relation-classification,4,Analysis & Discussion,experiment,Analysis & Discussion,0,177,42,1,0,experiment : Analysis & Discussion,0.6755725190839694,1.0,1.0
relation-classification,4,Ablation Study,ablation,Ablation Study,0,178,1,1,0,ablation : Ablation Study,0.6793893129770993,0.05,0.16666666666666666
relation-classification,4,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",ablation,Ablation Study,1,179,2,2,0,ablation : Ablation Study,0.683206106870229,0.1,0.3333333333333333
relation-classification,4,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,ablation,Ablation Study,1,180,3,3,0,ablation : Ablation Study,0.6870229007633588,0.15,0.5
relation-classification,4,"2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .",ablation,Ablation Study,1,181,4,4,0,ablation : Ablation Study,0.6908396946564885,0.2,0.6666666666666666
relation-classification,4,"3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",ablation,Ablation Study,1,182,5,5,0,ablation : Ablation Study,0.6946564885496184,0.25,0.8333333333333334
relation-classification,4,"4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",ablation,Ablation Study,1,183,6,6,0,ablation : Ablation Study,0.6984732824427481,0.3,1.0
relation-classification,4,Complementary Strengths of GCNs and PA - LSTMs,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,184,7,1,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7022900763358778,0.35,0.07142857142857142
relation-classification,4,"To understand what the GCN models are capturing and how they differ from a sequence model such as the PA - LSTM , we compared their performance :",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,185,8,2,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7061068702290076,0.4,0.14285714285714285
relation-classification,4,The three dependency edges that contribute the most to the classification of different relations in the TACRED dev set .,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,186,9,3,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7099236641221374,0.45,0.21428571428571427
relation-classification,4,"For clarity , we removed edges which 1 ) connect to common punctuation ( i.e. , commas , periods , and quotation marks ) , 2 ) connect to common prepositions ( i.e. , of , to , by ) , and 3 ) connect between tokens within the same entity .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,187,10,4,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7137404580152672,0.5,0.2857142857142857
relation-classification,4,"We use PER , ORG for entity types of PERSON , ORGANIZATION .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,188,11,5,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7175572519083969,0.55,0.35714285714285715
relation-classification,4,"We use S - and O - to denote subject and object entities , respectively .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,189,12,6,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7213740458015268,0.6,0.42857142857142855
relation-classification,4,We also include edges for more relations in the supplementary material .,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,190,13,7,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7251908396946565,0.65,0.5
relation-classification,4,over examples in the TACRED dev set .,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,191,14,8,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7290076335877863,0.7,0.5714285714285714
relation-classification,4,"Specifically , for each model , we trained it for 5 independent runs with different seeds , and for each example we evaluated the model 's accuracy over these 5 runs .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,192,15,9,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.732824427480916,0.75,0.6428571428571429
relation-classification,4,"For instance , if a model correctly classifies an example for 3 out of 5 times , it achieves an accuracy of 60 % on this example .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,193,16,10,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7366412213740458,0.8,0.7142857142857143
relation-classification,4,"We observe that on 847 ( 3.7 % ) dev examples , our C - GCN model achieves an accuracy at least 60 % higher than that of the PA - LSTM , while on 629 ( 2.8 % ) examples the PA - LSTM achieves 60 % higher .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,194,17,11,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7404580152671756,0.85,0.7857142857142857
relation-classification,4,This complementary performance explains the gain we see in when the two models are combined .,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,195,18,12,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7442748091603053,0.9,0.8571428571428571
relation-classification,4,"We further show that this difference is due to each model 's competitive advantage : dependency - based models are better at handling sentences with entities farther apart , while sequence models can better leverage local word patterns regardless of parsing quality ( see also .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,196,19,13,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7480916030534351,0.95,0.9285714285714286
relation-classification,4,We include further analysis in the supplementary material .,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,197,20,14,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7519083969465649,1.0,1.0
relation-classification,4,Understanding Model Behavior,model,Understanding Model Behavior,0,198,1,1,0,model : Understanding Model Behavior,0.7557251908396947,0.045454545454545456,0.125
relation-classification,4,"To gain more insights into the C - GCN model 's behavior , we visualized the partial dependency tree it is processing and how much each token 's final representation contributed to h sent ( ) .",model,Understanding Model Behavior,0,199,2,2,0,model : Understanding Model Behavior,0.7595419847328244,0.09090909090909091,0.25
relation-classification,4,"We find that the model often focuses on the dependency path , but sometimes also incorporates offpath information to help reinforce its prediction .",model,Understanding Model Behavior,0,200,3,3,0,model : Understanding Model Behavior,0.7633587786259542,0.13636363636363635,0.375
relation-classification,4,"The model also learns to ignore determiners ( e.g. , "" the "" ) as they rarely affect relation prediction .",model,Understanding Model Behavior,0,201,4,4,0,model : Understanding Model Behavior,0.767175572519084,0.18181818181818182,0.5
relation-classification,4,"To further understand what dependency edges contribute most to the classification of different relations , we scored each dependency edge by summing up the number of dimensions each of its connected nodes contributed to h sent .",model,Understanding Model Behavior,0,202,5,5,0,model : Understanding Model Behavior,0.7709923664122137,0.22727272727272727,0.625
relation-classification,4,We present the top scoring edges in .,model,Understanding Model Behavior,0,203,6,6,0,model : Understanding Model Behavior,0.7748091603053435,0.2727272727272727,0.75
relation-classification,4,"As can be seen in the table , most of these edges are associated with indicative nouns or verbs of each relation .",model,Understanding Model Behavior,0,204,7,7,0,model : Understanding Model Behavior,0.7786259541984732,0.3181818181818182,0.875
relation-classification,4,5,model,Understanding Model Behavior,0,205,8,8,0,model : Understanding Model Behavior,0.7824427480916031,0.36363636363636365,1.0
relation-classification,4,Entity Bias in the SemEval Dataset,model,Entity Bias in the SemEval Dataset,0,206,9,1,0,model : Entity Bias in the SemEval Dataset,0.7862595419847328,0.4090909090909091,0.07142857142857142
relation-classification,4,"In our study , we observed a high correlation between the entity mentions in a sentence and its relation label in the SemEval dataset .",model,Entity Bias in the SemEval Dataset,0,207,10,2,0,model : Entity Bias in the SemEval Dataset,0.7900763358778626,0.45454545454545453,0.14285714285714285
relation-classification,4,"We experimented with PA - LSTM models to analyze this dependency tree corresponding to K = 1 in path-centric pruning is shown , and the shortest dependency path is thickened .",model,Entity Bias in the SemEval Dataset,0,208,11,3,0,model : Entity Bias in the SemEval Dataset,0.7938931297709924,0.5,0.21428571428571427
relation-classification,4,We omit edges to punctuation for clarity .,model,Entity Bias in the SemEval Dataset,0,209,12,4,0,model : Entity Bias in the SemEval Dataset,0.7977099236641222,0.5454545454545454,0.2857142857142857
relation-classification,4,"The first example shows that the C - GCN is effective at leveraging long - range dependencies while reducing noise with the help of pruning ( while the PA - LSTM predicts no relation twice , org : alternate names twice , and org : parents once in this case ) .",model,Entity Bias in the SemEval Dataset,0,210,13,5,0,model : Entity Bias in the SemEval Dataset,0.8015267175572519,0.5909090909090909,0.35714285714285715
relation-classification,4,"The second example shows that the PA - LSTM is better at leveraging the proximity of the word "" migrated "" regardless of attachment errors in the parse ( while the C - GCN is misled to predict per :country of birth three times , and no relation twice ) .",model,Entity Bias in the SemEval Dataset,0,211,14,6,0,model : Entity Bias in the SemEval Dataset,0.8053435114503816,0.6363636363636364,0.42857142857142855
relation-classification,4,phenomenon .,model,Entity Bias in the SemEval Dataset,0,212,15,7,0,model : Entity Bias in the SemEval Dataset,0.8091603053435115,0.6818181818181818,0.5
relation-classification,4,"We started by simplifying every sentence in the SemEval training and dev sets to "" subject and object "" , where subject and object are the actual entities in the sentence .",model,Entity Bias in the SemEval Dataset,0,213,16,8,0,model : Entity Bias in the SemEval Dataset,0.8129770992366412,0.7272727272727273,0.5714285714285714
relation-classification,4,"Surprisingly , a trained PA - LSTM model on this data is able to achieve 65.1 F 1 on the dev set if Glo Ve is used to initialize word vectors , and 47.9 dev F 1 even without GloVe initialization .",model,Entity Bias in the SemEval Dataset,0,214,17,9,0,model : Entity Bias in the SemEval Dataset,0.816793893129771,0.7727272727272727,0.6428571428571429
relation-classification,4,"To further evaluate the model in a more realistic setting , we trained one model with the original SemEval training set ( unmasked ) and one with mentions masked in the training set , following what we have done for TACRED ( masked ) .",model,Entity Bias in the SemEval Dataset,0,215,18,10,0,model : Entity Bias in the SemEval Dataset,0.8206106870229007,0.8181818181818182,0.7142857142857143
relation-classification,4,"While the unmasked model achieves a 83.6 F 1 on the original SemEval dev set , F 1 drops drastically to 62.4 if we replace dev set entity mentions with a special < UNK > token to simulate the presence of unseen entities .",model,Entity Bias in the SemEval Dataset,0,216,19,11,0,model : Entity Bias in the SemEval Dataset,0.8244274809160306,0.8636363636363636,0.7857142857142857
relation-classification,4,"In contrast , the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74.7 .",model,Entity Bias in the SemEval Dataset,0,217,20,12,0,model : Entity Bias in the SemEval Dataset,0.8282442748091603,0.9090909090909091,0.8571428571428571
relation-classification,4,This suggests that models trained without entities masked generalize poorly to new examples with unseen entities .,model,Entity Bias in the SemEval Dataset,0,218,21,13,0,model : Entity Bias in the SemEval Dataset,0.8320610687022901,0.9545454545454546,0.9285714285714286
relation-classification,4,Our findings call for more careful evaluation that takes dataset biases into account in future relation extraction studies .,model,Entity Bias in the SemEval Dataset,0,219,22,14,0,model : Entity Bias in the SemEval Dataset,0.8358778625954199,1.0,1.0
relation-classification,4,Conclusion,conclusion,Conclusion,0,220,1,1,0,conclusion : Conclusion,0.8396946564885496,0.25,0.25
relation-classification,4,We showed the success of a neural architecture based on a graph convolutional network for relation extraction .,conclusion,Conclusion,0,221,2,2,0,conclusion : Conclusion,0.8435114503816794,0.5,0.5
relation-classification,4,We also proposed path - centric pruning to improve the robustness of dependencybased models by removing irrelevant content without ignoring crucial information .,conclusion,Conclusion,0,222,3,3,0,conclusion : Conclusion,0.8473282442748091,0.75,0.75
relation-classification,4,"We showed through detailed analysis that our model has complementary strengths to sequence models , and that the proposed pruning technique can be effectively applied to other dependency - based models .",conclusion,Conclusion,0,223,4,4,0,conclusion : Conclusion,0.851145038167939,1.0,1.0
relation-classification,4,Experimental Details,experiment,Experimental Details,0,224,1,1,0,experiment : Experimental Details,0.8549618320610687,0.02564102564102564,1.0
relation-classification,4,Hyperparameters TACRED,experiment,hyperparameters,0,225,2,1,0,experiment : hyperparameters,0.8587786259541985,0.05128205128205128,0.06666666666666667
relation-classification,4,We set LSTM hidden size to 200 in all neural models .,experiment,hyperparameters,0,226,3,2,0,experiment : hyperparameters,0.8625954198473282,0.07692307692307693,0.13333333333333333
relation-classification,4,We also use hidden size 200 for the output feedforward layers in the GCN model .,experiment,hyperparameters,0,227,4,3,0,experiment : hyperparameters,0.8664122137404581,0.10256410256410256,0.2
relation-classification,4,We use 2 GCN layers and 2 feedforward ( FFNN ) layers in our experiments .,experiment,hyperparameters,0,228,5,4,0,experiment : hyperparameters,0.8702290076335878,0.1282051282051282,0.26666666666666666
relation-classification,4,We employ the ReLU function for all nonlinearities in the GCN layers and the standard max pooling operations in all pooling layers .,experiment,hyperparameters,0,229,6,5,0,experiment : hyperparameters,0.8740458015267175,0.15384615384615385,0.3333333333333333
relation-classification,4,"For the Tree - LSTM model , we find a 2 - layer architecture works substantially better than the vanilla 1 - layer model , and use it in all our experiments .",experiment,hyperparameters,0,230,7,6,0,experiment : hyperparameters,0.8778625954198473,0.1794871794871795,0.4
relation-classification,4,"For both the Tree - LSTM and our models , we apply path - centric pruning with K = 1 , as we find that this generates best results for all models ( also see ) .",experiment,hyperparameters,0,231,8,7,0,experiment : hyperparameters,0.8816793893129771,0.20512820512820512,0.4666666666666667
relation-classification,4,"We use the pretrained 300 - dimensional Glo Ve vectors to initialize word embeddings , and we use embedding size of 30 for all other embeddings ( i.e. , POS , NER ) .",experiment,hyperparameters,0,232,9,8,0,experiment : hyperparameters,0.8854961832061069,0.23076923076923078,0.5333333333333333
relation-classification,4,"We use the dependency parse trees , POS and NER sequences as included in the original release of the dataset , which was generated with Stanford CoreNLP .",experiment,hyperparameters,0,233,10,9,0,experiment : hyperparameters,0.8893129770992366,0.2564102564102564,0.6
relation-classification,4,For regularization we apply dropout with p = 0.5 to all LSTM layers and all but the last GCN layers .,experiment,hyperparameters,0,234,11,10,0,experiment : hyperparameters,0.8931297709923665,0.28205128205128205,0.6666666666666666
relation-classification,4,Sem Eval,experiment,hyperparameters,0,235,12,11,0,experiment : hyperparameters,0.8969465648854962,0.3076923076923077,0.7333333333333333
relation-classification,4,We use LSTM hidden size of 100 and use 1 GCN layer for the SemEval dataset .,experiment,hyperparameters,0,236,13,12,0,experiment : hyperparameters,0.9007633587786259,0.3333333333333333,0.8
relation-classification,4,"We preprocess the dataset with Stanford CoreNLP to generate the dependency parse trees , POS and NER annotations .",experiment,hyperparameters,0,237,14,13,0,experiment : hyperparameters,0.9045801526717557,0.358974358974359,0.8666666666666667
relation-classification,4,All other hyperparameters are set to be the same .,experiment,hyperparameters,0,238,15,14,0,experiment : hyperparameters,0.9083969465648855,0.38461538461538464,0.9333333333333333
relation-classification,4,"For both datasets , we work with the Universal Dependencies v 1 formalism .",experiment,hyperparameters,0,239,16,15,0,experiment : hyperparameters,0.9122137404580153,0.41025641025641024,1.0
relation-classification,4,Training,experiment,Training,0,240,17,1,0,experiment : Training,0.916030534351145,0.4358974358974359,0.06666666666666667
relation-classification,4,For training we use Stochastic Gradient Descent with an initial learning rate of 1.0 .,experiment,Training,0,241,18,2,0,experiment : Training,0.9198473282442748,0.46153846153846156,0.13333333333333333
relation-classification,4,We use a cutoff of 5 for gradient clipping .,experiment,Training,0,242,19,3,0,experiment : Training,0.9236641221374046,0.48717948717948717,0.2
relation-classification,4,"For GCN models , we train every model for 100 epochs on the TAC - RED dataset , and from epoch 5 we start to anneal the learning rate by a factor of 0.9 every time the F 1 score on the dev set does not increase after an epoch .",experiment,Training,0,243,20,4,0,experiment : Training,0.9274809160305344,0.5128205128205128,0.26666666666666666
relation-classification,4,For Tree - LSTM models we find 30 total epochs to be enough .,experiment,Training,0,244,21,5,0,experiment : Training,0.9312977099236641,0.5384615384615384,0.3333333333333333
relation-classification,4,"Due to the small size of the SemEval dataset , we train all models for 150 epochs , and use an initial learning rate of 0.5 with a decay rate of 0.95 .",experiment,Training,0,245,22,6,0,experiment : Training,0.9351145038167938,0.5641025641025641,0.4
relation-classification,4,"In our experiments we found that the output vector h sent tends to have large magnitude , and : Aggregated 5 - run difference compared to PA - LSTM on the TACRED dev set .",experiment,Training,0,246,23,7,0,experiment : Training,0.9389312977099237,0.5897435897435898,0.4666666666666667
relation-classification,4,"For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ? Y .",experiment,Training,0,247,24,8,0,experiment : Training,0.9427480916030534,0.6153846153846154,0.5333333333333333
relation-classification,4,"For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ? Y .",experiment,Training,0,248,25,9,0,experiment : Training,0.9465648854961832,0.6410256410256411,0.6
relation-classification,4,"0 "" is omitted due to redundancy .",experiment,Training,0,249,26,10,0,experiment : Training,0.950381679389313,0.6666666666666666,0.6666666666666666
relation-classification,4,therefore adding the following regularization term to the cross entropy loss of each example improves the results :,experiment,Training,0,250,27,11,0,experiment : Training,0.9541984732824428,0.6923076923076923,0.7333333333333333
relation-classification,4,"Here , reg functions as an l 2 regularization on the learned sentence representations .",experiment,Training,0,251,28,12,0,experiment : Training,0.9580152671755725,0.717948717948718,0.8
relation-classification,4,controls the regularization strength and we set ? = 0.003 .,experiment,Training,0,252,29,13,0,experiment : Training,0.9618320610687023,0.7435897435897436,0.8666666666666667
relation-classification,4,controls the regularization strength and we set ? = 0.003 .,experiment,Training,0,253,30,14,0,experiment : Training,0.9656488549618321,0.7692307692307693,0.9333333333333333
relation-classification,4,We empirically found this to be more effective than applying l 2 regularization on the convolutional weights .,experiment,Training,0,254,31,15,0,experiment : Training,0.9694656488549618,0.7948717948717948,1.0
relation-classification,4,Comparing GCN models and PA - LSTM on TACRED,experiment,Comparing GCN models and PA-LSTM on TACRED,0,255,32,1,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9732824427480916,0.8205128205128205,0.125
relation-classification,4,We compared the performance of both GCN models with the PA - LSTM on the TACRED dev set .,experiment,Comparing GCN models and PA-LSTM on TACRED,0,256,33,2,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9770992366412213,0.8461538461538461,0.25
relation-classification,4,"To minimize randomness that is not inherent to these models , we accumulate statistics over 5 independent runs of each model , and report them in .",experiment,Comparing GCN models and PA-LSTM on TACRED,0,257,34,3,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9809160305343512,0.8717948717948718,0.375
relation-classification,4,"As is shown in the figure , both GCN models capture very different examples from the PA - LSTM model .",experiment,Comparing GCN models and PA-LSTM on TACRED,0,258,35,4,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9847328244274809,0.8974358974358975,0.5
relation-classification,4,"In the entire dev set of 22,631 examples , 1,450 had at least 3 more GCN models predicting the label correctly compared to the PA - LSTM , and 1,550 saw an improvement from using the PA - LSTM .",experiment,Comparing GCN models and PA-LSTM on TACRED,0,259,36,5,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9885496183206107,0.9230769230769231,0.625
relation-classification,4,"The C - GCN , on the other hand , outperformed the PA - LSTM by at least 3 models on a total of 847 examples , and lost by a margin of at least 3 on another 629 examples , as reported in the main text .",experiment,Comparing GCN models and PA-LSTM on TACRED,0,260,37,6,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9923664122137404,0.9487179487179487,0.75
relation-classification,4,This smaller difference is also reflected in the diminished gain from ensembling with the PA - LSTM shown in .,experiment,Comparing GCN models and PA-LSTM on TACRED,0,261,38,7,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.9961832061068703,0.9743589743589743,0.875
relation-classification,4,We hypoth -,experiment,Comparing GCN models and PA-LSTM on TACRED,0,262,39,8,0,experiment : Comparing GCN models and PA-LSTM on TACRED,1.0,1.0,1.0
relation-classification,5,End - to - end neural relation extraction using deep biaffine attention,title,title,1,2,1,1,0,title : title,0.017699115044247787,1.0,1.0
relation-classification,5,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.02654867256637168,0.25,0.25
relation-classification,5,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.035398230088495575,0.5,0.5
relation-classification,5,"The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.04424778761061947,0.75,0.75
relation-classification,5,"On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.05309734513274336,1.0,1.0
relation-classification,5,Introduction,introduction,introduction,0,7,1,1,0,introduction : introduction,0.061946902654867256,0.03571428571428571,0.03571428571428571
relation-classification,5,Extracting entities and their semantic relations from raw text is a key information extraction task .,introduction,introduction,1,8,2,2,0,introduction : introduction,0.07079646017699115,0.07142857142857142,0.07142857142857142
relation-classification,5,"For example , given the sentence "" David Foster is the AP 's Northwest regional reporter , based in Seattle "" in the CoNLL04 dataset , our goal is to recognize "" David Foster "" as person , "" AP "" as organization , and "" Northwest "" and "" Seattle "" as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .",introduction,introduction,0,9,3,3,0,introduction : introduction,0.07964601769911504,0.10714285714285714,0.10714285714285714
relation-classification,5,Such information is useful in many other NLP tasks .,introduction,introduction,0,10,4,4,0,introduction : introduction,0.08849557522123894,0.14285714285714285,0.14285714285714285
relation-classification,5,"Especially in IR applications such as entity search , structured search and question answering , it helps provide end users with significantly better search experience .",introduction,introduction,0,11,5,5,0,introduction : introduction,0.09734513274336283,0.17857142857142858,0.17857142857142858
relation-classification,5,common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification .,introduction,introduction,0,12,6,6,0,introduction : introduction,0.10619469026548672,0.21428571428571427,0.21428571428571427
relation-classification,5,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",introduction,introduction,1,13,7,7,0,introduction : introduction,0.11504424778761062,0.25,0.25
relation-classification,5,Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .,introduction,introduction,0,14,8,8,0,introduction : introduction,0.12389380530973451,0.2857142857142857,0.2857142857142857
relation-classification,5,State - of - the - art relation extraction performance has been obtained by end - to - end models based on neural networks .,introduction,introduction,0,15,9,9,0,introduction : introduction,0.13274336283185842,0.32142857142857145,0.32142857142857145
relation-classification,5,"Specifically , proposed a RNNbased model which achieved top results on the CoNLL04 dataset .",introduction,introduction,0,16,10,10,0,introduction : introduction,0.1415929203539823,0.35714285714285715,0.35714285714285715
relation-classification,5,Their approach relies on various manually extracted features .,introduction,introduction,0,17,11,11,0,introduction : introduction,0.1504424778761062,0.39285714285714285,0.39285714285714285
relation-classification,5,Other neural models employ dependency parsing - based information .,introduction,introduction,0,18,12,12,0,introduction : introduction,0.1592920353982301,0.42857142857142855,0.42857142857142855
relation-classification,5,"In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .",introduction,introduction,0,19,13,13,0,introduction : introduction,0.168141592920354,0.4642857142857143,0.4642857142857143
relation-classification,5,integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM - based dependency parser .,introduction,introduction,0,20,14,14,0,introduction : introduction,0.17699115044247787,0.5,0.5
relation-classification,5,"entity recognition , and a CNN on top of the BiLSTM for classifying relations .",introduction,introduction,0,21,15,15,0,introduction : introduction,0.18584070796460178,0.5357142857142857,0.5357142857142857
relation-classification,5,"Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .",introduction,introduction,0,22,16,16,0,introduction : introduction,0.19469026548672566,0.5714285714285714,0.5714285714285714
relation-classification,5,"Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .",introduction,introduction,0,23,17,17,0,introduction : introduction,0.20353982300884957,0.6071428571428571,0.6071428571428571
relation-classification,5,"In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .",introduction,introduction,0,24,18,18,0,introduction : introduction,0.21238938053097345,0.6428571428571429,0.6428571428571429
relation-classification,5,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",introduction,introduction,1,25,19,19,0,introduction : introduction,0.22123893805309736,0.6785714285714286,0.6785714285714286
relation-classification,5,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .",introduction,introduction,1,26,20,20,0,introduction : introduction,0.23008849557522124,0.7142857142857143,0.7142857142857143
relation-classification,5,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,introduction,introduction,1,27,21,21,0,introduction : introduction,0.23893805309734514,0.75,0.75
relation-classification,5,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .",introduction,introduction,1,28,22,22,0,introduction : introduction,0.24778761061946902,0.7857142857142857,0.7857142857142857
relation-classification,5,"In most previous neural joint models , the relation classification part relies on a common "" linear "" concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .",introduction,introduction,0,29,23,23,0,introduction : introduction,0.25663716814159293,0.8214285714285714,0.8214285714285714
relation-classification,5,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .",introduction,introduction,1,30,24,24,0,introduction : introduction,0.26548672566371684,0.8571428571428571,0.8571428571428571
relation-classification,5,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .",introduction,introduction,1,31,25,25,0,introduction : introduction,0.2743362831858407,0.8928571428571429,0.8928571428571429
relation-classification,5,"Experimental results on the benchmark "" relation and entity recognition "" dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .",introduction,introduction,0,32,26,26,0,introduction : introduction,0.2831858407079646,0.9285714285714286,0.9285714285714286
relation-classification,5,"In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .",introduction,introduction,0,33,27,27,0,introduction : introduction,0.2920353982300885,0.9642857142857143,0.9642857142857143
relation-classification,5,We also provide an ablation study to investigate effects of different contributing factors in our model .,introduction,introduction,0,34,28,28,0,introduction : introduction,0.3008849557522124,1.0,1.0
relation-classification,5,Our proposed model,model,Our proposed model,0,35,1,1,0,model : Our proposed model,0.30973451327433627,0.043478260869565216,0.043478260869565216
relation-classification,5,This section details our end - to - end relation extraction model .,model,Our proposed model,0,36,2,2,0,model : Our proposed model,0.3185840707964602,0.08695652173913043,0.08695652173913043
relation-classification,5,"Given an input sequence of n word tokens w 1 , w 2 , ... , w n , we use a vector vi to represent each i th word w i by concatenating word embedding e",model,Our proposed model,0,37,3,3,0,model : Our proposed model,0.3274336283185841,0.13043478260869565,0.13043478260869565
relation-classification,5,"Here , for each word type w , we use a one - layer BiLSTM ( BiLSTM char ) to learn its character - level word embedding e ( C ) w.",model,Our proposed model,0,38,4,4,0,model : Our proposed model,0.336283185840708,0.17391304347826086,0.17391304347826086
relation-classification,5,Named entity recognition ( NER ) :,model,Our proposed model,0,39,5,5,0,model : Our proposed model,0.34513274336283184,0.21739130434782608,0.21739130434782608
relation-classification,5,"The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a "" latent "" feature vector representing the i th word token .",model,Our proposed model,0,40,6,6,0,model : Our proposed model,0.35398230088495575,0.2608695652173913,0.2608695652173913
relation-classification,5,Then the NER component performs linear transformation of each latent feature vector by using a single - layer feed - forward network ( FFNN NER ) :,model,Our proposed model,0,41,7,7,0,model : Our proposed model,0.36283185840707965,0.30434782608695654,0.30434782608695654
relation-classification,5,The output layer size of FFNN NER is the number of BIOLU - based NER labels .,model,Our proposed model,0,42,8,8,0,model : Our proposed model,0.37168141592920356,0.34782608695652173,0.34782608695652173
relation-classification,5,The NER component feeds the output vectors h 1:n into a linear - chain CRF layer for NER label prediction .,model,Our proposed model,0,43,9,9,0,model : Our proposed model,0.3805309734513274,0.391304347826087,0.391304347826087
relation-classification,5,"cross-entropy loss L NER is computed during training , while the Viterbi algorithm is used for decoding .",model,Our proposed model,0,44,10,10,0,model : Our proposed model,0.3893805309734513,0.43478260869565216,0.43478260869565216
relation-classification,5,Our NER component thus is the BiLSTM - CRF model with additional LSTM - based character - level word embeddings .,model,Our proposed model,0,45,11,11,0,model : Our proposed model,0.39823008849557523,0.4782608695652174,0.4782608695652174
relation-classification,5,"Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .",model,Our proposed model,0,46,12,12,0,model : Our proposed model,0.40707964601769914,0.5217391304347826,0.5217391304347826
relation-classification,5,We represent each i th predicted label by a vector embedding e ti .,model,Our proposed model,0,47,13,13,0,model : Our proposed model,0.415929203539823,0.5652173913043478,0.5652173913043478
relation-classification,5,We create a sequence of vectors x 1:n in which each x i is computed as :,model,Our proposed model,0,48,14,14,0,model : Our proposed model,0.4247787610619469,0.6086956521739131,0.6086956521739131
relation-classification,5,"As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :",model,Our proposed model,0,49,15,15,0,model : Our proposed model,0.4336283185840708,0.6521739130434783,0.6521739130434783
relation-classification,5,The RC component further uses these latent vectors r i for relation classification .,model,Our proposed model,0,50,16,16,0,model : Our proposed model,0.4424778761061947,0.6956521739130435,0.6956521739130435
relation-classification,5,We propose a novel use of the deep biaffine attention mechanism for relation classification .,model,Our proposed model,0,51,17,17,0,model : Our proposed model,0.45132743362831856,0.7391304347826086,0.7391304347826086
relation-classification,5,"The biaffine attention mechanism was proposed for dependency parsing , helping to produce the best reported parsing performance to date .",model,Our proposed model,0,52,18,18,0,model : Our proposed model,0.46017699115044247,0.782608695652174,0.782608695652174
relation-classification,5,"First , to encode the directionality of a relation , we use two single - layer feed - forward networks to project each r i into head and tail vector representations which correspond to whether the i th word serves as the head or tail argument of the relation :",model,Our proposed model,0,53,19,19,0,model : Our proposed model,0.4690265486725664,0.8260869565217391,0.8260869565217391
relation-classification,5,"Following , our RC component incrementally constructs relation candidates using all possible combinations of the last word tokens of predicted entities , i.e. words with L or U labels .",model,Our proposed model,0,54,20,20,0,model : Our proposed model,0.4778761061946903,0.8695652173913043,0.8695652173913043
relation-classification,5,We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .,model,Our proposed model,0,55,21,21,0,model : Our proposed model,0.48672566371681414,0.9130434782608695,0.9130434782608695
relation-classification,5,"For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .",model,Our proposed model,0,56,22,22,0,model : Our proposed model,0.49557522123893805,0.9565217391304348,0.9565217391304348
relation-classification,5,"Then for each head - tail candidate pair ( w j , wk ) , we apply the biaffine attention operator :",model,Our proposed model,0,57,23,23,0,model : Our proposed model,0.504424778761062,1.0,1.0
relation-classification,5,Experiments,experiment,experiment,0,58,1,1,0,experiment : experiment,0.5132743362831859,0.043478260869565216,1.0
relation-classification,5,Experimental setup,experiment,Experimental setup,0,59,2,1,0,experiment : Experimental setup,0.5221238938053098,0.08695652173913043,0.045454545454545456
relation-classification,5,Evaluation scenarios :,experiment,Experimental setup,0,60,3,2,0,experiment : Experimental setup,0.5309734513274337,0.13043478260869565,0.09090909090909091
relation-classification,5,We evaluate our joint model on two evaluation setup scenarios :,experiment,Experimental setup,0,61,4,3,0,experiment : Experimental setup,0.5398230088495575,0.17391304347826086,0.13636363636363635
relation-classification,5,1 ) NER&RC :,experiment,Experimental setup,0,62,5,4,0,experiment : Experimental setup,0.5486725663716814,0.21739130434782608,0.18181818181818182
relation-classification,5,realistic scenario where entity boundaries are not given .,experiment,Experimental setup,0,63,6,5,0,experiment : Experimental setup,0.5575221238938053,0.2608695652173913,0.22727272727272727
relation-classification,5,2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .,experiment,Experimental setup,0,64,7,6,0,experiment : Experimental setup,0.5663716814159292,0.30434782608695654,0.2727272727272727
relation-classification,5,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,experiment,Experimental setup,0,65,8,7,0,experiment : Experimental setup,0.5752212389380531,0.34782608695652173,0.3181818181818182
relation-classification,5,"Following , we encode the gold entity boundaries in the BILOU scheme .",experiment,Experimental setup,0,66,9,8,0,experiment : Experimental setup,0.584070796460177,0.391304347826087,0.36363636363636365
relation-classification,5,"Then we represent each B , I , O , L or U boundary tag as a vector embedding .",experiment,Experimental setup,0,67,10,9,0,experiment : Experimental setup,0.5929203539823009,0.43478260869565216,0.4090909090909091
relation-classification,5,"As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .",experiment,Experimental setup,0,68,11,10,0,experiment : Experimental setup,0.6017699115044248,0.4782608695652174,0.45454545454545453
relation-classification,5,"Dataset : We use the benchmark "" entity and relation recognition "" dataset CoNLL04 from .",experiment,Experimental setup,0,69,12,11,0,experiment : Experimental setup,0.6106194690265486,0.5217391304347826,0.5
relation-classification,5,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .",experiment,Experimental setup,0,70,13,12,0,experiment : Experimental setup,0.6194690265486725,0.5652173913043478,0.5454545454545454
relation-classification,5,Implementation :,experiment,Experimental setup,0,71,14,13,0,experiment : Experimental setup,0.6283185840707964,0.6086956521739131,0.5909090909090909
relation-classification,5,Our model is implemented using DYNET v 2.0 .,experiment,Experimental setup,1,72,15,14,0,experiment : Experimental setup,0.6371681415929203,0.6521739130434783,0.6363636363636364
relation-classification,5,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",experiment,Experimental setup,1,73,16,15,0,experiment : Experimental setup,0.6460176991150443,0.6956521739130435,0.6818181818181818
relation-classification,5,We compute the average of NER / EC score and RC score after each training epoch .,experiment,Experimental setup,0,74,17,16,0,experiment : Experimental setup,0.6548672566371682,0.7391304347826086,0.7272727272727273
relation-classification,5,"We choose the model with the highest average score on the development set , which is then applied to the test set for the final evaluation phase .",experiment,Experimental setup,0,75,18,17,0,experiment : Experimental setup,0.6637168141592921,0.782608695652174,0.7727272727272727
relation-classification,5,More details of the implementation as well as optimal hyper - parameters are in the Appendix .,experiment,Experimental setup,0,76,19,18,0,experiment : Experimental setup,0.672566371681416,0.8260869565217391,0.8181818181818182
relation-classification,5,Our code is available at : https : //github.com/datquocnguyen/jointRE,experiment,Experimental setup,1,77,20,19,0,experiment : Experimental setup,0.6814159292035398,0.8695652173913043,0.8636363636363636
relation-classification,5,"Metric : Similar to previous works in , we use the macro -averaged F1 - score over the entity classes to score NER / EC and over the relation classes to score RC .",experiment,Experimental setup,0,78,21,20,0,experiment : Experimental setup,0.6902654867256637,0.9130434782608695,0.9090909090909091
relation-classification,5,More details of the metric are also in the Appendix .,experiment,Experimental setup,0,79,22,21,0,experiment : Experimental setup,0.6991150442477876,0.9565217391304348,0.9545454545454546
relation-classification,5,"Unlike previous neural models , we report results as mean and standard deviation of the scores over 10 runs with 10 random seeds .",experiment,Experimental setup,0,80,23,22,0,experiment : Experimental setup,0.7079646017699115,1.0,1.0
relation-classification,5,Main results,result,Main results,0,81,1,1,0,result : Main results,0.7168141592920354,0.034482758620689655,0.034482758620689655
relation-classification,5,End - to - end results :,result,Main results,0,82,2,2,0,result : Main results,0.7256637168141593,0.06896551724137931,0.06896551724137931
relation-classification,5,The first six rows in compare our results with previous state - of - the - art published results on the same test set .,result,Main results,0,83,3,3,0,result : Main results,0.7345132743362832,0.10344827586206896,0.10344827586206896
relation-classification,5,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .",result,Main results,0,84,4,4,0,result : Main results,0.7433628318584071,0.13793103448275862,0.13793103448275862
relation-classification,5,We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .,result,Main results,0,85,5,5,0,result : Main results,0.7522123893805309,0.1724137931034483,0.1724137931034483
relation-classification,5,"Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",result,Main results,0,86,6,6,0,result : Main results,0.7610619469026548,0.20689655172413793,0.20689655172413793
relation-classification,5,"These results show that our model performs better than previous state - of - the - art models , using the same setup .",result,Main results,0,87,7,7,0,result : Main results,0.7699115044247787,0.2413793103448276,0.2413793103448276
relation-classification,5,"In , the last two rows present results reported in and on the dataset CoNLL04 .",result,Main results,0,88,8,8,0,result : Main results,0.7787610619469026,0.27586206896551724,0.27586206896551724
relation-classification,5,"However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .",result,Main results,0,89,9,9,0,result : Main results,0.7876106194690266,0.3103448275862069,0.3103448275862069
relation-classification,5,Both and employ additional extra features based on external NLP tools and use larger training sets than ours .,result,Main results,0,90,10,10,0,result : Main results,0.7964601769911505,0.3448275862068966,0.3448275862068966
relation-classification,5,"Specifically , integrate syntactic features by using a pre-trained BiLSTM - based dependency parser to extract BiLSTM - based latent feature representations for words in the input sentence , and then using these latent representations directly as part of the input embeddings in their model .",result,Main results,0,91,11,11,0,result : Main results,0.8053097345132744,0.3793103448275862,0.3793103448275862
relation-classification,5,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,result,Main results,0,92,12,12,0,result : Main results,0.8141592920353983,0.41379310344827586,0.41379310344827586
relation-classification,5,Ablation analysis :,result,Main results,0,93,13,13,0,result : Main results,0.8230088495575221,0.4482758620689655,0.4482758620689655
relation-classification,5,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",result,Main results,1,94,14,14,0,result : Main results,0.831858407079646,0.4827586206896552,0.4827586206896552
relation-classification,5,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",result,Main results,0,95,15,15,0,result : Main results,0.8407079646017699,0.5172413793103449,0.5172413793103449
relation-classification,5,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .",result,Main results,1,96,16,16,0,result : Main results,0.8495575221238938,0.5517241379310345,0.5517241379310345
relation-classification,5,"Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .",result,Main results,0,97,17,17,0,result : Main results,0.8584070796460177,0.5862068965517241,0.5862068965517241
relation-classification,5,differences are not significant .,result,Main results,0,98,18,18,0,result : Main results,0.8672566371681416,0.6206896551724138,0.6206896551724138
relation-classification,5,similar observation is also found in .,result,Main results,0,99,19,19,0,result : Main results,0.8761061946902655,0.6551724137931034,0.6551724137931034
relation-classification,5,"Also , in preliminary experiments , we do not find any significant difference in performance of our joint model when feeding gold NER labels instead of predicted NER labels into the RC component during training .",result,Main results,0,100,20,20,0,result : Main results,0.8849557522123894,0.6896551724137931,0.6896551724137931
relation-classification,5,This is not surprising as the training NER score is at 99 +% .,result,Main results,0,101,21,21,0,result : Main results,0.8938053097345132,0.7241379310344828,0.7241379310344828
relation-classification,5,also presents ablation tests over 5 factors of our joint model on the development set .,result,Main results,0,102,22,22,0,result : Main results,0.9026548672566371,0.7586206896551724,0.7586206896551724
relation-classification,5,"In particular , Setup 1 performances significantly degrade by 4 + % absolutely , when not using the character - level word embeddings .",result,Main results,0,103,23,23,0,result : Main results,0.911504424778761,0.7931034482758621,0.7931034482758621
relation-classification,5,"The performances also decrease when using a softmax classifier for NER label prediction rather than a CRF layer ( here , the decrease is significant ) .",result,Main results,0,104,24,24,0,result : Main results,0.9203539823008849,0.8275862068965517,0.8275862068965517
relation-classification,5,"In contrast , we do not find any significant difference in Setup 2 scores when not using either the character - level embeddings or the CRF layer , clearly showing the usefulness of the given gold entity boundaries .",result,Main results,0,105,25,25,0,result : Main results,0.9292035398230089,0.8620689655172413,0.8620689655172413
relation-classification,5,"The 3 remaining factors , including removing NER label embeddings and not taking either the Bilinear or Linear part ( in Equation 8 ) into the Biaffine attention layer , do not affect the NER / EC score .",result,Main results,0,106,26,26,0,result : Main results,0.9380530973451328,0.896551724137931,0.896551724137931
relation-classification,5,"However , they significantly decrease the RC score .",result,Main results,0,107,27,27,0,result : Main results,0.9469026548672567,0.9310344827586207,0.9310344827586207
relation-classification,5,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",result,Main results,0,108,28,28,0,result : Main results,0.9557522123893806,0.9655172413793104,0.9655172413793104
relation-classification,5,"More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , "" w / o Bilinear "" results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .",result,Main results,0,109,29,29,0,result : Main results,0.9646017699115044,1.0,1.0
relation-classification,5,Conclusion,conclusion,Conclusion,0,110,1,1,0,conclusion : Conclusion,0.9734513274336283,0.25,0.25
relation-classification,5,"In this paper , we have presented an end - to - end neural network - based relation extraction model .",conclusion,Conclusion,0,111,2,2,0,conclusion : Conclusion,0.9823008849557522,0.5,0.5
relation-classification,5,Our model employs a BiLSTM - CRF architecture for entity recognition and a biaffine attention mechanism for relation classification .,conclusion,Conclusion,0,112,3,3,0,conclusion : Conclusion,0.9911504424778761,0.75,0.75
relation-classification,5,"On the benchmark CoNLL04 dataset , our model produces new state - of - the - art performance .",conclusion,Conclusion,0,113,4,4,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,6,Semantic Relation Classification via Bidirectional LSTM Networks with Entity - aware Attention using Latent Entity Typing,title,title,1,2,1,1,0,title : title,0.011049723756906077,1.0,1.0
relation-classification,6,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.016574585635359115,0.14285714285714285,0.14285714285714285
relation-classification,6,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.022099447513812154,0.2857142857142857,0.2857142857142857
relation-classification,6,"Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.027624309392265192,0.42857142857142855,0.42857142857142855
relation-classification,6,"In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.03314917127071823,0.5714285714285714,0.5714285714285714
relation-classification,6,"To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03867403314917127,0.7142857142857143,0.7142857142857143
relation-classification,6,Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.04419889502762431,0.8571428571428571,0.8571428571428571
relation-classification,6,"Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.049723756906077346,1.0,1.0
relation-classification,6,Introduction,introduction,introduction,0,10,1,1,0,introduction : introduction,0.055248618784530384,0.058823529411764705,0.058823529411764705
relation-classification,6,"Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks , such as information extraction , question answering and knowledge base population .",introduction,introduction,0,11,2,2,0,introduction : introduction,0.06077348066298342,0.11764705882352941,0.11764705882352941
relation-classification,6,task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,introduction,introduction,1,12,3,3,0,introduction : introduction,0.06629834254143646,0.17647058823529413,0.17647058823529413
relation-classification,6,"For example , given a sentence with tagged entity pair , crash and attack , this sentence is classified into the re-lation Cause - Effect ( e1 , e2 ) 1 between the entity pair like .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.0718232044198895,0.23529411764705882,0.23529411764705882
relation-classification,6,"first entity is surrounded by e 1 and / e 1 , and a second entity is surrounded by e 2 and / e 2 .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.07734806629834254,0.29411764705882354,0.29411764705882354
relation-classification,6,"Most previous relation classification models rely heavily on high - level lexical and syntactic features obtained from NLP tools such as WordNet , dependency parser , part - of - speech ( POS ) tagger , and named entity recognizer ( NER ) .",introduction,introduction,0,15,6,6,0,introduction : introduction,0.08287292817679558,0.35294117647058826,0.35294117647058826
relation-classification,6,The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive .,introduction,introduction,0,16,7,7,0,introduction : introduction,0.08839779005524862,0.4117647058823529,0.4117647058823529
relation-classification,6,"Recently , many studies therefore propose end - toend neural models without the high - level features .",introduction,introduction,0,17,8,8,0,introduction : introduction,0.09392265193370165,0.47058823529411764,0.47058823529411764
relation-classification,6,"Among them , attention - based models , which focus to the most important semantic information in a sentence , show state - of - the - art results in a lot of NLP tasks .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.09944751381215469,0.5294117647058824,0.5294117647058824
relation-classification,6,"Since these models are mainly proposed for solving translation and language modeling tasks , they could not fully utilize the information of tagged entities in relation classification task .",introduction,introduction,0,19,10,10,0,introduction : introduction,0.10497237569060773,0.5882352941176471,0.5882352941176471
relation-classification,6,"However , tagged entity pairs could be powerful hints for solving relation classification task .",introduction,introduction,0,20,11,11,0,introduction : introduction,0.11049723756906077,0.6470588235294118,0.6470588235294118
relation-classification,6,"For example , even if we do not consider other words except the crash and attack , we intuitively know that the entity pair has a relation Cause - Effect ( e1 , e2 ) 1 better than Component - Whole ( e1 , e2 ) 1 in To address these issues , We propose a novel endto - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) .",introduction,introduction,0,21,12,12,0,introduction : introduction,0.11602209944751381,0.7058823529411765,0.7058823529411765
relation-classification,6,"To capture the context of sentences , We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short - Term Memory ( LSTM ) networks .",introduction,introduction,0,22,13,13,0,introduction : introduction,0.12154696132596685,0.7647058823529411,0.7647058823529411
relation-classification,6,Entity - aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET .,introduction,introduction,0,23,14,14,0,introduction : introduction,0.1270718232044199,0.8235294117647058,0.8235294117647058
relation-classification,6,The contributions of our work are summarized as follows :,introduction,introduction,0,24,15,15,0,introduction : introduction,0.13259668508287292,0.8823529411764706,0.8823529411764706
relation-classification,6,"We propose an novel end - to - end recurrent neural model and an entity - aware attention mechanism with a LET which focuses to semantic information of entities and their latent types ; ( 2 ) Our model obtains 85.2 % F1 - score in SemEval- 2010 Task 8 and it outper - forms existing state - of - the - art models without any highlevel features ; We show that our model is more interpretable since it 's decision making process could be visualized with self attention , entity - aware attention , and LET .",introduction,introduction,0,25,16,16,0,introduction : introduction,0.13812154696132597,0.9411764705882353,0.9411764705882353
relation-classification,6,"We propose an novel end - to - end recurrent neural model and an entity - aware attention mechanism with a LET which focuses to semantic information of entities and their latent types ; ( 2 ) Our model obtains 85.2 % F1 - score in SemEval- 2010 Task 8 and it outper - forms existing state - of - the - art models without any highlevel features ; We show that our model is more interpretable since it 's decision making process could be visualized with self attention , entity - aware attention , and LET .",introduction,introduction,0,26,17,17,0,introduction : introduction,0.143646408839779,1.0,1.0
relation-classification,6,Related Work,related work,Related Work,0,27,1,1,0,related work : Related Work,0.14917127071823205,0.05,0.05
relation-classification,6,There are several studies for solving relation classification task .,related work,Related Work,0,28,2,2,0,related work : Related Work,0.15469613259668508,0.1,0.1
relation-classification,6,Early methods used handcrafted features through a series of NLP tools or manually designing kernels .,related work,Related Work,0,29,3,3,0,related work : Related Work,0.16022099447513813,0.15,0.15
relation-classification,6,"These approaches use high - level lexical and syntactic features obtained from NLP tools and manually designing kernels , but the classification models relying on such features suffer from propagation of implicit error of the tools .",related work,Related Work,0,30,4,4,0,related work : Related Work,0.16574585635359115,0.2,0.2
relation-classification,6,"On the other hands , deep neural networks have shown outperform previous models using handcraft features .",related work,Related Work,0,31,5,5,0,related work : Related Work,0.1712707182320442,0.25,0.25
relation-classification,6,"Especially , many researches tried to solve the problem based on end - to - end models using only raw sentences and pre-trained word representations learned by Skip - gram and Continuous Bag - of - Words .",related work,Related Work,0,32,6,6,0,related work : Related Work,0.17679558011049723,0.3,0.3
relation-classification,6,Zeng et al . employed a deep convolutional neural network ( CNN ) for extracting lexical and sentence level features .,related work,Related Work,0,33,7,7,0,related work : Related Work,0.18232044198895028,0.35,0.35
relation-classification,6,Zeng et al . employed a deep convolutional neural network ( CNN ) for extracting lexical and sentence level features .,related work,Related Work,0,34,8,8,0,related work : Related Work,0.1878453038674033,0.4,0.4
relation-classification,6,Dos Santos et al. proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes .,related work,Related Work,0,35,9,9,0,related work : Related Work,0.19337016574585636,0.45,0.45
relation-classification,6,Dos Santos et al. proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes .,related work,Related Work,0,36,10,10,0,related work : Related Work,0.19889502762430938,0.5,0.5
relation-classification,6,Zhang and Wang used bidirectional recurrent neural network ( RNN ) to learn long - term dependency between entity pairs .,related work,Related Work,0,37,11,11,0,related work : Related Work,0.20441988950276244,0.55,0.55
relation-classification,6,"Fur-thermore , Zhang et al. proposed bidirectional LSTM network ( BLSTM ) utilizing position of words , POS tags , named entity information , dependency parse .",related work,Related Work,0,38,12,12,0,related work : Related Work,0.20994475138121546,0.6,0.6
relation-classification,6,This model resolved vanishing gradient problem appeared in RNNs by using BLSTM .,related work,Related Work,0,39,13,13,0,related work : Related Work,0.2154696132596685,0.65,0.65
relation-classification,6,"Recently , some researcher have proposed attentionbased models which can focus to the most important semantic information in a sentence .",related work,Related Work,0,40,14,14,0,related work : Related Work,0.22099447513812154,0.7,0.7
relation-classification,6,Zhou et al. combined attention mechanisms with BLSTM .,related work,Related Work,0,41,15,15,0,related work : Related Work,0.2265193370165746,0.75,0.75
relation-classification,6,Xiao and Liu split the sentence into two entities and used two attention - based BLSTM hierarchically .,related work,Related Work,0,42,16,16,0,related work : Related Work,0.23204419889502761,0.8,0.8
relation-classification,6,Shen and Huang proposed attention - based CNN using word level attention mechanism that is able to better determine which parts of the sentence are more influential .,related work,Related Work,0,43,17,17,0,related work : Related Work,0.23756906077348067,0.85,0.85
relation-classification,6,"In contrast with end - to - end model , several works proposed models utilizing the shortest dependency path ( SDP ) between entity pairs of dependency parse trees .",related work,Related Work,0,44,18,18,0,related work : Related Work,0.2430939226519337,0.9,0.9
relation-classification,6,SDP - LSTM model proposed by Yan et al . and deep recurrent neural networks ( DRNNs ) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP .,related work,Related Work,0,45,19,19,0,related work : Related Work,0.24861878453038674,0.95,0.95
relation-classification,6,SDP - LSTM model proposed by Yan et al . and deep recurrent neural networks ( DRNNs ) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP .,related work,Related Work,0,46,20,20,0,related work : Related Work,0.2541436464088398,1.0,1.0
relation-classification,6,Model,model,Model,0,47,1,1,0,model : Model,0.2596685082872928,0.012987012987012988,0.09090909090909091
relation-classification,6,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .",model,Model,1,48,2,2,0,model : Model,0.26519337016574585,0.025974025974025976,0.18181818181818182
relation-classification,6,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .",model,Model,1,49,3,3,0,model : Model,0.27071823204419887,0.03896103896103896,0.2727272727272727
relation-classification,6,"After that , the features are averaged along the time steps to produce the sentencelevel features .",model,Model,0,50,4,4,0,model : Model,0.27624309392265195,0.05194805194805195,0.36363636363636365
relation-classification,6,Word Representation,model,Model,0,51,5,5,0,model : Model,0.281767955801105,0.06493506493506493,0.45454545454545453
relation-classification,6,Let a input sentence is denoted by,model,Model,0,52,6,6,0,model : Model,0.287292817679558,0.07792207792207792,0.5454545454545454
relation-classification,6,where n is the number of words .,model,Model,0,53,7,7,0,model : Model,0.292817679558011,0.09090909090909091,0.6363636363636364
relation-classification,6,"We transform each word into vector representations by looking up word embedding matrix W word ? R dw |V | , where d w is the dimension of the vector and | V | is the size of vocabulary .",model,Model,0,54,8,8,0,model : Model,0.2983425414364641,0.1038961038961039,0.7272727272727273
relation-classification,6,"We transform each word into vector representations by looking up word embedding matrix W word ? R dw |V | , where d w is the dimension of the vector and | V | is the size of vocabulary .",model,Model,0,55,9,9,0,model : Model,0.30386740331491713,0.11688311688311688,0.8181818181818182
relation-classification,6,"Then the word representations X = {x 1 , x 2 , ... , x n } are obtained by mapping w i , the i - th word , to a column vector xi ? R dw are fed into the next layer .",model,Model,0,56,10,10,0,model : Model,0.30939226519337015,0.12987012987012986,0.9090909090909091
relation-classification,6,"Then the word representations X = {x 1 , x 2 , ... , x n } are obtained by mapping w i , the i - th word , to a column vector xi ? R dw are fed into the next layer .",model,Model,0,57,11,11,0,model : Model,0.3149171270718232,0.14285714285714285,1.0
relation-classification,6,Self Attention,model,Self Attention,0,58,12,1,0,model : Self Attention,0.32044198895027626,0.15584415584415584,0.038461538461538464
relation-classification,6,We can obtain the richer word representations by using self attentions .,model,Self Attention,0,59,13,2,0,model : Self Attention,0.3259668508287293,0.16883116883116883,0.07692307692307693
relation-classification,6,These word representations are considered the context based on correlation between words in a sentence .,model,Self Attention,0,60,14,3,0,model : Self Attention,0.3314917127071823,0.18181818181818182,0.11538461538461539
relation-classification,6,"The illustrates the results of the self attention in the sentence , "" the ?e1 ? pollution ? / e1 ?was caused by the ?e2 ? shipwrek ?/e2 ? "" , which is labeled Cause - Effect ( e1 , e2 ) .",model,Self Attention,0,61,15,4,0,model : Self Attention,0.3370165745856354,0.19480519480519481,0.15384615384615385
relation-classification,6,There are visualizations of the two heads in the multi-head attention applied for self attention .,model,Self Attention,0,62,16,5,0,model : Self Attention,0.3425414364640884,0.2077922077922078,0.19230769230769232
relation-classification,6,"The color density indicates the attention values , results of Equation 3.1 , which means how much an entity focuses on each word in a sentence .",model,Self Attention,0,63,17,6,0,model : Self Attention,0.34806629834254144,0.22077922077922077,0.23076923076923078
relation-classification,6,"In , the left represents the words that pollution , the first entity , focuses on and the right represents the words that shipwreck , the second entity , focuses on .",model,Self Attention,0,64,18,7,0,model : Self Attention,0.35359116022099446,0.23376623376623376,0.2692307692307692
relation-classification,6,"We can recognize that the entity pair is commonly concentrated on was , caused , and each other .",model,Self Attention,0,65,19,8,0,model : Self Attention,0.35911602209944754,0.24675324675324675,0.3076923076923077
relation-classification,6,"Actually , these words play the most important role in semantically predicting the Cause - Effect ( e1 , e2 ) , which is the relation class of this entity pair .",model,Self Attention,0,66,20,9,0,model : Self Attention,0.36464088397790057,0.2597402597402597,0.34615384615384615
relation-classification,6,"shows where the model focuses on the sentence to compute relations between entity pairs , which is the result of visualizing the alpha vectors in Equation 3.9 .",model,Self Attention,0,67,21,10,0,model : Self Attention,0.3701657458563536,0.2727272727272727,0.38461538461538464
relation-classification,6,"The important words in sentence are highlighted in yellow , which means that the more clearly the color is , the more important it is .",model,Self Attention,0,68,22,11,0,model : Self Attention,0.3756906077348066,0.2857142857142857,0.4230769230769231
relation-classification,6,"For example , in the first sentence , the inside is strongly highlighted , which is actually the best word representing the relation Component - whole ( e 1 , e2 ) between the given entity pair .",model,Self Attention,0,69,23,12,0,model : Self Attention,0.3812154696132597,0.2987012987012987,0.46153846153846156
relation-classification,6,"As another example , in the third sentence , the highlighted assess and using represent the relation , Instrument - Agency ( e2 , e1 ) between entity pair , analysts and frequency , well .",model,Self Attention,0,70,24,13,0,model : Self Attention,0.3867403314917127,0.3116883116883117,0.5
relation-classification,6,"We can see that the using is more highlighted than the assess , because the former represents the relation better .",model,Self Attention,0,71,25,14,0,model : Self Attention,0.39226519337016574,0.3246753246753247,0.5384615384615384
relation-classification,6,"visualizes latent type representation t j? { 1 , 2 } in Equation 3.12",model,Self Attention,0,72,26,15,0,model : Self Attention,0.39779005524861877,0.33766233766233766,0.5769230769230769
relation-classification,6,"Since the dimensionality of representation vectors are too large to visualize , we applied the t - SNE , one of the most popular dimensionality reduction methods .",model,Self Attention,0,73,27,16,0,model : Self Attention,0.40331491712707185,0.35064935064935066,0.6153846153846154
relation-classification,6,"In , the red points represent latent type vectors c i?K and the rests are latent type representations t j , where the colors of points are determined by the closest of the latent type vectors in the vector space of the original dimensionality .",model,Self Attention,0,74,28,17,0,model : Self Attention,0.4088397790055249,0.36363636363636365,0.6538461538461539
relation-classification,6,The points are generally well divided and are almost uniformly distributed without being biased to one side .,model,Self Attention,0,75,29,18,0,model : Self Attention,0.4143646408839779,0.37662337662337664,0.6923076923076923
relation-classification,6,summarizes the results of extracting 50 entities in close order with each latent type vector .,model,Self Attention,0,76,30,19,0,model : Self Attention,0.4198895027624309,0.38961038961038963,0.7307692307692307
relation-classification,6,This allows us to roughly understand what latent types of entities are .,model,Self Attention,0,77,31,20,0,model : Self Attention,0.425414364640884,0.4025974025974026,0.7692307692307693
relation-classification,6,We use a total of three types and find that similar characteristics appear in words grouped by together .,model,Self Attention,0,78,32,21,0,model : Self Attention,0.430939226519337,0.4155844155844156,0.8076923076923077
relation-classification,6,"In the type 1 , the words are related to human 's jobs and foods .",model,Self Attention,0,79,33,22,0,model : Self Attention,0.43646408839779005,0.42857142857142855,0.8461538461538461
relation-classification,6,"The type2 has a lot of entities related to machines and engineering like engine , woofer , and motor .",model,Self Attention,0,80,34,23,0,model : Self Attention,0.4419889502762431,0.44155844155844154,0.8846153846153846
relation-classification,6,"Finally , in type3 , there are many words with bad meanings related associated with dis asters and :",model,Self Attention,0,81,35,24,0,model : Self Attention,0.44751381215469616,0.45454545454545453,0.9230769230769231
relation-classification,6,Sets of Entities grouped by Latent Types drugs .,model,Self Attention,0,82,36,25,0,model : Self Attention,0.4530386740331492,0.4675324675324675,0.9615384615384616
relation-classification,6,"As a result , each type has a set of words with similar characteristics , which can prove that LET works effectively .",model,Self Attention,0,83,37,26,0,model : Self Attention,0.4585635359116022,0.4805194805194805,1.0
relation-classification,6,Bidirectional LSTM,model,Bidirectional LSTM Network,0,84,38,1,0,model : Bidirectional LSTM Network,0.46408839779005523,0.4935064935064935,0.08333333333333333
relation-classification,6,Network,model,Bidirectional LSTM Network,0,85,39,2,0,model : Bidirectional LSTM Network,0.4696132596685083,0.5064935064935064,0.16666666666666666
relation-classification,6,"For sequentially encoding the output of self attention layer , we use a BLSTM that consists of two sub LSTM networks : a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence .",model,Bidirectional LSTM Network,0,86,40,3,0,model : Bidirectional LSTM Network,0.47513812154696133,0.5194805194805194,0.25
relation-classification,6,"More formally , BLSTM works as follows :",model,Bidirectional LSTM Network,0,87,41,4,0,model : Bidirectional LSTM Network,0.48066298342541436,0.5324675324675324,0.3333333333333333
relation-classification,6,The representation vectors M obtained from self attention layer are forwarded into to the network step by step .,model,Bidirectional LSTM Network,0,88,42,5,0,model : Bidirectional LSTM Network,0.4861878453038674,0.5454545454545454,0.4166666666666667
relation-classification,6,"At the time step t , the hidden state",model,Bidirectional LSTM Network,0,89,43,6,0,model : Bidirectional LSTM Network,0.49171270718232046,0.5584415584415584,0.5
relation-classification,6,Entity - aware Attention Mechanism,model,Bidirectional LSTM Network,0,90,44,7,0,model : Bidirectional LSTM Network,0.4972375690607735,0.5714285714285714,0.5833333333333334
relation-classification,6,Although many models with attention mechanism achieved state - of - the - art performance in many NLP tasks .,model,Bidirectional LSTM Network,0,91,45,8,0,model : Bidirectional LSTM Network,0.5027624309392266,0.5844155844155844,0.6666666666666666
relation-classification,6,"However , for the relation classification task , these models lack of prior knowledge forgiven entity pairs , which could be powerful hints for solving the task .",model,Bidirectional LSTM Network,0,92,46,9,0,model : Bidirectional LSTM Network,0.5082872928176796,0.5974025974025974,0.75
relation-classification,6,Relation classification differs from sentence classification in that information about entities is given along with sentences .,model,Bidirectional LSTM Network,0,93,47,10,0,model : Bidirectional LSTM Network,0.5138121546961326,0.6103896103896104,0.8333333333333334
relation-classification,6,We propose a novel entity - aware attention mechanism for fully utilizing informative factors in given entity pairs .,model,Bidirectional LSTM Network,0,94,48,11,0,model : Bidirectional LSTM Network,0.5193370165745856,0.6233766233766234,0.9166666666666666
relation-classification,6,"Entity - aware attention utilizes the two additional features except H = {h 1 , h 2 , ... , h n } , ( 1 ) relative position features , ( 2 ) entity features with LET , and the final sentence representation z , result of the attention , is computed as follows :",model,Bidirectional LSTM Network,0,95,49,12,0,model : Bidirectional LSTM Network,0.5248618784530387,0.6363636363636364,1.0
relation-classification,6,Relative Position Features,model,Relative Position Features,0,96,50,1,0,model : Relative Position Features,0.5303867403314917,0.6493506493506493,0.07692307692307693
relation-classification,6,"In relation classification , the position of each word relative to entities has been widely used for word representations .",model,Relative Position Features,0,97,51,2,0,model : Relative Position Features,0.5359116022099447,0.6623376623376623,0.15384615384615385
relation-classification,6,"Recently , position - aware attention is published as away to use the relative position features more effectively .",model,Relative Position Features,0,98,52,3,0,model : Relative Position Features,0.5414364640883977,0.6753246753246753,0.23076923076923078
relation-classification,6,"It is a variant of attention mechanisms , which use not only outputs of BLSTM but also the relative position features when calculating attention weights .",model,Relative Position Features,0,99,53,4,0,model : Relative Position Features,0.5469613259668509,0.6883116883116883,0.3076923076923077
relation-classification,6,We adopt this method with slightly modification as shown in Equation 3.8 .,model,Relative Position Features,0,100,54,5,0,model : Relative Position Features,0.5524861878453039,0.7012987012987013,0.38461538461538464
relation-classification,6,"In the equation , p e 1 i ? R dp and p e 2 i ? R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",model,Relative Position Features,0,101,55,6,0,model : Relative Position Features,0.5580110497237569,0.7142857142857143,0.46153846153846156
relation-classification,6,"In the equation , p e 1 i ? R dp and p e 2 i ? R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",model,Relative Position Features,0,102,56,7,0,model : Relative Position Features,0.56353591160221,0.7272727272727273,0.5384615384615384
relation-classification,6,"In the equation , p e 1 i ? R dp and p e 2 i ? R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",model,Relative Position Features,0,103,57,8,0,model : Relative Position Features,0.569060773480663,0.7402597402597403,0.6153846153846154
relation-classification,6,"Similar to word embeddings , the relative positions are converted to vector representations by looking up learnable embedding matrix W pos ? R dp ( 2L?1 ) , where d p is the dimension of the relative position vectors and L is the maximum sentence length .",model,Relative Position Features,0,104,58,9,0,model : Relative Position Features,0.574585635359116,0.7532467532467533,0.6923076923076923
relation-classification,6,"Similar to word embeddings , the relative positions are converted to vector representations by looking up learnable embedding matrix W pos ? R dp ( 2L?1 ) , where d p is the dimension of the relative position vectors and L is the maximum sentence length .",model,Relative Position Features,0,105,59,10,0,model : Relative Position Features,0.580110497237569,0.7662337662337663,0.7692307692307693
relation-classification,6,"Finally , the representations of BLSTM layer take into account the context and the positional relationship with entities by concatenating hi , p e 1 i , and p e 2 i .",model,Relative Position Features,0,106,60,11,0,model : Relative Position Features,0.585635359116022,0.7792207792207793,0.8461538461538461
relation-classification,6,The representation is linearly transformed by W H ? R da ( 2 d h + 2 dp ) as in the Equation 3.8 .,model,Relative Position Features,0,107,61,12,0,model : Relative Position Features,0.5911602209944752,0.7922077922077922,0.9230769230769231
relation-classification,6,The representation is linearly transformed by W H ? R da ( 2 d h + 2 dp ) as in the Equation 3.8 .,model,Relative Position Features,0,108,62,13,0,model : Relative Position Features,0.5966850828729282,0.8051948051948052,1.0
relation-classification,6,Entity Features with Latent Type,model,Entity Features with Latent Type,0,109,63,1,0,model : Entity Features with Latent Type,0.6022099447513812,0.8181818181818182,0.06666666666666667
relation-classification,6,"Since entity pairs are powerful hints for solving relation classification task , we involve the entity pairs and their types in the attention mechanism to effectively train relations between entity pairs and other words in a sentence .",model,Entity Features with Latent Type,0,110,64,2,0,model : Entity Features with Latent Type,0.6077348066298343,0.8311688311688312,0.13333333333333333
relation-classification,6,We employ the two entity - aware features .,model,Entity Features with Latent Type,0,111,65,3,0,model : Entity Features with Latent Type,0.6132596685082873,0.8441558441558441,0.2
relation-classification,6,"The first is the hidden states of BLSTM corresponding to positions of entity pairs , which are high - level features representing entities .",model,Entity Features with Latent Type,0,112,66,4,0,model : Entity Features with Latent Type,0.6187845303867403,0.8571428571428571,0.26666666666666666
relation-classification,6,"These are denoted by h ei ? R 2d h , where e i is index of i - th entity .",model,Entity Features with Latent Type,0,113,67,5,0,model : Entity Features with Latent Type,0.6243093922651933,0.8701298701298701,0.3333333333333333
relation-classification,6,"These are denoted by h ei ? R 2d h , where e i is index of i - th entity .",model,Entity Features with Latent Type,0,114,68,6,0,model : Entity Features with Latent Type,0.6298342541436464,0.8831168831168831,0.4
relation-classification,6,"In addition , latent types of the entities obtained by LET , our proposed novel method , are the second one .",model,Entity Features with Latent Type,0,115,69,7,0,model : Entity Features with Latent Type,0.6353591160220995,0.8961038961038961,0.4666666666666667
relation-classification,6,"Using types as features can be a great way to improve performance , since the types of entities alone can be inferred the approximate relations .",model,Entity Features with Latent Type,0,116,70,8,0,model : Entity Features with Latent Type,0.6408839779005525,0.9090909090909091,0.5333333333333333
relation-classification,6,"Because the annotated types are not given , we use the latent type representations by applying the LET inspired by latent topic clustering , a method for predicting latent topic of texts in question answering task .",model,Entity Features with Latent Type,0,117,71,9,0,model : Entity Features with Latent Type,0.6464088397790055,0.922077922077922,0.6
relation-classification,6,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms .,model,Entity Features with Latent Type,0,118,72,10,0,model : Entity Features with Latent Type,0.6519337016574586,0.935064935064935,0.6666666666666666
relation-classification,6,The mathematical formulation is the follows :,model,Entity Features with Latent Type,0,119,73,11,0,model : Entity Features with Latent Type,0.6574585635359116,0.948051948051948,0.7333333333333333
relation-classification,6,where c i is the i - th latent type vector and K is the number of latent entity types .,model,Entity Features with Latent Type,0,120,74,12,0,model : Entity Features with Latent Type,0.6629834254143646,0.961038961038961,0.8
relation-classification,6,"As a result , entity features are constructed by concatenating the hidden states corresponding entity positions and types of entity pairs .",model,Entity Features with Latent Type,0,121,75,13,0,model : Entity Features with Latent Type,0.6685082872928176,0.974025974025974,0.8666666666666667
relation-classification,6,"After linear transformation of the entity features , they add up with the representations of BLSTM layer as in Equation 3.8 , and the representation of sentence z ? R 2 d h is computed by Equations from 3.8 to 3.10 .",model,Entity Features with Latent Type,0,122,76,14,0,model : Entity Features with Latent Type,0.6740331491712708,0.987012987012987,0.9333333333333333
relation-classification,6,"After linear transformation of the entity features , they add up with the representations of BLSTM layer as in Equation 3.8 , and the representation of sentence z ? R 2 d h is computed by Equations from 3.8 to 3.10 .",model,Entity Features with Latent Type,0,123,77,15,0,model : Entity Features with Latent Type,0.6795580110497238,1.0,1.0
relation-classification,6,Classification and Training,training,Classification and Training,0,124,1,1,0,training : Classification and Training,0.6850828729281768,0.08333333333333333,0.08333333333333333
relation-classification,6,The sentence representation obtained from the entity - aware attention z is fed into a fully connected softmax layer for classification .,training,Classification and Training,0,125,2,2,0,training : Classification and Training,0.6906077348066298,0.16666666666666666,0.16666666666666666
relation-classification,6,"It produces the conditional probability p ( y|S , ? ) over all relation types :",training,Classification and Training,0,126,3,3,0,training : Classification and Training,0.6961325966850829,0.25,0.25
relation-classification,6,where y is a target relation class and S is the input sentence .,training,Classification and Training,0,127,4,4,0,training : Classification and Training,0.7016574585635359,0.3333333333333333,0.3333333333333333
relation-classification,6,The ? is whole learnable parameters in the whole network including,training,Classification and Training,0,128,5,5,0,training : Classification and Training,0.7071823204419889,0.4166666666666667,0.4166666666666667
relation-classification,6,The ? is whole learnable parameters in the whole network including,training,Classification and Training,0,129,6,6,0,training : Classification and Training,0.712707182320442,0.5,0.5
relation-classification,6,where | R| is the number of relation classes .,training,Classification and Training,0,130,7,7,0,training : Classification and Training,0.7182320441988951,0.5833333333333334,0.5833333333333334
relation-classification,6,"loss function L is the cross entropy between the predictions and the ground truths , which is defined as :",training,Classification and Training,0,131,8,8,0,training : Classification and Training,0.7237569060773481,0.6666666666666666,0.6666666666666666
relation-classification,6,"where | D| is the size of training dataset and ( S ( i ) , y ( i ) ) is the i - th sample in the dataset .",training,Classification and Training,0,132,9,9,0,training : Classification and Training,0.7292817679558011,0.75,0.75
relation-classification,6,We minimize the loss L using AdaDelta optimizer to compute the parameters ? of our model .,training,Classification and Training,0,133,10,10,0,training : Classification and Training,0.7348066298342542,0.8333333333333334,0.8333333333333334
relation-classification,6,"To alleviate overfitting , we constrain the L2 regularization with the coefficient ?.",training,Classification and Training,0,134,11,11,0,training : Classification and Training,0.7403314917127072,0.9166666666666666,0.9166666666666666
relation-classification,6,"In addition , the dropout method is applied afterword embedding , LSTM network , and entity - aware attention to prevent co-adaptation of hidden units by randomly omitting feature detectors .",training,Classification and Training,0,135,12,12,0,training : Classification and Training,0.7458563535911602,1.0,1.0
relation-classification,6,Experiments,experiment,Experiments,0,136,1,1,0,experiment : Experiments,0.7513812154696132,1.0,1.0
relation-classification,6,Dataset and Evaluation Metrics,dataset,Dataset and Evaluation Metrics,0,137,1,1,0,dataset : Dataset and Evaluation Metrics,0.7569060773480663,0.16666666666666666,0.16666666666666666
relation-classification,6,"We evaluate our model on the SemEval - 2010 Task 8 dataset , which is an commonly used benchmark for relation classification and compare the results with the state - of - the - art models in this are a .",dataset,Dataset and Evaluation Metrics,0,138,2,2,0,dataset : Dataset and Evaluation Metrics,0.7624309392265194,0.3333333333333333,0.3333333333333333
relation-classification,6,"The dataset contains 10 distinguished relations , Cause - Effect , Instrument - Agency , Product - Producer , Content - Container , Entity - Origin , Entity - Destination , Component - Whole , Member - Collection , Message - Topic , and Other .",dataset,Dataset and Evaluation Metrics,0,139,3,3,0,dataset : Dataset and Evaluation Metrics,0.7679558011049724,0.5,0.5
relation-classification,6,"The former 9 relations have two directions , whereas Other is not directional , so the total number of relations is 19 .",dataset,Dataset and Evaluation Metrics,0,140,4,4,0,dataset : Dataset and Evaluation Metrics,0.7734806629834254,0.6666666666666666,0.6666666666666666
relation-classification,6,"There are 10,717 annotated sentences which consist of 8,000 samples for training and 2,717 samples for testing .",dataset,Dataset and Evaluation Metrics,0,141,5,5,0,dataset : Dataset and Evaluation Metrics,0.7790055248618785,0.8333333333333334,0.8333333333333334
relation-classification,6,"We adopt the official evaluation metric of SemEval - 2010 Task 8 , which is based on the macro -averaged F1 - score ( excluding Other ) , and takes into consideration the directionality .",dataset,Dataset and Evaluation Metrics,0,142,6,6,0,dataset : Dataset and Evaluation Metrics,0.7845303867403315,1.0,1.0
relation-classification,6,Implementation Details,implementation,Implementation Details,0,143,1,1,0,implementation : Implementation Details,0.7900552486187845,0.06666666666666667,0.3333333333333333
relation-classification,6,We tune the hyperparameters for our model on the development set randomly sampled 800 sentences for validation .,implementation,Implementation Details,0,144,2,2,0,implementation : Implementation Details,0.7955801104972375,0.13333333333333333,0.6666666666666666
relation-classification,6,The best hyperparameters in our proposed model are shown in following .,implementation,Implementation Details,0,145,3,3,0,implementation : Implementation Details,0.8011049723756906,0.2,1.0
relation-classification,6,Hyperparameter,implementation,Hyperparameter,0,146,4,1,0,implementation : Hyperparameter,0.8066298342541437,0.26666666666666666,0.08333333333333333
relation-classification,6,"Description Value We use pre-trained weights of the publicly available Glo Ve model to initialize word embeddings in our model , and other weights are randomly initialized from zero-mean Gaussian distribution .",implementation,Hyperparameter,0,147,5,2,0,implementation : Hyperparameter,0.8121546961325967,0.3333333333333333,0.16666666666666666
relation-classification,6,compares our Entity - aware Attention LSTM model with state - of - theart models on this relation classification dataset .,implementation,Hyperparameter,0,148,6,3,0,implementation : Hyperparameter,0.8176795580110497,0.4,0.25
relation-classification,6,"We divide the models into three groups , Non-Neural Model , SDP - based Model , and End - to - End Model .",implementation,Hyperparameter,0,149,7,4,0,implementation : Hyperparameter,0.8232044198895028,0.4666666666666667,0.3333333333333333
relation-classification,6,"First , the SVM , Non-Neural Model , was top of the SemEval - 2010 task , during the official competition period .",implementation,Hyperparameter,0,150,8,5,0,implementation : Hyperparameter,0.8287292817679558,0.5333333333333333,0.4166666666666667
relation-classification,6,They used many handcraft feature and SVM classifier .,implementation,Hyperparameter,0,151,9,6,0,implementation : Hyperparameter,0.8342541436464088,0.6,0.5
relation-classification,6,"As a result , they achieved an F1-score of 82.2 % .",implementation,Hyperparameter,0,152,10,7,0,implementation : Hyperparameter,0.8397790055248618,0.6666666666666666,0.5833333333333334
relation-classification,6,"The second is SDP - based Model such as MVRNN , FCM , DepNN , de pLCNN + NS , SDP - LSTM , and DRNNs .",implementation,Hyperparameter,0,153,11,8,0,implementation : Hyperparameter,0.8453038674033149,0.7333333333333333,0.6666666666666666
relation-classification,6,The SDP is reasonable features for detecting semantic structure of sentences .,implementation,Hyperparameter,0,154,12,9,0,implementation : Hyperparameter,0.850828729281768,0.8,0.75
relation-classification,6,"Actually , the SDP - based models show high performance , but SDP may not always be accurate and the parsing time is exponentially increased by long sentences .",implementation,Hyperparameter,0,155,13,10,0,implementation : Hyperparameter,0.856353591160221,0.8666666666666667,0.8333333333333334
relation-classification,6,The last model is End - to - End Model automatically learned internal representations can occur between the original inputs and the final outputs in deep learning .,implementation,Hyperparameter,0,156,14,11,0,implementation : Hyperparameter,0.861878453038674,0.9333333333333333,0.9166666666666666
relation-classification,6,"There are CNN - based models such as CNN , CR - CNN , and Attention - CNN and RNN - based models such as BLSTM , Attention - BLSTM , and Hierarchical - BLSTM ( Hier - BLSTM ) for this task .",implementation,Hyperparameter,0,157,15,12,0,implementation : Hyperparameter,0.8674033149171271,1.0,1.0
relation-classification,6,Experimental Results,experiment,Experimental Results,0,158,1,1,0,experiment : Experimental Results,0.8729281767955801,0.06666666666666667,1.0
relation-classification,6,Model F1,experiment,Model F1,0,159,2,1,0,experiment : Model F1,0.8784530386740331,0.13333333333333333,0.16666666666666666
relation-classification,6,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .",experiment,Model F1,1,160,3,2,0,experiment : Model F1,0.8839779005524862,0.2,0.3333333333333333
relation-classification,6,"However , they rely on high - level lexical features such as WordNet , dependency parse trees , POS tags , and NER tags from NLP tools .",experiment,Model F1,0,161,4,3,0,experiment : Model F1,0.8895027624309392,0.26666666666666666,0.5
relation-classification,6,The experimental results show that the LET is effective for relation classification .,experiment,Model F1,0,162,5,4,0,experiment : Model F1,0.8950276243093923,0.3333333333333333,0.6666666666666666
relation-classification,6,The LET improve a performance of 0.5 % than the model not applied it .,experiment,Model F1,0,163,6,5,0,experiment : Model F1,0.9005524861878453,0.4,0.8333333333333334
relation-classification,6,The model showed the best performance with three types .,experiment,Model F1,0,164,7,6,0,experiment : Model F1,0.9060773480662984,0.4666666666666667,1.0
relation-classification,6,Visualization,experiment,Visualization,0,165,8,1,0,experiment : Visualization,0.9116022099447514,0.5333333333333333,0.14285714285714285
relation-classification,6,There are three different visualization to demonstrate that our model is more interpretable .,experiment,Visualization,0,166,9,2,0,experiment : Visualization,0.9171270718232044,0.6,0.2857142857142857
relation-classification,6,"First , the visualization of self attention shows where each word focus on parts of a sentence .",experiment,Visualization,0,167,10,3,0,experiment : Visualization,0.9226519337016574,0.6666666666666666,0.42857142857142855
relation-classification,6,"By showing the words that the entity pair attends , we can find the words that well represent the relation between them .",experiment,Visualization,0,168,11,4,0,experiment : Visualization,0.9281767955801105,0.7333333333333333,0.5714285714285714
relation-classification,6,"Next , the entity - aware attention visualization shows where the model pays attend to a sentence .",experiment,Visualization,0,169,12,5,0,experiment : Visualization,0.9337016574585635,0.8,0.7142857142857143
relation-classification,6,"This visualization result highlights important words in a sentence , which are usually important keywords for classification .",experiment,Visualization,0,170,13,6,0,experiment : Visualization,0.9392265193370166,0.8666666666666667,0.8571428571428571
relation-classification,6,"Finally , we visualize representation of type in LET by using t- SNE , a method for dimensionality reduction , and group the whole entities in the dataset by the its latent types .",experiment,Visualization,0,171,14,7,0,experiment : Visualization,0.9447513812154696,0.9333333333333333,1.0
relation-classification,6,Entity - aware Attention,experiment,Entity-aware Attention,0,172,15,1,0,experiment : Entity-aware Attention,0.9502762430939227,1.0,1.0
relation-classification,6,Latent Entity Type,system description,Latent Entity Type,0,173,1,1,0,system description : Latent Entity Type,0.9558011049723757,1.0,1.0
relation-classification,6,Conclusion,conclusion,Conclusion,0,174,1,1,0,conclusion : Conclusion,0.9613259668508287,0.125,0.125
relation-classification,6,"In this paper , we proposed entity - aware attention mechanism with latent entity typing and a novel end - to - end recurrent neural model which incorporates this mechanism for relation classification .",conclusion,Conclusion,0,175,2,2,0,conclusion : Conclusion,0.9668508287292817,0.25,0.25
relation-classification,6,Our model achieves 85.2 % F1 - score in SemEval- 2010,conclusion,Conclusion,0,176,3,3,0,conclusion : Conclusion,0.9723756906077348,0.375,0.375
relation-classification,6,Task 8 using only raw sentence and word embeddings without any high - level features from NLP tools and it outperforms existing state - of - the - art methods .,conclusion,Conclusion,0,177,4,4,0,conclusion : Conclusion,0.9779005524861878,0.5,0.5
relation-classification,6,"In addition , our three visualizations of attention mechanisms applied to the model demonstrate that our model is more interpretable than previous models .",conclusion,Conclusion,0,178,5,5,0,conclusion : Conclusion,0.9834254143646409,0.625,0.625
relation-classification,6,We expect our model to be extended not only the relation classification task but also other tasks that entity plays an important role .,conclusion,Conclusion,0,179,6,6,0,conclusion : Conclusion,0.988950276243094,0.75,0.75
relation-classification,6,"Especially , latent entity typing can be effectively applied to sequence modeling task using entity information without NER .",conclusion,Conclusion,0,180,7,7,0,conclusion : Conclusion,0.994475138121547,0.875,0.875
relation-classification,6,"In the future , we will propose a new method in question answering or knowledge base population based on relations between entities extracted from our model .",conclusion,Conclusion,0,181,8,8,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,7,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,title,1,2,1,1,0,title : title,0.010050251256281407,1.0,1.0
relation-classification,7,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01507537688442211,0.07692307692307693,0.07692307692307693
relation-classification,7,Motivation :,abstract,abstract,0,4,2,2,0,abstract : abstract,0.020100502512562814,0.15384615384615385,0.15384615384615385
relation-classification,7,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.02512562814070352,0.23076923076923078,0.23076923076923078
relation-classification,7,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.03015075376884422,0.3076923076923077,0.3076923076923077
relation-classification,7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.035175879396984924,0.38461538461538464,0.38461538461538464
relation-classification,7,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,abstract,1,8,6,6,0,abstract : abstract,0.04020100502512563,0.46153846153846156,0.46153846153846156
relation-classification,7,Results :,abstract,abstract,0,9,7,7,0,abstract : abstract,0.04522613065326633,0.5384615384615384,0.5384615384615384
relation-classification,7,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.05025125628140704,0.6153846153846154,0.6153846153846154
relation-classification,7,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.05527638190954774,0.6923076923076923,0.6923076923076923
relation-classification,7,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.06030150753768844,0.7692307692307693,0.7692307692307693
relation-classification,7,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.06532663316582915,0.8461538461538461,0.8461538461538461
relation-classification,7,Availability and implementation :,abstract,abstract,0,14,12,12,0,abstract : abstract,0.07035175879396985,0.9230769230769231,0.9230769230769231
relation-classification,7,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract,abstract,1,15,13,13,0,abstract : abstract,0.07537688442211055,1.0,1.0
relation-classification,7,Introduction,introduction,introduction,0,16,1,1,0,introduction : introduction,0.08040201005025126,0.058823529411764705,0.058823529411764705
relation-classification,7,The volume of biomedical literature continues to rapidly increase .,introduction,introduction,0,17,2,2,0,introduction : introduction,0.08542713567839195,0.11764705882352941,0.11764705882352941
relation-classification,7,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",introduction,introduction,0,18,3,3,0,introduction : introduction,0.09045226130653267,0.17647058823529413,0.17647058823529413
relation-classification,7,PubMed alone has a total of 29M articles as of January 2019 .,introduction,introduction,0,19,4,4,0,introduction : introduction,0.09547738693467336,0.23529411764705882,0.23529411764705882
relation-classification,7,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,introduction,introduction,0,20,5,5,0,introduction : introduction,0.10050251256281408,0.29411764705882354,0.29411764705882354
relation-classification,7,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",introduction,introduction,0,21,6,6,0,introduction : introduction,0.10552763819095477,0.35294117647058826,0.35294117647058826
relation-classification,7,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,introduction,introduction,0,22,7,7,0,introduction : introduction,0.11055276381909548,0.4117647058823529,0.4117647058823529
relation-classification,7,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",introduction,introduction,0,23,8,8,0,introduction : introduction,0.11557788944723618,0.47058823529411764,0.47058823529411764
relation-classification,7,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,introduction,introduction,0,24,9,9,0,introduction : introduction,0.12060301507537688,0.5294117647058824,0.5294117647058824
relation-classification,7,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",introduction,introduction,0,25,10,10,0,introduction : introduction,0.12562814070351758,0.5882352941176471,0.5882352941176471
relation-classification,7,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",introduction,introduction,0,26,11,11,0,introduction : introduction,0.1306532663316583,0.6470588235294118,0.6470588235294118
relation-classification,7,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",introduction,introduction,1,27,12,12,0,introduction : introduction,0.135678391959799,0.7058823529411765,0.7058823529411765
relation-classification,7,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",introduction,introduction,0,28,13,13,0,introduction : introduction,0.1407035175879397,0.7647058823529411,0.7647058823529411
relation-classification,7,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",introduction,introduction,0,29,14,14,0,introduction : introduction,0.1457286432160804,0.8235294117647058,0.8235294117647058
relation-classification,7,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions thatare usually not included in a general domain corpus .",introduction,introduction,0,30,15,15,0,introduction : introduction,0.1507537688442211,0.8823529411764706,0.8823529411764706
relation-classification,7,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",introduction,introduction,0,31,16,16,0,introduction : introduction,0.15577889447236182,0.9411764705882353,0.9411764705882353
relation-classification,7,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",introduction,introduction,0,32,17,17,0,introduction : introduction,0.16080402010050251,1.0,1.0
relation-classification,7,Approach,approach,Approach,0,33,1,1,0,approach : Approach,0.1658291457286432,0.07142857142857142,0.07142857142857142
relation-classification,7,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",approach,Approach,1,34,2,2,0,approach : Approach,0.1708542713567839,0.14285714285714285,0.14285714285714285
relation-classification,7,The over all process of pre-training and fine - tuning BioBERT is illustrated in .,approach,Approach,0,35,3,3,0,approach : Approach,0.17587939698492464,0.21428571428571427,0.21428571428571427
relation-classification,7,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",approach,Approach,1,36,4,4,0,approach : Approach,0.18090452261306533,0.2857142857142857,0.2857142857142857
relation-classification,7,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",approach,Approach,1,37,5,5,0,approach : Approach,0.18592964824120603,0.35714285714285715,0.35714285714285715
relation-classification,7,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",approach,Approach,1,38,6,6,0,approach : Approach,0.19095477386934673,0.42857142857142855,0.42857142857142855
relation-classification,7,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",approach,Approach,0,39,7,7,0,approach : Approach,0.19597989949748743,0.5,0.5
relation-classification,7,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,approach,Approach,0,40,8,8,0,approach : Approach,0.20100502512562815,0.5714285714285714,0.5714285714285714
relation-classification,7,The contributions of our paper are as follows :,approach,Approach,0,41,9,9,0,approach : Approach,0.20603015075376885,0.6428571428571429,0.6428571428571429
relation-classification,7,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,approach,Approach,0,42,10,10,0,approach : Approach,0.21105527638190955,0.7142857142857143,0.7142857142857143
relation-classification,7,We show that pre-training BERT on biomedical corpora largely improves its performance .,approach,Approach,0,43,11,11,0,approach : Approach,0.21608040201005024,0.7857142857142857,0.7857142857142857
relation-classification,7,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",approach,Approach,0,44,12,12,0,approach : Approach,0.22110552763819097,0.8571428571428571,0.8571428571428571
relation-classification,7,"Compared with most previous biomedical text mining models thatare mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",approach,Approach,0,45,13,13,0,approach : Approach,0.22613065326633167,0.9285714285714286,0.9285714285714286
relation-classification,7,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",approach,Approach,0,46,14,14,0,approach : Approach,0.23115577889447236,1.0,1.0
relation-classification,7,Materials and methods,method,Materials and methods,0,47,1,1,0,method : Materials and methods,0.23618090452261306,0.02,0.3333333333333333
relation-classification,7,BioBERT basically has the same structure as BERT .,method,Materials and methods,0,48,2,2,0,method : Materials and methods,0.24120603015075376,0.04,0.6666666666666666
relation-classification,7,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",method,Materials and methods,0,49,3,3,0,method : Materials and methods,0.24623115577889448,0.06,1.0
relation-classification,7,BERT : bidirectional encoder representations from transformers,method,BERT: bidirectional encoder representations from transformers,0,50,4,1,0,method : BERT: bidirectional encoder representations from transformers,0.25125628140703515,0.08,0.09090909090909091
relation-classification,7,Learning word representations from a large amount of unannotated text is a long - established method .,method,BERT: bidirectional encoder representations from transformers,0,51,5,2,0,method : BERT: bidirectional encoder representations from transformers,0.2562814070351759,0.1,0.18181818181818182
relation-classification,7,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",method,BERT: bidirectional encoder representations from transformers,0,52,6,3,0,method : BERT: bidirectional encoder representations from transformers,0.2613065326633166,0.12,0.2727272727272727
relation-classification,7,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",method,BERT: bidirectional encoder representations from transformers,0,53,7,4,0,method : BERT: bidirectional encoder representations from transformers,0.2663316582914573,0.14,0.36363636363636365
relation-classification,7,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,method,BERT: bidirectional encoder representations from transformers,0,54,8,5,0,method : BERT: bidirectional encoder representations from transformers,0.271356783919598,0.16,0.45454545454545453
relation-classification,7,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",method,BERT: bidirectional encoder representations from transformers,0,55,9,6,0,method : BERT: bidirectional encoder representations from transformers,0.27638190954773867,0.18,0.5454545454545454
relation-classification,7,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",method,BERT: bidirectional encoder representations from transformers,0,56,10,7,0,method : BERT: bidirectional encoder representations from transformers,0.2814070351758794,0.2,0.6363636363636364
relation-classification,7,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",method,BERT: bidirectional encoder representations from transformers,0,57,11,8,0,method : BERT: bidirectional encoder representations from transformers,0.2864321608040201,0.22,0.7272727272727273
relation-classification,7,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",method,BERT: bidirectional encoder representations from transformers,0,58,12,9,0,method : BERT: bidirectional encoder representations from transformers,0.2914572864321608,0.24,0.8181818181818182
relation-classification,7,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,method,BERT: bidirectional encoder representations from transformers,0,59,13,10,0,method : BERT: bidirectional encoder representations from transformers,0.2964824120603015,0.26,0.9090909090909091
relation-classification,7,"Due to the space limitations , we refer readers to for a more detailed description of BERT .",method,BERT: bidirectional encoder representations from transformers,0,60,14,11,0,method : BERT: bidirectional encoder representations from transformers,0.3015075376884422,0.28,1.0
relation-classification,7,Pre-training BioBERT,method,Pre-training BioBERT,0,61,15,1,0,method : Pre-training BioBERT,0.3065326633165829,0.3,0.07142857142857142
relation-classification,7,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",method,Pre-training BioBERT,0,62,16,2,0,method : Pre-training BioBERT,0.31155778894472363,0.32,0.14285714285714285
relation-classification,7,"However , biomedical domain texts contain a considerable number of domain - specific .",method,Pre-training BioBERT,0,63,17,3,0,method : Pre-training BioBERT,0.3165829145728643,0.34,0.21428571428571427
relation-classification,7,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",method,Pre-training BioBERT,0,64,18,4,0,method : Pre-training BioBERT,0.32160804020100503,0.36,0.2857142857142857
relation-classification,7,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",method,Pre-training BioBERT,0,65,19,5,0,method : Pre-training BioBERT,0.32663316582914576,0.38,0.35714285714285715
relation-classification,7,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",method,Pre-training BioBERT,0,66,20,6,0,method : Pre-training BioBERT,0.3316582914572864,0.4,0.42857142857142855
relation-classification,7,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",method,Pre-training BioBERT,0,67,21,7,0,method : Pre-training BioBERT,0.33668341708542715,0.42,0.5
relation-classification,7,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",method,Pre-training BioBERT,0,68,22,8,0,method : Pre-training BioBERT,0.3417085427135678,0.44,0.5714285714285714
relation-classification,7,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,method,Pre-training BioBERT,0,69,23,9,0,method : Pre-training BioBERT,0.34673366834170855,0.46,0.6428571428571429
relation-classification,7,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",method,Pre-training BioBERT,0,70,24,10,0,method : Pre-training BioBERT,0.35175879396984927,0.48,0.7142857142857143
relation-classification,7,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",method,Pre-training BioBERT,0,71,25,11,0,method : Pre-training BioBERT,0.35678391959798994,0.5,0.7857142857142857
relation-classification,7,##mm ##uno ##g ##lo # #bul # #in ) .,method,Pre-training BioBERT,0,72,26,12,0,method : Pre-training BioBERT,0.36180904522613067,0.52,0.8571428571428571
relation-classification,7,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,method,Pre-training BioBERT,0,73,27,13,0,method : Pre-training BioBERT,0.36683417085427134,0.54,0.9285714285714286
relation-classification,7,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",method,Pre-training BioBERT,0,74,28,14,0,method : Pre-training BioBERT,0.37185929648241206,0.56,1.0
relation-classification,7,Fine-tuning BioBERT,method,Fine-tuning BioBERT,0,75,29,1,0,method : Fine-tuning BioBERT,0.3768844221105528,0.58,0.045454545454545456
relation-classification,7,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",method,Fine-tuning BioBERT,0,76,30,2,0,method : Fine-tuning BioBERT,0.38190954773869346,0.6,0.09090909090909091
relation-classification,7,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",method,Fine-tuning BioBERT,0,77,31,3,0,method : Fine-tuning BioBERT,0.3869346733668342,0.62,0.13636363636363635
relation-classification,7,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",method,Fine-tuning BioBERT,0,78,32,4,0,method : Fine-tuning BioBERT,0.39195979899497485,0.64,0.18181818181818182
relation-classification,7,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",method,Fine-tuning BioBERT,0,79,33,5,0,method : Fine-tuning BioBERT,0.3969849246231156,0.66,0.22727272727272727
relation-classification,7,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,method,Fine-tuning BioBERT,0,80,34,6,0,method : Fine-tuning BioBERT,0.4020100502512563,0.68,0.2727272727272727
relation-classification,7,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",method,Fine-tuning BioBERT,0,81,35,7,0,method : Fine-tuning BioBERT,0.40703517587939697,0.7,0.3181818181818182
relation-classification,7,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",method,Fine-tuning BioBERT,0,82,36,8,0,method : Fine-tuning BioBERT,0.4120603015075377,0.72,0.36363636363636365
relation-classification,7,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,method,Fine-tuning BioBERT,0,83,37,9,0,method : Fine-tuning BioBERT,0.41708542713567837,0.74,0.4090909090909091
relation-classification,7,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",method,Fine-tuning BioBERT,0,84,38,10,0,method : Fine-tuning BioBERT,0.4221105527638191,0.76,0.45454545454545453
relation-classification,7,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,method,Fine-tuning BioBERT,0,85,39,11,0,method : Fine-tuning BioBERT,0.4271356783919598,0.78,0.5
relation-classification,7,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,method,Fine-tuning BioBERT,0,86,40,12,0,method : Fine-tuning BioBERT,0.4321608040201005,0.8,0.5454545454545454
relation-classification,7,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,method,Fine-tuning BioBERT,0,87,41,13,0,method : Fine-tuning BioBERT,0.4371859296482412,0.82,0.5909090909090909
relation-classification,7,"The precision , recall and F 1 scores on the RE task are reported .",method,Fine-tuning BioBERT,0,88,42,14,0,method : Fine-tuning BioBERT,0.44221105527638194,0.84,0.6363636363636364
relation-classification,7,Question answering is a task of answering questions posed in natural language given related passages .,method,Fine-tuning BioBERT,0,89,43,15,0,method : Fine-tuning BioBERT,0.4472361809045226,0.86,0.6818181818181818
relation-classification,7,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",method,Fine-tuning BioBERT,0,90,44,16,0,method : Fine-tuning BioBERT,0.45226130653266333,0.88,0.7272727272727273
relation-classification,7,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,method,Fine-tuning BioBERT,0,91,45,17,0,method : Fine-tuning BioBERT,0.457286432160804,0.9,0.7727272727272727
relation-classification,7,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,method,Fine-tuning BioBERT,0,92,46,18,0,method : Fine-tuning BioBERT,0.4623115577889447,0.92,0.8181818181818182
relation-classification,7,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",method,Fine-tuning BioBERT,0,93,47,19,0,method : Fine-tuning BioBERT,0.46733668341708545,0.94,0.8636363636363636
relation-classification,7,"Like , we excluded the samples with unanswerable questions from the training sets .",method,Fine-tuning BioBERT,0,94,48,20,0,method : Fine-tuning BioBERT,0.4723618090452261,0.96,0.9090909090909091
relation-classification,7,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",method,Fine-tuning BioBERT,0,95,49,21,0,method : Fine-tuning BioBERT,0.47738693467336685,0.98,0.9545454545454546
relation-classification,7,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",method,Fine-tuning BioBERT,0,96,50,22,0,method : Fine-tuning BioBERT,0.4824120603015075,1.0,1.0
relation-classification,7,Results,result,Results,0,97,1,1,0,result : Results,0.48743718592964824,1.0,1.0
relation-classification,7,Datasets,dataset,Datasets,0,98,1,1,0,dataset : Datasets,0.49246231155778897,0.05,0.05
relation-classification,7,The statistics of biomedical NER datasets are listed in .,dataset,Datasets,0,99,2,2,0,dataset : Datasets,0.49748743718592964,0.1,0.1
relation-classification,7,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",dataset,Datasets,0,100,3,3,0,dataset : Datasets,0.5025125628140703,0.15,0.15
relation-classification,7,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,dataset,Datasets,0,101,4,4,0,dataset : Datasets,0.507537688442211,0.2,0.2
relation-classification,7,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,dataset,Datasets,0,102,5,5,0,dataset : Datasets,0.5125628140703518,0.25,0.25
relation-classification,7,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,dataset,Datasets,0,103,6,6,0,dataset : Datasets,0.5175879396984925,0.3,0.3
relation-classification,7,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",dataset,Datasets,0,104,7,7,0,dataset : Datasets,0.5226130653266332,0.35,0.35
relation-classification,7,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets thatare frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",dataset,Datasets,0,105,8,8,0,dataset : Datasets,0.5276381909547738,0.4,0.4
relation-classification,7,The RE datasets contain gene - disease relations and protein - chemical relations ) .,dataset,Datasets,0,106,9,9,0,dataset : Datasets,0.5326633165829145,0.45,0.45
relation-classification,7,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,dataset,Datasets,0,107,10,10,0,dataset : Datasets,0.5376884422110553,0.5,0.5
relation-classification,7,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",dataset,Datasets,0,108,11,11,0,dataset : Datasets,0.542713567839196,0.55,0.55
relation-classification,7,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",dataset,Datasets,0,109,12,12,0,dataset : Datasets,0.5477386934673367,0.6,0.6
relation-classification,7,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,dataset,Datasets,0,110,13,13,0,dataset : Datasets,0.5527638190954773,0.65,0.65
relation-classification,7,We have made the pre-processed BioASQ datasets publicly available .,dataset,Datasets,0,111,14,14,0,dataset : Datasets,0.5577889447236181,0.7,0.7
relation-classification,7,"For all the datasets , we used the same dataset splits used in previous works ) for a fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",dataset,Datasets,0,112,15,15,0,dataset : Datasets,0.5628140703517588,0.75,0.75
relation-classification,7,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",dataset,Datasets,0,113,16,16,0,dataset : Datasets,0.5678391959798995,0.8,0.8
relation-classification,7,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,dataset,Datasets,0,114,17,17,0,dataset : Datasets,0.5728643216080402,0.85,0.85
relation-classification,7,Note that the state - of - the - art models each have a different architecture and training procedure .,dataset,Datasets,0,115,18,18,0,dataset : Datasets,0.5778894472361809,0.9,0.9
relation-classification,7,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",dataset,Datasets,0,116,19,19,0,dataset : Datasets,0.5829145728643216,0.95,0.95
relation-classification,7,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",dataset,Datasets,0,117,20,20,0,dataset : Datasets,0.5879396984924623,1.0,1.0
relation-classification,7,Experimental setups,experiment,Experimental setups,0,118,1,1,0,experiment : Experimental setups,0.592964824120603,0.03125,0.058823529411764705
relation-classification,7,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,experiment,Experimental setups,1,119,2,2,0,experiment : Experimental setups,0.5979899497487438,0.0625,0.11764705882352941
relation-classification,7,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,experiment,Experimental setups,1,120,3,3,0,experiment : Experimental setups,0.6030150753768844,0.09375,0.17647058823529413
relation-classification,7,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",experiment,Experimental setups,0,121,4,4,0,experiment : Experimental setups,0.6080402010050251,0.125,0.23529411764705882
relation-classification,7,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",experiment,Experimental setups,0,122,5,5,0,experiment : Experimental setups,0.6130653266331658,0.15625,0.29411764705882354
relation-classification,7,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",experiment,Experimental setups,0,123,6,6,0,experiment : Experimental setups,0.6180904522613065,0.1875,0.35294117647058826
relation-classification,7,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,experiment,Experimental setups,0,124,7,7,0,experiment : Experimental setups,0.6231155778894473,0.21875,0.4117647058823529
relation-classification,7,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",experiment,Experimental setups,0,125,8,8,0,experiment : Experimental setups,0.628140703517588,0.25,0.47058823529411764
relation-classification,7,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,experiment,Experimental setups,1,126,9,9,0,experiment : Experimental setups,0.6331658291457286,0.28125,0.5294117647058824
relation-classification,7,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",experiment,Experimental setups,1,127,10,10,0,experiment : Experimental setups,0.6381909547738693,0.3125,0.5882352941176471
relation-classification,7,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,experiment,Experimental setups,0,128,11,11,0,experiment : Experimental setups,0.6432160804020101,0.34375,0.6470588235294118
relation-classification,7,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",experiment,Experimental setups,0,129,12,12,0,experiment : Experimental setups,0.6482412060301508,0.375,0.7058823529411765
relation-classification,7,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,experiment,Experimental setups,1,130,13,13,0,experiment : Experimental setups,0.6532663316582915,0.40625,0.7647058823529411
relation-classification,7,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,experiment,Experimental setups,0,131,14,14,0,experiment : Experimental setups,0.6582914572864321,0.4375,0.8235294117647058
relation-classification,7,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",experiment,Experimental setups,0,132,15,15,0,experiment : Experimental setups,0.6633165829145728,0.46875,0.8823529411764706
relation-classification,7,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,experiment,Experimental setups,0,133,16,16,0,experiment : Experimental setups,0.6683417085427136,0.5,0.9411764705882353
relation-classification,7,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",experiment,Experimental setups,0,134,17,17,0,experiment : Experimental setups,0.6733668341708543,0.53125,1.0
relation-classification,7,Experimental results,experiment,Experimental results,0,135,18,1,0,experiment : Experimental results,0.678391959798995,0.5625,0.06666666666666667
relation-classification,7,The results of NER are shown in .,experiment,Experimental results,1,136,19,2,0,experiment : Experimental results,0.6834170854271356,0.59375,0.13333333333333333
relation-classification,7,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",experiment,Experimental results,0,137,20,3,0,experiment : Experimental results,0.6884422110552764,0.625,0.2
relation-classification,7,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",experiment,Experimental results,1,138,21,4,0,experiment : Experimental results,0.6934673366834171,0.65625,0.26666666666666666
relation-classification,7,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",experiment,Experimental results,1,139,22,5,0,experiment : Experimental results,0.6984924623115578,0.6875,0.3333333333333333
relation-classification,7,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",experiment,Experimental results,0,140,23,6,0,experiment : Experimental results,0.7035175879396985,0.71875,0.4
relation-classification,7,The RE results of each model are shown in .,experiment,Experimental results,1,141,24,7,0,experiment : Experimental results,0.7085427135678392,0.75,0.4666666666666667
relation-classification,7,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",experiment,Experimental results,0,142,25,8,0,experiment : Experimental results,0.7135678391959799,0.78125,0.5333333333333333
relation-classification,7,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",experiment,Experimental results,1,143,26,9,0,experiment : Experimental results,0.7185929648241206,0.8125,0.6
relation-classification,7,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",experiment,Experimental results,1,144,27,10,0,experiment : Experimental results,0.7236180904522613,0.84375,0.6666666666666666
relation-classification,7,The QA results are shown in .,experiment,Experimental results,1,145,28,11,0,experiment : Experimental results,0.7286432160804021,0.875,0.7333333333333333
relation-classification,7,We micro averaged the best scores of the state - of - the - art models from each batch .,experiment,Experimental results,0,146,29,12,0,experiment : Experimental results,0.7336683417085427,0.90625,0.8
relation-classification,7,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,experiment,Experimental results,0,147,30,13,0,experiment : Experimental results,0.7386934673366834,0.9375,0.8666666666666667
relation-classification,7,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",experiment,Experimental results,1,148,31,14,0,experiment : Experimental results,0.7437185929648241,0.96875,0.9333333333333333
relation-classification,7,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",experiment,Experimental results,1,149,32,15,0,experiment : Experimental results,0.7487437185929648,1.0,1.0
relation-classification,7,Discussion,discussion,Discussion,0,150,1,1,0,discussion : Discussion,0.7537688442211056,0.058823529411764705,0.058823529411764705
relation-classification,7,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,discussion,Discussion,0,151,2,2,0,discussion : Discussion,0.7587939698492462,0.11764705882352941,0.11764705882352941
relation-classification,7,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",discussion,Discussion,0,152,3,3,0,discussion : Discussion,0.7638190954773869,0.17647058823529413,0.17647058823529413
relation-classification,7,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",discussion,Discussion,0,153,4,4,0,discussion : Discussion,0.7688442211055276,0.23529411764705882,0.23529411764705882
relation-classification,7,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",discussion,Discussion,0,154,5,5,0,discussion : Discussion,0.7738693467336684,0.29411764705882354,0.29411764705882354
relation-classification,7,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,discussion,Discussion,0,155,6,6,0,discussion : Discussion,0.7788944723618091,0.35294117647058826,0.35294117647058826
relation-classification,7,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,discussion,Discussion,0,156,7,7,0,discussion : Discussion,0.7839195979899497,0.4117647058823529,0.4117647058823529
relation-classification,7,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,discussion,Discussion,0,157,8,8,0,discussion : Discussion,0.7889447236180904,0.47058823529411764,0.47058823529411764
relation-classification,7,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",discussion,Discussion,0,158,9,9,0,discussion : Discussion,0.7939698492462312,0.5294117647058824,0.5294117647058824
relation-classification,7,"F1 scores were used for NER / RE , and MRR scores were used for QA .",discussion,Discussion,0,159,10,10,0,discussion : Discussion,0.7989949748743719,0.5882352941176471,0.5882352941176471
relation-classification,7,BioBERT significantly improves performance on most of the datasets .,discussion,Discussion,0,160,11,11,0,discussion : Discussion,0.8040201005025126,0.6470588235294118,0.6470588235294118
relation-classification,7,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to see the effect of pre-training on downstream tasks .",discussion,Discussion,0,161,12,12,0,discussion : Discussion,0.8090452261306532,0.7058823529411765,0.7058823529411765
relation-classification,7,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,discussion,Discussion,0,162,13,13,0,discussion : Discussion,0.8140703517587939,0.7647058823529411,0.7647058823529411
relation-classification,7,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",discussion,Discussion,0,163,14,14,0,discussion : Discussion,0.8190954773869347,0.8235294117647058,0.8235294117647058
relation-classification,7,entities .,discussion,Discussion,0,164,15,15,0,discussion : Discussion,0.8241206030150754,0.8823529411764706,0.8823529411764706
relation-classification,7,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",discussion,Discussion,0,165,16,16,0,discussion : Discussion,0.8291457286432161,0.9411764705882353,0.9411764705882353
relation-classification,7,"Also , BioBERT can provide longer named entities as answers .",discussion,Discussion,0,166,17,17,0,discussion : Discussion,0.8341708542713567,1.0,1.0
relation-classification,7,Conclusion,conclusion,Conclusion,0,167,1,1,0,conclusion : Conclusion,0.8391959798994975,0.06666666666666667,0.06666666666666667
relation-classification,7,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",conclusion,Conclusion,0,168,2,2,0,conclusion : Conclusion,0.8442211055276382,0.13333333333333333,0.13333333333333333
relation-classification,7,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,conclusion,Conclusion,0,169,3,3,0,conclusion : Conclusion,0.8492462311557789,0.2,0.2
relation-classification,7,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",conclusion,Conclusion,0,170,4,4,0,conclusion : Conclusion,0.8542713567839196,0.26666666666666666,0.26666666666666666
relation-classification,7,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",conclusion,Conclusion,0,171,5,5,0,conclusion : Conclusion,0.8592964824120602,0.3333333333333333,0.3333333333333333
relation-classification,7,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,conclusion,Conclusion,0,172,6,6,0,conclusion : Conclusion,0.864321608040201,0.4,0.4
relation-classification,7,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",conclusion,Conclusion,0,173,7,7,0,conclusion : Conclusion,0.8693467336683417,0.4666666666666667,0.4666666666666667
relation-classification,7,"The best scores are in bold , and the second best scores are underlined .",conclusion,Conclusion,0,174,8,8,0,conclusion : Conclusion,0.8743718592964824,0.5333333333333333,0.5333333333333333
relation-classification,7,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",conclusion,Conclusion,0,175,9,9,0,conclusion : Conclusion,0.8793969849246231,0.6,0.6
relation-classification,7,"The best scores are in bold , and the second best scores are underlined .",conclusion,Conclusion,0,176,10,10,0,conclusion : Conclusion,0.8844221105527639,0.6666666666666666,0.6666666666666666
relation-classification,7,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",conclusion,Conclusion,0,177,11,11,0,conclusion : Conclusion,0.8894472361809045,0.7333333333333333,0.7333333333333333
relation-classification,7,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",conclusion,Conclusion,0,178,12,12,0,conclusion : Conclusion,0.8944723618090452,0.8,0.8
relation-classification,7,"The best scores are in bold , and the second best scores are underlined .",conclusion,Conclusion,0,179,13,13,0,conclusion : Conclusion,0.8994974874371859,0.8666666666666667,0.8666666666666667
relation-classification,7,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-are a.bioasq.org ) .,conclusion,Conclusion,0,180,14,14,0,conclusion : Conclusion,0.9045226130653267,0.9333333333333333,0.9333333333333333
relation-classification,7,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",conclusion,Conclusion,0,181,15,15,0,conclusion : Conclusion,0.9095477386934674,1.0,1.0
relation-classification,7,BioBERT,BioBERT,BioBERT,0,182,1,1,0,BioBERT : BioBERT,0.914572864321608,0.05555555555555555,0.5
relation-classification,7,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,BioBERT,BioBERT,0,183,2,2,0,BioBERT : BioBERT,0.9195979899497487,0.1111111111111111,1.0
relation-classification,7,BC2GM,BioBERT,BC2GM,0,184,3,1,0,BioBERT : BC2GM,0.9246231155778895,0.16666666666666666,0.3333333333333333
relation-classification,7,BERT,BioBERT,BC2GM,0,185,4,2,0,BioBERT : BC2GM,0.9296482412060302,0.2222222222222222,0.6666666666666666
relation-classification,7,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",BioBERT,BC2GM,0,186,5,3,0,BioBERT : BC2GM,0.9346733668341709,0.2777777777777778,1.0
relation-classification,7,QA,BioBERT,QA,0,187,6,1,0,BioBERT : QA,0.9396984924623115,0.3333333333333333,0.14285714285714285
relation-classification,7,BioASQ 6 b - factoid,BioBERT,QA,0,188,7,2,0,BioBERT : QA,0.9447236180904522,0.3888888888888889,0.2857142857142857
relation-classification,7,: Which type of urinary incontinence is diagnosed with the Q tip test ? BERT,BioBERT,QA,0,189,8,3,0,BioBERT : QA,0.949748743718593,0.4444444444444444,0.42857142857142855
relation-classification,7,: Which type of urinary incontinence is diagnosed with the Q tip test ? BERT,BioBERT,QA,0,190,9,4,0,BioBERT : QA,0.9547738693467337,0.5,0.5714285714285714
relation-classification,7,total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,BioBERT,QA,0,191,10,5,0,BioBERT : QA,0.9597989949748744,0.5555555555555556,0.7142857142857143
relation-classification,7,After undergoing ( . . .),BioBERT,QA,0,192,11,6,0,BioBERT : QA,0.964824120603015,0.6111111111111112,0.8571428571428571
relation-classification,7,"Q-tip test , . . .",BioBERT,QA,0,193,12,7,0,BioBERT : QA,0.9698492462311558,0.6666666666666666,1.0
relation-classification,7,: Which bacteria causes erythrasma ? BERT,BioBERT,Q: Which bacteria causes erythrasma? BERT,0,194,13,1,0,BioBERT : Q: Which bacteria causes erythrasma? BERT,0.9748743718592965,0.7222222222222222,0.3333333333333333
relation-classification,7,: Which bacteria causes erythrasma ? BERT,BioBERT,Q: Which bacteria causes erythrasma? BERT,0,195,14,2,0,BioBERT : Q: Which bacteria causes erythrasma? BERT,0.9798994974874372,0.7777777777777778,0.6666666666666666
relation-classification,7,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,BioBERT,Q: Which bacteria causes erythrasma? BERT,0,196,15,3,0,BioBERT : Q: Which bacteria causes erythrasma? BERT,0.9849246231155779,0.8333333333333334,1.0
relation-classification,7,Note :,BioBERT,Note:,0,197,16,1,0,BioBERT : Note:,0.9899497487437185,0.8888888888888888,0.5
relation-classification,7,Predicted named entities for NER and predicted answers for QA are in bold .,BioBERT,Note:,0,198,17,2,0,BioBERT : Note:,0.9949748743718593,0.9444444444444444,1.0
relation-classification,7,Funding,BioBERT,Funding,0,199,18,1,0,BioBERT : Funding,1.0,1.0,1.0
relation-classification,8,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,title,title,1,2,1,1,0,title : title,0.014598540145985401,1.0,1.0
relation-classification,8,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.021897810218978103,0.16666666666666666,0.16666666666666666
relation-classification,8,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.029197080291970802,0.3333333333333333,0.3333333333333333
relation-classification,8,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",abstract,abstract,1,5,3,3,0,abstract : abstract,0.0364963503649635,0.5,0.5
relation-classification,8,Our solution is built on top of the pre-trained self - attentive models ( Transformer ) .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.043795620437956206,0.6666666666666666,0.6666666666666666
relation-classification,8,"Since our method uses a single - pass to compute all relations at once , it scales to larger datasets easily ; which makes it more usable in real - world applications .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.051094890510948905,0.8333333333333334,0.8333333333333334
relation-classification,8,1,abstract,abstract,0,8,6,6,0,abstract : abstract,0.058394160583941604,1.0,1.0
relation-classification,8,Introduction,introduction,introduction,0,9,1,1,0,introduction : introduction,0.06569343065693431,0.08333333333333333,0.08333333333333333
relation-classification,8,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,introduction,introduction,1,10,2,2,0,introduction : introduction,0.072992700729927,0.16666666666666666,0.16666666666666666
relation-classification,8,"solution to this task is essential for many downstream NLP applications such as automatic knowledge - base completion , knowledge base question answering , and symbolic approaches for visual question answering , etc .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.08029197080291971,0.25,0.25
relation-classification,8,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,introduction,introduction,1,12,4,4,0,introduction : introduction,0.08759124087591241,0.3333333333333333,0.3333333333333333
relation-classification,8,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",introduction,introduction,0,13,5,5,0,introduction : introduction,0.0948905109489051,0.4166666666666667,0.4166666666666667
relation-classification,8,"However , nearly all existing approaches for MRE tasks 2014 ; adopt some variations of the singlerelation extraction ( SRE ) approach , which treats each pair of entity mentions as an independent instance , and requires multiple passes of encoding for the multiple pairs of entities .",introduction,introduction,0,14,6,6,0,introduction : introduction,0.10218978102189781,0.5,0.5
relation-classification,8,"The drawback of this approach is obvious - it is computationally expensive and this issue becomes more severe when the input paragraph is large , making this solution impossible to implement when the encoding step involves deep models .",introduction,introduction,0,15,7,7,0,introduction : introduction,0.10948905109489052,0.5833333333333334,0.5833333333333334
relation-classification,8,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.11678832116788321,0.6666666666666666,0.6666666666666666
relation-classification,8,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .",introduction,introduction,0,17,9,9,0,introduction : introduction,0.12408759124087591,0.75,0.75
relation-classification,8,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .",introduction,introduction,0,18,10,10,0,introduction : introduction,0.13138686131386862,0.8333333333333334,0.8333333333333334
relation-classification,8,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,introduction,introduction,0,19,11,11,0,introduction : introduction,0.1386861313868613,0.9166666666666666,0.9166666666666666
relation-classification,8,"To the best of our knowledge , this work is the first promising solution that can solve MRE tasks with such high efficiency ( encoding the input in one - pass ) and effectiveness ( achieve a new state - of - the - art performance ) , as proved on the ACE 2005 benchmark .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.145985401459854,1.0,1.0
relation-classification,8,Background,background,Background,0,21,1,1,0,background : Background,0.15328467153284672,0.14285714285714285,0.14285714285714285
relation-classification,8,MRE is an important task as it is an essential prior step for many downstream tasks such as automatic knowledge - base completion and questionanswering .,background,Background,0,22,2,2,0,background : Background,0.16058394160583941,0.2857142857142857,0.2857142857142857
relation-classification,8,Popular MRE benchmarks include ACE and ERE .,background,Background,0,23,3,3,0,background : Background,0.1678832116788321,0.42857142857142855,0.42857142857142855
relation-classification,8,"In MRE , given as a text paragraph x = {x 1 , . . . , x N } and M mentions e = {e 1 , . . . , e M } as input , the goal is to predict the relation r ij for each mention pair ( e i , e j ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation .",background,Background,0,24,4,4,0,background : Background,0.17518248175182483,0.5714285714285714,0.5714285714285714
relation-classification,8,"This paper uses "" entity mention "" , "" mention "" and "" entity "" interchangeably .",background,Background,0,25,5,5,0,background : Background,0.18248175182481752,0.7142857142857143,0.7142857142857143
relation-classification,8,"Existing MRE approaches are based on either feature and model architecture selection techniques , or domain adaptations approaches .",background,Background,0,26,6,6,0,background : Background,0.1897810218978102,0.8571428571428571,0.8571428571428571
relation-classification,8,"But these approaches require multiple passes of encoding over the paragraph , as they treat a MRE task as multiple passes of a SRE task .",background,Background,0,27,7,7,0,background : Background,0.19708029197080293,1.0,1.0
relation-classification,8,Proposed Approach,approach,Proposed Approach,0,28,1,1,0,approach : Proposed Approach,0.20437956204379562,0.02857142857142857,0.2
relation-classification,8,This section describes the proposed one - pass encoding MRE solution .,approach,Proposed Approach,1,29,2,2,0,approach : Proposed Approach,0.2116788321167883,0.05714285714285714,0.4
relation-classification,8,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .",approach,Proposed Approach,1,30,3,3,0,approach : Proposed Approach,0.21897810218978103,0.08571428571428572,0.6
relation-classification,8,The framework is illustrated in 1 .,approach,Proposed Approach,0,31,4,4,0,approach : Proposed Approach,0.22627737226277372,0.11428571428571428,0.8
relation-classification,8,"It is worth mentioning that our solution can easily use other transformer - based encoders besides BERT , e.g..",approach,Proposed Approach,0,32,5,5,0,approach : Proposed Approach,0.23357664233576642,0.14285714285714285,1.0
relation-classification,8,Structured Prediction with BERT for MRE,approach,Structured Prediction with BERT for MRE,0,33,6,1,0,approach : Structured Prediction with BERT for MRE,0.24087591240875914,0.17142857142857143,0.09090909090909091
relation-classification,8,The BERT model has been successfully applied to various NLP tasks .,approach,Structured Prediction with BERT for MRE,0,34,7,2,0,approach : Structured Prediction with BERT for MRE,0.24817518248175183,0.2,0.18181818181818182
relation-classification,8,"However , the final prediction layers used in the original model is not applicable to MRE tasks .",approach,Structured Prediction with BERT for MRE,0,35,8,3,0,approach : Structured Prediction with BERT for MRE,0.25547445255474455,0.22857142857142856,0.2727272727272727
relation-classification,8,The MRE task essentially requires to perform edge predictions over a graph with entities as nodes .,approach,Structured Prediction with BERT for MRE,0,36,9,4,0,approach : Structured Prediction with BERT for MRE,0.26277372262773724,0.2571428571428571,0.36363636363636365
relation-classification,8,"Inspired by , we propose that we can first encode the input paragraph using BERT .",approach,Structured Prediction with BERT for MRE,0,37,10,5,0,approach : Structured Prediction with BERT for MRE,0.27007299270072993,0.2857142857142857,0.45454545454545453
relation-classification,8,"Thus , the representation for a pair of entity mentions ( e i , e j ) can be denoted as oi and o j respectively .",approach,Structured Prediction with BERT for MRE,0,38,11,6,0,approach : Structured Prediction with BERT for MRE,0.2773722627737226,0.3142857142857143,0.5454545454545454
relation-classification,8,"In the case of a mention e i consist of multiple hidden states ( due to the byte pair encoding ) , oi is aggregated via average - pooling over the hidden states of the corresponding tokens in the last BERT layer .",approach,Structured Prediction with BERT for MRE,0,39,12,7,0,approach : Structured Prediction with BERT for MRE,0.2846715328467153,0.34285714285714286,0.6363636363636364
relation-classification,8,"We then concatenate oi and o j denoted as [ o i : o j ] , and pass it to a linear classifier 2 to predict the relation",approach,Structured Prediction with BERT for MRE,0,40,13,8,0,approach : Structured Prediction with BERT for MRE,0.291970802919708,0.37142857142857144,0.7272727272727273
relation-classification,8,where W L ? R 2 dzl .,approach,Structured Prediction with BERT for MRE,0,41,14,9,0,approach : Structured Prediction with BERT for MRE,0.29927007299270075,0.4,0.8181818181818182
relation-classification,8,where W L ? R 2 dzl .,approach,Structured Prediction with BERT for MRE,0,42,15,10,0,approach : Structured Prediction with BERT for MRE,0.30656934306569344,0.42857142857142855,0.9090909090909091
relation-classification,8,"dz is the dimension of BERT embedding at each token position , and l is the number of relation labels .",approach,Structured Prediction with BERT for MRE,0,43,16,11,0,approach : Structured Prediction with BERT for MRE,0.31386861313868614,0.45714285714285713,1.0
relation-classification,8,Entity - Aware Self - Attention based on Relative Distance,approach,Entity-Aware Self-Attention based on Relative Distance,0,44,17,1,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.32116788321167883,0.4857142857142857,0.05263157894736842
relation-classification,8,This section describes how we encode multiplerelations information into the model .,approach,Entity-Aware Self-Attention based on Relative Distance,0,45,18,2,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.3284671532846715,0.5142857142857142,0.10526315789473684
relation-classification,8,The key concept is to use the relative distances between words and entities to encode the positional information for each entity .,approach,Entity-Aware Self-Attention based on Relative Distance,0,46,19,3,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.3357664233576642,0.5428571428571428,0.15789473684210525
relation-classification,8,This information is propagated through different layers via attention computations .,approach,Entity-Aware Self-Attention based on Relative Distance,0,47,20,4,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.34306569343065696,0.5714285714285714,0.21052631578947367
relation-classification,8,"Following , for each pair of word tokens ( x i , x j ) with the input representations from the previous layer ash i and h j , we extend the computation of self - attention z i as :",approach,Entity-Aware Self-Attention based on Relative Distance,0,48,21,5,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.35036496350364965,0.6,0.2631578947368421
relation-classification,8,"are the parameters of the model , and dz is the dimension of the output from the self - attention layer .",approach,Entity-Aware Self-Attention based on Relative Distance,0,49,22,6,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.35766423357664234,0.6285714285714286,0.3157894736842105
relation-classification,8,"Compared to standard BERT 's self - attention , a V ij , a K ij ? R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",approach,Entity-Aware Self-Attention based on Relative Distance,0,50,23,7,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.36496350364963503,0.6571428571428571,0.3684210526315789
relation-classification,8,"Compared to standard BERT 's self - attention , a V ij , a K ij ? R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",approach,Entity-Aware Self-Attention based on Relative Distance,0,51,24,8,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.3722627737226277,0.6857142857142857,0.42105263157894735
relation-classification,8,"Specifically , we devise a V ij and a K ij to encourage each token to be aware of the relative distance to different entity mentions , and vice versa .",approach,Entity-Aware Self-Attention based on Relative Distance,0,52,25,9,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.3795620437956204,0.7142857142857143,0.47368421052631576
relation-classification,8,"Adapted from , we argue that the relative distance information will not help if the distance is beyond a certain threshold .",approach,Entity-Aware Self-Attention based on Relative Distance,0,53,26,10,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.38686131386861317,0.7428571428571429,0.5263157894736842
relation-classification,8,Hence we first define the distance function as :,approach,Entity-Aware Self-Attention based on Relative Distance,0,54,27,11,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.39416058394160586,0.7714285714285715,0.5789473684210527
relation-classification,8,"This distance definition clips all distances to a region [?k , k].",approach,Entity-Aware Self-Attention based on Relative Distance,0,55,28,12,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.40145985401459855,0.8,0.631578947368421
relation-classification,8,is a hyper - parameter to be tuned on the development set .,approach,Entity-Aware Self-Attention based on Relative Distance,0,56,29,13,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.40875912408759124,0.8285714285714286,0.6842105263157895
relation-classification,8,We can now define a V ij and a K ij formally as :,approach,Entity-Aware Self-Attention based on Relative Distance,0,57,30,14,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.41605839416058393,0.8571428571428571,0.7368421052631579
relation-classification,8,"As defined above , if either token xi or x j belongs to an entity , we will introduce a relative positional representation according to their distance .",approach,Entity-Aware Self-Attention based on Relative Distance,0,58,31,15,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.4233576642335766,0.8857142857142857,0.7894736842105263
relation-classification,8,The distance is defined in an entity - centric way as we always compute the distance from the entity mention to the other token .,approach,Entity-Aware Self-Attention based on Relative Distance,0,59,32,16,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.4306569343065693,0.9142857142857143,0.8421052631578947
relation-classification,8,"If neither xi nor x j are entity mentions , we explicitly assign a zero vector to a K ij and a V ij .",approach,Entity-Aware Self-Attention based on Relative Distance,0,60,33,17,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.43795620437956206,0.9428571428571428,0.8947368421052632
relation-classification,8,"When both xi and x j are inside entity mentions , we take the distance as d ( i , j ) to make row - wise attention computation coherent as depicted in .",approach,Entity-Aware Self-Attention based on Relative Distance,0,61,34,18,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.44525547445255476,0.9714285714285714,0.9473684210526315
relation-classification,8,"During the model fine - tuning , the newly introduced parameters {w K ?k , ... , w K k } and {w V ?k , ... , w V k } are trained from scratch .",approach,Entity-Aware Self-Attention based on Relative Distance,0,62,35,19,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.45255474452554745,1.0,1.0
relation-classification,8,Experiments,experiment,Experiments,0,63,1,1,0,experiment : Experiments,0.45985401459854014,0.08333333333333333,0.3333333333333333
relation-classification,8,"We demonstrate the advantage of our method on a popular MRE benchmark , ACE 2005 , and a more recent MRE benchmark , SemEval 2018 Task 7 .",experiment,Experiments,0,64,2,2,0,experiment : Experiments,0.46715328467153283,0.16666666666666666,0.6666666666666666
relation-classification,8,"We also evaluate on a commonly used SRE benchmark SemEval 2010 task 8 , and achieve state - of - the - art performance .",experiment,Experiments,0,65,3,3,0,experiment : Experiments,0.4744525547445255,0.25,1.0
relation-classification,8,Settings,experiment,Settings,0,66,4,1,0,experiment : Settings,0.48175182481751827,0.3333333333333333,0.1111111111111111
relation-classification,8,"Data For ACE 2005 , we adopt the multi-domain setting and split the data following : we train on the union of news domain ( nw and bn ) , tune hyperparameters on half of the broadcast conversation ( bc ) domain , and evaluate on the remainder of broadcast conversation ( bc ) , the telephone speech ( cts ) , usenet newsgroups ( un ) , and weblogs ( wl ) domains .",experiment,Settings,0,67,5,2,0,experiment : Settings,0.48905109489051096,0.4166666666666667,0.2222222222222222
relation-classification,8,For Se-m,experiment,Settings,0,68,6,3,0,experiment : Settings,0.49635036496350365,0.5,0.3333333333333333
relation-classification,8,Eval 2018,experiment,Settings,0,69,7,4,0,experiment : Settings,0.5036496350364964,0.5833333333333334,0.4444444444444444
relation-classification,8,"Task 7 , we evaluate on its sub-task 1.1 .",experiment,Settings,0,70,8,5,0,experiment : Settings,0.5109489051094891,0.6666666666666666,0.5555555555555556
relation-classification,8,We use the same data split in the shared task .,experiment,Settings,0,71,9,6,0,experiment : Settings,0.5182481751824818,0.75,0.6666666666666666
relation-classification,8,The passages in this task is usually much longer compared to ACE .,experiment,Settings,0,72,10,7,0,experiment : Settings,0.5255474452554745,0.8333333333333334,0.7777777777777778
relation-classification,8,"Therefore we adopt the following pre-processing step - for the entity pair in each relation , we assume the tokens related to their relation labeling are always within a range from the fifth token ahead of the pair to the fifth token after it .",experiment,Settings,0,73,11,8,0,experiment : Settings,0.5328467153284672,0.9166666666666666,0.8888888888888888
relation-classification,8,"Therefore , the tokens in the original passage thatare not covered by the range of ANY input relations , will be removed from the input .",experiment,Settings,0,74,12,9,0,experiment : Settings,0.5401459854014599,1.0,1.0
relation-classification,8,Methods,method,Methods,0,75,1,1,0,method : Methods,0.5474452554744526,0.034482758620689655,0.1111111111111111
relation-classification,8,"We compare our solution with previous works that predict a single relation per pass , our model that predicts single relation per pass for MRE , and with the following naive modifications of BERT that could achieve MRE in one - pass .",method,Methods,1,76,2,2,0,method : Methods,0.5547445255474452,0.06896551724137931,0.2222222222222222
relation-classification,8,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .",method,Methods,1,77,3,3,0,method : Methods,0.5620437956204379,0.10344827586206896,0.3333333333333333
relation-classification,8,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .",method,Methods,1,78,4,4,0,method : Methods,0.5693430656934306,0.13793103448275862,0.4444444444444444
relation-classification,8,BERT SP with position embedding on the final attention layer .,method,Methods,1,79,5,5,0,method : Methods,0.5766423357664233,0.1724137931034483,0.5555555555555556
relation-classification,8,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,method,Methods,0,80,6,6,0,method : Methods,0.583941605839416,0.20689655172413793,0.6666666666666666
relation-classification,8,"In this method , the BERT model encode the paragraph to the last attention - layer .",method,Methods,1,81,7,7,0,method : Methods,0.5912408759124088,0.2413793103448276,0.7777777777777778
relation-classification,8,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .",method,Methods,1,82,8,8,0,method : Methods,0.5985401459854015,0.27586206896551724,0.8888888888888888
relation-classification,8,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )",method,Methods,0,83,9,9,0,method : Methods,0.6058394160583942,0.3103448275862069,1.0
relation-classification,8,Results on ACE 2005,method,Results on ACE 2005,1,84,10,1,0,method : Results on ACE 2005,0.6131386861313869,0.3448275862068966,0.08333333333333333
relation-classification,8,Main Results gives the over all results on ACE 2005 .,method,Results on ACE 2005,0,85,11,2,0,method : Results on ACE 2005,0.6204379562043796,0.3793103448275862,0.16666666666666666
relation-classification,8,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,method,Results on ACE 2005,1,86,12,3,0,method : Results on ACE 2005,0.6277372262773723,0.41379310344827586,0.25
relation-classification,8,"Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .",method,Results on ACE 2005,0,87,13,4,0,method : Results on ACE 2005,0.635036496350365,0.4482758620689655,0.3333333333333333
relation-classification,8,This result further demonstrates its effectiveness .,method,Results on ACE 2005,0,88,14,5,0,method : Results on ACE 2005,0.6423357664233577,0.4827586206896552,0.4166666666666667
relation-classification,8,"Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .",method,Results on ACE 2005,0,89,15,6,0,method : Results on ACE 2005,0.6496350364963503,0.5172413793103449,0.5
relation-classification,8,"The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .",method,Results on ACE 2005,0,90,16,7,0,method : Results on ACE 2005,0.656934306569343,0.5517241379310345,0.5833333333333334
relation-classification,8,Summing up the vectors confuses this information .,method,Results on ACE 2005,0,91,17,8,0,method : Results on ACE 2005,0.6642335766423357,0.5862068965517241,0.6666666666666666
relation-classification,8,"It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .",method,Results on ACE 2005,0,92,18,9,0,method : Results on ACE 2005,0.6715328467153284,0.6206896551724138,0.75
relation-classification,8,prior state - of - the - art level of the methods without domain adaptation .,method,Results on ACE 2005,0,93,19,10,0,method : Results on ACE 2005,0.6788321167883211,0.6551724137931034,0.8333333333333334
relation-classification,8,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .",method,Results on ACE 2005,1,94,20,11,0,method : Results on ACE 2005,0.6861313868613139,0.6896551724137931,0.9166666666666666
relation-classification,8,It also beats the other two methods on BERT in Multi- Relation per Pass .,method,Results on ACE 2005,0,95,21,12,0,method : Results on ACE 2005,0.6934306569343066,0.7241379310344828,1.0
relation-classification,8,Performance Gap between MRE in One - Pass and Multi - Pass,method,Performance Gap between MRE in One-Pass and Multi-Pass,0,96,22,1,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7007299270072993,0.7586206896551724,0.125
relation-classification,8,The MRE - in - one - pass models can also be used to train and test with one entity pair per pass ( Single - Relation per Pass results in ) .,method,Performance Gap between MRE in One-Pass and Multi-Pass,0,97,23,2,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.708029197080292,0.7931034482758621,0.25
relation-classification,8,"Therefore , we compare the same methods when applied to the multi-relation and singlerelation settings .",method,Performance Gap between MRE in One-Pass and Multi-Pass,0,98,24,3,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7153284671532847,0.8275862068965517,0.375
relation-classification,8,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .",method,Performance Gap between MRE in One-Pass and Multi-Pass,0,99,25,4,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7226277372262774,0.8620689655172413,0.5
relation-classification,8,2 % gap is observed as expected .,method,Performance Gap between MRE in One-Pass and Multi-Pass,0,100,26,5,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7299270072992701,0.896551724137931,0.625
relation-classification,8,"By comparison , our full model has a much smaller performance gap between two different settings ( and no consistent performance drop over different domains ) .",method,Performance Gap between MRE in One-Pass and Multi-Pass,0,101,27,6,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7372262773722628,0.9310344827586207,0.75
relation-classification,8,The BERT SP is not expected to have a gap as shown in the table .,method,Performance Gap between MRE in One-Pass and Multi-Pass,0,102,28,7,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7445255474452555,0.9655172413793104,0.875
relation-classification,8,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .",method,Performance Gap between MRE in One-Pass and Multi-Pass,0,103,29,8,0,method : Performance Gap between MRE in One-Pass and Multi-Pass,0.7518248175182481,1.0,1.0
relation-classification,8,Training and Inference Time,training,training,0,104,1,1,0,training : training,0.7591240875912408,0.05555555555555555,0.16666666666666666
relation-classification,8,"Through our experiment , 4 we verify that the full model with MRE is significantly faster compared to all other methods for both training and inference .",training,training,0,105,2,2,0,training : training,0.7664233576642335,0.1111111111111111,0.3333333333333333
relation-classification,8,The training time for full model with MRE is 3.5 x faster than it with SRE .,training,training,0,106,3,3,0,training : training,0.7737226277372263,0.16666666666666666,0.5
relation-classification,8,"As for inference speed , the former could reach 126 relation per second compared the later at 23 relation per second .",training,training,0,107,4,4,0,training : training,0.781021897810219,0.2222222222222222,0.6666666666666666
relation-classification,8,"It is also much faster when compared to the second best performing approach , BERT SP w / pos-emb on final attlayer , which is at 76 relation per second , as it runs the last layer for every entity pair .",training,training,0,108,5,5,0,training : training,0.7883211678832117,0.2777777777777778,0.8333333333333334
relation-classification,8,"evaluates the usage of different prediction layers , including replacing our linear layer in Eq .",training,training,0,109,6,6,0,training : training,0.7956204379562044,0.3333333333333333,1.0
relation-classification,8,Prediction Module Selection,training,Prediction Module Selection,0,110,7,1,0,training : Prediction Module Selection,0.8029197080291971,0.3888888888888889,0.25
relation-classification,8,1 ) with MLP or Biaff .,training,Prediction Module Selection,0,111,8,2,0,training : Prediction Module Selection,0.8102189781021898,0.4444444444444444,0.5
relation-classification,8,Results show that the usage of the linear predictor gives better results .,training,Prediction Module Selection,0,112,9,3,0,training : Prediction Module Selection,0.8175182481751825,0.5,0.75
relation-classification,8,This is consistent with the motivation of the pre-trained encoders : by unsupervised pre-training the encoders are expected to be sufficiently powerful thus adding more complex layers on top does not improve the capacity but leads to more free parameters and higher risk of over-fitting .,training,Prediction Module Selection,0,113,10,4,0,training : Prediction Module Selection,0.8248175182481752,0.5555555555555556,1.0
relation-classification,8,Results on SemEval 2018,training,Results on SemEval 2018 Task 7,0,114,11,1,0,training : Results on SemEval 2018 Task 7,0.8321167883211679,0.6111111111111112,0.14285714285714285
relation-classification,8,Task 7,training,Results on SemEval 2018 Task 7,0,115,12,2,0,training : Results on SemEval 2018 Task 7,0.8394160583941606,0.6666666666666666,0.2857142857142857
relation-classification,8,The results on SemEval 2018 Task 7 are shown in .,training,Results on SemEval 2018 Task 7,1,116,13,3,0,training : Results on SemEval 2018 Task 7,0.8467153284671532,0.7222222222222222,0.42857142857142855
relation-classification,8,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .",training,Results on SemEval 2018 Task 7,1,117,14,4,0,training : Results on SemEval 2018 Task 7,0.8540145985401459,0.7777777777777778,0.5714285714285714
relation-classification,8,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .",training,Results on SemEval 2018 Task 7,1,118,15,5,0,training : Results on SemEval 2018 Task 7,0.8613138686131386,0.8333333333333334,0.7142857142857143
relation-classification,8,"Note that the system ( Rotsztejn et al. , 2018 ) integrates many techniques like feature - engineering , model combination , pretraining embeddings on in - domain data , and artificial data generation , while our model is almost a direct adaption from the ACE architecture .",training,Results on SemEval 2018 Task 7,0,119,16,6,0,training : Results on SemEval 2018 Task 7,0.8686131386861314,0.8888888888888888,0.8571428571428571
relation-classification,8,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .",training,Results on SemEval 2018 Task 7,1,120,17,7,0,training : Results on SemEval 2018 Task 7,0.8759124087591241,0.9444444444444444,1.0
relation-classification,8,Additional SRE,training,Additional SRE Results,0,121,18,1,0,training : Additional SRE Results,0.8832116788321168,1.0,1.0
relation-classification,8,Results,result,result,0,122,1,1,0,result : result,0.8905109489051095,0.5,0.5
relation-classification,8,"We conduct additional experiments on the relation classification task , SemEval 2010 Task 8 , to com -",result,result,0,123,2,2,0,result : result,0.8978102189781022,1.0,1.0
relation-classification,8,Method,method,Method,0,124,1,1,0,method : Method,0.9051094890510949,0.1111111111111111,0.1111111111111111
relation-classification,8,Averaged F1 Macro Micro,method,Method,0,125,2,2,0,method : Method,0.9124087591240876,0.2222222222222222,0.2222222222222222
relation-classification,8,Top 3 in the Shared Task 81.7 82.8 78.9 - 76 . pare with models developed on this benchmark .,method,Method,0,126,3,3,0,method : Method,0.9197080291970803,0.3333333333333333,0.3333333333333333
relation-classification,8,"From the results in , our proposed techniques also outperforms the state - of - the - art on this single - relation benchmark .",method,Method,0,127,4,4,0,method : Method,0.927007299270073,0.4444444444444444,0.4444444444444444
relation-classification,8,"On this single relation task , the out - of - box BERT achieves a reasonable result after finetuning .",method,Method,0,128,5,5,0,method : Method,0.9343065693430657,0.5555555555555556,0.5555555555555556
relation-classification,8,"Adding the entity - aware attention gives about 8 % improvement , due to the availability of the entity information during encoding .",method,Method,0,129,6,6,0,method : Method,0.9416058394160584,0.6666666666666666,0.6666666666666666
relation-classification,8,"Adding structured prediction layer to BERT ( i.e. , BERT SP ) also leads to a similar amount of improvement .",method,Method,0,130,7,7,0,method : Method,0.948905109489051,0.7777777777777778,0.7777777777777778
relation-classification,8,"However , the gap between BERT SP method with and without entity - aware attention is small .",method,Method,0,131,8,8,0,method : Method,0.9562043795620438,0.8888888888888888,0.8888888888888888
relation-classification,8,"This is likely because of the bias of data distribution : the assumption that only two target entities exist , makes the two techniques have similar effects .",method,Method,0,132,9,9,0,method : Method,0.9635036496350365,1.0,1.0
relation-classification,8,Conclusion,conclusion,Conclusion,0,133,1,1,0,conclusion : Conclusion,0.9708029197080292,0.2,0.2
relation-classification,8,"In summary , we propose a first - of - its - kind solution that can simultaneously extract multiple relations with one - pass encoding of an input paragraph for MRE tasks .",conclusion,Conclusion,0,134,2,2,0,conclusion : Conclusion,0.9781021897810219,0.4,0.4
relation-classification,8,"With the proposed structured prediction and entity - aware self - attention layers on top of BERT , we achieve a new state - of - the - art results with high efficiency on the ACE 2005 benchmark .",conclusion,Conclusion,0,135,3,3,0,conclusion : Conclusion,0.9854014598540146,0.6,0.6
relation-classification,8,"Our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction , e.g. , entity - centric passage encoding in question answering .",conclusion,Conclusion,0,136,4,4,0,conclusion : Conclusion,0.9927007299270073,0.8,0.8
relation-classification,8,"In the future work , we will explore the usage of this method with other applications .",conclusion,Conclusion,0,137,5,5,0,conclusion : Conclusion,1.0,1.0,1.0
relation-classification,9,SCIBERT : A Pretrained Language Model for Scientific Text,title,title,1,2,1,1,0,title : title,0.013605442176870748,1.0,1.0
relation-classification,9,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.02040816326530612,0.14285714285714285,0.14285714285714285
relation-classification,9,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.027210884353741496,0.2857142857142857,0.2857142857142857
relation-classification,9,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",abstract,abstract,1,5,3,3,0,abstract : abstract,0.034013605442176874,0.42857142857142855,0.42857142857142855
relation-classification,9,SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.04081632653061224,0.5714285714285714,0.5714285714285714
relation-classification,9,"We evaluate on a suite of tasks including sequence tagging , sentence classification and dependency parsing , with datasets from a variety of scientific domains .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.047619047619047616,0.7142857142857143,0.7142857142857143
relation-classification,9,We demonstrate statistically significant improvements over BERT and achieve new state - of - theart results on several of these tasks .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.05442176870748299,0.8571428571428571,0.8571428571428571
relation-classification,9,The code and pretrained models are available at https://github.com/allenai/scibert/.,abstract,abstract,1,9,7,7,0,abstract : abstract,0.061224489795918366,1.0,1.0
relation-classification,9,Introduction,introduction,introduction,0,10,1,1,0,introduction : introduction,0.06802721088435375,0.07692307692307693,0.07692307692307693
relation-classification,9,The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these documents .,introduction,introduction,0,11,2,2,0,introduction : introduction,0.07482993197278912,0.15384615384615385,0.15384615384615385
relation-classification,9,"Recent progress in NLP has been driven by the adoption of deep neural models , but training such models often requires large amounts of labeled data .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.08163265306122448,0.23076923076923078,0.23076923076923078
relation-classification,9,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",introduction,introduction,1,13,4,4,0,introduction : introduction,0.08843537414965986,0.3076923076923077,0.3076923076923077
relation-classification,9,"As shown through ELMo , and BERT , unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.09523809523809523,0.38461538461538464,0.38461538461538464
relation-classification,9,These models return contextualized embeddings for each token which can be passed into minimal task - specific neural architectures .,introduction,introduction,0,15,6,6,0,introduction : introduction,0.10204081632653061,0.46153846153846156,0.46153846153846156
relation-classification,9,"Leveraging the success of unsupervised pretraining has become especially important especially when task - specific annotations are difficult to obtain , like in scientific NLP .",introduction,introduction,0,16,7,7,0,introduction : introduction,0.10884353741496598,0.5384615384615384,0.5384615384615384
relation-classification,9,"Yet while both BERT and ELMo have released pretrained models , they are still trained on general domain corpora such as news articles and Wikipedia .",introduction,introduction,0,17,8,8,0,introduction : introduction,0.11564625850340136,0.6153846153846154,0.6153846153846154
relation-classification,9,"In this work , we make the following contributions :",introduction,introduction,0,18,9,9,0,introduction : introduction,0.12244897959183673,0.6923076923076923,0.6923076923076923
relation-classification,9,"i ) We release SCIBERT , a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain .",introduction,introduction,0,19,10,10,0,introduction : introduction,0.1292517006802721,0.7692307692307693,0.7692307692307693
relation-classification,9,SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text .,introduction,introduction,0,20,11,11,0,introduction : introduction,0.1360544217687075,0.8461538461538461,0.8461538461538461
relation-classification,9,"ii ) We perform extensive experimentation to investigate the performance of finetuning versus task - specific architectures atop frozen embeddings , and the effect of having an in - domain vocabulary .",introduction,introduction,0,21,12,12,0,introduction : introduction,0.14285714285714285,0.9230769230769231,0.9230769230769231
relation-classification,9,"iii ) We evaluate SCIBERT on a suite of tasks in the scientific domain , and achieve new state - of the - art ( SOTA ) results on many of these tasks .",introduction,introduction,0,22,13,13,0,introduction : introduction,0.14965986394557823,1.0,1.0
relation-classification,9,Methods,method,Methods,0,23,1,1,0,method : Methods,0.1564625850340136,1.0,1.0
relation-classification,9,Background,background,background,0,24,1,1,0,background : background,0.16326530612244897,0.1,0.1
relation-classification,9,The BERT model architecture is based on a multilayer bidirectional Transformer .,background,background,0,25,2,2,0,background : background,0.17006802721088435,0.2,0.2
relation-classification,9,"Instead of the traditional left - to - right language modeling objective , BERT is trained on two tasks : predicting randomly masked tokens and predicting whether two sentences follow each other .",background,background,0,26,3,3,0,background : background,0.17687074829931973,0.3,0.3
relation-classification,9,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,background,background,1,27,4,4,0,background : background,0.1836734693877551,0.4,0.4
relation-classification,9,Vocabulary BERT uses WordPiece for unsupervised tokenization of the input text .,background,background,0,28,5,5,0,background : background,0.19047619047619047,0.5,0.5
relation-classification,9,The vocabulary is built such that it contains the most frequently used words or subword units .,background,background,0,29,6,6,0,background : background,0.19727891156462585,0.6,0.6
relation-classification,9,We refer to the original vocabulary released with BERT as BASEVOCAB .,background,background,0,30,7,7,0,background : background,0.20408163265306123,0.7,0.7
relation-classification,9,"We construct SCIVOCAB , a new WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .",background,background,1,31,8,8,0,background : background,0.2108843537414966,0.8,0.8
relation-classification,9,We produce both cased and uncased vocabularies and set the vocabulary size to 30 K to match the size of BASEVOCAB .,background,background,0,32,9,9,0,background : background,0.21768707482993196,0.9,0.9
relation-classification,9,"The resulting token overlap between BASEVOCAB and SCIVOCAB is 42 % , illustrating a substantial difference in frequently used words between scientific and general domain texts .",background,background,0,33,10,10,0,background : background,0.22448979591836735,1.0,1.0
relation-classification,9,Corpus,system description,Corpus,1,34,1,1,0,system description : Corpus,0.23129251700680273,0.16666666666666666,0.16666666666666666
relation-classification,9,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,system description,Corpus,1,35,2,2,0,system description : Corpus,0.23809523809523808,0.3333333333333333,0.3333333333333333
relation-classification,9,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,system description,Corpus,1,36,3,3,0,system description : Corpus,0.24489795918367346,0.5,0.5
relation-classification,9,"We use the full text of the papers , not just the abstracts .",system description,Corpus,0,37,4,4,0,system description : Corpus,0.25170068027210885,0.6666666666666666,0.6666666666666666
relation-classification,9,"The average paper length is 154 sentences ( 2,769 tokens ) resulting in a corpus size of 3.17B tokens , similar to the 3.3B tokens on which BERT was trained .",system description,Corpus,0,38,5,5,0,system description : Corpus,0.2585034013605442,0.8333333333333334,0.8333333333333334
relation-classification,9,"We split sentences using Scispa Cy , 2 which is optimized for scientific text .",system description,Corpus,0,39,6,6,0,system description : Corpus,0.2653061224489796,1.0,1.0
relation-classification,9,Experimental Setup,experiment,experiment,0,40,1,1,0,experiment : experiment,0.272108843537415,0.09090909090909091,1.0
relation-classification,9,Tasks,experiment,Tasks,0,41,2,1,0,experiment : Tasks,0.2789115646258503,0.18181818181818182,0.1
relation-classification,9,We experiment on the following core NLP tasks :,experiment,Tasks,0,42,3,2,0,experiment : Tasks,0.2857142857142857,0.2727272727272727,0.2
relation-classification,9,1 .,experiment,Tasks,0,43,4,3,0,experiment : Tasks,0.2925170068027211,0.36363636363636365,0.3
relation-classification,9,Named Entity Recognition ( NER ),experiment,Tasks,1,44,5,4,0,experiment : Tasks,0.29931972789115646,0.45454545454545453,0.4
relation-classification,9,. PICO Extraction ( PICO ),experiment,Tasks,1,45,6,5,0,experiment : Tasks,0.30612244897959184,0.5454545454545454,0.5
relation-classification,9,. Text Classification ( CLS ),experiment,Tasks,1,46,7,6,0,experiment : Tasks,0.3129251700680272,0.6363636363636364,0.6
relation-classification,9,. Relation Classification ( REL ),experiment,Tasks,1,47,8,7,0,experiment : Tasks,0.3197278911564626,0.7272727272727273,0.7
relation-classification,9,. Dependency Parsing ( DEP ),experiment,Tasks,1,48,9,8,0,experiment : Tasks,0.32653061224489793,0.8181818181818182,0.8
relation-classification,9,"PICO , like NER , is a sequence labeling task where the model extracts spans describing the Participants , Interventions , Comparisons , and Outcomes in a clinical trial paper .",experiment,Tasks,0,49,10,9,0,experiment : Tasks,0.3333333333333333,0.9090909090909091,0.9
relation-classification,9,"REL is a special case of text classification where the model predicts the type of relation expressed between two entities , which are encapsulated in the sentence by inserted special tokens .",experiment,Tasks,0,50,11,10,0,experiment : Tasks,0.3401360544217687,1.0,1.0
relation-classification,9,Datasets,dataset,Datasets,0,51,1,1,0,dataset : Datasets,0.3469387755102041,0.14285714285714285,0.14285714285714285
relation-classification,9,"For brevity , we only describe the newer datasets here , and refer the reader to the references in Table 1 for the older datasets .",dataset,Datasets,0,52,2,2,0,dataset : Datasets,0.35374149659863946,0.2857142857142857,0.2857142857142857
relation-classification,9,EBM - NLP annotates PICO spans in clinical trial abstracts .,dataset,Datasets,0,53,3,3,0,dataset : Datasets,0.36054421768707484,0.42857142857142855,0.42857142857142855
relation-classification,9,SciERC annotates entities and relations from computer science ab - 1 https://github.com/google/sentencepiece,dataset,Datasets,0,54,4,4,0,dataset : Datasets,0.3673469387755102,0.5714285714285714,0.5714285714285714
relation-classification,9,"https://github.com/allenai/SciSpaCy stracts . ACL - ARC and Sci -Cite assign intent labels ( e.g. Comparison , Extension , etc. ) to sentences from scientific papers that cite other papers .",dataset,Datasets,0,55,5,5,0,dataset : Datasets,0.3741496598639456,0.7142857142857143,0.7142857142857143
relation-classification,9,The Paper Field dataset is built from the Microsoft Academic Graph 3 and maps paper titles to one of 7 fields of study .,dataset,Datasets,0,56,6,6,0,dataset : Datasets,0.38095238095238093,0.8571428571428571,0.8571428571428571
relation-classification,9,"Each field of study ( i.e. geography , politics , economics , business , sociology , medicine , and psychology ) has approximately 12 K training examples .",dataset,Datasets,0,57,7,7,0,dataset : Datasets,0.3877551020408163,1.0,1.0
relation-classification,9,Pretrained BERT,system description,Pretrained BERT Variants,0,58,1,1,0,system description : Pretrained BERT Variants,0.3945578231292517,0.02127659574468085,0.16666666666666666
relation-classification,9,Variants,system description,Pretrained BERT Variants,0,59,2,2,0,system description : Pretrained BERT Variants,0.4013605442176871,0.0425531914893617,0.3333333333333333
relation-classification,9,BERT - Base,system description,Pretrained BERT Variants,0,60,3,3,0,system description : Pretrained BERT Variants,0.40816326530612246,0.06382978723404255,0.5
relation-classification,9,We use the pretrained weights for BERT - Base released with the original BERT code .,system description,Pretrained BERT Variants,0,61,4,4,0,system description : Pretrained BERT Variants,0.41496598639455784,0.0851063829787234,0.6666666666666666
relation-classification,9,The vocabulary is BASE - VOCAB .,system description,Pretrained BERT Variants,0,62,5,5,0,system description : Pretrained BERT Variants,0.4217687074829932,0.10638297872340426,0.8333333333333334
relation-classification,9,We evaluate both cased and uncased versions of this model .,system description,Pretrained BERT Variants,0,63,6,6,0,system description : Pretrained BERT Variants,0.42857142857142855,0.1276595744680851,1.0
relation-classification,9,SCIBERT,system description,SCIBERT,0,64,7,1,0,system description : SCIBERT,0.43537414965986393,0.14893617021276595,0.058823529411764705
relation-classification,9,We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT - Base .,system description,SCIBERT,0,65,8,2,0,system description : SCIBERT,0.4421768707482993,0.1702127659574468,0.11764705882352941
relation-classification,9,We train 4 different versions of SCIBERT : ( i ) cased or uncased and ( ii ) BASEVOCAB or SCIVOCAB .,system description,SCIBERT,0,66,9,3,0,system description : SCIBERT,0.4489795918367347,0.19148936170212766,0.17647058823529413
relation-classification,9,The two models that use BASEVOCAB are finetuned from the corresponding BERT - Base models .,system description,SCIBERT,0,67,10,4,0,system description : SCIBERT,0.4557823129251701,0.2127659574468085,0.23529411764705882
relation-classification,9,The other two models that use the new SCIVOCAB are trained from scratch .,system description,SCIBERT,0,68,11,5,0,system description : SCIBERT,0.46258503401360546,0.23404255319148937,0.29411764705882354
relation-classification,9,Pretraining BERT for long sentences can be slow .,system description,SCIBERT,0,69,12,6,0,system description : SCIBERT,0.46938775510204084,0.2553191489361702,0.35294117647058826
relation-classification,9,"Following the original BERT code , we set a maximum sentence length of 128 tokens , and train the model until the training loss stops decreasing .",system description,SCIBERT,0,70,13,7,0,system description : SCIBERT,0.47619047619047616,0.2765957446808511,0.4117647058823529
relation-classification,9,We then continue training the model allowing sentence lengths up to 512 tokens .,system description,SCIBERT,0,71,14,8,0,system description : SCIBERT,0.48299319727891155,0.2978723404255319,0.47058823529411764
relation-classification,9,We use a single TPU v 3 with 8 cores .,system description,SCIBERT,0,72,15,9,0,system description : SCIBERT,0.4897959183673469,0.3191489361702128,0.5294117647058824
relation-classification,9,"Training the SCIVOCAB models from scratch on our corpus takes 1 week 5 ( 5 days with max length 128 , then 2 days with max length 512 ) .",system description,SCIBERT,0,73,16,10,0,system description : SCIBERT,0.4965986394557823,0.3404255319148936,0.5882352941176471
relation-classification,9,The BASEVOCAB models take 2 fewer days of training because they are n't trained from scratch .,system description,SCIBERT,0,74,17,11,0,system description : SCIBERT,0.5034013605442177,0.3617021276595745,0.6470588235294118
relation-classification,9,All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library .,system description,SCIBERT,0,75,18,12,0,system description : SCIBERT,0.5102040816326531,0.3829787234042553,0.7058823529411765
relation-classification,9,All our models ( Sections 3.4 and 3.5 ) are implemented in PyTorch using AllenNLP .,system description,SCIBERT,0,76,19,13,0,system description : SCIBERT,0.5170068027210885,0.40425531914893614,0.7647058823529411
relation-classification,9,Casing,system description,SCIBERT,0,77,20,14,0,system description : SCIBERT,0.5238095238095238,0.425531914893617,0.8235294117647058
relation-classification,9,We follow in using the cased models for NER and the uncased models for all other tasks .,system description,SCIBERT,0,78,21,15,0,system description : SCIBERT,0.5306122448979592,0.44680851063829785,0.8823529411764706
relation-classification,9,We also use the cased models for parsing .,system description,SCIBERT,0,79,22,16,0,system description : SCIBERT,0.5374149659863946,0.46808510638297873,0.9411764705882353
relation-classification,9,Some light experimentation showed that the uncased models perform slightly better ( even sometimes on NER ) than cased models .,system description,SCIBERT,0,80,23,17,0,system description : SCIBERT,0.54421768707483,0.48936170212765956,1.0
relation-classification,9,Finetuning BERT,system description,Finetuning BERT,0,81,24,1,0,system description : Finetuning BERT,0.5510204081632653,0.5106382978723404,0.09090909090909091
relation-classification,9,"We mostly follow the same architecture , optimization , and hyperparameter choices used in .",system description,Finetuning BERT,0,82,25,2,0,system description : Finetuning BERT,0.5578231292517006,0.5319148936170213,0.18181818181818182
relation-classification,9,"For text classification ( i.e. CLS and REL ) , we feed the final BERT vector for the [ CLS ] token into a linear classification layer .",system description,Finetuning BERT,0,83,26,3,0,system description : Finetuning BERT,0.564625850340136,0.5531914893617021,0.2727272727272727
relation-classification,9,"For sequence labeling ( i.e. NER and PICO ) , we feed the final BERT vector for each token into a linear classification layer with softmax output .",system description,Finetuning BERT,0,84,27,4,0,system description : Finetuning BERT,0.5714285714285714,0.574468085106383,0.36363636363636365
relation-classification,9,"We differ slightly in using an additional conditional random field , which made evaluation easier by guaranteeing well - formed entities .",system description,Finetuning BERT,0,85,28,5,0,system description : Finetuning BERT,0.5782312925170068,0.5957446808510638,0.45454545454545453
relation-classification,9,"For DEP , we use the model from with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs .",system description,Finetuning BERT,0,86,29,6,0,system description : Finetuning BERT,0.5850340136054422,0.6170212765957447,0.5454545454545454
relation-classification,9,"In all settings , we apply a dropout of 0.1 and optimize cross entropy loss using Adam .",system description,Finetuning BERT,0,87,30,7,0,system description : Finetuning BERT,0.5918367346938775,0.6382978723404256,0.6363636363636364
relation-classification,9,"We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5 e - 6 , 1 e - 5 , 2 e - 5 , or 5 e - 5 with a slanted triangular schedule which is equivalent to the linear warmup followed by linear decay .",system description,Finetuning BERT,0,88,31,8,0,system description : Finetuning BERT,0.5986394557823129,0.6595744680851063,0.7272727272727273
relation-classification,9,"For each dataset and BERT variant , we pick the best learning rate and number of epochs on the development set and report the corresponding test results .",system description,Finetuning BERT,0,89,32,9,0,system description : Finetuning BERT,0.6054421768707483,0.6808510638297872,0.8181818181818182
relation-classification,9,We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2 e - 5 .,system description,Finetuning BERT,0,90,33,10,0,system description : Finetuning BERT,0.6122448979591837,0.7021276595744681,0.9090909090909091
relation-classification,9,"While task - dependent , optimal hyperparameters for each task are often the same across BERT variants .",system description,Finetuning BERT,0,91,34,11,0,system description : Finetuning BERT,0.6190476190476191,0.723404255319149,1.0
relation-classification,9,Frozen BERT,system description,Frozen BERT Embeddings,0,92,35,1,0,system description : Frozen BERT Embeddings,0.6258503401360545,0.7446808510638298,0.07692307692307693
relation-classification,9,Embeddings,system description,Frozen BERT Embeddings,0,93,36,2,0,system description : Frozen BERT Embeddings,0.6326530612244898,0.7659574468085106,0.15384615384615385
relation-classification,9,"We also explore the usage of BERT as pretrained contextualized word embeddings , like ELMo ) , by training simple task - specific models atop frozen BERT embeddings .",system description,Frozen BERT Embeddings,0,94,37,3,0,system description : Frozen BERT Embeddings,0.6394557823129252,0.7872340425531915,0.23076923076923078
relation-classification,9,"For text classification , we feed each sentence of BERT vectors into a 2 - layer BiLSTM of size 200 and apply a multilayer perceptron ( with hidden size 200 ) on the concatenated first and last BiLSTM vectors .",system description,Frozen BERT Embeddings,0,95,38,4,0,system description : Frozen BERT Embeddings,0.6462585034013606,0.8085106382978723,0.3076923076923077
relation-classification,9,"For sequence labeling , we use the same BiLSTM layers and use a conditional random field to guarantee well - formed predictions .",system description,Frozen BERT Embeddings,0,96,39,5,0,system description : Frozen BERT Embeddings,0.6530612244897959,0.8297872340425532,0.38461538461538464
relation-classification,9,"For DEP , we use the full model from with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks .",system description,Frozen BERT Embeddings,0,97,40,6,0,system description : Frozen BERT Embeddings,0.6598639455782312,0.851063829787234,0.46153846153846156
relation-classification,9,We did not find changing the depth or size of the BiLSTMs to significantly impact results .,system description,Frozen BERT Embeddings,0,98,41,7,0,system description : Frozen BERT Embeddings,0.6666666666666666,0.8723404255319149,0.5384615384615384
relation-classification,9,"We optimize cross entropy loss using Adam , but holding BERT weights frozen and applying a dropout of 0.5 .",system description,Frozen BERT Embeddings,0,99,42,8,0,system description : Frozen BERT Embeddings,0.673469387755102,0.8936170212765957,0.6153846153846154
relation-classification,9,We train with early stopping on the development set ( patience of 10 ) using a batch size of 32 and a learning rate of 0.001 .,system description,Frozen BERT Embeddings,0,100,43,9,0,system description : Frozen BERT Embeddings,0.6802721088435374,0.9148936170212766,0.6923076923076923
relation-classification,9,"We did not perform extensive hyperparameter search , but while optimal hyperparameters are going to be task - dependent , some light experimentation showed these settings work fairly well across most tasks and BERT variants .",system description,Frozen BERT Embeddings,0,101,44,10,0,system description : Frozen BERT Embeddings,0.6870748299319728,0.9361702127659575,0.7692307692307693
relation-classification,9,summarizes the experimental results .,system description,Frozen BERT Embeddings,0,102,45,11,0,system description : Frozen BERT Embeddings,0.6938775510204082,0.9574468085106383,0.8461538461538461
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),system description,Frozen BERT Embeddings,1,103,46,12,0,system description : Frozen BERT Embeddings,0.7006802721088435,0.9787234042553191,0.9230769230769231
relation-classification,9,. We also achieve new SOTA results on many of these tasks using SCIBERT .,system description,Frozen BERT Embeddings,0,104,47,13,0,system description : Frozen BERT Embeddings,0.7074829931972789,1.0,1.0
relation-classification,9,Results,result,Results,0,105,1,1,0,result : Results,0.7142857142857143,0.047619047619047616,0.058823529411764705
relation-classification,9,Biomedical Domain,result,Results,1,106,2,2,0,result : Results,0.7210884353741497,0.09523809523809523,0.11764705882352941
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,result,Results,1,107,3,3,0,result : Results,0.7278911564625851,0.14285714285714285,0.17647058823529413
relation-classification,9,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .",result,Results,1,108,4,4,0,result : Results,0.7346938775510204,0.19047619047619047,0.23529411764705882
relation-classification,9,SCIBERT performs slightly worse than SOTA on 3 datasets .,result,Results,0,109,5,5,0,result : Results,0.7414965986394558,0.23809523809523808,0.29411764705882354
relation-classification,9,The SOTA model for JNLPBA is a BiLSTM - CRF ensemble trained on multiple NER datasets not just JNLPBA .,result,Results,0,110,6,6,0,result : Results,0.7482993197278912,0.2857142857142857,0.35294117647058826
relation-classification,9,"The SOTA model for NCBI - disease is BIOBERT , which is BERT - Base finetuned on 18B tokens from biomedical papers .",result,Results,0,111,7,7,0,result : Results,0.7551020408163265,0.3333333333333333,0.4117647058823529
relation-classification,9,"The SOTA result for GENIA is in Nguyen and Verspoor ( 2019 ) which uses the model from with partof - speech ( POS ) features , which we do not use .",result,Results,0,112,8,8,0,result : Results,0.7619047619047619,0.38095238095238093,0.47058823529411764
relation-classification,9,"In , we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in .",result,Results,0,113,9,9,0,result : Results,0.7687074829931972,0.42857142857142855,0.5294117647058824
relation-classification,9,"Interesting , SCIBERT outperforms BIOBERT results on 7 The SOTA paper did not report a single score .",result,Results,0,114,10,10,0,result : Results,0.7755102040816326,0.47619047619047616,0.5882352941176471
relation-classification,9,We compute the average of the reported results for each class weighted by number of examples in each class .,result,Results,0,115,11,11,0,result : Results,0.782312925170068,0.5238095238095238,0.6470588235294118
relation-classification,9,"Forrest of this paper , all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS .",result,Results,0,116,12,12,0,result : Results,0.7891156462585034,0.5714285714285714,0.7058823529411765
relation-classification,9,"BC5 CDR and ChemProt , and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus .",result,Results,0,117,13,13,0,result : Results,0.7959183673469388,0.6190476190476191,0.7647058823529411
relation-classification,9,Computer Science Domain,result,Results,1,118,14,14,0,result : Results,0.8027210884353742,0.6666666666666666,0.8235294117647058
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,result,Results,1,119,15,15,0,result : Results,0.8095238095238095,0.7142857142857143,0.8823529411764706
relation-classification,9,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .",result,Results,1,120,16,16,0,result : Results,0.8163265306122449,0.7619047619047619,0.9411764705882353
relation-classification,9,"For relations in Sci - ERC , our results are not comparable with those in because we are performing relation classification given gold entities , while they perform joint entity and relation extraction .",result,Results,0,121,17,17,0,result : Results,0.8231292517006803,0.8095238095238095,1.0
relation-classification,9,Multiple Domains,result,Multiple Domains,1,122,18,1,0,result : Multiple Domains,0.8299319727891157,0.8571428571428571,0.25
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,result,Multiple Domains,1,123,19,2,0,result : Multiple Domains,0.8367346938775511,0.9047619047619048,0.5
relation-classification,9,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .",result,Multiple Domains,1,124,20,3,0,result : Multiple Domains,0.8435374149659864,0.9523809523809523,0.75
relation-classification,9,No prior published SOTA results exist for the Paper Field dataset .,result,Multiple Domains,0,125,21,4,0,result : Multiple Domains,0.8503401360544217,1.0,1.0
relation-classification,9,Discussion,discussion,Discussion,0,126,1,1,0,discussion : Discussion,0.8571428571428571,1.0,1.0
relation-classification,9,Effect of Finetuning,Effect of Finetuning,Effect of Finetuning,0,127,1,1,0,Effect of Finetuning : Effect of Finetuning,0.8639455782312925,0.1,0.25
relation-classification,9,"We observe improved results via BERT finetuning rather than task - specific architectures atop frozen embeddings ( + 3.25 F1 with SCIBERT and + 3.58 with BERT - Base , on average ) .",Effect of Finetuning,Effect of Finetuning,0,128,2,2,0,Effect of Finetuning : Effect of Finetuning,0.8707482993197279,0.2,0.5
relation-classification,9,"For each scientific domain , we observe the largest effects of finetuning on the computer science ( + 5.59 F1 with SCIB - ERT and + 3.17 F1 with BERT - Base ) and biomedical tasks ( + 2.94 F1 with SCIBERT and + 4.61 F1 with BERT - Base ) , and the smallest effect on multidomain tasks ( + 0.7 F1 with SCIBERT and + 1.14 F1 with BERT - Base ) .",Effect of Finetuning,Effect of Finetuning,0,129,3,3,0,Effect of Finetuning : Effect of Finetuning,0.8775510204081632,0.3,0.75
relation-classification,9,"On every dataset except BC5 CDR and SciCite , BERT - Base with finetuning outperforms ( or performs similarly to ) a model using frozen SCIBERT embeddings .",Effect of Finetuning,Effect of Finetuning,0,130,4,4,0,Effect of Finetuning : Effect of Finetuning,0.8843537414965986,0.4,1.0
relation-classification,9,Effect of SCIVOCAB,Effect of Finetuning,Effect of SCIVOCAB,0,131,5,1,0,Effect of Finetuning : Effect of SCIVOCAB,0.891156462585034,0.5,0.16666666666666666
relation-classification,9,We assess the importance of an in - domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB .,Effect of Finetuning,Effect of SCIVOCAB,0,132,6,2,0,Effect of Finetuning : Effect of SCIVOCAB,0.8979591836734694,0.6,0.3333333333333333
relation-classification,9,We find the optimal hyperparameters for SCIBERT - BASEVOCAB often coincide with those of SCIB - ERT - SCIVOCAB .,Effect of Finetuning,Effect of SCIVOCAB,0,133,7,3,0,Effect of Finetuning : Effect of SCIVOCAB,0.9047619047619048,0.7,0.5
relation-classification,9,"Averaged across datasets , we observe + 0.60 F1 when using SCIVOCAB .",Effect of Finetuning,Effect of SCIVOCAB,0,134,8,4,0,Effect of Finetuning : Effect of SCIVOCAB,0.9115646258503401,0.8,0.6666666666666666
relation-classification,9,"For each scientific do - main , we observe + 0.76 F1 for biomedical tasks , + 0.61 F1 for computer science tasks , and + 0.11 F1 for multidomain tasks .",Effect of Finetuning,Effect of SCIVOCAB,0,135,9,5,0,Effect of Finetuning : Effect of SCIVOCAB,0.9183673469387755,0.9,0.8333333333333334
relation-classification,9,"Given the disjoint vocabularies ( Section 2 ) and the magnitude of improvement over BERT - Base ( Section 4 ) , we suspect that while an in - domain vocabulary is helpful , SCIBERT benefits most from the scientific corpus pretraining .",Effect of Finetuning,Effect of SCIVOCAB,0,136,10,6,0,Effect of Finetuning : Effect of SCIVOCAB,0.9251700680272109,1.0,1.0
relation-classification,9,Related Work,related work,Related Work,0,137,1,1,0,related work : Related Work,0.9319727891156463,0.2,0.2
relation-classification,9,Recent work on domain adaptation of BERT includes BIOBERT and CLIN - ICALBERT .,related work,Related Work,0,138,2,2,0,related work : Related Work,0.9387755102040817,0.4,0.4
relation-classification,9,"BIOBERT is trained on PubMed abstracts and PMC full text articles , and CLIN - ICALBERT is trained on clinical text from the MIMIC - III data base .",related work,Related Work,0,139,3,3,0,related work : Related Work,0.9455782312925171,0.6,0.6
relation-classification,9,"In contrast , SCIBERT is trained on the full text of 1.14 M biomedical and computer science papers from the Semantic Scholar corpus .",related work,Related Work,0,140,4,4,0,related work : Related Work,0.9523809523809523,0.8,0.8
relation-classification,9,"Furthermore , SCIBERT uses an in - domain vocabulary ( SCIVOCAB ) while the other abovementioned models use the original BERT vocabulary ( BASEVOCAB ) .",related work,Related Work,0,141,5,5,0,related work : Related Work,0.9591836734693877,1.0,1.0
relation-classification,9,Conclusion and Future Work,conclusion,Conclusion and Future Work,0,142,1,1,0,conclusion : Conclusion and Future Work,0.9659863945578231,0.16666666666666666,0.16666666666666666
relation-classification,9,"We released SCIBERT , a pretrained language model for scientific text based on BERT .",conclusion,Conclusion and Future Work,0,143,2,2,0,conclusion : Conclusion and Future Work,0.9727891156462585,0.3333333333333333,0.3333333333333333
relation-classification,9,We evaluated SCIBERT on a suite of tasks and datasets from scientific domains .,conclusion,Conclusion and Future Work,0,144,3,3,0,conclusion : Conclusion and Future Work,0.9795918367346939,0.5,0.5
relation-classification,9,"SCIBERT significantly outperformed BERT - Base and achieves new SOTA results on several of these tasks , even compared to some reported BIOBERT ) results on biomedical tasks .",conclusion,Conclusion and Future Work,0,145,4,4,0,conclusion : Conclusion and Future Work,0.9863945578231292,0.6666666666666666,0.6666666666666666
relation-classification,9,"For future work , we will release a version of SCIBERT analogous to BERT - Large , as well as experiment with different proportions of papers from each domain .",conclusion,Conclusion and Future Work,0,146,5,5,0,conclusion : Conclusion and Future Work,0.9931972789115646,0.8333333333333334,0.8333333333333334
relation-classification,9,"Because these language models are costly to train , we aim to build a single resource that 's useful across multiple domains .",conclusion,Conclusion and Future Work,0,147,6,6,0,conclusion : Conclusion and Future Work,1.0,1.0,1.0
text-classification,0,Character - level Convolutional Networks for Text Classification,title,title,1,2,1,1,0,title : title,0.00881057268722467,0.5,1.0
text-classification,0,*,title,abstract,0,3,2,1,0,title : abstract,0.013215859030837005,1.0,1.0
text-classification,0,abstract,abstract,abstract,0,4,1,1,0,abstract : abstract,0.01762114537444934,0.125,0.125
text-classification,0,This article offers an empirical exploration on the use of character - level convolutional networks ( ConvNets ) for text classification .,abstract,abstract,0,5,2,2,0,abstract : abstract,0.022026431718061675,0.25,0.25
text-classification,0,We constructed several largescale datasets to show that character - level convolutional networks could achieve state - of - the - art or competitive results .,abstract,abstract,0,6,3,3,0,abstract : abstract,0.02643171806167401,0.375,0.375
text-classification,0,"Comparisons are offered against traditional models such as bag of words , n-grams and their TFIDF variants , and deep learning models such as word - based ConvNets and recurrent neural networks .",abstract,abstract,0,7,4,4,0,abstract : abstract,0.030837004405286344,0.5,0.5
text-classification,0,There are also related works that use character - level features for language processing .,abstract,abstract,0,8,5,5,0,abstract : abstract,0.03524229074889868,0.625,0.625
text-classification,0,"These include using character - level n-grams with linear classifiers [ 15 ] , and incorporating character - level features to ConvNets [ 28 ] [ 29 ] .",abstract,abstract,0,9,6,6,0,abstract : abstract,0.039647577092511016,0.75,0.75
text-classification,0,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",abstract,abstract,0,10,7,7,0,abstract : abstract,0.04405286343612335,0.875,0.875
text-classification,0,Improvements for part - of - speech tagging and information retrieval were observed .,abstract,abstract,0,11,8,8,0,abstract : abstract,0.048458149779735685,1.0,1.0
text-classification,0,Introduction,introduction,introduction,0,12,1,1,0,introduction : introduction,0.05286343612334802,0.0625,0.0625
text-classification,0,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",introduction,introduction,1,13,2,2,0,introduction : introduction,0.05726872246696035,0.125,0.125
text-classification,0,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,introduction,introduction,0,14,3,3,0,introduction : introduction,0.06167400881057269,0.1875,0.1875
text-classification,0,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",introduction,introduction,0,15,4,4,0,introduction : introduction,0.06607929515418502,0.25,0.25
text-classification,0,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",introduction,introduction,0,16,5,5,0,introduction : introduction,0.07048458149779736,0.3125,0.3125
text-classification,0,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",introduction,introduction,0,17,6,6,0,introduction : introduction,0.07488986784140969,0.375,0.375
text-classification,0,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",introduction,introduction,1,18,7,7,0,introduction : introduction,0.07929515418502203,0.4375,0.4375
text-classification,0,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,introduction,introduction,1,19,8,8,0,introduction : introduction,0.08370044052863436,0.5,0.5
text-classification,0,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",introduction,introduction,0,20,9,9,0,introduction : introduction,0.0881057268722467,0.5625,0.5625
text-classification,0,An extensive set of comparisons is offered with traditional models and other deep learning models .,introduction,introduction,0,21,10,10,0,introduction : introduction,0.09251101321585903,0.625,0.625
text-classification,0,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,introduction,introduction,0,22,11,11,0,introduction : introduction,0.09691629955947137,0.6875,0.6875
text-classification,0,"It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .",introduction,introduction,0,23,12,12,0,introduction : introduction,0.1013215859030837,0.75,0.75
text-classification,0,These approaches have been proven to be competitive to traditional models .,introduction,introduction,0,24,13,13,0,introduction : introduction,0.10572687224669604,0.8125,0.8125
text-classification,0,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,introduction,introduction,0,25,14,14,0,introduction : introduction,0.11013215859030837,0.875,0.875
text-classification,0,"This simplification of engineering could be crucial for a single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",introduction,introduction,0,26,15,15,0,introduction : introduction,0.1145374449339207,0.9375,0.9375
text-classification,0,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,introduction,introduction,0,27,16,16,0,introduction : introduction,0.11894273127753303,1.0,1.0
text-classification,0,Character - level Convolutional Networks,system description,Character-level Convolutional Networks,0,28,1,1,0,system description : Character-level Convolutional Networks,0.12334801762114538,0.029411764705882353,0.3333333333333333
text-classification,0,"In this section , we introduce the design of character - level ConvNets for text classification .",system description,Character-level Convolutional Networks,0,29,2,2,0,system description : Character-level Convolutional Networks,0.1277533039647577,0.058823529411764705,0.6666666666666666
text-classification,0,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",system description,Character-level Convolutional Networks,0,30,3,3,0,system description : Character-level Convolutional Networks,0.13215859030837004,0.08823529411764706,1.0
text-classification,0,Key Modules,system description,Key Modules,0,31,4,1,0,system description : Key Modules,0.13656387665198239,0.11764705882352941,0.041666666666666664
text-classification,0,"The main component is the temporal convolutional module , which simply computes a 1 - D convolution .",system description,Key Modules,0,32,5,2,0,system description : Key Modules,0.14096916299559473,0.14705882352941177,0.08333333333333333
text-classification,0,"Suppose we have a discrete input function g ( x ) ? [ 1 , l ] ? Rand a discrete kernel function",system description,Key Modules,0,33,6,3,0,system description : Key Modules,0.14537444933920704,0.17647058823529413,0.125
text-classification,0,"Suppose we have a discrete input function g ( x ) ? [ 1 , l ] ? Rand a discrete kernel function",system description,Key Modules,0,34,7,4,0,system description : Key Modules,0.14977973568281938,0.20588235294117646,0.16666666666666666
text-classification,0,"Suppose we have a discrete input function g ( x ) ? [ 1 , l ] ? Rand a discrete kernel function",system description,Key Modules,0,35,8,5,0,system description : Key Modules,0.15418502202643172,0.23529411764705882,0.20833333333333334
text-classification,0,where c = k ? d + 1 is an offset constant .,system description,Key Modules,0,36,9,6,0,system description : Key Modules,0.15859030837004406,0.2647058823529412,0.25
text-classification,0,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",system description,Key Modules,0,37,10,7,0,system description : Key Modules,0.16299559471365638,0.29411764705882354,0.2916666666666667
text-classification,0,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",system description,Key Modules,0,38,11,8,0,system description : Key Modules,0.16740088105726872,0.3235294117647059,0.3333333333333333
text-classification,0,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,system description,Key Modules,0,39,12,9,0,system description : Key Modules,0.17180616740088106,0.35294117647058826,0.375
text-classification,0,One key module that helped us to train deeper models is temporal max - pooling .,system description,Key Modules,0,40,13,10,0,system description : Key Modules,0.1762114537444934,0.38235294117647056,0.4166666666666667
text-classification,0,It is the 1 - D version of the max - pooling module used in computer vision .,system description,Key Modules,0,41,14,11,0,system description : Key Modules,0.18061674008810572,0.4117647058823529,0.4583333333333333
text-classification,0,"Given a discrete input function g ( x ) ? [ 1 , l ] ? R , the max - pooling function h ( y ) ? [ 1 , ( l ? k) / d + 1 ] ? R of g ( x ) is defined as",system description,Key Modules,0,42,15,12,0,system description : Key Modules,0.18502202643171806,0.4411764705882353,0.5
text-classification,0,"Given a discrete input function g ( x ) ? [ 1 , l ] ? R , the max - pooling function h ( y ) ? [ 1 , ( l ? k) / d + 1 ] ? R of g ( x ) is defined as",system description,Key Modules,0,43,16,13,0,system description : Key Modules,0.1894273127753304,0.47058823529411764,0.5416666666666666
text-classification,0,"Given a discrete input function g ( x ) ? [ 1 , l ] ? R , the max - pooling function h ( y ) ? [ 1 , ( l ? k) / d + 1 ] ? R of g ( x ) is defined as",system description,Key Modules,0,44,17,14,0,system description : Key Modules,0.19383259911894274,0.5,0.5833333333333334
text-classification,0,"Given a discrete input function g ( x ) ? [ 1 , l ] ? R , the max - pooling function h ( y ) ? [ 1 , ( l ? k) / d + 1 ] ? R of g ( x ) is defined as",system description,Key Modules,0,45,18,15,0,system description : Key Modules,0.19823788546255505,0.5294117647058824,0.625
text-classification,0,"Given a discrete input function g ( x ) ? [ 1 , l ] ? R , the max - pooling function h ( y ) ? [ 1 , ( l ? k) / d + 1 ] ? R of g ( x ) is defined as",system description,Key Modules,0,46,19,16,0,system description : Key Modules,0.2026431718061674,0.5588235294117647,0.6666666666666666
text-classification,0,where c = k ? d + 1 is an offset constant .,system description,Key Modules,0,47,20,17,0,system description : Key Modules,0.20704845814977973,0.5882352941176471,0.7083333333333334
text-classification,0,"This very pooling module enabled us to train ConvNets deeper than 6 layers , where all others fail .",system description,Key Modules,0,48,21,18,0,system description : Key Modules,0.21145374449339208,0.6176470588235294,0.75
text-classification,0,The analysis by might shed some light on this .,system description,Key Modules,0,49,22,19,0,system description : Key Modules,0.21585903083700442,0.6470588235294118,0.7916666666666666
text-classification,0,"The non-linearity used in our model is the rectifier or thresholding function h ( x ) = max {0 , x} , which makes our convolutional layers similar to rectified linear units ( ReLUs ) .",system description,Key Modules,0,50,23,20,0,system description : Key Modules,0.22026431718061673,0.6764705882352942,0.8333333333333334
text-classification,0,"The algorithm used is stochastic gradient descent ( SGD ) with a minibatch of size 128 , using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times .",system description,Key Modules,0,51,24,21,0,system description : Key Modules,0.22466960352422907,0.7058823529411765,0.875
text-classification,0,Each epoch takes a fixed number of random training samples uniformly sampled across classes .,system description,Key Modules,0,52,25,22,0,system description : Key Modules,0.2290748898678414,0.7352941176470589,0.9166666666666666
text-classification,0,This number will later be detailed for each dataset sparately .,system description,Key Modules,0,53,26,23,0,system description : Key Modules,0.23348017621145375,0.7647058823529411,0.9583333333333334
text-classification,0,The implementation is done using Torch 7 .,system description,Key Modules,0,54,27,24,0,system description : Key Modules,0.23788546255506607,0.7941176470588235,1.0
text-classification,0,Character quantization,system description,Character quantization,0,55,28,1,0,system description : Character quantization,0.2422907488986784,0.8235294117647058,0.14285714285714285
text-classification,0,Our models accept a sequence of encoded characters as input .,system description,Character quantization,0,56,29,2,0,system description : Character quantization,0.24669603524229075,0.8529411764705882,0.2857142857142857
text-classification,0,"The encoding is done by prescribing an alphabet of size m for the input language , and then quantize each character using 1 - of - m encoding ( or "" one - hot "" encoding ) .",system description,Character quantization,0,57,30,3,0,system description : Character quantization,0.2511013215859031,0.8823529411764706,0.42857142857142855
text-classification,0,"Then , the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l 0 .",system description,Character quantization,0,58,31,4,0,system description : Character quantization,0.2555066079295154,0.9117647058823529,0.5714285714285714
text-classification,0,"Any character exceeding length l 0 is ignored , and any characters thatare not in the alphabet including blank characters are quantized as all - zero vectors .",system description,Character quantization,0,59,32,5,0,system description : Character quantization,0.2599118942731278,0.9411764705882353,0.7142857142857143
text-classification,0,"The character quantization order is backward so that the latest reading on characters is always placed near the begin of the output , making it easy for fully connected layers to associate weights with the latest reading .",system description,Character quantization,0,60,33,6,0,system description : Character quantization,0.2643171806167401,0.9705882352941176,0.8571428571428571
text-classification,0,Later we also compare with models that use a different alphabet in which we distinguish between upper-case and lower - case letters .,system description,Character quantization,0,61,34,7,0,system description : Character quantization,0.2687224669603524,1.0,1.0
text-classification,0,Model Design,model,Model Design,0,62,1,1,0,model : Model Design,0.27312775330396477,0.1111111111111111,0.25
text-classification,0,We designed 2 ConvNets - one large and one small .,model,Model Design,0,63,2,2,0,model : Model Design,0.2775330396475771,0.2222222222222222,0.5
text-classification,0,They are both 9 layers deep with 6 convolutional layers and 3 fully - connected layers .,model,Model Design,0,64,3,3,0,model : Model Design,0.28193832599118945,0.3333333333333333,0.75
text-classification,0,gives an illustration .,model,Model Design,0,65,4,4,0,model : Model Design,0.28634361233480177,0.4444444444444444,1.0
text-classification,0,Some Text,model,Some Text,0,66,5,1,0,model : Some Text,0.2907488986784141,0.5555555555555556,1.0
text-classification,0,Convolutions,model,Convolutions,0,67,6,1,0,model : Convolutions,0.29515418502202645,0.6666666666666666,0.25
text-classification,0,Max - pooling Length Feature Quantization ...,model,Convolutions,0,68,7,2,0,model : Convolutions,0.29955947136563876,0.7777777777777778,0.5
text-classification,0,Conv. and Pool. layers,model,Convolutions,0,69,8,3,0,model : Convolutions,0.3039647577092511,0.8888888888888888,0.75
text-classification,0,Fully - connected :,model,Convolutions,0,70,9,4,0,model : Convolutions,0.30837004405286345,1.0,1.0
text-classification,0,Illustration of our model,model,model,0,71,1,1,0,model : model,0.31277533039647576,0.03125,0.06666666666666667
text-classification,0,"The input have number of features equal to 70 due to our character quantization method , and the input feature length is 1014 .",model,model,0,72,2,2,0,model : model,0.31718061674008813,0.0625,0.13333333333333333
text-classification,0,It seems that 1014 characters could already capture most of the texts of interest .,model,model,0,73,3,3,0,model : model,0.32158590308370044,0.09375,0.2
text-classification,0,We also insert 2 dropout modules in between the 3 fully - connected layers to regularize .,model,model,0,74,4,4,0,model : model,0.32599118942731276,0.125,0.26666666666666666
text-classification,0,They have dropout probability of 0.5 .,model,model,0,75,5,5,0,model : model,0.3303964757709251,0.15625,0.3333333333333333
text-classification,0,"lists the configurations for convolutional layers , and table 2 lists the configurations for fully - connected ( linear ) layers .",model,model,0,76,6,6,0,model : model,0.33480176211453744,0.1875,0.4
text-classification,0,We initialize the weights using a Gaussian distribution .,model,model,0,77,7,7,0,model : model,0.3392070484581498,0.21875,0.4666666666666667
text-classification,0,"The mean and standard deviation used for initializing the large model is ( 0 , 0.02 ) and small model ( 0 , 0.05 ) . :",model,model,0,78,8,8,0,model : model,0.3436123348017621,0.25,0.5333333333333333
text-classification,0,Fully - connected layers used in our experiments .,model,model,0,79,9,9,0,model : model,0.34801762114537443,0.28125,0.6
text-classification,0,The number of output units for the last layer is determined by the problem .,model,model,0,80,10,10,0,model : model,0.3524229074889868,0.3125,0.6666666666666666
text-classification,0,"For example , for a 10 - class classification problem it will be 10 .",model,model,0,81,11,11,0,model : model,0.3568281938325991,0.34375,0.7333333333333333
text-classification,0,Depends on the problem,model,model,0,82,12,12,0,model : model,0.36123348017621143,0.375,0.8
text-classification,0,"For different problems the input lengths maybe different ( for example in our case l 0 = 1014 ) , and so are the frame lengths .",model,model,0,83,13,13,0,model : model,0.3656387665198238,0.40625,0.8666666666666667
text-classification,0,"From our model design , it is easy to know that given input length l 0 , the output frame length after the last convolutional layer ( but before any of the fully - connected layers ) isl 6 = ( l 0 ? 96 ) / 27 .",model,model,0,84,14,14,0,model : model,0.3700440528634361,0.4375,0.9333333333333333
text-classification,0,This number multiplied with the frame size at layer 6 will give the input dimension the first fully - connected layer accepts .,model,model,0,85,15,15,0,model : model,0.3744493392070485,0.46875,1.0
text-classification,0,Layer Output Units Large Output Units,model,Layer Output Units Large Output Units,0,86,16,1,0,model : Layer Output Units Large Output Units,0.3788546255506608,0.5,1.0
text-classification,0,Data Augmentation using Thesaurus,model,Data Augmentation using Thesaurus,0,87,17,1,0,model : Data Augmentation using Thesaurus,0.3832599118942731,0.53125,0.07692307692307693
text-classification,0,Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models .,model,Data Augmentation using Thesaurus,0,88,18,2,0,model : Data Augmentation using Thesaurus,0.3876651982378855,0.5625,0.15384615384615385
text-classification,0,These techniques usually work well when we could find appropriate invariance properties that the model should possess .,model,Data Augmentation using Thesaurus,0,89,19,3,0,model : Data Augmentation using Thesaurus,0.3920704845814978,0.59375,0.23076923076923078
text-classification,0,"In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .",model,Data Augmentation using Thesaurus,0,90,20,4,0,model : Data Augmentation using Thesaurus,0.3964757709251101,0.625,0.3076923076923077
text-classification,0,"Therefore , the best way to do data augmentation would have been using human rephrases of sentences , but this is unrealistic and expensive due the large volume of samples in our datasets .",model,Data Augmentation using Thesaurus,0,91,21,5,0,model : Data Augmentation using Thesaurus,0.4008810572687225,0.65625,0.38461538461538464
text-classification,0,"As a result , the most natural choice in data augmentation for us is to replace words or phrases with their synonyms .",model,Data Augmentation using Thesaurus,0,92,22,6,0,model : Data Augmentation using Thesaurus,0.4052863436123348,0.6875,0.46153846153846156
text-classification,0,"We experimented data augmentation by using an English thesaurus , which is obtained from the mytheas component used in LibreOffice 1 project .",model,Data Augmentation using Thesaurus,0,93,23,7,0,model : Data Augmentation using Thesaurus,0.40969162995594716,0.71875,0.5384615384615384
text-classification,0,"That thesaurus in turn was obtained from Word - Net , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning .",model,Data Augmentation using Thesaurus,0,94,24,8,0,model : Data Augmentation using Thesaurus,0.41409691629955947,0.75,0.6153846153846154
text-classification,0,"To decide on how many words to replace , we extract all replaceable words from the given text and randomly chooser of them to be replaced .",model,Data Augmentation using Thesaurus,0,95,25,9,0,model : Data Augmentation using Thesaurus,0.4185022026431718,0.78125,0.6923076923076923
text-classification,0,The probability of number r is determined by a geometric distribution with parameter pin which P [ r ] ? pr .,model,Data Augmentation using Thesaurus,0,96,26,10,0,model : Data Augmentation using Thesaurus,0.42290748898678415,0.8125,0.7692307692307693
text-classification,0,The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [ s ] ? q s .,model,Data Augmentation using Thesaurus,0,97,27,11,0,model : Data Augmentation using Thesaurus,0.42731277533039647,0.84375,0.8461538461538461
text-classification,0,"This way , the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning .",model,Data Augmentation using Thesaurus,0,98,28,12,0,model : Data Augmentation using Thesaurus,0.43171806167400884,0.875,0.9230769230769231
text-classification,0,We will report the results using this new data augmentation technique with p = 0.5 and q = 0.5 .,model,Data Augmentation using Thesaurus,0,99,29,13,0,model : Data Augmentation using Thesaurus,0.43612334801762115,0.90625,1.0
text-classification,0,Comparison Models,model,Comparison Models,0,100,30,1,0,model : Comparison Models,0.44052863436123346,0.9375,0.3333333333333333
text-classification,0,"To offer fair comparisons to competitive models , we conducted a series of experiments with both traditional and deep learning methods .",model,Comparison Models,0,101,31,2,0,model : Comparison Models,0.44493392070484583,0.96875,0.6666666666666666
text-classification,0,"We tried our best to choose models that can provide comparable and competitive results , and the results are reported faithfully without any model selection .",model,Comparison Models,0,102,32,3,0,model : Comparison Models,0.44933920704845814,1.0,1.0
text-classification,0,Traditional Methods,method,Traditional Methods,0,103,1,1,0,method : Traditional Methods,0.45374449339207046,0.008403361344537815,0.05555555555555555
text-classification,0,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,method,Traditional Methods,0,104,2,2,0,method : Traditional Methods,0.4581497797356828,0.01680672268907563,0.1111111111111111
text-classification,0,The classifier used is a multinomial logistic regression in all these models .,method,Traditional Methods,0,105,3,3,0,method : Traditional Methods,0.46255506607929514,0.025210084033613446,0.16666666666666666
text-classification,0,Bag - of - words and its TFIDF .,method,Traditional Methods,0,106,4,4,0,method : Traditional Methods,0.4669603524229075,0.03361344537815126,0.2222222222222222
text-classification,0,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",method,Traditional Methods,0,107,5,5,0,method : Traditional Methods,0.4713656387665198,0.04201680672268908,0.2777777777777778
text-classification,0,"For the normal bag - of - words , we use the counts of each word as the features .",method,Traditional Methods,0,108,6,6,0,method : Traditional Methods,0.47577092511013214,0.05042016806722689,0.3333333333333333
text-classification,0,"For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",method,Traditional Methods,0,109,7,7,0,method : Traditional Methods,0.4801762114537445,0.058823529411764705,0.3888888888888889
text-classification,0,The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .,method,Traditional Methods,0,110,8,8,0,method : Traditional Methods,0.4845814977973568,0.06722689075630252,0.4444444444444444
text-classification,0,The features are normalized by dividing the largest feature value .,method,Traditional Methods,0,111,9,9,0,method : Traditional Methods,0.4889867841409692,0.07563025210084033,0.5
text-classification,0,Bag - of - ngrams and its TFIDF .,method,Traditional Methods,0,112,10,10,0,method : Traditional Methods,0.4933920704845815,0.08403361344537816,0.5555555555555556
text-classification,0,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",method,Traditional Methods,0,113,11,11,0,method : Traditional Methods,0.4977973568281938,0.09243697478991597,0.6111111111111112
text-classification,0,The feature values are computed the same way as in the bag - of - words model .,method,Traditional Methods,0,114,12,12,0,method : Traditional Methods,0.5022026431718062,0.10084033613445378,0.6666666666666666
text-classification,0,Bag - of - means on word embedding .,method,Traditional Methods,0,115,13,13,0,method : Traditional Methods,0.5066079295154186,0.1092436974789916,0.7222222222222222
text-classification,0,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",method,Traditional Methods,0,116,14,14,0,method : Traditional Methods,0.5110132158590308,0.11764705882352941,0.7777777777777778
text-classification,0,We take into consideration all the words that appeared more than 5 times in the training subset .,method,Traditional Methods,0,117,15,15,0,method : Traditional Methods,0.5154185022026432,0.12605042016806722,0.8333333333333334
text-classification,0,The dimension of the embedding is 300 .,method,Traditional Methods,0,118,16,16,0,method : Traditional Methods,0.5198237885462555,0.13445378151260504,0.8888888888888888
text-classification,0,The bag - of - means features are computed the same way as in the bag - of - words model .,method,Traditional Methods,0,119,17,17,0,method : Traditional Methods,0.5242290748898678,0.14285714285714285,0.9444444444444444
text-classification,0,The number of means is 5000 .,method,Traditional Methods,0,120,18,18,0,method : Traditional Methods,0.5286343612334802,0.15126050420168066,1.0
text-classification,0,Deep Learning Methods,method,Deep Learning Methods,0,121,19,1,0,method : Deep Learning Methods,0.5330396475770925,0.15966386554621848,0.058823529411764705
text-classification,0,Recently deep learning methods have started to be applied to text classification .,method,Deep Learning Methods,0,122,20,2,0,method : Deep Learning Methods,0.5374449339207048,0.16806722689075632,0.11764705882352941
text-classification,0,"We choose two simple and representative models for comparison , in which one is word - based ConvNet and the other a simple long - short term memory ( LSTM ) recurrent neural network model .",method,Deep Learning Methods,0,123,21,3,0,method : Deep Learning Methods,0.5418502202643172,0.17647058823529413,0.17647058823529413
text-classification,0,Word - based ConvNets .,method,Deep Learning Methods,0,124,22,4,0,method : Deep Learning Methods,0.5462555066079295,0.18487394957983194,0.23529411764705882
text-classification,0,"Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",method,Deep Learning Methods,0,125,23,5,0,method : Deep Learning Methods,0.5506607929515418,0.19327731092436976,0.29411764705882354
text-classification,0,We offer comparisons with both using the pretrained word2vec embedding and using lookup tables .,method,Deep Learning Methods,0,126,24,6,0,method : Deep Learning Methods,0.5550660792951542,0.20168067226890757,0.35294117647058826
text-classification,0,"The embedding size is 300 in both cases , in the same way as our bagof - means model .",method,Deep Learning Methods,0,127,25,7,0,method : Deep Learning Methods,0.5594713656387665,0.21008403361344538,0.4117647058823529
text-classification,0,"To ensure fair comparison , the models for each case are of the same size as our character - level ConvNets , in terms of both the number of layers and each layer 's output size .",method,Deep Learning Methods,0,128,26,8,0,method : Deep Learning Methods,0.5638766519823789,0.2184873949579832,0.47058823529411764
text-classification,0,Experiments using a thesaurus for data augmentation are also conducted .,method,Deep Learning Methods,0,129,27,9,0,method : Deep Learning Methods,0.5682819383259912,0.226890756302521,0.5294117647058824
text-classification,0,LSTM LSTM LSTM ... : long - short term memory,method,Deep Learning Methods,0,130,28,10,0,method : Deep Learning Methods,0.5726872246696035,0.23529411764705882,0.5882352941176471
text-classification,0,Long - short term memory .,method,Deep Learning Methods,0,131,29,11,0,method : Deep Learning Methods,0.5770925110132159,0.24369747899159663,0.6470588235294118
text-classification,0,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",method,Deep Learning Methods,0,132,30,12,0,method : Deep Learning Methods,0.5814977973568282,0.25210084033613445,0.7058823529411765
text-classification,0,"The LSTM model used in our case is word - based , using pretrained word2vec embedding of size 300 as in previous models .",method,Deep Learning Methods,0,133,31,13,0,method : Deep Learning Methods,0.5859030837004405,0.2605042016806723,0.7647058823529411
text-classification,0,"The model is formed by taking mean of the outputs of all LSTM cells to form a feature vector , and then using multinomial logistic regression on this feature vector .",method,Deep Learning Methods,0,134,32,14,0,method : Deep Learning Methods,0.5903083700440529,0.2689075630252101,0.8235294117647058
text-classification,0,The output dimension is 512 .,method,Deep Learning Methods,0,135,33,15,0,method : Deep Learning Methods,0.5947136563876652,0.2773109243697479,0.8823529411764706
text-classification,0,"The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",method,Deep Learning Methods,0,136,34,16,0,method : Deep Learning Methods,0.5991189427312775,0.2857142857142857,0.9411764705882353
text-classification,0,We also used gradient clipping in which the gradient norm is limited to 5 . gives an illustration .,method,Deep Learning Methods,0,137,35,17,0,method : Deep Learning Methods,0.6035242290748899,0.29411764705882354,1.0
text-classification,0,Mean,method,Mean,0,138,36,1,0,method : Mean,0.6079295154185022,0.3025210084033613,1.0
text-classification,0,Choice of Alphabet,method,Choice of Alphabet,0,139,37,1,0,method : Choice of Alphabet,0.6123348017621145,0.31092436974789917,0.25
text-classification,0,"For the alphabet of English , one apparent choice is whether to distinguish between upper-case and lower - case letters .",method,Choice of Alphabet,0,140,38,2,0,method : Choice of Alphabet,0.6167400881057269,0.31932773109243695,0.5
text-classification,0,We report experiments on this choice and observed that it usually ( but not always ) gives worse results when such distinction is made .,method,Choice of Alphabet,0,141,39,3,0,method : Choice of Alphabet,0.6211453744493393,0.3277310924369748,0.75
text-classification,0,"One possible explanation might be that semantics do not change with different letter cases , therefore there is a benefit of regularization .",method,Choice of Alphabet,0,142,40,4,0,method : Choice of Alphabet,0.6255506607929515,0.33613445378151263,1.0
text-classification,0,Large - scale Datasets and Results,method,Large-scale Datasets and Results,0,143,41,1,0,method : Large-scale Datasets and Results,0.6299559471365639,0.3445378151260504,0.022222222222222223
text-classification,0,"Previous research on ConvNets in different are as has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .",method,Large-scale Datasets and Results,0,144,42,2,0,method : Large-scale Datasets and Results,0.6343612334801763,0.35294117647058826,0.044444444444444446
text-classification,0,"However , most open datasets for text classification are quite small , and large - scale datasets are splitted with a significantly smaller training set than testing .",method,Large-scale Datasets and Results,0,145,43,3,0,method : Large-scale Datasets and Results,0.6387665198237885,0.36134453781512604,0.06666666666666667
text-classification,0,"Therefore , instead of confusing our community more by using them , we built several large - scale datasets for our experiments , ranging from hundreds of thousands to several millions of samples .",method,Large-scale Datasets and Results,0,146,44,4,0,method : Large-scale Datasets and Results,0.6431718061674009,0.3697478991596639,0.08888888888888889
text-classification,0,is a summary .,method,Large-scale Datasets and Results,0,147,45,5,0,method : Large-scale Datasets and Results,0.6475770925110133,0.37815126050420167,0.1111111111111111
text-classification,0,Sogou news corpus .,method,Large-scale Datasets and Results,0,148,46,6,0,method : Large-scale Datasets and Results,0.6519823788546255,0.3865546218487395,0.13333333333333333
text-classification,0,"This dataset is a combination of the Sogo u CA and Sogo uCS news corpora , containing in total 2,909,551 news articles in various topic channels .",method,Large-scale Datasets and Results,0,149,47,7,0,method : Large-scale Datasets and Results,0.6563876651982379,0.3949579831932773,0.15555555555555556
text-classification,0,"We then labeled each piece of news using its URL , by manually classifying the their domain names .",method,Large-scale Datasets and Results,0,150,48,8,0,method : Large-scale Datasets and Results,0.6607929515418502,0.40336134453781514,0.17777777777777778
text-classification,0,This gives us a large corpus of news articles labeled with their categories .,method,Large-scale Datasets and Results,0,151,49,9,0,method : Large-scale Datasets and Results,0.6651982378854625,0.4117647058823529,0.2
text-classification,0,There are a large number categories but most of them contain only few articles .,method,Large-scale Datasets and Results,0,152,50,10,0,method : Large-scale Datasets and Results,0.6696035242290749,0.42016806722689076,0.2222222222222222
text-classification,0,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",method,Large-scale Datasets and Results,0,153,51,11,0,method : Large-scale Datasets and Results,0.6740088105726872,0.42857142857142855,0.24444444444444444
text-classification,0,"The number of training samples selected for each class is 90,000 and testing 12,000 .",method,Large-scale Datasets and Results,0,154,52,12,0,method : Large-scale Datasets and Results,0.6784140969162996,0.4369747899159664,0.26666666666666666
text-classification,0,"Although this is a dataset in Chinese , we used pypinyin package combined with jieba Chinese segmentation system to produce Pinyin - a phonetic romanization of Chinese .",method,Large-scale Datasets and Results,0,155,53,13,0,method : Large-scale Datasets and Results,0.6828193832599119,0.44537815126050423,0.28888888888888886
text-classification,0,The models for English can then be applied to this dataset without change .,method,Large-scale Datasets and Results,0,156,54,14,0,method : Large-scale Datasets and Results,0.6872246696035242,0.453781512605042,0.3111111111111111
text-classification,0,The fields used are title and content . :,method,Large-scale Datasets and Results,0,157,55,15,0,method : Large-scale Datasets and Results,0.6916299559471366,0.46218487394957986,0.3333333333333333
text-classification,0,Testing errors of all the models .,method,Large-scale Datasets and Results,0,158,56,16,0,method : Large-scale Datasets and Results,0.6960352422907489,0.47058823529411764,0.35555555555555557
text-classification,0,Numbers are in percentage .,method,Large-scale Datasets and Results,0,159,57,17,0,method : Large-scale Datasets and Results,0.7004405286343612,0.4789915966386555,0.37777777777777777
text-classification,0,"Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",method,Large-scale Datasets and Results,0,160,58,18,0,method : Large-scale Datasets and Results,0.7048458149779736,0.48739495798319327,0.4
text-classification,0,"w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",method,Large-scale Datasets and Results,0,161,59,19,0,method : Large-scale Datasets and Results,0.7092511013215859,0.4957983193277311,0.4222222222222222
text-classification,0,DBpedia is a crowd - sourced community effort to extract structured information from Wikipedia .,method,Large-scale Datasets and Results,0,162,60,20,0,method : Large-scale Datasets and Results,0.7136563876651982,0.5042016806722689,0.4444444444444444
text-classification,0,The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014 .,method,Large-scale Datasets and Results,0,163,61,21,0,method : Large-scale Datasets and Results,0.7180616740088106,0.5126050420168067,0.4666666666666667
text-classification,0,"From each of these 14 ontology classes , we randomly choose 40,000 training samples and 5,000 testing samples .",method,Large-scale Datasets and Results,0,164,62,22,0,method : Large-scale Datasets and Results,0.7224669603524229,0.5210084033613446,0.4888888888888889
text-classification,0,The fields we used for this dataset contain title and abstract of each Wikipedia article .,method,Large-scale Datasets and Results,0,165,63,23,0,method : Large-scale Datasets and Results,0.7268722466960352,0.5294117647058824,0.5111111111111111
text-classification,0,Yelp reviews .,method,Large-scale Datasets and Results,0,166,64,24,0,method : Large-scale Datasets and Results,0.7312775330396476,0.5378151260504201,0.5333333333333333
text-classification,0,The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 .,method,Large-scale Datasets and Results,0,167,65,25,0,method : Large-scale Datasets and Results,0.73568281938326,0.5462184873949579,0.5555555555555556
text-classification,0,"This dataset contains 1,569,264 samples that have review texts .",method,Large-scale Datasets and Results,0,168,66,26,0,method : Large-scale Datasets and Results,0.7400881057268722,0.5546218487394958,0.5777777777777777
text-classification,0,"Two classification tasks are constructed from this dataset - one predicting full number of stars the user has given , and the other predicting a polarity label by considering stars 1 and 2 negative , and 3 and 4 positive .",method,Large-scale Datasets and Results,0,169,67,27,0,method : Large-scale Datasets and Results,0.7444933920704846,0.5630252100840336,0.6
text-classification,0,"The full dataset has 130,000 training samples and 10,000 testing samples in each star , and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity .",method,Large-scale Datasets and Results,0,170,68,28,0,method : Large-scale Datasets and Results,0.748898678414097,0.5714285714285714,0.6222222222222222
text-classification,0,Yahoo!,method,Large-scale Datasets and Results,0,171,69,29,0,method : Large-scale Datasets and Results,0.7533039647577092,0.5798319327731093,0.6444444444444445
text-classification,0,Answers dataset .,method,Large-scale Datasets and Results,0,172,70,30,0,method : Large-scale Datasets and Results,0.7577092511013216,0.5882352941176471,0.6666666666666666
text-classification,0,We obtained Yahoo!,method,Large-scale Datasets and Results,0,173,71,31,0,method : Large-scale Datasets and Results,0.762114537444934,0.5966386554621849,0.6888888888888889
text-classification,0,Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo !,method,Large-scale Datasets and Results,0,174,72,32,0,method : Large-scale Datasets and Results,0.7665198237885462,0.6050420168067226,0.7111111111111111
text-classification,0,Webscope program .,method,Large-scale Datasets and Results,0,175,73,33,0,method : Large-scale Datasets and Results,0.7709251101321586,0.6134453781512605,0.7333333333333333
text-classification,0,"The corpus contains 4,483,032 questions and their answers .",method,Large-scale Datasets and Results,0,176,74,34,0,method : Large-scale Datasets and Results,0.775330396475771,0.6218487394957983,0.7555555555555555
text-classification,0,We constructed a topic classification dataset from this corpus using 10 largest main categories .,method,Large-scale Datasets and Results,0,177,75,35,0,method : Large-scale Datasets and Results,0.7797356828193832,0.6302521008403361,0.7777777777777778
text-classification,0,"Each class contains 140,000 training samples and 5,000 testing samples .",method,Large-scale Datasets and Results,0,178,76,36,0,method : Large-scale Datasets and Results,0.7841409691629956,0.6386554621848739,0.8
text-classification,0,"The fields we used include question title , question content and best answer .",method,Large-scale Datasets and Results,0,179,77,37,0,method : Large-scale Datasets and Results,0.788546255506608,0.6470588235294118,0.8222222222222222
text-classification,0,Amazon reviews .,method,Large-scale Datasets and Results,0,180,78,38,0,method : Large-scale Datasets and Results,0.7929515418502202,0.6554621848739496,0.8444444444444444
text-classification,0,"We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products .",method,Large-scale Datasets and Results,0,181,79,39,0,method : Large-scale Datasets and Results,0.7973568281938326,0.6638655462184874,0.8666666666666667
text-classification,0,"Similarly to the Yelp review dataset , we also constructed 2 datasets - one full score prediction and another polarity prediction .",method,Large-scale Datasets and Results,0,182,80,40,0,method : Large-scale Datasets and Results,0.801762114537445,0.6722689075630253,0.8888888888888888
text-classification,0,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",method,Large-scale Datasets and Results,0,183,81,41,0,method : Large-scale Datasets and Results,0.8061674008810573,0.680672268907563,0.9111111111111111
text-classification,0,The fields used are review title and review content .,method,Large-scale Datasets and Results,0,184,82,42,0,method : Large-scale Datasets and Results,0.8105726872246696,0.6890756302521008,0.9333333333333333
text-classification,0,lists all the testing errors we obtained from these datasets for all the applicable models .,method,Large-scale Datasets and Results,0,185,83,43,0,method : Large-scale Datasets and Results,0.8149779735682819,0.6974789915966386,0.9555555555555556
text-classification,0,"Note that since we do not have a Chinese thesaurus , the Sogou News dataset does not have any results using thesaurus augmentation .",method,Large-scale Datasets and Results,0,186,84,44,0,method : Large-scale Datasets and Results,0.8193832599118943,0.7058823529411765,0.9777777777777777
text-classification,0,We labeled the best result in blue and worse result in red .,method,Large-scale Datasets and Results,0,187,85,45,0,method : Large-scale Datasets and Results,0.8237885462555066,0.7142857142857143,1.0
text-classification,0,Figure 3 : Relative errors with comparison models,method,Figure 3: Relative errors with comparison models,0,188,86,1,0,method : Figure 3: Relative errors with comparison models,0.8281938325991189,0.7226890756302521,0.029411764705882353
text-classification,0,"To understand the results in table 4 further , we offer some empirical analysis in this section .",method,Figure 3: Relative errors with comparison models,0,189,87,2,0,method : Figure 3: Relative errors with comparison models,0.8325991189427313,0.7310924369747899,0.058823529411764705
text-classification,0,"To facilitate our analysis , we present the relative errors in with respect to comparison models .",method,Figure 3: Relative errors with comparison models,0,190,88,3,0,method : Figure 3: Relative errors with comparison models,0.8370044052863436,0.7394957983193278,0.08823529411764706
text-classification,0,"Each of these plots is computed by taking the difference between errors on comparison model and our character - level ConvNet model , then divided by the comparison model error .",method,Figure 3: Relative errors with comparison models,0,191,89,4,0,method : Figure 3: Relative errors with comparison models,0.8414096916299559,0.7478991596638656,0.11764705882352941
text-classification,0,All ConvNets in the figure are the large models with thesaurus augmentation respectively .,method,Figure 3: Relative errors with comparison models,0,192,90,5,0,method : Figure 3: Relative errors with comparison models,0.8458149779735683,0.7563025210084033,0.14705882352941177
text-classification,0,Character - level,method,Figure 3: Relative errors with comparison models,0,193,91,6,0,method : Figure 3: Relative errors with comparison models,0.8502202643171806,0.7647058823529411,0.17647058823529413
text-classification,0,ConvNet is an effective method .,method,Figure 3: Relative errors with comparison models,0,194,92,7,0,method : Figure 3: Relative errors with comparison models,0.8546255506607929,0.773109243697479,0.20588235294117646
text-classification,0,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,method,Figure 3: Relative errors with comparison models,1,195,93,8,0,method : Figure 3: Relative errors with comparison models,0.8590308370044053,0.7815126050420168,0.23529411764705882
text-classification,0,This is a strong indication that language could also bethought of as a signal no different from any other kind .,method,Figure 3: Relative errors with comparison models,0,196,94,9,0,method : Figure 3: Relative errors with comparison models,0.8634361233480177,0.7899159663865546,0.2647058823529412
text-classification,0,shows 12 random first - layer patches learnt by one of our character - level ConvNets for DBPedia dataset .,method,Figure 3: Relative errors with comparison models,0,197,95,10,0,method : Figure 3: Relative errors with comparison models,0.8678414096916299,0.7983193277310925,0.29411764705882354
text-classification,0,Dataset size forms a dichotomy between traditional and ConvNets models .,method,Figure 3: Relative errors with comparison models,0,198,96,11,0,method : Figure 3: Relative errors with comparison models,0.8722466960352423,0.8067226890756303,0.3235294117647059
text-classification,0,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,method,Figure 3: Relative errors with comparison models,1,199,97,12,0,method : Figure 3: Relative errors with comparison models,0.8766519823788547,0.8151260504201681,0.35294117647058826
text-classification,0,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",method,Figure 3: Relative errors with comparison models,0,200,98,13,0,method : Figure 3: Relative errors with comparison models,0.8810572687224669,0.8235294117647058,0.38235294117647056
text-classification,0,Conv Nets may work well for user - generated data .,method,Figure 3: Relative errors with comparison models,0,201,99,14,0,method : Figure 3: Relative errors with comparison models,0.8854625550660793,0.8319327731092437,0.4117647058823529
text-classification,0,User- generated data vary in the degree of how well the texts are curated .,method,Figure 3: Relative errors with comparison models,0,202,100,15,0,method : Figure 3: Relative errors with comparison models,0.8898678414096917,0.8403361344537815,0.4411764705882353
text-classification,0,"For example , in our million scale datasets , Amazon reviews tend to be raw user-inputs , whereas users might be extra careful in their writings on Yahoo !",method,Figure 3: Relative errors with comparison models,0,203,101,16,0,method : Figure 3: Relative errors with comparison models,0.8942731277533039,0.8487394957983193,0.47058823529411764
text-classification,0,Answers .,method,Figure 3: Relative errors with comparison models,0,204,102,17,0,method : Figure 3: Relative errors with comparison models,0.8986784140969163,0.8571428571428571,0.5
text-classification,0,"Plots comparing word - based deep models ( figures 3 c , 3 d and 3 e ) show that character - level ConvNets work better for less curated user - generated texts .",method,Figure 3: Relative errors with comparison models,0,205,103,18,0,method : Figure 3: Relative errors with comparison models,0.9030837004405287,0.865546218487395,0.5294117647058824
text-classification,0,This property suggests that ConvNets may have better applicability to real - world scenarios .,method,Figure 3: Relative errors with comparison models,0,206,104,19,0,method : Figure 3: Relative errors with comparison models,0.9074889867841409,0.8739495798319328,0.5588235294117647
text-classification,0,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",method,Figure 3: Relative errors with comparison models,1,207,105,20,0,method : Figure 3: Relative errors with comparison models,0.9118942731277533,0.8823529411764706,0.5882352941176471
text-classification,0,Choice of alphabet makes a difference .,method,Figure 3: Relative errors with comparison models,0,208,106,21,0,method : Figure 3: Relative errors with comparison models,0.9162995594713657,0.8907563025210085,0.6176470588235294
text-classification,0,shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,method,Figure 3: Relative errors with comparison models,0,209,107,22,0,method : Figure 3: Relative errors with comparison models,0.920704845814978,0.8991596638655462,0.6470588235294118
text-classification,0,"For million - scale datasets , it seems that not making such distinction usually works better .",method,Figure 3: Relative errors with comparison models,0,210,108,23,0,method : Figure 3: Relative errors with comparison models,0.9251101321585903,0.907563025210084,0.6764705882352942
text-classification,0,"One possible explanation is that there is a regularization effect , but this is to be validated .",method,Figure 3: Relative errors with comparison models,0,211,109,24,0,method : Figure 3: Relative errors with comparison models,0.9295154185022027,0.9159663865546218,0.7058823529411765
text-classification,0,Semantics of tasks may not matter .,method,Figure 3: Relative errors with comparison models,0,212,110,25,0,method : Figure 3: Relative errors with comparison models,0.933920704845815,0.9243697478991597,0.7352941176470589
text-classification,0,Our datasets consist of two kinds of tasks : sentiment analysis ( Yelp and Amazon reviews ) and topic classification ( all others ) .,method,Figure 3: Relative errors with comparison models,0,213,111,26,0,method : Figure 3: Relative errors with comparison models,0.9383259911894273,0.9327731092436975,0.7647058823529411
text-classification,0,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,method,Figure 3: Relative errors with comparison models,0,214,112,27,0,method : Figure 3: Relative errors with comparison models,0.9427312775330396,0.9411764705882353,0.7941176470588235
text-classification,0,Bag - of - means is a misuse of word2vec .,method,Figure 3: Relative errors with comparison models,0,215,113,28,0,method : Figure 3: Relative errors with comparison models,0.947136563876652,0.9495798319327731,0.8235294117647058
text-classification,0,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,method,Figure 3: Relative errors with comparison models,0,216,114,29,0,method : Figure 3: Relative errors with comparison models,0.9515418502202643,0.957983193277311,0.8529411764705882
text-classification,0,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",method,Figure 3: Relative errors with comparison models,1,217,115,30,0,method : Figure 3: Relative errors with comparison models,0.9559471365638766,0.9663865546218487,0.8823529411764706
text-classification,0,"However , our experiments does not speak for any other language processing tasks or use of word2vec in any other way .",method,Figure 3: Relative errors with comparison models,0,218,116,31,0,method : Figure 3: Relative errors with comparison models,0.960352422907489,0.9747899159663865,0.9117647058823529
text-classification,0,There is no free lunch .,method,Figure 3: Relative errors with comparison models,0,219,117,32,0,method : Figure 3: Relative errors with comparison models,0.9647577092511013,0.9831932773109243,0.9411764705882353
text-classification,0,Our experiments once again verifies that there is not a single machine learning model that can work for all kinds of datasets .,method,Figure 3: Relative errors with comparison models,0,220,118,33,0,method : Figure 3: Relative errors with comparison models,0.9691629955947136,0.9915966386554622,0.9705882352941176
text-classification,0,The factors discussed in this section could all play a role in deciding which method is the best for some specific application .,method,Figure 3: Relative errors with comparison models,0,221,119,34,0,method : Figure 3: Relative errors with comparison models,0.973568281938326,1.0,1.0
text-classification,0,Conclusion and Outlook,conclusion,Conclusion and Outlook,0,222,1,1,0,conclusion : Conclusion and Outlook,0.9779735682819384,0.16666666666666666,0.16666666666666666
text-classification,0,This article offers an empirical study on character - level convolutional networks for text classification .,conclusion,Conclusion and Outlook,0,223,2,2,0,conclusion : Conclusion and Outlook,0.9823788546255506,0.3333333333333333,0.3333333333333333
text-classification,0,We compared with a large number of traditional and deep learning models using several largescale datasets .,conclusion,Conclusion and Outlook,0,224,3,3,0,conclusion : Conclusion and Outlook,0.986784140969163,0.5,0.5
text-classification,0,"On one hand , analysis shows that character - level ConvNet is an effective method .",conclusion,Conclusion and Outlook,0,225,4,4,0,conclusion : Conclusion and Outlook,0.9911894273127754,0.6666666666666666,0.6666666666666666
text-classification,0,"On the other hand , how well our model performs in comparisons depends on many factors , such as dataset size , whether the texts are curated and choice of alphabet .",conclusion,Conclusion and Outlook,0,226,5,5,0,conclusion : Conclusion and Outlook,0.9955947136563876,0.8333333333333334,0.8333333333333334
text-classification,0,"In the future , we hope to apply character - level ConvNets for a broader range of language processing tasks especially when structured outputs are needed .",conclusion,Conclusion and Outlook,0,227,6,6,0,conclusion : Conclusion and Outlook,1.0,1.0,1.0
text-classification,1,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,title,title,1,2,1,1,0,title : title,0.0078125,1.0,1.0
text-classification,1,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.01171875,0.1111111111111111,0.1111111111111111
text-classification,1,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.015625,0.2222222222222222,0.2222222222222222
text-classification,1,We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ' text region embedding + pooling ' .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.01953125,0.3333333333333333,0.3333333333333333
text-classification,1,"Under this framework , we explore a more sophisticated region embedding method using Long Short - Term Memory ( LSTM ) .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.0234375,0.4444444444444444,0.4444444444444444
text-classification,1,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02734375,0.5555555555555556,0.5555555555555556
text-classification,1,We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03125,0.6666666666666666,0.6666666666666666
text-classification,1,The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03515625,0.7777777777777778,0.7777777777777778
text-classification,1,"The results indicate that on this task , embeddings of text regions , which can convey complex concepts , are more useful than embeddings of single words in isolation .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.0390625,0.8888888888888888,0.8888888888888888
text-classification,1,We report performances exceeding the previous best results on four benchmark datasets .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.04296875,1.0,1.0
text-classification,1,Introduction,introduction,introduction,0,12,1,1,0,introduction : introduction,0.046875,0.02857142857142857,0.02857142857142857
text-classification,1,"Text categorization is the task of assigning labels to documents written in a natural language , and it has numerous real - world applications including sentiment analysis as well as traditional topic assignment tasks .",introduction,introduction,0,13,2,2,0,introduction : introduction,0.05078125,0.05714285714285714,0.05714285714285714
text-classification,1,"The state - of - the art methods for text categorization had long been linear predictors ( e.g. , SVM with a linear kernel ) with either bag - ofword or bag - of - n- gram vectors ( hereafter bow ) as input , e.g. , .",introduction,introduction,0,14,3,3,0,introduction : introduction,0.0546875,0.08571428571428572,0.08571428571428572
text-classification,1,"This , however , A convolutional neural network ( CNN ) ) is a feedforward neural network with convolution layers interleaved with pooling layers , originally developed for image processing .",introduction,introduction,0,15,4,4,0,introduction : introduction,0.05859375,0.11428571428571428,0.11428571428571428
text-classification,1,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .",introduction,introduction,0,16,5,5,0,introduction : introduction,0.0625,0.14285714285714285,0.14285714285714285
text-classification,1,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .",introduction,introduction,0,17,6,6,0,introduction : introduction,0.06640625,0.17142857142857143,0.17142857142857143
text-classification,1,"In its simplest form , onehot CNN works as follows .",introduction,introduction,0,18,7,7,0,introduction : introduction,0.0703125,0.2,0.2
text-classification,1,"document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",introduction,introduction,0,19,8,8,0,introduction : introduction,0.07421875,0.22857142857142856,0.22857142857142856
text-classification,1,The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,introduction,introduction,0,20,9,9,0,introduction : introduction,0.078125,0.2571428571428571,0.2571428571428571
text-classification,1,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",introduction,introduction,1,21,10,10,0,introduction : introduction,0.08203125,0.2857142857142857,0.2857142857142857
text-classification,1,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) , where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",introduction,introduction,0,22,11,11,0,introduction : introduction,0.0859375,0.3142857142857143,0.3142857142857143
text-classification,1,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) , where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",introduction,introduction,0,23,12,12,0,introduction : introduction,0.08984375,0.34285714285714286,0.34285714285714286
text-classification,1,"It is simple and fast to compute , and considering its simplicity , the method works surprisingly well if the region size is appropriately set .",introduction,introduction,0,24,13,13,0,introduction : introduction,0.09375,0.37142857142857144,0.37142857142857144
text-classification,1,"However , there are also potential shortcomings .",introduction,introduction,0,25,14,14,0,introduction : introduction,0.09765625,0.4,0.4
text-classification,1,"The region size must be fixed , which may not be optimal as the size of relevant regions may vary .",introduction,introduction,0,26,15,15,0,introduction : introduction,0.1015625,0.42857142857142855,0.42857142857142855
text-classification,1,"Practically , the region size can not be very large as the number of parameters to be learned ( components of W ) depends on it .",introduction,introduction,0,27,16,16,0,introduction : introduction,0.10546875,0.45714285714285713,0.45714285714285713
text-classification,1,JZ15 proposed variations to alleviate these issues .,introduction,introduction,0,28,17,17,0,introduction : introduction,0.109375,0.4857142857142857,0.4857142857142857
text-classification,1,"For example , a bow - input variation allows x above to be a bow vector of the region .",introduction,introduction,0,29,18,18,0,introduction : introduction,0.11328125,0.5142857142857142,0.5142857142857142
text-classification,1,"This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",introduction,introduction,0,30,19,19,0,introduction : introduction,0.1171875,0.5428571428571428,0.5428571428571428
text-classification,1,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",introduction,introduction,1,31,20,20,0,introduction : introduction,0.12109375,0.5714285714285714,0.5714285714285714
text-classification,1,LSTM ) is a recurrent neural network .,introduction,introduction,0,32,21,21,0,introduction : introduction,0.125,0.6,0.6
text-classification,1,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ? 1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .",introduction,introduction,0,33,22,22,0,introduction : introduction,0.12890625,0.6285714285714286,0.6285714285714286
text-classification,1,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ? 1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .",introduction,introduction,0,34,23,23,0,introduction : introduction,0.1328125,0.6571428571428571,0.6571428571428571
text-classification,1,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,introduction,introduction,0,35,24,24,0,introduction : introduction,0.13671875,0.6857142857142857,0.6857142857142857
text-classification,1,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",introduction,introduction,0,36,25,25,0,introduction : introduction,0.140625,0.7142857142857143,0.7142857142857143
text-classification,1,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",introduction,introduction,1,37,26,26,0,introduction : introduction,0.14453125,0.7428571428571429,0.7428571428571429
text-classification,1,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",introduction,introduction,1,38,27,27,0,introduction : introduction,0.1484375,0.7714285714285715,0.7714285714285715
text-classification,1,Our findings are threefold .,introduction,introduction,0,39,28,28,0,introduction : introduction,0.15234375,0.8,0.8
text-classification,1,"First , in the supervised setting , our simplification strategy leads to higher accuracy and faster training than previous LSTM .",introduction,introduction,0,40,29,29,0,introduction : introduction,0.15625,0.8285714285714286,0.8285714285714286
text-classification,1,"Second , accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input .",introduction,introduction,0,41,30,30,0,introduction : introduction,0.16015625,0.8571428571428571,0.8571428571428571
text-classification,1,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .",introduction,introduction,0,42,31,31,0,introduction : introduction,0.1640625,0.8857142857142857,0.8857142857142857
text-classification,1,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .",introduction,introduction,0,43,32,32,0,introduction : introduction,0.16796875,0.9142857142857143,0.9142857142857143
text-classification,1,"Overall , our results show that for text categorization , embeddings of text regions , which can convey higher - level concepts than single words in isolation , are useful , and that useful region embeddings can be learned without going through word embedding learning .",introduction,introduction,0,44,33,33,0,introduction : introduction,0.171875,0.9428571428571428,0.9428571428571428
text-classification,1,We report performances exceeding the previous best results on four benchmark datasets .,introduction,introduction,0,45,34,34,0,introduction : introduction,0.17578125,0.9714285714285714,0.9714285714285714
text-classification,1,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,introduction,introduction,0,46,35,35,0,introduction : introduction,0.1796875,1.0,1.0
text-classification,1,Preliminary,system description,Preliminary,0,47,1,1,0,system description : Preliminary,0.18359375,0.005,0.14285714285714285
text-classification,1,"On text , LSTM has been used for labeling or generating words .",system description,Preliminary,0,48,2,2,0,system description : Preliminary,0.1875,0.01,0.2857142857142857
text-classification,1,"It has been also used for representing short sentences mostly for sentiment analysis , and some of them rely on syntactic parse trees ; see e.g. , .",system description,Preliminary,0,49,3,3,0,system description : Preliminary,0.19140625,0.015,0.42857142857142855
text-classification,1,"Unlike these studies , this work as well as JZ15 focuses on classifying general full - length documents without any special linguistic knowledge .",system description,Preliminary,0,50,4,4,0,system description : Preliminary,0.1953125,0.02,0.5714285714285714
text-classification,1,"Similarly , DL15 applied LSTM to categorizing general full - length documents .",system description,Preliminary,0,51,5,5,0,system description : Preliminary,0.19921875,0.025,0.7142857142857143
text-classification,1,"Therefore , our empirical comparisons will focus on DL15 and JZ15 , both of which reported new state of the art results .",system description,Preliminary,0,52,6,6,0,system description : Preliminary,0.203125,0.03,0.8571428571428571
text-classification,1,"Let us first introduce the general LSTM formulation , and then briefly describe DL15 's model as it illustrates the challenges in using LSTMs for this task .",system description,Preliminary,0,53,7,7,0,system description : Preliminary,0.20703125,0.035,1.0
text-classification,1,LSTM,system description,Supervised LSTM for text categorization,0,54,8,1,0,system description : Supervised LSTM for text categorization,0.2109375,0.04,0.037037037037037035
text-classification,1,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. , where denotes element - wise multiplication and ? is an element - wise squash function to make the gating values in [ 0 , 1 ] .",system description,Supervised LSTM for text categorization,0,55,9,2,0,system description : Supervised LSTM for text categorization,0.21484375,0.045,0.07407407407407407
text-classification,1,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. , where denotes element - wise multiplication and ? is an element - wise squash function to make the gating values in [ 0 , 1 ] .",system description,Supervised LSTM for text categorization,0,56,10,3,0,system description : Supervised LSTM for text categorization,0.21875,0.05,0.1111111111111111
text-classification,1,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. , where denotes element - wise multiplication and ? is an element - wise squash function to make the gating values in [ 0 , 1 ] .",system description,Supervised LSTM for text categorization,0,57,11,4,0,system description : Supervised LSTM for text categorization,0.22265625,0.055,0.14814814814814814
text-classification,1,We fix ? to sigmoid .,system description,Supervised LSTM for text categorization,0,58,12,5,0,system description : Supervised LSTM for text categorization,0.2265625,0.06,0.18518518518518517
text-classification,1,"t ? Rd is the input from the lower layer at time step t , where d would be , for example , size of vocabulary if the input was a one - hot vector representing a word , or the dimensionality of word vector if the lower layer was a word embedding layer .",system description,Supervised LSTM for text categorization,0,59,13,6,0,system description : Supervised LSTM for text categorization,0.23046875,0.065,0.2222222222222222
text-classification,1,"t ? Rd is the input from the lower layer at time step t , where d would be , for example , size of vocabulary if the input was a one - hot vector representing a word , or the dimensionality of word vector if the lower layer was a word embedding layer .",system description,Supervised LSTM for text categorization,0,60,14,7,0,system description : Supervised LSTM for text categorization,0.234375,0.07,0.25925925925925924
text-classification,1,"With q LSTM units , the dimensionality of the weight matrices and bias vectors , which need to be trained , are W ( ) ? R qd , U ( ) ? R qq , and b ( ) ? R q for all types .",system description,Supervised LSTM for text categorization,0,61,15,8,0,system description : Supervised LSTM for text categorization,0.23828125,0.075,0.2962962962962963
text-classification,1,"With q LSTM units , the dimensionality of the weight matrices and bias vectors , which need to be trained , are W ( ) ? R qd , U ( ) ? R qq , and b ( ) ? R q for all types .",system description,Supervised LSTM for text categorization,0,62,16,9,0,system description : Supervised LSTM for text categorization,0.2421875,0.08,0.3333333333333333
text-classification,1,"The centerpiece of LSTM is the memory cells ct , designed to counteract the risk of vanishing / exploding gradients , thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks .",system description,Supervised LSTM for text categorization,0,63,17,10,0,system description : Supervised LSTM for text categorization,0.24609375,0.085,0.37037037037037035
text-classification,1,The forget gate ft is for resetting the memory cells .,system description,Supervised LSTM for text categorization,0,64,18,11,0,system description : Supervised LSTM for text categorization,0.25,0.09,0.4074074074074074
text-classification,1,The input gate it and output gate o t control the input and output of the memory cells .,system description,Supervised LSTM for text categorization,0,65,19,12,0,system description : Supervised LSTM for text categorization,0.25390625,0.095,0.4444444444444444
text-classification,1,Word - vector LSTM ( wv - LSTM ) [ DL15 ] DL15 's application of LSTM to text categorization is straightforward .,system description,Supervised LSTM for text categorization,0,66,20,13,0,system description : Supervised LSTM for text categorization,0.2578125,0.1,0.48148148148148145
text-classification,1,"As illustrated in , for each document , the output of the LSTM layer is the output of the last time step ( corresponding to the last word of the document ) , which represents the whole document ( document embedding ) .",system description,Supervised LSTM for text categorization,0,67,21,14,0,system description : Supervised LSTM for text categorization,0.26171875,0.105,0.5185185185185185
text-classification,1,"Like many other studies of LSTM on text , words are first converted to low - dimensional dense word vectors via a word embedding layer ; therefore , we call it word - vector LSTM or wv - LSTM .",system description,Supervised LSTM for text categorization,0,68,22,15,0,system description : Supervised LSTM for text categorization,0.265625,0.11,0.5555555555555556
text-classification,1,DL15 observed that wv - LSTM underperformed linear predictors and its training was unstable .,system description,Supervised LSTM for text categorization,0,69,23,16,0,system description : Supervised LSTM for text categorization,0.26953125,0.115,0.5925925925925926
text-classification,1,This was attributed to the fact that documents are long .,system description,Supervised LSTM for text categorization,0,70,24,17,0,system description : Supervised LSTM for text categorization,0.2734375,0.12,0.6296296296296297
text-classification,1,"In addition , we found that training and testing of wv - LSTM is time / resource consuming .",system description,Supervised LSTM for text categorization,0,71,25,18,0,system description : Supervised LSTM for text categorization,0.27734375,0.125,0.6666666666666666
text-classification,1,"To put it into perspective , using a GPU , one epoch of wv - LSTM training takes nearly 20 times longer than that of one - hot CNN training even though it achieves poorer accuracy ( the first two rows of ) .",system description,Supervised LSTM for text categorization,0,72,26,19,0,system description : Supervised LSTM for text categorization,0.28125,0.13,0.7037037037037037
text-classification,1,"This is due to the sequential nature of LSTM , i.e. , computation at time t requires the output of time t ? 1 , whereas modern computation depends on parallelization for speedup .",system description,Supervised LSTM for text categorization,0,73,27,20,0,system description : Supervised LSTM for text categorization,0.28515625,0.135,0.7407407407407407
text-classification,1,"Documents in a mini-batch can be processed in parallel , but the variability of document lengths reduces the degree of parallelization 1 .",system description,Supervised LSTM for text categorization,0,74,28,21,0,system description : Supervised LSTM for text categorization,0.2890625,0.14,0.7777777777777778
text-classification,1,It was shown in DL15 that training becomes stable and accuracy improves drastically when LSTM and the word embedding layer are jointly pre-trained with either the language model learning objective ( predicting the next word ) or autoencoder objective ( memorizing the document ) .,system description,Supervised LSTM for text categorization,0,75,29,22,0,system description : Supervised LSTM for text categorization,0.29296875,0.145,0.8148148148148148
text-classification,1,Supervised LSTM for text categorization,system description,Supervised LSTM for text categorization,0,76,30,23,0,system description : Supervised LSTM for text categorization,0.296875,0.15,0.8518518518518519
text-classification,1,"Within the framework of ' region embedding + pooling ' for text categorization , we seek effective and efficient use of LSTM as an alternative region embedding method .",system description,Supervised LSTM for text categorization,0,77,31,24,0,system description : Supervised LSTM for text categorization,0.30078125,0.155,0.8888888888888888
text-classification,1,"This section focuses on an end - to - end supervised setting so that there is no additional data ( e.g. , unlabeled data ) or additional algorithm ( e.g. , for learning a word embedding ) .",system description,Supervised LSTM for text categorization,0,78,32,25,0,system description : Supervised LSTM for text categorization,0.3046875,0.16,0.9259259259259259
text-classification,1,Our general strategy is to simplify the model as much as possible .,system description,Supervised LSTM for text categorization,0,79,33,26,0,system description : Supervised LSTM for text categorization,0.30859375,0.165,0.9629629629629629
text-classification,1,"We start with elimination of the word embedding layer so that one - hot vectors are directly fed to LSTM , which we call one - hot LSTM in short .",system description,Supervised LSTM for text categorization,0,80,34,27,0,system description : Supervised LSTM for text categorization,0.3125,0.17,1.0
text-classification,1,Elimination of the word embedding layer,system description,Elimination of the word embedding layer,0,81,35,1,0,system description : Elimination of the word embedding layer,0.31640625,0.175,0.1
text-classification,1,Facts : A word embedding is a linear operation that can be written as Vx t with x t being a one - hot vector and columns of V being word vectors .,system description,Elimination of the word embedding layer,0,82,36,2,0,system description : Elimination of the word embedding layer,0.3203125,0.18,0.2
text-classification,1,"Therefore , by replacing the LSTM weights W ( ) with W ( ) V and removing the word embedding layer , a word - vector LSTM can be turned into a one - hot LSTM without changing the model behavior .",system description,Elimination of the word embedding layer,0,83,37,3,0,system description : Elimination of the word embedding layer,0.32421875,0.185,0.3
text-classification,1,"Thus , word - vector LSTM is not more expressive than one - hot LSTM ; rather , a merit , if any , of training with a word embedding layer would be through imposing restrictions ( e.g. , a low - rank V makes a less expressive model ) to achieve good prior / regularization effects .",system description,Elimination of the word embedding layer,0,84,38,4,0,system description : Elimination of the word embedding layer,0.328125,0.19,0.4
text-classification,1,"In the end - to - end supervised setting , a word embedding matrix V would need to be initialized randomly and trained as part of the model .",system description,Elimination of the word embedding layer,0,85,39,5,0,system description : Elimination of the word embedding layer,0.33203125,0.195,0.5
text-classification,1,"In the preliminary experiments under our framework , we were unable to improve accuracy over one - hot LSTM by inclusion of such a randomly initialized word embedding layer ; i.e. , random vectors failed to provide good prior effects .",system description,Elimination of the word embedding layer,0,86,40,6,0,system description : Elimination of the word embedding layer,0.3359375,0.2,0.6
text-classification,1,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .",system description,Elimination of the word embedding layer,0,87,41,7,0,system description : Elimination of the word embedding layer,0.33984375,0.205,0.7
text-classification,1,"If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",system description,Elimination of the word embedding layer,0,88,42,8,0,system description : Elimination of the word embedding layer,0.34375,0.21,0.8
text-classification,1,"We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .",system description,Elimination of the word embedding layer,0,89,43,9,0,system description : Elimination of the word embedding layer,0.34765625,0.215,0.9
text-classification,1,"Altogether , elimination of the word embedding layer was found to be useful ; thus , we base our work on one - hot LSTM .",system description,Elimination of the word embedding layer,0,90,44,10,0,system description : Elimination of the word embedding layer,0.3515625,0.22,1.0
text-classification,1,More simplifications,system description,More simplifications,0,91,45,1,0,system description : More simplifications,0.35546875,0.225,0.030303030303030304
text-classification,1,We introduce four more useful modifications to wv - LSTM that lead to higher accuracy or faster training .,system description,More simplifications,0,92,46,2,0,system description : More simplifications,0.359375,0.23,0.06060606060606061
text-classification,1,Pooling : simplifying sub - problems,system description,More simplifications,0,93,47,3,0,system description : More simplifications,0.36328125,0.235,0.09090909090909091
text-classification,1,Our framework of ' region embedding + pooling ' has a simplification effect as follows .,system description,More simplifications,0,94,48,4,0,system description : More simplifications,0.3671875,0.24,0.12121212121212122
text-classification,1,"In wv - LSTM , the sub-problem that LSTM needs to solve is to represent the entire document by one vector ( document embedding ) .",system description,More simplifications,0,95,49,5,0,system description : More simplifications,0.37109375,0.245,0.15151515151515152
text-classification,1,We make this easy by changing it to detecting regions of text ( of arbitrary sizes ) thatare relevant to the task and representing them by vectors ( region embedding ) .,system description,More simplifications,0,96,50,6,0,system description : More simplifications,0.375,0.25,0.18181818181818182
text-classification,1,"As illustrated in , we let the LSTM layer emit vectors ht at each time step , and let pooling aggregate them into a document vector .",system description,More simplifications,0,97,51,7,0,system description : More simplifications,0.37890625,0.255,0.21212121212121213
text-classification,1,"With wv - LSTM , LSTM has to remember relevant information until it gets to the end of the document even if relevant information was observed 10 K words away .",system description,More simplifications,0,98,52,8,0,system description : More simplifications,0.3828125,0.26,0.24242424242424243
text-classification,1,The task of our LSTM is easier as it is allowed to forget old things via the forget gate and can focus on representing the concepts conveyed by smaller segments such as phrases or sentences .,system description,More simplifications,0,99,53,9,0,system description : More simplifications,0.38671875,0.265,0.2727272727272727
text-classification,1,related architecture appears in the Deep Learning Tutorials 2 though it uses a word embedding .,system description,More simplifications,0,100,54,10,0,system description : More simplifications,0.390625,0.27,0.30303030303030304
text-classification,1,"Another related work is , which combined pooling with non -LSTM recurrent networks and a word embedding .",system description,More simplifications,0,101,55,11,0,system description : More simplifications,0.39453125,0.275,0.3333333333333333
text-classification,1,Chopping for speeding up training,system description,More simplifications,0,102,56,12,0,system description : More simplifications,0.3984375,0.28,0.36363636363636365
text-classification,1,"In addition to simplifying the sub-problem , pooling has the merit of enabling faster training via chopping .",system description,More simplifications,0,103,57,13,0,system description : More simplifications,0.40234375,0.285,0.3939393939393939
text-classification,1,"Since we set the goal of LSTM to embedding text regions instead of documents , it is no longer crucial to go through the document from the beginning to the end sequentially .",system description,More simplifications,0,104,58,14,0,system description : More simplifications,0.40625,0.29,0.42424242424242425
text-classification,1,"At the time of training , we can chop each document into segments of a fixed length that is sufficiently long ( e.g. , 50 or 100 ) and process all the segments in a mini batch in parallel as if these segments were individual documents .",system description,More simplifications,0,105,59,15,0,system description : More simplifications,0.41015625,0.295,0.45454545454545453
text-classification,1,Note that this is done only in the LSTM layer and pooling is done over the entire document . ),system description,More simplifications,0,106,60,16,0,system description : More simplifications,0.4140625,0.3,0.48484848484848486
text-classification,1,We perform testing without chopping .,system description,More simplifications,0,107,61,17,0,system description : More simplifications,0.41796875,0.305,0.5151515151515151
text-classification,1,"That is , we train LSTM with approximations of sequences for speedup and test with real sequences for better accuracy .",system description,More simplifications,0,108,62,18,0,system description : More simplifications,0.421875,0.31,0.5454545454545454
text-classification,1,"There is a risk of chopping important phrases ( e.g. , "" do n't | like it "" ) , and this can be easily avoided by having segments slightly overlap .",system description,More simplifications,0,109,63,19,0,system description : More simplifications,0.42578125,0.315,0.5757575757575758
text-classification,1,"However , we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping .",system description,More simplifications,0,110,64,20,0,system description : More simplifications,0.4296875,0.32,0.6060606060606061
text-classification,1,Removing the input / output gates,system description,More simplifications,0,111,65,21,0,system description : More simplifications,0.43359375,0.325,0.6363636363636364
text-classification,1,"We found that when LSTM is followed by pooling , the presence of input and output gates typically does not improve accuracy , while removing them nearly halves the time and memory required for training and testing .",system description,More simplifications,0,112,66,22,0,system description : More simplifications,0.4375,0.33,0.6666666666666666
text-classification,1,"It is intuitive , in particular , that pooling can make the output gate unnecessary ; the role of the output gate is to prevent undesirable information from entering the output ht , and such irrelevant information can be filtered out by max - pooling .",system description,More simplifications,0,113,67,23,0,system description : More simplifications,0.44140625,0.335,0.696969696969697
text-classification,1,"Without the input and output gates , the LSTM formulation can be simplified to :",system description,More simplifications,0,114,68,24,0,system description : More simplifications,0.4453125,0.34,0.7272727272727273
text-classification,1,2 ),system description,More simplifications,0,115,69,25,0,system description : More simplifications,0.44921875,0.345,0.7575757575757576
text-classification,1,This is equivalent to fixing it and o t to all ones .,system description,More simplifications,0,116,70,26,0,system description : More simplifications,0.453125,0.35,0.7878787878787878
text-classification,1,"It is in spirit similar to Gated Recurrent Units but simpler , having fewer gates .",system description,More simplifications,0,117,71,27,0,system description : More simplifications,0.45703125,0.355,0.8181818181818182
text-classification,1,Bidirectional LSTM for better accuracy,system description,More simplifications,0,118,72,28,0,system description : More simplifications,0.4609375,0.36,0.8484848484848485
text-classification,1,The changes from wv - LSTM above substantially reduce the time and 2 http://deeplearning.net/tutorial/lstm.html,system description,More simplifications,0,119,73,29,0,system description : More simplifications,0.46484375,0.365,0.8787878787878788
text-classification,1,One - hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement .,system description,More simplifications,0,120,74,30,0,system description : More simplifications,0.46875,0.37,0.9090909090909091
text-classification,1,"As shown in , we concatenate the output of a forward LSTM ( left to right ) and a backward LSTM ( right to left ) , which is referred to as bidirectional LSTM in the literature .",system description,More simplifications,0,121,75,31,0,system description : More simplifications,0.47265625,0.375,0.9393939393939394
text-classification,1,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .",system description,More simplifications,0,122,76,32,0,system description : More simplifications,0.4765625,0.38,0.9696969696969697
text-classification,1,"shows how much accuracy and / or training speed can be improved by elimination of the word embedding layer , pooling , chopping , removing the input / output gates , and adding the backward LSTM .",system description,More simplifications,0,123,77,33,0,system description : More simplifications,0.48046875,0.385,1.0
text-classification,1,Experiments ( supervised ),system description,Experiments (supervised),1,124,78,1,0,system description : Experiments (supervised),0.484375,0.39,0.016666666666666666
text-classification,1,"We used four datasets : IMDB , Elec , RCV1 ( second - level topics ) , and 20 - newsgroup ( 20 NG ) 3 , to facilitate direct comparison with JZ15 and DL15 .",system description,Experiments (supervised),0,125,79,2,0,system description : Experiments (supervised),0.48828125,0.395,0.03333333333333333
text-classification,1,The first three were used in JZ15 .,system description,Experiments (supervised),0,126,80,3,0,system description : Experiments (supervised),0.4921875,0.4,0.05
text-classification,1,IMDB and 20 NG were used in DL15 .,system description,Experiments (supervised),0,127,81,4,0,system description : Experiments (supervised),0.49609375,0.405,0.06666666666666667
text-classification,1,The datasets are summarized in .,system description,Experiments (supervised),0,128,82,5,0,system description : Experiments (supervised),0.5,0.41,0.08333333333333333
text-classification,1,The data was converted to lower - case letters .,system description,Experiments (supervised),0,129,83,6,0,system description : Experiments (supervised),0.50390625,0.415,0.1
text-classification,1,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .",system description,Experiments (supervised),0,130,84,7,0,system description : Experiments (supervised),0.5078125,0.42,0.11666666666666667
text-classification,1,"Data. "" avg "" / "" max "" : the average / maximum length of documents ( #words ) of the training / test data .",system description,Experiments (supervised),0,131,85,8,0,system description : Experiments (supervised),0.51171875,0.425,0.13333333333333333
text-classification,1,"IMDB and Elec are for sentiment classification ( positive vs. negative ) of movie reviews and Amazon electronics product reviews , respectively .",system description,Experiments (supervised),0,132,86,9,0,system description : Experiments (supervised),0.515625,0.43,0.15
text-classification,1,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .",system description,Experiments (supervised),0,133,87,10,0,system description : Experiments (supervised),0.51953125,0.435,0.16666666666666666
text-classification,1,zero mean and standard deviation 0.01 .,system description,Experiments (supervised),0,134,88,11,0,system description : Experiments (supervised),0.5234375,0.44,0.18333333333333332
text-classification,1,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,system description,Experiments (supervised),0,135,89,12,0,system description : Experiments (supervised),0.52734375,0.445,0.2
text-classification,1,"Hyper parameters such as learning rates were chosen based on the performance on the development data , which was a held - out portion of the training data , and training was redone using all the training data with the chosen parameters .",system description,Experiments (supervised),0,136,90,13,0,system description : Experiments (supervised),0.53125,0.45,0.21666666666666667
text-classification,1,"We used the same pooling method as used in JZ15 , which parameterizes the number of pooling regions so that pooling is done fork non-overlapping regions of equal size , and the resulting k vectors are concatenated to make one vector per document .",system description,Experiments (supervised),0,137,91,14,0,system description : Experiments (supervised),0.53515625,0.455,0.23333333333333334
text-classification,1,"The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",system description,Experiments (supervised),0,138,92,15,0,system description : Experiments (supervised),0.5390625,0.46,0.25
text-classification,1,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",system description,Experiments (supervised),1,139,93,16,0,system description : Experiments (supervised),0.54296875,0.465,0.26666666666666666
text-classification,1,Now we review the non -LSTM baseline methods .,system description,Experiments (supervised),0,140,94,17,0,system description : Experiments (supervised),0.546875,0.47,0.2833333333333333
text-classification,1,The last row of shows the best one - hot CNN results within the constraints above .,system description,Experiments (supervised),0,141,95,18,0,system description : Experiments (supervised),0.55078125,0.475,0.3
text-classification,1,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .",system description,Experiments (supervised),0,142,96,19,0,system description : Experiments (supervised),0.5546875,0.48,0.31666666666666665
text-classification,1,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",system description,Experiments (supervised),1,143,97,20,0,system description : Experiments (supervised),0.55859375,0.485,0.3333333333333333
text-classification,1,"However , on RCV1 , it underperforms both .",system description,Experiments (supervised),0,144,98,21,0,system description : Experiments (supervised),0.5625,0.49,0.35
text-classification,1,We conjecture that this is because strict word order is not very useful on RCV1 .,system description,Experiments (supervised),0,145,99,22,0,system description : Experiments (supervised),0.56640625,0.495,0.36666666666666664
text-classification,1,This point can also be observed in the SVM and CNN performances .,system description,Experiments (supervised),0,146,100,23,0,system description : Experiments (supervised),0.5703125,0.5,0.38333333333333336
text-classification,1,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",system description,Experiments (supervised),0,147,101,24,0,system description : Experiments (supervised),0.57421875,0.505,0.4
text-classification,1,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",system description,Experiments (supervised),0,148,102,25,0,system description : Experiments (supervised),0.578125,0.51,0.4166666666666667
text-classification,1,This is presumably because the former can more easily cover variability of expressions indicative of topics .,system description,Experiments (supervised),0,149,103,26,0,system description : Experiments (supervised),0.58203125,0.515,0.43333333333333335
text-classification,1,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .",system description,Experiments (supervised),0,150,104,27,0,system description : Experiments (supervised),0.5859375,0.52,0.45
text-classification,1,More on one - hot CNN vs. one - hot LSTM LSTM can embed regions of variable ( and possibly large ) sizes whereas CNN requires the region size to be fixed .,system description,Experiments (supervised),0,151,105,28,0,system description : Experiments (supervised),0.58984375,0.525,0.4666666666666667
text-classification,1,We attribute to this fact the small improvements of oh - 2 LSTMp over oh - CNN in .,system description,Experiments (supervised),0,152,106,29,0,system description : Experiments (supervised),0.59375,0.53,0.48333333333333334
text-classification,1,"However , this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes .",system description,Experiments (supervised),0,153,107,30,0,system description : Experiments (supervised),0.59765625,0.535,0.5
text-classification,1,We show in the table above that one - hot CNNs with two layers ( of 1000 feature maps each ) with two different region sizes 4 rival oh - 2 LST Mp .,system description,Experiments (supervised),0,154,108,31,0,system description : Experiments (supervised),0.6015625,0.54,0.5166666666666667
text-classification,1,"Although these models are larger than those in , training / testing is still faster than the LSTM models due to simplicity of the region embeddings .",system description,Experiments (supervised),0,155,109,32,0,system description : Experiments (supervised),0.60546875,0.545,0.5333333333333333
text-classification,1,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .",system description,Experiments (supervised),0,156,110,33,0,system description : Experiments (supervised),0.609375,0.55,0.55
text-classification,1,This maybe because the amount of training data is not sufficient enough to learn the relevance of longer word sequences .,system description,Experiments (supervised),0,157,111,34,0,system description : Experiments (supervised),0.61328125,0.555,0.5666666666666667
text-classification,1,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",system description,Experiments (supervised),0,158,112,35,0,system description : Experiments (supervised),0.6171875,0.56,0.5833333333333334
text-classification,1,Comparison with the previous best results on 20 NG,system description,Experiments (supervised),0,159,113,36,0,system description : Experiments (supervised),0.62109375,0.565,0.6
text-classification,1,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",system description,Experiments (supervised),0,160,114,37,0,system description : Experiments (supervised),0.625,0.57,0.6166666666666667
text-classification,1,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",system description,Experiments (supervised),0,161,115,38,0,system description : Experiments (supervised),0.62890625,0.575,0.6333333333333333
text-classification,1,"The previous best results on the other datasets use unlabeled data , and we will review them with our semi-supervised results .",system description,Experiments (supervised),0,162,116,39,0,system description : Experiments (supervised),0.6328125,0.58,0.65
text-classification,1,Semi-supervised LSTM,system description,Experiments (supervised),0,163,117,40,0,system description : Experiments (supervised),0.63671875,0.585,0.6666666666666666
text-classification,1,"To exploit unlabeled data as an additional resource , we use a non-linear extension of two - view feature learning , whose linear version appeared in our earlier work .",system description,Experiments (supervised),0,164,118,41,0,system description : Experiments (supervised),0.640625,0.59,0.6833333333333333
text-classification,1,This was used in JZ15 b to learn from unlabeled data a region embedding embodied by a convolution layer .,system description,Experiments (supervised),0,165,119,42,0,system description : Experiments (supervised),0.64453125,0.595,0.7
text-classification,1,In this work we use it to learn a region embedding embodied by a one - hot LSTM .,system description,Experiments (supervised),0,166,120,43,0,system description : Experiments (supervised),0.6484375,0.6,0.7166666666666667
text-classification,1,Let us start with a brief review of non-linear two - view feature learning .,system description,Experiments (supervised),0,167,121,44,0,system description : Experiments (supervised),0.65234375,0.605,0.7333333333333333
text-classification,1,Two - view embedding ( tv-embedding ) [ JZ15 b ],system description,Experiments (supervised),0,168,122,45,0,system description : Experiments (supervised),0.65625,0.61,0.75
text-classification,1,rough sketch is as follows .,system description,Experiments (supervised),0,169,123,46,0,system description : Experiments (supervised),0.66015625,0.615,0.7666666666666667
text-classification,1,Consider two views of the input .,system description,Experiments (supervised),0,170,124,47,0,system description : Experiments (supervised),0.6640625,0.62,0.7833333333333333
text-classification,1,An embedding is called a tv-embedding if the embedded view is as good as the original view for the purpose of predicting the other view .,system description,Experiments (supervised),0,171,125,48,0,system description : Experiments (supervised),0.66796875,0.625,0.8
text-classification,1,"If the two views and the labels ( classification targets ) are related to one another only through some hidden states , then the tv-embedded view is as good as the original view for the purpose of classification .",system description,Experiments (supervised),0,172,126,49,0,system description : Experiments (supervised),0.671875,0.63,0.8166666666666667
text-classification,1,Such an embedding is useful provided that its dimensionality is much lower than the original view .,system description,Experiments (supervised),0,173,127,50,0,system description : Experiments (supervised),0.67578125,0.635,0.8333333333333334
text-classification,1,JZ15 b applied this idea by regarding text regions embedded by the convolution layer as one view and their surrounding context as the other view and training a tv-embedding ( embodied by a convolution layer ) on unlabeled data .,system description,Experiments (supervised),0,174,128,51,0,system description : Experiments (supervised),0.6796875,0.64,0.85
text-classification,1,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .",system description,Experiments (supervised),0,175,129,52,0,system description : Experiments (supervised),0.68359375,0.645,0.8666666666666667
text-classification,1,"we consider the following two views : the words we have already seen in the document ( view - 1 ) , and the next few words ( view - 2 ) .",system description,Experiments (supervised),0,176,130,53,0,system description : Experiments (supervised),0.6875,0.65,0.8833333333333333
text-classification,1,The task of tv-embedding learning is to predict view - 2 based on view - 1 .,system description,Experiments (supervised),0,177,131,54,0,system description : Experiments (supervised),0.69140625,0.655,0.9
text-classification,1,"We train one - hot LSTMs in both directions , as in , on unlabeled data .",system description,Experiments (supervised),0,178,132,55,0,system description : Experiments (supervised),0.6953125,0.66,0.9166666666666666
text-classification,1,"For this purpose , we use the input and output gates as well as the forget gate as we found them to be useful .",system description,Experiments (supervised),0,179,133,56,0,system description : Experiments (supervised),0.69921875,0.665,0.9333333333333333
text-classification,1,Learning LSTM tv-embeddings,system description,Experiments (supervised),0,180,134,57,0,system description : Experiments (supervised),0.703125,0.67,0.95
text-classification,1,The theory of tv-embedding says that the region embeddings obtained in this way are useful for the task of interest if the two views are related to each other through the concepts relevant to the task .,system description,Experiments (supervised),0,181,135,58,0,system description : Experiments (supervised),0.70703125,0.675,0.9666666666666667
text-classification,1,"To reduce undesirable relations between the views such as syntactic relations , JZ15 b performed vocabulary control to remove function words from ( and only from ) the vocabulary of the target view , which we found useful also for LSTM .",system description,Experiments (supervised),0,182,136,59,0,system description : Experiments (supervised),0.7109375,0.68,0.9833333333333333
text-classification,1,We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :,system description,Experiments (supervised),0,183,137,60,0,system description : Experiments (supervised),0.71484375,0.685,1.0
text-classification,1,) .,system description,Comparison with the previous best results on 20NG,0,184,138,1,0,system description : Comparison with the previous best results on 20NG,0.71875,0.69,0.015873015873015872
text-classification,1,"j t is the output of a tv-embedding ( an LSTM trained with unlabeled data ) indexed by j at time step t , and S is a set of tv-embeddings which contains the two LSTMs going forward and backward as in .",system description,Comparison with the previous best results on 20NG,0,185,139,2,0,system description : Comparison with the previous best results on 20NG,0.72265625,0.695,0.031746031746031744
text-classification,1,"Although it is possible to fine - tune the tv-embeddings with labeled data , for simplicity and faster training , we fixed them in our experiments .",system description,Comparison with the previous best results on 20NG,0,186,140,3,0,system description : Comparison with the previous best results on 20NG,0.7265625,0.7,0.047619047619047616
text-classification,1,Combining LSTM tv-embeddings and CNN tv-embeddings,system description,Comparison with the previous best results on 20NG,0,187,141,4,0,system description : Comparison with the previous best results on 20NG,0.73046875,0.705,0.06349206349206349
text-classification,1,"It is easy to see that the set S above can be expanded with any tv-embeddings , not only those in the form of LSTM ( LSTM tv-embeddings ) but also with the tv-embeddings in the form of convolution layers ( CNN tv-embeddings ) such as those obtained in JZ15 b .",system description,Comparison with the previous best results on 20NG,0,188,142,5,0,system description : Comparison with the previous best results on 20NG,0.734375,0.71,0.07936507936507936
text-classification,1,"Similarly , it is possible to use LSTM tv-embeddings to produce additional input to CNN .",system description,Comparison with the previous best results on 20NG,0,189,143,6,0,system description : Comparison with the previous best results on 20NG,0.73828125,0.715,0.09523809523809523
text-classification,1,"While both LSTM tv-embeddings and CNN tv-embeddings are region embeddings , their formulations are very different from each other ; therefore , we expect that they complement each other and bring further performance improvements when combined .",system description,Comparison with the previous best results on 20NG,0,190,144,7,0,system description : Comparison with the previous best results on 20NG,0.7421875,0.72,0.1111111111111111
text-classification,1,We will empirically confirm these conjectures in the experiments below .,system description,Comparison with the previous best results on 20NG,0,191,145,8,0,system description : Comparison with the previous best results on 20NG,0.74609375,0.725,0.12698412698412698
text-classification,1,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,system description,Comparison with the previous best results on 20NG,0,192,146,9,0,system description : Comparison with the previous best results on 20NG,0.75,0.73,0.14285714285714285
text-classification,1,6.08 9.24 5 oh -CNN 1200 - dim CNN tv-embed .,system description,Comparison with the previous best results on 20NG,0,193,147,10,0,system description : Comparison with the previous best results on 20NG,0.75390625,0.735,0.15873015873015872
text-classification,1,"6.57 7.97 our framework , which uses unlabeled data to produce additional input to LSTM instead of pre-training .",system description,Comparison with the previous best results on 20NG,0,194,148,11,0,system description : Comparison with the previous best results on 20NG,0.7578125,0.74,0.1746031746031746
text-classification,1,Semi-supervised experiments,system description,Comparison with the previous best results on 20NG,1,195,149,12,0,system description : Comparison with the previous best results on 20NG,0.76171875,0.745,0.19047619047619047
text-classification,1,"We used IMDB , Elec , and RCV1 for our semi-supervised experiments ; 20 NG was excluded due to the absence of standard unlabeled data .",system description,Comparison with the previous best results on 20NG,0,196,150,13,0,system description : Comparison with the previous best results on 20NG,0.765625,0.75,0.20634920634920634
text-classification,1,summarizes the unlabeled data .,system description,Comparison with the previous best results on 20NG,0,197,151,14,0,system description : Comparison with the previous best results on 20NG,0.76953125,0.755,0.2222222222222222
text-classification,1,"To experiment with LSTM tv-embeddings , we trained two LSTMs ( forward and backward ) with 100 units each on unlabeled data .",system description,Comparison with the previous best results on 20NG,0,198,152,15,0,system description : Comparison with the previous best results on 20NG,0.7734375,0.76,0.23809523809523808
text-classification,1,The training objective was to predict the next k words where k was set to 20 for RCV1 and 5 for others .,system description,Comparison with the previous best results on 20NG,0,199,153,16,0,system description : Comparison with the previous best results on 20NG,0.77734375,0.765,0.25396825396825395
text-classification,1,"Similar to JZ15 b , we minimized weighted square",system description,Comparison with the previous best results on 20NG,0,200,154,17,0,system description : Comparison with the previous best results on 20NG,0.78125,0.77,0.2698412698412698
text-classification,1,"2 where i goes through the time steps , z represents the next k words by a bow vector , and p is the model output ; ? i , j were set to achieve negative sampling effect for speed - up ; vocabulary control was performed for reducing undesirable relations between views , which sets the vocabulary of the target ( i.e. , the k words ) to the 30 K most frequent words excluding function words ( or stop words on RCV1 ) .",system description,Comparison with the previous best results on 20NG,0,201,155,18,0,system description : Comparison with the previous best results on 20NG,0.78515625,0.775,0.2857142857142857
text-classification,1,"2 where i goes through the time steps , z represents the next k words by a bow vector , and p is the model output ; ? i , j were set to achieve negative sampling effect for speed - up ; vocabulary control was performed for reducing undesirable relations between views , which sets the vocabulary of the target ( i.e. , the k words ) to the 30 K most frequent words excluding function words ( or stop words on RCV1 ) .",system description,Comparison with the previous best results on 20NG,0,202,156,19,0,system description : Comparison with the previous best results on 20NG,0.7890625,0.78,0.30158730158730157
text-classification,1,Other details followed the supervised experiments .,system description,Comparison with the previous best results on 20NG,0,203,157,20,0,system description : Comparison with the previous best results on 20NG,0.79296875,0.785,0.31746031746031744
text-classification,1,"Our semi-supervised one - hot bidirectional LSTM with pooling ( oh - 2 LSTM p ) in row # 4 of used the two LSTM tv-embeddings trained on unlabeled data as described above , to produce additional input to one - hot LSTMs in two directions ( 500 units each ) .",system description,Comparison with the previous best results on 20NG,0,204,158,21,0,system description : Comparison with the previous best results on 20NG,0.796875,0.79,0.3333333333333333
text-classification,1,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .",system description,Comparison with the previous best results on 20NG,0,205,159,22,0,system description : Comparison with the previous best results on 20NG,0.80078125,0.795,0.3492063492063492
text-classification,1,We review the semi-supervised performance of wv - LSTMs ) .,system description,Comparison with the previous best results on 20NG,0,206,160,23,0,system description : Comparison with the previous best results on 20NG,0.8046875,0.8,0.36507936507936506
text-classification,1,"In DL15 the model consisted of a word embedding layer of 512 dimensions , an LSTM layer with 1024 units , and 30 hidden units on top of the LSTM layer ; the word embedding layer and the LSTM were pre-trained with unlabeled data and were fine - tuned with labeled data ; pre-training used either the language model objective or autoencoder objective .",system description,Comparison with the previous best results on 20NG,0,207,161,24,0,system description : Comparison with the previous best results on 20NG,0.80859375,0.805,0.38095238095238093
text-classification,1,"The error rate on IMDB is from DL15 , and those on Elec and RCV1 are our best effort to perform pre-training with the language model objective .",system description,Comparison with the previous best results on 20NG,0,208,162,25,0,system description : Comparison with the previous best results on 20NG,0.8125,0.81,0.3968253968253968
text-classification,1,"We used the same configuration on Elec as DL15 ; however , on RCV1 , which has 55 classes , 30 hidden units turned out to be too few and we changed it to 1000 .",system description,Comparison with the previous best results on 20NG,0,209,163,26,0,system description : Comparison with the previous best results on 20NG,0.81640625,0.815,0.4126984126984127
text-classification,1,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .",system description,Comparison with the previous best results on 20NG,0,210,164,27,0,system description : Comparison with the previous best results on 20NG,0.8203125,0.82,0.42857142857142855
text-classification,1,"Previous studies on LSTM for text often convert words into pre-trained word vectors , and word2vec is a popular choice for this purpose .",system description,Comparison with the previous best results on 20NG,0,211,165,28,0,system description : Comparison with the previous best results on 20NG,0.82421875,0.825,0.4444444444444444
text-classification,1,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .",system description,Comparison with the previous best results on 20NG,1,212,166,29,0,system description : Comparison with the previous best results on 20NG,0.828125,0.83,0.4603174603174603
text-classification,1,The word vectors were optionally updated ( finetuned ) during training .,system description,Comparison with the previous best results on 20NG,0,213,167,30,0,system description : Comparison with the previous best results on 20NG,0.83203125,0.835,0.47619047619047616
text-classification,1,Two types of word vectors were tested .,system description,Comparison with the previous best results on 20NG,0,214,168,31,0,system description : Comparison with the previous best results on 20NG,0.8359375,0.84,0.49206349206349204
text-classification,1,The Google News word vectors were trained by word2vec on a huge ( 100 billion - word ) news corpus and are provided publicly .,system description,Comparison with the previous best results on 20NG,0,215,169,32,0,system description : Comparison with the previous best results on 20NG,0.83984375,0.845,0.5079365079365079
text-classification,1,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",system description,Comparison with the previous best results on 20NG,0,216,170,33,0,system description : Comparison with the previous best results on 20NG,0.84375,0.85,0.5238095238095238
text-classification,1,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .",system description,Comparison with the previous best results on 20NG,0,217,171,34,0,system description : Comparison with the previous best results on 20NG,0.84765625,0.855,0.5396825396825397
text-classification,1,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .",system description,Comparison with the previous best results on 20NG,0,218,172,35,0,system description : Comparison with the previous best results on 20NG,0.8515625,0.86,0.5555555555555556
text-classification,1,"We attribute the superiority of the models with tv-embeddings to the fact that they learn , from unlabeled data , embeddings of text regions , which can convey higher - level concepts than single words in isolation .",system description,Comparison with the previous best results on 20NG,0,219,173,36,0,system description : Comparison with the previous best results on 20NG,0.85546875,0.865,0.5714285714285714
text-classification,1,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .",system description,Comparison with the previous best results on 20NG,1,220,174,37,0,system description : Comparison with the previous best results on 20NG,0.859375,0.87,0.5873015873015873
text-classification,1,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,system description,Comparison with the previous best results on 20NG,0,221,175,38,0,system description : Comparison with the previous best results on 20NG,0.86328125,0.875,0.6031746031746031
text-classification,1,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",system description,Comparison with the previous best results on 20NG,0,222,176,39,0,system description : Comparison with the previous best results on 20NG,0.8671875,0.88,0.6190476190476191
text-classification,1,"As discussed earlier , we attribute the superiority of one - hot CNN on RCV1 to its unique way of representing parts of documents via bow input .",system description,Comparison with the previous best results on 20NG,0,223,177,40,0,system description : Comparison with the previous best results on 20NG,0.87109375,0.885,0.6349206349206349
text-classification,1,Experiments combining CNN tv-embeddings and LSTM tv-embeddings,system description,Comparison with the previous best results on 20NG,0,224,178,41,0,system description : Comparison with the previous best results on 20NG,0.875,0.89,0.6507936507936508
text-classification,1,In Section 3.3 we noted that LSTM tv-embeddings and CNN tv-embeddings can be naturally combined .,system description,Comparison with the previous best results on 20NG,0,225,179,42,0,system description : Comparison with the previous best results on 20NG,0.87890625,0.895,0.6666666666666666
text-classification,1,We experimented with this idea in the following two settings ..,system description,Comparison with the previous best results on 20NG,0,226,180,43,0,system description : Comparison with the previous best results on 20NG,0.8828125,0.9,0.6825396825396826
text-classification,1,Comparison with previous best results .,system description,Comparison with the previous best results on 20NG,0,227,181,44,0,system description : Comparison with the previous best results on 20NG,0.88671875,0.905,0.6984126984126984
text-classification,1,Error rates ( % ) .,system description,Comparison with the previous best results on 20NG,0,228,182,45,0,system description : Comparison with the previous best results on 20NG,0.890625,0.91,0.7142857142857143
text-classification,1,"U "" : Was unlabeled data used ? "" Co - tr. optimized "" : co-training using oh - CNN as a base learner with parameters ( e.g. , when to stop ) optimized on the test data ; it demonstrates the difficulty of exploiting unlabeled data on these tasks .",system description,Comparison with the previous best results on 20NG,0,229,183,46,0,system description : Comparison with the previous best results on 20NG,0.89453125,0.915,0.7301587301587301
text-classification,1,"U "" : Was unlabeled data used ? "" Co - tr. optimized "" : co-training using oh - CNN as a base learner with parameters ( e.g. , when to stop ) optimized on the test data ; it demonstrates the difficulty of exploiting unlabeled data on these tasks .",system description,Comparison with the previous best results on 20NG,0,230,184,47,0,system description : Comparison with the previous best results on 20NG,0.8984375,0.92,0.746031746031746
text-classification,1,"In one setting , oh - 2 LSTMp takes additional input from five embeddings : two LSTM tv-embeddings used in and three CNN tv-embeddings from JZ15 b obtained by three distinct combinations of training objectives and input representations , which are publicly provided .",system description,Comparison with the previous best results on 20NG,0,231,185,48,0,system description : Comparison with the previous best results on 20NG,0.90234375,0.925,0.7619047619047619
text-classification,1,"These CNN tv-embeddings were trained to be applied to text regions of size k at every location taking bow input , where k is 5 on IMDB / Elec and 20 on RCV1 .",system description,Comparison with the previous best results on 20NG,0,232,186,49,0,system description : Comparison with the previous best results on 20NG,0.90625,0.93,0.7777777777777778
text-classification,1,"We connect each of the CNN tv-embeddings to an LSTM by aligning the centers of the regions of the former with the LSTM time steps ; e.g. , the CNN tv-embedding result on the first five words is passed to the LSTM at the time step on the third word .",system description,Comparison with the previous best results on 20NG,0,233,187,50,0,system description : Comparison with the previous best results on 20NG,0.91015625,0.935,0.7936507936507936
text-classification,1,"In the second setting , we trained one - hot CNN with these five types of tv-embeddings by replacing ( 1 ) max ( 0 , Wx + b ) by max ( 0 , Wx + j W ( j ) x j + b ) where x j is the output of the j - th tv-embedding with the same alignment as above .",system description,Comparison with the previous best results on 20NG,0,234,188,51,0,system description : Comparison with the previous best results on 20NG,0.9140625,0.94,0.8095238095238095
text-classification,1,Rows 3 - 4 of show the results of these two types of models .,system description,Comparison with the previous best results on 20NG,0,235,189,52,0,system description : Comparison with the previous best results on 20NG,0.91796875,0.945,0.8253968253968254
text-classification,1,"For comparison , we also show the results of the LSTM with LSTM tv-embeddings only ( row# 1 ) and the CNN with CNN tv-embeddings only ( row # 2 ) .",system description,Comparison with the previous best results on 20NG,0,236,190,53,0,system description : Comparison with the previous best results on 20NG,0.921875,0.95,0.8412698412698413
text-classification,1,"To see the effects of combination , compare row# 3 with row# 1 , and compare row # 4 with row # 2 .",system description,Comparison with the previous best results on 20NG,0,237,191,54,0,system description : Comparison with the previous best results on 20NG,0.92578125,0.955,0.8571428571428571
text-classification,1,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .",system description,Comparison with the previous best results on 20NG,0,238,192,55,0,system description : Comparison with the previous best results on 20NG,0.9296875,0.96,0.873015873015873
text-classification,1,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .",system description,Comparison with the previous best results on 20NG,0,239,193,56,0,system description : Comparison with the previous best results on 20NG,0.93359375,0.965,0.8888888888888888
text-classification,1,Comparison with the previous best results,system description,Comparison with the previous best results on 20NG,0,240,194,57,0,system description : Comparison with the previous best results on 20NG,0.9375,0.97,0.9047619047619048
text-classification,1,The previous best results in the literature are shown in Table 7 .,system description,Comparison with the previous best results on 20NG,0,241,195,58,0,system description : Comparison with the previous best results on 20NG,0.94140625,0.975,0.9206349206349206
text-classification,1,"More results of previous semi-supervised models can be found in JZ15b , all of which clearly underperform the semi-supervised one - hot CNN of .",system description,Comparison with the previous best results on 20NG,0,242,196,59,0,system description : Comparison with the previous best results on 20NG,0.9453125,0.98,0.9365079365079365
text-classification,1,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .",system description,Comparison with the previous best results on 20NG,0,243,197,60,0,system description : Comparison with the previous best results on 20NG,0.94921875,0.985,0.9523809523809523
text-classification,1,"Many more of the previous results on IMDB can be found in , all of which are over 10 % except for 8.78 by bi-gram NBSVM .",system description,Comparison with the previous best results on 20NG,0,244,198,61,0,system description : Comparison with the previous best results on 20NG,0.953125,0.99,0.9682539682539683
text-classification,1,by paragraph vectors ) and 6.51 by JZ15 b were considered to be large improvements .,system description,Comparison with the previous best results on 20NG,0,245,199,62,0,system description : Comparison with the previous best results on 20NG,0.95703125,0.995,0.9841269841269841
text-classification,1,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .",system description,Comparison with the previous best results on 20NG,0,246,200,63,0,system description : Comparison with the previous best results on 20NG,0.9609375,1.0,1.0
text-classification,1,Conclusion,conclusion,conclusion,0,247,1,1,0,conclusion : conclusion,0.96484375,0.1,0.1
text-classification,1,"Within the general framework of ' region embedding + pooling ' for text categorization , we explored region embeddings via one - hot LSTM .",conclusion,conclusion,0,248,2,2,0,conclusion : conclusion,0.96875,0.2,0.2
text-classification,1,"The region embedding of onehot LSTM rivaled or outperformed that of the state - of - the art one - hot CNN , proving its effectiveness .",conclusion,conclusion,0,249,3,3,0,conclusion : conclusion,0.97265625,0.3,0.3
text-classification,1,We also found that the models with either one of these two types of region embedding strongly outperformed other methods including previous LSTM .,conclusion,conclusion,0,250,4,4,0,conclusion : conclusion,0.9765625,0.4,0.4
text-classification,1,"The best results were obtained by combining the two types of region embedding trained on unlabeled data , suggesting that their strengths are complementary .",conclusion,conclusion,0,251,5,5,0,conclusion : conclusion,0.98046875,0.5,0.5
text-classification,1,"As a result , we reported substantial improvements over the previous best results on benchmark datasets .",conclusion,conclusion,0,252,6,6,0,conclusion : conclusion,0.984375,0.6,0.6
text-classification,1,"At a high level , our results indicate the following .",conclusion,conclusion,0,253,7,7,0,conclusion : conclusion,0.98828125,0.7,0.7
text-classification,1,"First , on this task , embeddings of text regions , which can convey higher - level concepts , are more useful than embeddings of single words in isolation .",conclusion,conclusion,0,254,8,8,0,conclusion : conclusion,0.9921875,0.8,0.8
text-classification,1,"Second , useful region embeddings can be learned by working with one - hot vectors directly , either on labeled data or unlabeled data .",conclusion,conclusion,0,255,9,9,0,conclusion : conclusion,0.99609375,0.9,0.9
text-classification,1,"Finally , a promising future direction might be to seek , under this framework , new region embedding methods with complementary benefits .",conclusion,conclusion,0,256,10,10,0,conclusion : conclusion,1.0,1.0,1.0
text-classification,2,Bag of Tricks for Efficient Text Classification,title,title,1,2,1,1,0,title : title,0.021505376344086023,1.0,1.0
text-classification,2,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.03225806451612903,0.25,0.25
text-classification,2,This paper explores a simple and efficient baseline for text classification .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.043010752688172046,0.5,0.5
text-classification,2,"Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.053763440860215055,0.75,0.75
text-classification,2,"We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU , and classify half a million sentences among 312K classes in less than a minute .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.06451612903225806,1.0,1.0
text-classification,2,Introduction,introduction,introduction,0,7,1,1,0,introduction : introduction,0.07526881720430108,0.1,0.1
text-classification,2,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",introduction,introduction,0,8,2,2,0,introduction : introduction,0.08602150537634409,0.2,0.2
text-classification,2,"Recently , models based on neural networks have become increasingly popular .",introduction,introduction,0,9,3,3,0,introduction : introduction,0.0967741935483871,0.3,0.3
text-classification,2,"While these models achieve very good performance in practice , they tend to be relatively slow both at train and test time , limiting their use on very large datasets .",introduction,introduction,0,10,4,4,0,introduction : introduction,0.10752688172043011,0.4,0.4
text-classification,2,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",introduction,introduction,0,11,5,5,0,introduction : introduction,0.11827956989247312,0.5,0.5
text-classification,2,"Despite their simplicity , they often obtain stateof - the - art performances if the right features are used .",introduction,introduction,0,12,6,6,0,introduction : introduction,0.12903225806451613,0.6,0.6
text-classification,2,They also have the potential to scale to very large corpus .,introduction,introduction,0,13,7,7,0,introduction : introduction,0.13978494623655913,0.7,0.7
text-classification,2,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",introduction,introduction,0,14,8,8,0,introduction : introduction,0.15053763440860216,0.8,0.8
text-classification,2,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",introduction,introduction,0,15,9,9,0,introduction : introduction,0.16129032258064516,0.9,0.9
text-classification,2,"We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",introduction,introduction,0,16,10,10,0,introduction : introduction,0.17204301075268819,1.0,1.0
text-classification,2,Model architecture,model,Model architecture,0,17,1,1,0,model : Model architecture,0.1827956989247312,0.03225806451612903,0.07142857142857142
text-classification,2,"simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",model,Model architecture,0,18,2,2,0,model : Model architecture,0.1935483870967742,0.06451612903225806,0.14285714285714285
text-classification,2,"However , linear classifiers do not share parameters among features and classes .",model,Model architecture,0,19,3,3,0,model : Model architecture,0.20430107526881722,0.0967741935483871,0.21428571428571427
text-classification,2,This possibly limits their generalization in the context of large output space where some classes have very few examples .,model,Model architecture,0,20,4,4,0,model : Model architecture,0.21505376344086022,0.12903225806451613,0.2857142857142857
text-classification,2,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,model,Model architecture,0,21,5,5,0,model : Model architecture,0.22580645161290322,0.16129032258064516,0.35714285714285715
text-classification,2,shows a simple linear model with rank constraint .,model,Model architecture,1,22,6,6,0,model : Model architecture,0.23655913978494625,0.1935483870967742,0.42857142857142855
text-classification,2,The first weight matrix A is a look - up table over the words .,model,Model architecture,1,23,7,7,0,model : Model architecture,0.24731182795698925,0.22580645161290322,0.5
text-classification,2,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",model,Model architecture,1,24,8,8,0,model : Model architecture,0.25806451612903225,0.25806451612903225,0.5714285714285714
text-classification,2,The text representa - tion is an hidden variable which can be potentially be reused .,model,Model architecture,0,25,9,9,0,model : Model architecture,0.26881720430107525,0.2903225806451613,0.6428571428571429
text-classification,2,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",model,Model architecture,0,26,10,10,0,model : Model architecture,0.27956989247311825,0.3225806451612903,0.7142857142857143
text-classification,2,We use the softmax function f to compute the probability distribution over the predefined classes .,model,Model architecture,1,27,11,11,0,model : Model architecture,0.2903225806451613,0.3548387096774194,0.7857142857142857
text-classification,2,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",model,Model architecture,0,28,12,12,0,model : Model architecture,0.3010752688172043,0.3870967741935484,0.8571428571428571
text-classification,2,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",model,Model architecture,0,29,13,13,0,model : Model architecture,0.3118279569892473,0.41935483870967744,0.9285714285714286
text-classification,2,This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .,model,Model architecture,0,30,14,14,0,model : Model architecture,0.3225806451612903,0.45161290322580644,1.0
text-classification,2,Hierarchical softmax,model,Hierarchical softmax,0,31,15,1,0,model : Hierarchical softmax,0.3333333333333333,0.4838709677419355,0.08333333333333333
text-classification,2,"When the number of classes is large , computing the linear classifier is computationally expensive .",model,Hierarchical softmax,0,32,16,2,0,model : Hierarchical softmax,0.34408602150537637,0.5161290322580645,0.16666666666666666
text-classification,2,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",model,Hierarchical softmax,0,33,17,3,0,model : Hierarchical softmax,0.3548387096774194,0.5483870967741935,0.25
text-classification,2,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",model,Hierarchical softmax,1,34,18,4,0,model : Hierarchical softmax,0.3655913978494624,0.5806451612903226,0.3333333333333333
text-classification,2,"During training , the computational complexity drops to O ( h log 2 ( k ) ) .",model,Hierarchical softmax,0,35,19,5,0,model : Hierarchical softmax,0.3763440860215054,0.6129032258064516,0.4166666666666667
text-classification,2,The hierarchical softmax is also advantageous at test time when searching for the most likely class .,model,Hierarchical softmax,0,36,20,6,0,model : Hierarchical softmax,0.3870967741935484,0.6451612903225806,0.5
text-classification,2,Each node is associated with a probability that is the probability of the path from the root to that node .,model,Hierarchical softmax,0,37,21,7,0,model : Hierarchical softmax,0.3978494623655914,0.6774193548387096,0.5833333333333334
text-classification,2,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",model,Hierarchical softmax,0,38,22,8,0,model : Hierarchical softmax,0.40860215053763443,0.7096774193548387,0.6666666666666666
text-classification,2,This means that the probability of anode is always lower than the one of its parent .,model,Hierarchical softmax,0,39,23,9,0,model : Hierarchical softmax,0.41935483870967744,0.7419354838709677,0.75
text-classification,2,Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability .,model,Hierarchical softmax,0,40,24,10,0,model : Hierarchical softmax,0.43010752688172044,0.7741935483870968,0.8333333333333334
text-classification,2,"In practice , we observe a reduction of the complexity to O ( h log 2 ( k ) ) at test time .",model,Hierarchical softmax,0,41,25,11,0,model : Hierarchical softmax,0.44086021505376344,0.8064516129032258,0.9166666666666666
text-classification,2,"This approach is further extended to compute the T - top targets at the cost of O ( log ( T ) ) , using a binary heap .",model,Hierarchical softmax,0,42,26,12,0,model : Hierarchical softmax,0.45161290322580644,0.8387096774193549,1.0
text-classification,2,- gram features,model,N-gram features,0,43,27,1,0,model : N-gram features,0.46236559139784944,0.8709677419354839,0.2
text-classification,2,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,model,N-gram features,0,44,28,2,0,model : N-gram features,0.4731182795698925,0.9032258064516129,0.4
text-classification,2,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",model,N-gram features,1,45,29,3,0,model : N-gram features,0.4838709677419355,0.9354838709677419,0.6
text-classification,2,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,model,N-gram features,1,46,30,4,0,model : N-gram features,0.4946236559139785,0.967741935483871,0.8
text-classification,2,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",model,N-gram features,1,47,31,5,0,model : N-gram features,0.5053763440860215,1.0,1.0
text-classification,2,Experiments,experiment,Experiments,0,48,1,1,0,experiment : Experiments,0.5161290322580645,0.2,0.2
text-classification,2,We evaluate fastText on two different tasks .,experiment,Experiments,0,49,2,2,0,experiment : Experiments,0.5268817204301075,0.4,0.4
text-classification,2,"First , we compare it to existing text classifers on the problem of sentiment analysis .",experiment,Experiments,0,50,3,3,0,experiment : Experiments,0.5376344086021505,0.6,0.6
text-classification,2,"Then , we evaluate its capacity to scale to large output space on a tag prediction dataset .",experiment,Experiments,0,51,4,4,0,experiment : Experiments,0.5483870967741935,0.8,0.8
text-classification,2,"Note that our model could be implemented with the Vowpal Wabbit library , 2 but we observe in practice , that our tailored implementation is at least 2 - 5 faster .",experiment,Experiments,0,52,5,5,0,experiment : Experiments,0.5591397849462365,1.0,1.0
text-classification,2,Sentiment analysis,analysis,Sentiment analysis,1,53,1,1,0,analysis : Sentiment analysis,0.5698924731182796,0.02857142857142857,0.0625
text-classification,2,Datasets and baselines .,analysis,Sentiment analysis,0,54,2,2,0,analysis : Sentiment analysis,0.5806451612903226,0.05714285714285714,0.125
text-classification,2,We employ the same 8 datasets and evaluation protocol of .,analysis,Sentiment analysis,0,55,3,3,0,analysis : Sentiment analysis,0.5913978494623656,0.08571428571428572,0.1875
text-classification,2,We report the n-grams and TFIDF baselines from We also compare to following their evaluation protocol .,analysis,Sentiment analysis,0,56,4,4,0,analysis : Sentiment analysis,0.6021505376344086,0.11428571428571428,0.25
text-classification,2,We report their main baselines as well as their two approaches based on recurrent networks ( Conv - GRNN and LSTM - GRNN ) .,analysis,Sentiment analysis,0,57,5,5,0,analysis : Sentiment analysis,0.6129032258064516,0.14285714285714285,0.3125
text-classification,2,Results .,analysis,Sentiment analysis,0,58,6,6,0,analysis : Sentiment analysis,0.6236559139784946,0.17142857142857143,0.375
text-classification,2,We present the results in .,analysis,Sentiment analysis,0,59,7,7,0,analysis : Sentiment analysis,0.6344086021505376,0.2,0.4375
text-classification,2,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",analysis,Sentiment analysis,1,60,8,8,0,analysis : Sentiment analysis,0.6451612903225806,0.22857142857142856,0.5
text-classification,2,"On this task , adding bigram information improves the performance by 1 - 4 % .",analysis,Sentiment analysis,1,61,9,9,0,analysis : Sentiment analysis,0.6559139784946236,0.2571428571428571,0.5625
text-classification,2,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",analysis,Sentiment analysis,1,62,10,10,0,analysis : Sentiment analysis,0.6666666666666666,0.2857142857142857,0.625
text-classification,2,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",analysis,Sentiment analysis,1,63,11,11,0,analysis : Sentiment analysis,0.6774193548387096,0.3142857142857143,0.6875
text-classification,2,"Finally , shows that our method is competitive with the methods presented in .",analysis,Sentiment analysis,0,64,12,12,0,analysis : Sentiment analysis,0.6881720430107527,0.34285714285714286,0.75
text-classification,2,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,analysis,Sentiment analysis,0,65,13,13,0,analysis : Sentiment analysis,0.6989247311827957,0.37142857142857144,0.8125
text-classification,2,"Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference in accuracy .",analysis,Sentiment analysis,0,66,14,14,0,analysis : Sentiment analysis,0.7096774193548387,0.4,0.875
text-classification,2,We show a few correct and incorrect tag predictions .,analysis,Sentiment analysis,0,67,15,15,0,analysis : Sentiment analysis,0.7204301075268817,0.42857142857142855,0.9375
text-classification,2,"up compared to neural network based methods increases with the size of the dataset , going up to at least a 15,000 speed - up .",analysis,Sentiment analysis,0,68,16,16,0,analysis : Sentiment analysis,0.7311827956989247,0.45714285714285713,1.0
text-classification,2,Tag prediction,analysis,Tag prediction,1,69,17,1,0,analysis : Tag prediction,0.7419354838709677,0.4857142857142857,0.05263157894736842
text-classification,2,Dataset and baselines .,analysis,Tag prediction,0,70,18,2,0,analysis : Tag prediction,0.7526881720430108,0.5142857142857142,0.10526315789473684
text-classification,2,"To test scalability of our approach , further evaluation is carried on the YFCC100M dataset which consists of almost 100M images with captions , titles and tags .",analysis,Tag prediction,0,71,19,3,0,analysis : Tag prediction,0.7634408602150538,0.5428571428571428,0.15789473684210525
text-classification,2,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,analysis,Tag prediction,0,72,20,4,0,analysis : Tag prediction,0.7741935483870968,0.5714285714285714,0.21052631578947367
text-classification,2,"We remove the words and tags occurring less than 100 times and split the data into a train , validation and test set .",analysis,Tag prediction,0,73,21,5,0,analysis : Tag prediction,0.7849462365591398,0.6,0.2631578947368421
text-classification,2,"The train set contains 91,188,648 examples ( 1.5B tokens ) .",analysis,Tag prediction,0,74,22,6,0,analysis : Tag prediction,0.7956989247311828,0.6285714285714286,0.3157894736842105
text-classification,2,"The validation has 930,497 examples and the test set 543,424 .",analysis,Tag prediction,0,75,23,7,0,analysis : Tag prediction,0.8064516129032258,0.6571428571428571,0.3684210526315789
text-classification,2,"The vocabulary size is 297,141 and there are 312,116 unique tags .",analysis,Tag prediction,0,76,24,8,0,analysis : Tag prediction,0.8172043010752689,0.6857142857142857,0.42105263157894735
text-classification,2,We will release a script that recreates this dataset so that our numbers could be reproduced .,analysis,Tag prediction,0,77,25,9,0,analysis : Tag prediction,0.8279569892473119,0.7142857142857143,0.47368421052631576
text-classification,2,We report precision at 1 .,analysis,Tag prediction,0,78,26,10,0,analysis : Tag prediction,0.8387096774193549,0.7428571428571429,0.5263157894736842
text-classification,2,We consider a frequency - based baseline which predicts the most frequent tag .,analysis,Tag prediction,0,79,27,11,0,analysis : Tag prediction,0.8494623655913979,0.7714285714285715,0.5789473684210527
text-classification,2,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",analysis,Tag prediction,0,80,28,12,0,analysis : Tag prediction,0.8602150537634409,0.8,0.631578947368421
text-classification,2,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",analysis,Tag prediction,0,81,29,13,0,analysis : Tag prediction,0.8709677419354839,0.8285714285714286,0.6842105263157895
text-classification,2,Results and training time . and 200 .,analysis,Tag prediction,0,82,30,14,0,analysis : Tag prediction,0.8817204301075269,0.8571428571428571,0.7368421052631579
text-classification,2,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",analysis,Tag prediction,0,83,31,15,0,analysis : Tag prediction,0.8924731182795699,0.8857142857142857,0.7894736842105263
text-classification,2,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",analysis,Tag prediction,1,84,32,16,0,analysis : Tag prediction,0.9032258064516129,0.9142857142857143,0.8421052631578947
text-classification,2,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .",analysis,Tag prediction,1,85,33,17,0,analysis : Tag prediction,0.9139784946236559,0.9428571428571428,0.8947368421052632
text-classification,2,The speedup of the test phase is even more significant ( a 600 speedup ) .,analysis,Tag prediction,0,86,34,18,0,analysis : Tag prediction,0.9247311827956989,0.9714285714285714,0.9473684210526315
text-classification,2,shows some qualitative examples .,analysis,Tag prediction,0,87,35,19,0,analysis : Tag prediction,0.9354838709677419,1.0,1.0
text-classification,2,Discussion and conclusion,conclusion,Discussion and conclusion,0,88,1,1,0,conclusion : Discussion and conclusion,0.946236559139785,0.16666666666666666,0.16666666666666666
text-classification,2,"In this work , we propose a simple baseline method for text classification .",conclusion,Discussion and conclusion,0,89,2,2,0,conclusion : Discussion and conclusion,0.956989247311828,0.3333333333333333,0.3333333333333333
text-classification,2,"Unlike unsupervisedly trained word vectors from word2vec , our word features can be averaged together to form good sentence representations .",conclusion,Discussion and conclusion,0,90,3,3,0,conclusion : Discussion and conclusion,0.967741935483871,0.5,0.5
text-classification,2,"In several tasks , fastText obtains performance on par with recently proposed methods inspired by deep learning , while being much faster .",conclusion,Discussion and conclusion,0,91,4,4,0,conclusion : Discussion and conclusion,0.978494623655914,0.6666666666666666,0.6666666666666666
text-classification,2,"Although deep neural networks have in theory much higher representational power than shallow models , it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them .",conclusion,Discussion and conclusion,0,92,5,5,0,conclusion : Discussion and conclusion,0.989247311827957,0.8333333333333334,0.8333333333333334
text-classification,2,We will publish our code so that the research community can easily build on top of our work .,conclusion,Discussion and conclusion,0,93,6,6,0,conclusion : Discussion and conclusion,1.0,1.0,1.0
text-classification,3,On the Role of Text Preprocessing in Neural Network Architectures : An Evaluation Study on Text Categorization and Sentiment Analysis,title,title,1,2,1,1,0,title : title,0.016260162601626018,1.0,1.0
text-classification,3,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.024390243902439025,0.125,0.125
text-classification,3,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.032520325203252036,0.25,0.25
text-classification,3,"Despite its importance , text preprocessing has not received much attention in the deep learning literature .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.04065040650406504,0.375,0.375
text-classification,3,"In this paper we investigate the impact of simple text preprocessing decisions ( particularly tokenizing , lemmatizing , lowercasing and multiword grouping ) on the performance of a standard neural text classifier .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.04878048780487805,0.5,0.5
text-classification,3,We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.056910569105691054,0.625,0.625
text-classification,3,"While our experiments show that a simple tokenization of input text is generally adequate , they also highlight significant degrees of variability across preprocessing techniques .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.06504065040650407,0.75,0.75
text-classification,3,"This reveals the importance of paying attention to this usually - overlooked step in the pipeline , particularly when comparing different models .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.07317073170731707,0.875,0.875
text-classification,3,"Finally , our evaluation provides insights into the best preprocessing practices for training word embeddings .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.08130081300813008,1.0,1.0
text-classification,3,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.08943089430894309,0.045454545454545456,0.045454545454545456
text-classification,3,"Words are often considered as the basic constituents of texts for many languages , including English .",introduction,introduction,0,12,2,2,0,introduction : introduction,0.0975609756097561,0.09090909090909091,0.09090909090909091
text-classification,3,The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.10569105691056911,0.13636363636363635,0.13636363636363635
text-classification,3,"However , in practise , other preprocessing techniques can be ( and are ) further used together with tokenization .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.11382113821138211,0.18181818181818182,0.18181818181818182
text-classification,3,"These include lemmatization , lowercasing and 1 Note that although word - based models are mainstream in NLP in general and text classification in particular , recent work has also considered other linguistic units , such as characters or word senses .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.12195121951219512,0.22727272727272727,0.22727272727272727
text-classification,3,"These techniques require a different kind of preprocessing and , while they have been shown effective in various settings , in this work we only focus on the mainstream word - based models .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.13008130081300814,0.2727272727272727,0.2727272727272727
text-classification,3,"multiword grouping , among others .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.13821138211382114,0.3181818181818182,0.3181818181818182
text-classification,3,"Although these preprocessing decisions have been studied in the context of conventional text classification techniques , little attention has been paid to them in the more recent neural - based models .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.14634146341463414,0.36363636363636365,0.36363636363636365
text-classification,3,"The most similar study to ours is , which analyzed different encoding levels for English and Asian languages such as Chinese , Japanese and Korean .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.15447154471544716,0.4090909090909091,0.4090909090909091
text-classification,3,"As opposed to our work , their analysis was focused on UTF - 8 bytes , characters , words , romanized characters and romanized words as encoding levels , rather than the preprocessing techniques analyzed in this paper .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.16260162601626016,0.45454545454545453,0.45454545454545453
text-classification,3,"Additionally , word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.17073170731707318,0.5,0.5
text-classification,3,"However , while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus , the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.17886178861788618,0.5454545454545454,0.5454545454545454
text-classification,3,"In this paper we focus on the role of preprocessing the input text , particularly in how it is split into individual ( meaning - bearing ) tokens and how it affects the performance of standard neural text classification models based on .",introduction,introduction,1,23,13,13,0,introduction : introduction,0.18699186991869918,0.5909090909090909,0.5909090909090909
text-classification,3,"CNNs have proven to be effective in a wide range of NLP applications , in - cluding text classification tasks such as topic categorization and polarity detection , which are the tasks considered in this work .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.1951219512195122,0.6363636363636364,0.6363636363636364
text-classification,3,The goal of our evaluation study is to find answers to the following two questions :,introduction,introduction,0,25,15,15,0,introduction : introduction,0.2032520325203252,0.6818181818181818,0.6818181818181818
text-classification,3,1 .,introduction,introduction,0,26,16,16,0,introduction : introduction,0.21138211382113822,0.7272727272727273,0.7272727272727273
text-classification,3,Are neural network architectures ( in particular CNNs ) affected by seemingly small preprocessing decisions in the input text ? 2 .,introduction,introduction,0,27,17,17,0,introduction : introduction,0.21951219512195122,0.7727272727272727,0.7727272727272727
text-classification,3,Are neural network architectures ( in particular CNNs ) affected by seemingly small preprocessing decisions in the input text ? 2 .,introduction,introduction,0,28,18,18,0,introduction : introduction,0.22764227642276422,0.8181818181818182,0.8181818181818182
text-classification,3,"Does the preprocessing of the embeddings ' underlying training corpus have an impact on the final performance of a state - of - the - art neural network text classifier ? According to our experiments in topic categorization and polarity detection , these decisions are important in certain cases .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.23577235772357724,0.8636363636363636,0.8636363636363636
text-classification,3,"Does the preprocessing of the embeddings ' underlying training corpus have an impact on the final performance of a state - of - the - art neural network text classifier ? According to our experiments in topic categorization and polarity detection , these decisions are important in certain cases .",introduction,introduction,0,30,20,20,0,introduction : introduction,0.24390243902439024,0.9090909090909091,0.9090909090909091
text-classification,3,"Moreover , we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting .",introduction,introduction,0,31,21,21,0,introduction : introduction,0.25203252032520324,0.9545454545454546,0.9545454545454546
text-classification,3,The accompanying materials of this submission can be downloaded at the following repository : github.com/pedrada88/preproc-textclassification .,introduction,introduction,1,32,22,22,0,introduction : introduction,0.2601626016260163,1.0,1.0
text-classification,3,Text Preprocessing,system description,Text Preprocessing,0,33,1,1,0,system description : Text Preprocessing,0.2682926829268293,0.034482758620689655,0.16666666666666666
text-classification,3,"Given an input text , words are gathered as input units of classification models through tokenization .",system description,Text Preprocessing,0,34,2,2,0,system description : Text Preprocessing,0.2764227642276423,0.06896551724137931,0.3333333333333333
text-classification,3,We refer to the corpus which is only tokenized as vanilla .,system description,Text Preprocessing,0,35,3,3,0,system description : Text Preprocessing,0.2845528455284553,0.10344827586206896,0.5
text-classification,3,"For example , given the sentence "" Apple is asking its manufacturers to move Mac - Book Air production to the United States . "" ( running example ) , the vanilla tokenized text would be as follows ( white spaces delimiting different word units ) :",system description,Text Preprocessing,0,36,4,4,0,system description : Text Preprocessing,0.2926829268292683,0.13793103448275862,0.6666666666666666
text-classification,3,Apple is asking its manufacturers to move MacBook Air production to the United States .,system description,Text Preprocessing,0,37,5,5,0,system description : Text Preprocessing,0.3008130081300813,0.1724137931034483,0.8333333333333334
text-classification,3,"We additionally consider three simple preprocessing techniques to be applied to an input text : lowercasing ( Section 2.1 ) , lemmatizing ( Section 2.2 ) and multiword grouping ( Section 2.3 ) .",system description,Text Preprocessing,0,38,6,6,0,system description : Text Preprocessing,0.3089430894308943,0.20689655172413793,1.0
text-classification,3,Lowercasing,system description,Lowercasing,0,39,7,1,0,system description : Lowercasing,0.3170731707317073,0.2413793103448276,0.16666666666666666
text-classification,3,This is the simplest preprocessing technique which consists of lowercasing each single token of the input text :,system description,Lowercasing,0,40,8,2,0,system description : Lowercasing,0.3252032520325203,0.27586206896551724,0.3333333333333333
text-classification,3,apple is asking its manufacturers to move macbook air production to the united states .,system description,Lowercasing,0,41,9,3,0,system description : Lowercasing,0.3333333333333333,0.3103448275862069,0.5
text-classification,3,"Due to its simplicity , lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages .",system description,Lowercasing,0,42,10,4,0,system description : Lowercasing,0.34146341463414637,0.3448275862068966,0.6666666666666666
text-classification,3,"Despite its desirable property of reducing sparsity and vocabulary size , lowercasing may negatively impact system 's performance by increasing ambiguity .",system description,Lowercasing,0,43,11,5,0,system description : Lowercasing,0.34959349593495936,0.3793103448275862,0.8333333333333334
text-classification,3,"For instance , the Apple company in our example and the apple fruit would be considered as identical entities .",system description,Lowercasing,0,44,12,6,0,system description : Lowercasing,0.35772357723577236,0.41379310344827586,1.0
text-classification,3,Lemmatizing,system description,Lemmatizing,0,45,13,1,0,system description : Lemmatizing,0.36585365853658536,0.4482758620689655,0.14285714285714285
text-classification,3,The process of lemmatizing consists of replacing a given token with its corresponding lemma :,system description,Lemmatizing,0,46,14,2,0,system description : Lemmatizing,0.37398373983739835,0.4827586206896552,0.2857142857142857
text-classification,3,Apple be ask its manufacturer to move Mac - Book Air production to the United States .,system description,Lemmatizing,0,47,15,3,0,system description : Lemmatizing,0.3821138211382114,0.5172413793103449,0.42857142857142855
text-classification,3,Lemmatization has been traditionally a standard preprocessing technique for linear text classification systems .,system description,Lemmatizing,0,48,16,4,0,system description : Lemmatizing,0.3902439024390244,0.5517241379310345,0.5714285714285714
text-classification,3,"However , it is rarely used as a preprocessing stage in neural - based systems .",system description,Lemmatizing,0,49,17,5,0,system description : Lemmatizing,0.3983739837398374,0.5862068965517241,0.7142857142857143
text-classification,3,"The main idea behind lemmatization is to reduce sparsity , as different inflected forms of the same lemma may occur infrequently ( or not at all ) during training .",system description,Lemmatizing,0,50,18,6,0,system description : Lemmatizing,0.4065040650406504,0.6206896551724138,0.8571428571428571
text-classification,3,"However , this may come at the cost of neglecting important syntactic nuances .",system description,Lemmatizing,0,51,19,7,0,system description : Lemmatizing,0.4146341463414634,0.6551724137931034,1.0
text-classification,3,Multiword grouping,system description,Multiword grouping,0,52,20,1,0,system description : Multiword grouping,0.42276422764227645,0.6896551724137931,0.1
text-classification,3,This last preprocessing technique consists of grouping consecutive tokens together into a single token if found in a given inventory :,system description,Multiword grouping,0,53,21,2,0,system description : Multiword grouping,0.43089430894308944,0.7241379310344828,0.2
text-classification,3,Apple is asking its manufacturers to move MacBook Air production to the United States .,system description,Multiword grouping,0,54,22,3,0,system description : Multiword grouping,0.43902439024390244,0.7586206896551724,0.3
text-classification,3,"The motivation behind this step lies in the idiosyncratic nature of multiword expressions , e.g. United States in the example .",system description,Multiword grouping,0,55,23,4,0,system description : Multiword grouping,0.44715447154471544,0.7931034482758621,0.4
text-classification,3,The meaning of these multiword expressions are often hardly traceable from their individual tokens .,system description,Multiword grouping,0,56,24,5,0,system description : Multiword grouping,0.45528455284552843,0.8275862068965517,0.5
text-classification,3,"As a result , treating multiwords as single units may lead to better training of a given model .",system description,Multiword grouping,0,57,25,6,0,system description : Multiword grouping,0.4634146341463415,0.8620689655172413,0.6
text-classification,3,"Because of this , word embedding toolkits such as Word2vec propose statistical approaches for extracting these multiwords , or directly include multiwords along with single words in their pretrained embedding spaces .",system description,Multiword grouping,0,58,26,7,0,system description : Multiword grouping,0.4715447154471545,0.896551724137931,0.7
text-classification,3,"We considered two tasks for our experiments : topic categorization , i.e. assigning a topic to a given document from a pre-defined set of topics , and polarity detection , i.e. detecting if the sentiment of a given piece of text is positive or negative .",system description,Multiword grouping,0,59,27,8,0,system description : Multiword grouping,0.4796747967479675,0.9310344827586207,0.8
text-classification,3,Two different settings were studied : ( 1 ) word embedding 's training corpus and the evaluation dataset were preprocessed in a similar manner ( Section 3.2 ) ; and ( 2 ) the two were preprocessed differently ( Section 3.3 ) .,system description,Multiword grouping,0,60,28,9,0,system description : Multiword grouping,0.4878048780487805,0.9655172413793104,0.9
text-classification,3,In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation .,system description,Multiword grouping,0,61,29,10,0,system description : Multiword grouping,0.4959349593495935,1.0,1.0
text-classification,3,Experimental setup,experiment,Experimental setup,0,62,1,1,0,experiment : Experimental setup,0.5040650406504065,0.0196078431372549,0.07692307692307693
text-classification,3,We tried with two classification models .,experiment,Experimental setup,1,63,2,2,0,experiment : Experimental setup,0.5121951219512195,0.0392156862745098,0.15384615384615385
text-classification,3,"The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .",experiment,Experimental setup,1,64,3,3,0,experiment : Experimental setup,0.5203252032520326,0.058823529411764705,0.23076923076923078
text-classification,3,"In the second model , we add a recurrent layer ( specifically an LSTM ) before passing the pooled features directly to the fully connected softmax layer .",experiment,Experimental setup,1,65,4,4,0,experiment : Experimental setup,0.5284552845528455,0.0784313725490196,0.3076923076923077
text-classification,3,The inclusion of this LSTM layer has been shown to be able to effectively replace multiple layers of convolution and be beneficial particularly for large inputs .,experiment,Experimental setup,0,66,5,5,0,experiment : Experimental setup,0.5365853658536586,0.09803921568627451,0.38461538461538464
text-classification,3,"These models were used for both topic categorization and polarity detection tasks , with slight hyperparameter variations given their different natures ( mainly in their text size ) which were fixed across all datasets .",experiment,Experimental setup,0,67,6,6,0,experiment : Experimental setup,0.5447154471544715,0.11764705882352941,0.46153846153846156
text-classification,3,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,experiment,Experimental setup,1,68,7,7,0,experiment : Experimental setup,0.5528455284552846,0.13725490196078433,0.5384615384615384
text-classification,3,4 .,experiment,Experimental setup,0,69,8,8,0,experiment : Experimental setup,0.5609756097560976,0.1568627450980392,0.6153846153846154
text-classification,3,Evaluation datasets .,experiment,Experimental setup,0,70,9,9,0,experiment : Experimental setup,0.5691056910569106,0.17647058823529413,0.6923076923076923
text-classification,3,"For the topic categorization task we used the BBC news dataset 5 , 20 News , Reuters 6 and The code for this CNN implementation is the same as in , which is available at https://github.com/pilehvar/sensecnn",experiment,Experimental setup,0,71,10,10,0,experiment : Experimental setup,0.5772357723577236,0.19607843137254902,0.7692307692307693
text-classification,3,Context window of 5 words and hierarchical softmax .,experiment,Experimental setup,0,72,11,11,0,experiment : Experimental setup,0.5853658536585366,0.21568627450980393,0.8461538461538461
text-classification,3,http://mlg.ucd.ie/datasets/bbc.html,experiment,Experimental setup,0,73,12,12,0,experiment : Experimental setup,0.5934959349593496,0.23529411764705882,0.9230769230769231
text-classification,3,"Due to the large number of labels in the original Reuters ( i.e. 91 ) and to be consistent with the other datasets , we reduce the dataset to its 8 most frequent labels , a reduction already performed in previous works .",experiment,Experimental setup,0,74,13,13,0,experiment : Experimental setup,0.6016260162601627,0.2549019607843137,1.0
text-classification,3,Preprocessing .,experiment,Experiment 1: Preprocessing effect,0,75,14,1,0,experiment : Experiment 1: Preprocessing effect,0.6097560975609756,0.27450980392156865,0.045454545454545456
text-classification,3,Four different techniques ( see Section 2 ) were used to preprocess the datasets as well as the corpus which was used to train word embeddings ( i.e. UMBC ) .,experiment,Experiment 1: Preprocessing effect,0,76,15,2,0,experiment : Experiment 1: Preprocessing effect,0.6178861788617886,0.29411764705882354,0.09090909090909091
text-classification,3,For tokenization and lemmatization we relied on Stanford CoreNLP .,experiment,Experiment 1: Preprocessing effect,0,77,16,3,0,experiment : Experiment 1: Preprocessing effect,0.6260162601626016,0.3137254901960784,0.13636363636363635
text-classification,3,"As for multiwords , we used the phrases from the pre-trained Google News Word2vec vectors , which were obtained using a simple statistical approach .",experiment,Experiment 1: Preprocessing effect,0,78,17,4,0,experiment : Experiment 1: Preprocessing effect,0.6341463414634146,0.3333333333333333,0.18181818181818182
text-classification,3,12 shows the accuracy 13 of the classification models using our four preprocessing techniques .,experiment,Experiment 1: Preprocessing effect,0,79,18,5,0,experiment : Experiment 1: Preprocessing effect,0.6422764227642277,0.35294117647058826,0.22727272727272727
text-classification,3,We observe a certain variability of results depending on the preprocessing techniques used ( aver -7 ftp://medir.ohsu.edu/pub/ohsumed,experiment,Experiment 1: Preprocessing effect,0,80,19,6,0,experiment : Experiment 1: Preprocessing effect,0.6504065040650406,0.37254901960784315,0.2727272727272727
text-classification,3,Both PL04 and PL05 were downloaded from http://www.cs.cornell.edu/people/pabo/movie-review-data/,experiment,Experiment 1: Preprocessing effect,0,81,20,7,0,experiment : Experiment 1: Preprocessing effect,0.6585365853658537,0.39215686274509803,0.3181818181818182
text-classification,3,http://www.rottentomatoes.com,experiment,Experiment 1: Preprocessing effect,0,82,21,8,0,experiment : Experiment 1: Preprocessing effect,0.6666666666666666,0.4117647058823529,0.36363636363636365
text-classification,3,"10 We mapped the numerical value of phrases to either negative ( from 0 to 0.4 ) or positive ( from 0.6 to 1 ) , removing the neutral phrases according to the scale ( from 0.4 to 0.6 ) .",experiment,Experiment 1: Preprocessing effect,0,83,22,9,0,experiment : Experiment 1: Preprocessing effect,0.6747967479674797,0.43137254901960786,0.4090909090909091
text-classification,3,"For the datasets with train - test partitions , the sizes of the test sets are the following : 7,532 for 20 News ; 12,733 for Ohsumed ; 25,000 for IMDb ; and 1,000 for RTC .",experiment,Experiment 1: Preprocessing effect,0,84,23,10,0,experiment : Experiment 1: Preprocessing effect,0.6829268292682927,0.45098039215686275,0.45454545454545453
text-classification,3,For future work it would be interesting to explore more complex methods to learn embeddings for multiword expressions .,experiment,Experiment 1: Preprocessing effect,0,85,24,11,0,experiment : Experiment 1: Preprocessing effect,0.6910569105691057,0.47058823529411764,0.5
text-classification,3,Computed by averaging accuracy of two different runs .,experiment,Experiment 1: Preprocessing effect,0,86,25,12,0,experiment : Experiment 1: Preprocessing effect,0.6991869918699187,0.49019607843137253,0.5454545454545454
text-classification,3,"The statistical significance was calculated according to an unpaired t- test at the 5 % significance level . age variability 14 of 2.4 % for the CNN + LSTM model , including a statistical significance gap in seven of the nine datasets ) , which proves the influence of preprocessing on the final results .",experiment,Experiment 1: Preprocessing effect,0,87,26,13,0,experiment : Experiment 1: Preprocessing effect,0.7073170731707317,0.5098039215686274,0.5909090909090909
text-classification,3,It is perhaps not surprising that the lowest variance of results is seen in the datasets with the larger training data ( i.e. RTC and Stanford ) .,experiment,Experiment 1: Preprocessing effect,0,88,27,14,0,experiment : Experiment 1: Preprocessing effect,0.7154471544715447,0.5294117647058824,0.6363636363636364
text-classification,3,"This suggests that the preprocessing decisions are not so important when the training data is large enough , but they are indeed relevant in benchmarks where the training data is limited .",experiment,Experiment 1: Preprocessing effect,0,89,28,15,0,experiment : Experiment 1: Preprocessing effect,0.7235772357723578,0.5490196078431373,0.6818181818181818
text-classification,3,"As far as the individual preprocessing techniques are concerned , the vanilla setting ( tokenization only ) proves to be consistent across datasets and tasks , as it performs in the same ballpark as the best result in 8 of the 9 datasets for both models ( with no noticeable differences between topic categorization and polarity detection ) .",experiment,Experiment 1: Preprocessing effect,0,90,29,16,0,experiment : Experiment 1: Preprocessing effect,0.7317073170731707,0.5686274509803921,0.7272727272727273
text-classification,3,"The only topic categorization dataset in which tokenization does not seem enough is Ohsumed , which , unlike the more general nature of other categorization datasets ( news ) , belongs to a specialized domain ( medical ) for which fine - grained distinctions are required to classify cardiovascular diseases .",experiment,Experiment 1: Preprocessing effect,0,91,30,17,0,experiment : Experiment 1: Preprocessing effect,0.7398373983739838,0.5882352941176471,0.7727272727272727
text-classification,3,"In particular for this dataset , word embeddings trained on a general - domain corpus like UMBC may not accurately capture the specialized meaning of medical terms and hence , sparsity becomes an issue .",experiment,Experiment 1: Preprocessing effect,0,92,31,18,0,experiment : Experiment 1: Preprocessing effect,0.7479674796747967,0.6078431372549019,0.8181818181818182
text-classification,3,"In fact , lowercasing and lemmatizing , which are mainly aimed at reducing sparsity , outperform the vanilla setting by over six points in the CNN + LSTM setting and clearly outperform the other preprocessing techniques on the single CNN model as well .",experiment,Experiment 1: Preprocessing effect,0,93,32,19,0,experiment : Experiment 1: Preprocessing effect,0.7560975609756098,0.6274509803921569,0.8636363636363636
text-classification,3,Experiment 1 : Preprocessing effect,experiment,Experiment 1: Preprocessing effect,1,94,33,20,0,experiment : Experiment 1: Preprocessing effect,0.7642276422764228,0.6470588235294118,0.9090909090909091
text-classification,3,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .",experiment,Experiment 1: Preprocessing effect,1,95,34,21,0,experiment : Experiment 1: Preprocessing effect,0.7723577235772358,0.6666666666666666,0.9545454545454546
text-classification,3,"Even though lemmatization has proved useful in conventional linear models as an effective way to deal with sparsity , neural network architectures seem to be more capable of overcoming sparsity thanks to the generalization power of word embeddings .",experiment,Experiment 1: Preprocessing effect,0,96,35,22,0,experiment : Experiment 1: Preprocessing effect,0.7804878048780488,0.6862745098039216,1.0
text-classification,3,Experiment 2 : Cross-preprocessing,experiment,Experiment 2: Cross-preprocessing,1,97,36,1,0,experiment : Experiment 2: Cross-preprocessing,0.7886178861788617,0.7058823529411765,0.0625
text-classification,3,This experiment aims at studying the impact of using different word embeddings ( with differently preprocessed training corpora ) on tokenized datasets ( vanilla setting ) .,experiment,Experiment 2: Cross-preprocessing,0,98,37,2,0,experiment : Experiment 2: Cross-preprocessing,0.7967479674796748,0.7254901960784313,0.125
text-classification,3,shows the results for this experiment .,experiment,Experiment 2: Cross-preprocessing,0,99,38,3,0,experiment : Experiment 2: Cross-preprocessing,0.8048780487804879,0.7450980392156863,0.1875
text-classification,3,"In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best over all performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .",experiment,Experiment 2: Cross-preprocessing,1,100,39,4,0,experiment : Experiment 2: Cross-preprocessing,0.8130081300813008,0.7647058823529411,0.25
text-classification,3,In this case the same set of words is learnt but single tokens inside multiword expressions are not trained .,experiment,Experiment 2: Cross-preprocessing,0,101,40,5,0,experiment : Experiment 2: Cross-preprocessing,0.8211382113821138,0.7843137254901961,0.3125
text-classification,3,"Instead , these single tokens are considered in isolation only , without the added noise when considered inside the multiword expression as well .",experiment,Experiment 2: Cross-preprocessing,0,102,41,6,0,experiment : Experiment 2: Cross-preprocessing,0.8292682926829268,0.803921568627451,0.375
text-classification,3,"For instance , the word Apple has a clearly different meaning in isolation from the one inside :",experiment,Experiment 2: Cross-preprocessing,0,103,42,7,0,experiment : Experiment 2: Cross-preprocessing,0.8373983739837398,0.8235294117647058,0.4375
text-classification,3,Cross - preprocessing evaluation : accuracy on the topic categorization and polarity detection tasks using different sets of word embeddings to initialize the embedding layer of the two classifiers .,experiment,Experiment 2: Cross-preprocessing,0,104,43,8,0,experiment : Experiment 2: Cross-preprocessing,0.8455284552845529,0.8431372549019608,0.5
text-classification,3,All datasets were preprocessed similarly according to the vanilla setting .,experiment,Experiment 2: Cross-preprocessing,0,105,44,9,0,experiment : Experiment 2: Cross-preprocessing,0.8536585365853658,0.8627450980392157,0.5625
text-classification,3,indicates results thatare statistically significant with respect to the top result .,experiment,Experiment 2: Cross-preprocessing,0,106,45,10,0,experiment : Experiment 2: Cross-preprocessing,0.8617886178861789,0.8823529411764706,0.625
text-classification,3,"the multiword expression Big Apple , hence it can be seen as beneficial not to train the word",experiment,Experiment 2: Cross-preprocessing,0,107,46,11,0,experiment : Experiment 2: Cross-preprocessing,0.8699186991869918,0.9019607843137255,0.6875
text-classification,3,Apple when part of this multiword expression .,experiment,Experiment 2: Cross-preprocessing,0,108,47,12,0,experiment : Experiment 2: Cross-preprocessing,0.8780487804878049,0.9215686274509803,0.75
text-classification,3,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",experiment,Experiment 2: Cross-preprocessing,1,109,48,13,0,experiment : Experiment 2: Cross-preprocessing,0.8861788617886179,0.9411764705882353,0.8125
text-classification,3,"This could provide hints on the excellent results provided by pre-trained Word2vec embeddings trained on the Google News corpus , which learns multiwords similarly to our setting .",experiment,Experiment 2: Cross-preprocessing,0,110,49,14,0,experiment : Experiment 2: Cross-preprocessing,0.8943089430894309,0.9607843137254902,0.875
text-classification,3,"Apart from this somewhat surprising finding , the use of the embeddings trained on a simple tokenized corpus ( i.e. vanilla ) proved again competitive , as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help .",experiment,Experiment 2: Cross-preprocessing,1,111,50,15,0,experiment : Experiment 2: Cross-preprocessing,0.9024390243902439,0.9803921568627451,0.9375
text-classification,3,"In fact , the relatively weaker performance of lemmatization and lowercasing in this crossprocessing experiment is somehow expected as the coverage of word embeddings in vanilla - tokenized datasets is limited , e.g. , many entities which are capitalized in the datasets are not covered in the case of lowercasing , and inflected forms are missing in the case of lemmatizing .",experiment,Experiment 2: Cross-preprocessing,0,112,51,16,0,experiment : Experiment 2: Cross-preprocessing,0.9105691056910569,1.0,1.0
text-classification,3,Conclusions,conclusion,Conclusions,0,113,1,1,0,conclusion : Conclusions,0.9186991869918699,0.09090909090909091,0.09090909090909091
text-classification,3,In this paper we analyzed the impact of simple text preprocessing decisions on the performance of a standard word - based neural text classifier .,conclusion,Conclusions,0,114,2,2,0,conclusion : Conclusions,0.926829268292683,0.18181818181818182,0.18181818181818182
text-classification,3,Our evaluations highlight the importance of being careful in the choice of how to preprocess our data and to be consistent when comparing different systems .,conclusion,Conclusions,0,115,3,3,0,conclusion : Conclusions,0.9349593495934959,0.2727272727272727,0.2727272727272727
text-classification,3,"In general , a simple tokenization works equally or better than more complex pre-processing techniques such as lemmatization or multiword grouping , except for domain - specific datasets ( such as the medical dataset in our experiments ) in which sole tokenization performs poorly .",conclusion,Conclusions,0,116,4,4,0,conclusion : Conclusions,0.943089430894309,0.36363636363636365,0.36363636363636365
text-classification,3,"Additionally , word embeddings trained on multiword - grouped corpora perform surprisingly well when applied to simple tokenized datasets .",conclusion,Conclusions,0,117,5,5,0,conclusion : Conclusions,0.9512195121951219,0.45454545454545453,0.45454545454545453
text-classification,3,"This property has often been overlooked and , to the best of our knowledge , we test the hypothesis for the first time .",conclusion,Conclusions,0,118,6,6,0,conclusion : Conclusions,0.959349593495935,0.5454545454545454,0.5454545454545454
text-classification,3,"In fact , this finding could partially explain the long - lasting success of pre-trained Word2vec embeddings , which specifically learn multiword embeddings as part of their pipeline .",conclusion,Conclusions,0,119,7,7,0,conclusion : Conclusions,0.967479674796748,0.6363636363636364,0.6363636363636364
text-classification,3,"Moreover , our analysis shows that there is a high variance in the results depending on the preprocessing choice ( 2.4 % on average for the best performing model ) , especially when the training data is not large enough to generalize .",conclusion,Conclusions,0,120,8,8,0,conclusion : Conclusions,0.975609756097561,0.7272727272727273,0.7272727272727273
text-classification,3,"Further analysis and experimentation would be required to fully understand the significance of these results ; but , this work can be viewed as a starting point for studying the impact of text preprocessing in deep learning models .",conclusion,Conclusions,0,121,9,9,0,conclusion : Conclusions,0.983739837398374,0.8181818181818182,0.8181818181818182
text-classification,3,We hope that our findings will encourage future researchers to carefully select and report these preprocessing decisions when evaluating or comparing different models .,conclusion,Conclusions,0,122,10,10,0,conclusion : Conclusions,0.991869918699187,0.9090909090909091,0.9090909090909091
text-classification,3,"Finally , as future work , we plan to extend our analysis to other tasks ( e.g. question answering ) , languages ( particularly morphologically rich languages for which these results may vary ) and preprocessing techniques ( e.g. stopword removal or part - of - speech tagging ) .",conclusion,Conclusions,0,123,11,11,0,conclusion : Conclusions,1.0,1.0,1.0
text-classification,4,Learning Context - Sensitive Convolutional Filters for Text Processing,title,title,1,2,1,1,0,title : title,0.00904977375565611,1.0,1.0
text-classification,4,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.013574660633484163,0.125,0.125
text-classification,4,Convolutional neural networks ( CNNs ) have recently emerged as a popular building block for natural language processing ( NLP ) .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.01809954751131222,0.25,0.25
text-classification,4,"Despite their success , most existing CNN models employed in NLP share the same learned ( and static ) set of filters for all input sentences .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02262443438914027,0.375,0.375
text-classification,4,"In this paper , we consider an approach of using a small meta network to learn contextsensitive convolutional filters for text processing .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.027149321266968326,0.5,0.5
text-classification,4,The role of meta network is to abstract the contextual information of a sentence or document into a set of input -aware filters .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.03167420814479638,0.625,0.625
text-classification,4,"We further generalize this framework to model sentence pairs , where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03619909502262444,0.75,0.75
text-classification,4,"In our benchmarks on four different tasks , including ontology classification , sentiment analysis , answer sentence selection , and paraphrase identification , our proposed model , a modified CNN with context - sensitive filters , consistently outperforms the standard CNN and attention - based CNN baselines .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.04072398190045249,0.875,0.875
text-classification,4,"By visualizing the learned context - sensitive filters , we further validate and rationalize the effectiveness of proposed framework .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.04524886877828054,1.0,1.0
text-classification,4,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.049773755656108594,0.041666666666666664,0.041666666666666664
text-classification,4,"In the last few years , convolutional neural networks ( CNNs ) have demonstrated remarkable progress in various natural language processing applications , including sentence / document classification , text sequence matching , generic text representations , language modeling , machine translation and abstractive sentence summarization .",introduction,introduction,0,12,2,2,0,introduction : introduction,0.05429864253393665,0.08333333333333333,0.08333333333333333
text-classification,4,CNNs are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.058823529411764705,0.125,0.125
text-classification,4,"As an encoder network for text , CNNs typically convolve a set of filters , of window size n , with an inputsentence embedding matrix obtained via word2vec or Glove .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.06334841628959276,0.16666666666666666,0.16666666666666666
text-classification,4,"Different filter sizes n maybe used within the same model , exploiting meaningful semantic features from different n-gram fragments .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.06787330316742081,0.20833333333333334,0.20833333333333334
text-classification,4,"The learned weights of CNN filters , in most cases , are assumed to be fixed regardless of the input text .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.07239819004524888,0.25,0.25
text-classification,4,"As a result , the rich contextual information inherent in natural language sequences may not be fully captured .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.07692307692307693,0.2916666666666667,0.2916666666666667
text-classification,4,"As demonstrated in , the context of a word tends to greatly influence its contribution to the final supervised tasks .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.08144796380090498,0.3333333333333333,0.3333333333333333
text-classification,4,"This observation is consistent with the following intuition : when reading different types of documents , e.g. , academic papers or newspaper articles , people tend to adopt distinct strategies for better and more effective understanding , leveraging the fact that the same words or phrases may have different meaning or imply different things , depending on context .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.08597285067873303,0.375,0.375
text-classification,4,Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations .,introduction,introduction,0,20,10,10,0,introduction : introduction,0.09049773755656108,0.4166666666666667,0.4166666666666667
text-classification,4,"One common strategy is the attention mechanism , which is typically employed on top of a CNN ( or Long Short - Term Memory ( LSTM ) ) layer to guide the extraction of semantic features .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.09502262443438914,0.4583333333333333,0.4583333333333333
text-classification,4,"For the embedding of a single sentence , proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.09954751131221719,0.5,0.5
text-classification,4,"However , their model needs considerably more parameters to achieve performance gains over traditional CNNs .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.10407239819004525,0.5416666666666666,0.5416666666666666
text-classification,4,"To match sentence pairs , introduced an attentionbased CNN model , which re-weights the convolution inputs or outputs , to extract interdepen - dent sentence representations . ; explore a compare and aggregate framework to directly capture the wordby - word matching between two paired sentences .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.1085972850678733,0.5833333333333334,0.5833333333333334
text-classification,4,"However , these approaches suffer from the problem of high matching complexity , since a similarity matrix between pairwise words needs to be computed , and thus it is computationally inefficient or even prohibitive when applied to long sentences .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.11312217194570136,0.625,0.625
text-classification,4,"In this paper , we propose a generic approach to learn context - sensitive convolutional filters for natural language understanding .",introduction,introduction,1,26,16,16,0,introduction : introduction,0.11764705882352941,0.6666666666666666,0.6666666666666666
text-classification,4,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .",introduction,introduction,1,27,17,17,0,introduction : introduction,0.12217194570135746,0.7083333333333334,0.7083333333333334
text-classification,4,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .",introduction,introduction,1,28,18,18,0,introduction : introduction,0.12669683257918551,0.75,0.75
text-classification,4,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .",introduction,introduction,1,29,19,19,0,introduction : introduction,0.13122171945701358,0.7916666666666666,0.7916666666666666
text-classification,4,"Moreover , since the generated filters in our framework can adapt to different conditional information available ( labels or paired sentences ) , they can be naturally generalized to model sentence pairs .",introduction,introduction,0,30,20,20,0,introduction : introduction,0.13574660633484162,0.8333333333333334,0.8333333333333334
text-classification,4,"In this regard , we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context - sensitive representations .",introduction,introduction,1,31,21,21,0,introduction : introduction,0.14027149321266968,0.875,0.875
text-classification,4,"We investigate the effectiveness of our Adaptive Context - sensitive CNN ( ACNN ) framework on several text processing tasks : ontology classification , sentiment analysis , answer sentence selection and paraphrase identification .",introduction,introduction,0,32,22,22,0,introduction : introduction,0.14479638009049775,0.9166666666666666,0.9166666666666666
text-classification,4,We show that the proposed methods consistently outperforms the standard CNN and attention - based CNN baselines .,introduction,introduction,0,33,23,23,0,introduction : introduction,0.1493212669683258,0.9583333333333334,0.9583333333333334
text-classification,4,"Our work provides a new perspective on how to incorporate contextual information into text representations , which can be combined with more sophisticated structures to achieve even better performance in the future .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.15384615384615385,1.0,1.0
text-classification,4,Related Work,related work,Related Work,0,35,1,1,0,related work : Related Work,0.1583710407239819,0.07692307692307693,0.07692307692307693
text-classification,4,"Learning deep text representations has attracted much attention recently , since they can potentially benefit a wide range of NLP applications .",related work,Related Work,0,36,2,2,0,related work : Related Work,0.16289592760180996,0.15384615384615385,0.15384615384615385
text-classification,4,CNNs have been extensively investigated as the encoder networks of natural language .,related work,Related Work,0,37,3,3,0,related work : Related Work,0.167420814479638,0.23076923076923078,0.23076923076923078
text-classification,4,Our work is inline with previous efforts on improving the adaptivity and flexibility of convolutional neural networks .,related work,Related Work,0,38,4,4,0,related work : Related Work,0.17194570135746606,0.3076923076923077,0.3076923076923077
text-classification,4,proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation .,related work,Related Work,0,39,5,5,0,related work : Related Work,0.17647058823529413,0.38461538461538464,0.38461538461538464
text-classification,4,"De introduced an architecture to generate the future frames conditioned on given image ( s ) , by adapting the CNN filter weights to the motion within previous video frames .",related work,Related Work,0,40,6,6,0,related work : Related Work,0.18099547511312217,0.46153846153846156,0.46153846153846156
text-classification,4,"Although CNNs have been widely adopted in a large number of NLP applications , improving the adaptivity of vanilla CNN modules has been considerably less studied .",related work,Related Work,0,41,7,7,0,related work : Related Work,0.18552036199095023,0.5384615384615384,0.5384615384615384
text-classification,4,"To the best of our knowledge , the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences .",related work,Related Work,0,42,8,8,0,related work : Related Work,0.19004524886877827,0.6153846153846154,0.6153846153846154
text-classification,4,"Our use of a meta network to generate parameters for another network is directly inspired by the recent success of hypernetworks for textgeneration tasks , and dynamic parameter - prediction for video - frame generation .",related work,Related Work,0,43,9,9,0,related work : Related Work,0.19457013574660634,0.6923076923076923,0.6923076923076923
text-classification,4,"In contrast to these works that focus on generation problems , our model is based on context - sensitive CNN filters and is aimed at abstracting more informative and predictive sentence features .",related work,Related Work,0,44,10,10,0,related work : Related Work,0.19909502262443438,0.7692307692307693,0.7692307692307693
text-classification,4,"Most similar to our work , designed a meta network to generate compositional functions over tree - structured neural networks for encapsulating sentence features .",related work,Related Work,0,45,11,11,0,related work : Related Work,0.20361990950226244,0.8461538461538461,0.8461538461538461
text-classification,4,"However , their model is only suitable for encoding individual sentences , while our framework can be readily generalized to capture the interactions between sentence pairs .",related work,Related Work,0,46,12,12,0,related work : Related Work,0.2081447963800905,0.9230769230769231,0.9230769230769231
text-classification,4,"Moreover , our framework is based on CNN models , which is advantageous due to fewer parameters and highly parallelizable computations relative to sequential - based models .",related work,Related Work,0,47,13,13,0,related work : Related Work,0.21266968325791855,1.0,1.0
text-classification,4,Model,model,Model,0,48,1,1,0,model : Model,0.2171945701357466,0.011111111111111112,1.0
text-classification,4,Basic CNN for text representations,model,Basic CNN for text representations,0,49,2,1,0,model : Basic CNN for text representations,0.22171945701357465,0.022222222222222223,0.06666666666666667
text-classification,4,"The CNN architectures in are typically utilized for extracting sentence representations , by a composition of a convolutional layer and a max - pooling operation over all resulting feature maps .",model,Basic CNN for text representations,0,50,3,2,0,model : Basic CNN for text representations,0.22624434389140272,0.03333333333333333,0.13333333333333333
text-classification,4,"Let the words of a sentence of length T ( padded where necessary ) be x 1 , x 2 , ... , x T .",model,Basic CNN for text representations,0,51,4,3,0,model : Basic CNN for text representations,0.23076923076923078,0.044444444444444446,0.2
text-classification,4,"The sentence can be represented as a matrix X ? R d T , where each column represents a d-dimensional embedding of the corresponding word .",model,Basic CNN for text representations,0,52,5,4,0,model : Basic CNN for text representations,0.23529411764705882,0.05555555555555555,0.26666666666666666
text-classification,4,"The sentence can be represented as a matrix X ? R d T , where each column represents a d-dimensional embedding of the corresponding word .",model,Basic CNN for text representations,0,53,6,5,0,model : Basic CNN for text representations,0.2398190045248869,0.06666666666666667,0.3333333333333333
text-classification,4,"In the convolutional layer , a set of filters with weights W ? R Khd is convolved with every window of h words within the sentence , i.e. , {x 1:h , x 2:h+1 , . . . , x T ?h+1:T } , where K is the number of output feature maps ( and filters ) .",model,Basic CNN for text representations,0,54,7,6,0,model : Basic CNN for text representations,0.24434389140271492,0.07777777777777778,0.4
text-classification,4,"In the convolutional layer , a set of filters with weights W ? R Khd is convolved with every window of h words within the sentence , i.e. , {x 1:h , x 2:h+1 , . . . , x T ?h+1:T } , where K is the number of output feature maps ( and filters ) .",model,Basic CNN for text representations,0,55,8,7,0,model : Basic CNN for text representations,0.248868778280543,0.08888888888888889,0.4666666666666667
text-classification,4,"In this manner , feature maps p for these h-gram text fragments are generated as :",model,Basic CNN for text representations,0,56,9,8,0,model : Basic CNN for text representations,0.25339366515837103,0.1,0.5333333333333333
text-classification,4,"where i = 1 , 2 , ... , T ? h + 1 and denotes the convolution operator at the ith shift location .",model,Basic CNN for text representations,0,57,10,9,0,model : Basic CNN for text representations,0.2579185520361991,0.1111111111111111,0.6
text-classification,4,"Parameter b ? R K is the bias term and f ( ) is a non-linear function , implemented as a rectified linear unit ( ReLU ) in our experiments .",model,Basic CNN for text representations,0,58,11,10,0,model : Basic CNN for text representations,0.26244343891402716,0.12222222222222222,0.6666666666666666
text-classification,4,"Parameter b ? R K is the bias term and f ( ) is a non-linear function , implemented as a rectified linear unit ( ReLU ) in our experiments .",model,Basic CNN for text representations,0,59,12,11,0,model : Basic CNN for text representations,0.2669683257918552,0.13333333333333333,0.7333333333333333
text-classification,4,"The output feature maps of the convolutional layer , i.e. , p ? R K(T ?h+1 ) are then passed to the pooling layer , which takes the maximum value in every row of p , forming a K-dimensional vector , z.",model,Basic CNN for text representations,0,60,13,12,0,model : Basic CNN for text representations,0.27149321266968324,0.14444444444444443,0.8
text-classification,4,This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments .,model,Basic CNN for text representations,0,61,14,13,0,model : Basic CNN for text representations,0.27601809954751133,0.15555555555555556,0.8666666666666667
text-classification,4,"Moreover , the max - over - time nature of the pooling operation guarantees that the size of the obtained representation is independent of the sentence length .",model,Basic CNN for text representations,0,62,15,14,0,model : Basic CNN for text representations,0.28054298642533937,0.16666666666666666,0.9333333333333333
text-classification,4,"Note that in basic CNN sentence encoders , filter weights are the same for different inputs , which maybe suboptimal for feature extraction , especially in the case where conditional information is available .",model,Basic CNN for text representations,0,63,16,15,0,model : Basic CNN for text representations,0.2850678733031674,0.17777777777777778,1.0
text-classification,4,Learning context - sensitive filters,model,Learning context-sensitive filters,0,64,17,1,0,model : Learning context-sensitive filters,0.2895927601809955,0.18888888888888888,0.2
text-classification,4,"The proposed architecture to learn contextsensitive filters is composed of two principal modules : ( i ) a filter generation module , which produces a set of filters conditioned on the input sentence ; and ( ii ) an adaptive convolution module , which applies the generated filters to an input sentence ( this sentence maybe either the same as or different from the first input , as discussed further in Section 3.3 ) .",model,Learning context-sensitive filters,0,65,18,2,0,model : Learning context-sensitive filters,0.29411764705882354,0.2,0.4
text-classification,4,"The two modules are jointly differentiable , and the over all architecture can be trained in an end - to - end manner .",model,Learning context-sensitive filters,0,66,19,3,0,model : Learning context-sensitive filters,0.2986425339366516,0.2111111111111111,0.6
text-classification,4,"Since the generated filters are sample - specific , our ACNN feature extractor for text tends to have stronger predictive power than a basic CNN encoder .",model,Learning context-sensitive filters,0,67,20,4,0,model : Learning context-sensitive filters,0.3031674208144796,0.2222222222222222,0.8
text-classification,4,The general ACNN framework is shown schematically in .,model,Learning context-sensitive filters,0,68,21,5,0,model : Learning context-sensitive filters,0.3076923076923077,0.23333333333333334,1.0
text-classification,4,Convolution module,model,Convolution module,0,69,22,1,0,model : Convolution module,0.31221719457013575,0.24444444444444444,1.0
text-classification,4,Context - aware Filters,model,Context-aware Filters,0,70,23,1,0,model : Context-aware Filters,0.3167420814479638,0.25555555555555554,1.0
text-classification,4,Filter generation module,model,Filter generation module,0,71,24,1,0,model : Filter generation module,0.3212669683257919,0.26666666666666666,0.2
text-classification,4,"'ll go back and try other dishes Filter generation module Instead of utilizing fixed filter weights W for different inputs ( as ( 1 ) ) , our model generates a set of filters conditioned on the input sentence X. Given an input X , the filter - generation module can be implemented , in principle , as any deep ( differentiable ) architecture .",model,Filter generation module,0,72,25,2,0,model : Filter generation module,0.3257918552036199,0.2777777777777778,0.4
text-classification,4,"However , in order to handle input sentences of variable length common in natural language , we design a generic filter generation module to produce filters with a predefined size .",model,Filter generation module,0,73,26,3,0,model : Filter generation module,0.33031674208144796,0.28888888888888886,0.6
text-classification,4,"First , the input X is encapsulated into a fixedlength vector ( code ) z with the dimension of l , via a basic CNN model , where one convolutional layer is employed along with the pooling operation ( as described in Section 3.1 ) .",model,Filter generation module,0,74,27,4,0,model : Filter generation module,0.334841628959276,0.3,0.8
text-classification,4,"On top of this hidden representation z , a deconvolutional layer , which performs transposed operations of convolutions , is further applied to produce a unique set of filters for X ( as illustrated in ) :",model,Filter generation module,0,75,28,5,0,model : Filter generation module,0.3393665158371041,0.3111111111111111,1.0
text-classification,4,(,model,Extension to text sequence matching,0,76,29,1,0,model : Extension to text sequence matching,0.3438914027149321,0.32222222222222224,0.017543859649122806
text-classification,4,"where ? e and ? dare the learned parameters in each layer of the filter - generating module , respectively .",model,Extension to text sequence matching,0,77,30,2,0,model : Extension to text sequence matching,0.34841628959276016,0.3333333333333333,0.03508771929824561
text-classification,4,"where ? e and ? dare the learned parameters in each layer of the filter - generating module , respectively .",model,Extension to text sequence matching,0,78,31,3,0,model : Extension to text sequence matching,0.35294117647058826,0.34444444444444444,0.05263157894736842
text-classification,4,"where ? e and ? dare the learned parameters in each layer of the filter - generating module , respectively .",model,Extension to text sequence matching,0,79,32,4,0,model : Extension to text sequence matching,0.3574660633484163,0.35555555555555557,0.07017543859649122
text-classification,4,"Specifically , we convolve z with a filter of size ( f s , l , k x , k y ) , where f sis the number of generated filters and the kernel size is ( k x , k y ) .",model,Extension to text sequence matching,0,80,33,5,0,model : Extension to text sequence matching,0.36199095022624433,0.36666666666666664,0.08771929824561403
text-classification,4,"The output will be a tensor of shape ( f s , k x , k y ) .",model,Extension to text sequence matching,0,81,34,6,0,model : Extension to text sequence matching,0.3665158371040724,0.37777777777777777,0.10526315789473684
text-classification,4,"Since the dimension of hidden representation z is independent of input - sentence length , this framework guarantees that the generated filters are of the same shape and size for every sentence .",model,Extension to text sequence matching,0,82,35,7,0,model : Extension to text sequence matching,0.37104072398190047,0.3888888888888889,0.12280701754385964
text-classification,4,"Intuitively , the encoding part of filter generation module abstracts the information from sentence X into z .",model,Extension to text sequence matching,0,83,36,8,0,model : Extension to text sequence matching,0.3755656108597285,0.4,0.14035087719298245
text-classification,4,"Based on this representation , the deconvolutional up - sampling layer determines a set of fixedsize , fine - grained filters f for the specific input .",model,Extension to text sequence matching,0,84,37,9,0,model : Extension to text sequence matching,0.38009049773755654,0.4111111111111111,0.15789473684210525
text-classification,4,Adaptive convolution module,model,Extension to text sequence matching,0,85,38,10,0,model : Extension to text sequence matching,0.38461538461538464,0.4222222222222222,0.17543859649122806
text-classification,4,The adaptive convolution module takes as inputs the generated filters f and an input sentence .,model,Extension to text sequence matching,0,86,39,11,0,model : Extension to text sequence matching,0.3891402714932127,0.43333333333333335,0.19298245614035087
text-classification,4,This sentence and the input to the filter - generation module maybe identical ( as in ) or different ( as in .,model,Extension to text sequence matching,0,87,40,12,0,model : Extension to text sequence matching,0.3936651583710407,0.4444444444444444,0.21052631578947367
text-classification,4,"With the sample - specific filters , the input sentence is adaptively encoded , again , via a basic CNN architecture as in Section 3.1 , i.e. , one convolutional and one pooling layer .",model,Extension to text sequence matching,0,88,41,13,0,model : Extension to text sequence matching,0.39819004524886875,0.45555555555555555,0.22807017543859648
text-classification,4,"Notably , there are no additional parameters in the adaptive convolution module ( no bias term is employed ) .",model,Extension to text sequence matching,0,89,42,14,0,model : Extension to text sequence matching,0.40271493212669685,0.4666666666666667,0.24561403508771928
text-classification,4,"Our ACNN framework can be seen as a generalization of the basic CNN , which can be represented as an ACNN by setting the outputs of the filter - generation module to a constant , regardless of the contextual information from input sentence ( s ) .",model,Extension to text sequence matching,0,90,43,15,0,model : Extension to text sequence matching,0.4072398190045249,0.4777777777777778,0.2631578947368421
text-classification,4,"Because of the learning - to - learn nature of the proposed ACNN framework , it tends to have greater representational power than the basic CNN .",model,Extension to text sequence matching,0,91,44,16,0,model : Extension to text sequence matching,0.4117647058823529,0.4888888888888889,0.2807017543859649
text-classification,4,Extension to text sequence matching,model,Extension to text sequence matching,0,92,45,17,0,model : Extension to text sequence matching,0.416289592760181,0.5,0.2982456140350877
text-classification,4,"Considering the ability of our ACNN framework to generate context - sensitive filters , it can be naturally generalized to the task of text sequence matching .",model,Extension to text sequence matching,0,93,46,18,0,model : Extension to text sequence matching,0.42081447963800905,0.5111111111111111,0.3157894736842105
text-classification,4,"In this section , we will describe the proposed Adaptive Question Answering ( AdaQA ) model in the context of answer sentence selection task .",model,Extension to text sequence matching,0,94,47,19,0,model : Extension to text sequence matching,0.4253393665158371,0.5222222222222223,0.3333333333333333
text-classification,4,Note that the corresponding model can be readily adapted to other sentence matching problems as well ( see Section 5.2 ) .,model,Extension to text sequence matching,0,95,48,20,0,model : Extension to text sequence matching,0.4298642533936652,0.5333333333333333,0.3508771929824561
text-classification,4,"Given a factual question q ( associated with a list of candidate answers {a 1 , a 2 , . . . , am } and their corresponding labels y = {y 1 , y 2 , . . . , y m } ) , the goal of the model is to identify the correct answers from the set of candidates .",model,Extension to text sequence matching,0,96,49,21,0,model : Extension to text sequence matching,0.4343891402714932,0.5444444444444444,0.3684210526315789
text-classification,4,"For i = 1 , 2 , . . . , m , if a i correctly answers q , then y i = 1 , and otherwise y i =",model,Extension to text sequence matching,0,97,50,22,0,model : Extension to text sequence matching,0.43891402714932126,0.5555555555555556,0.38596491228070173
text-classification,4,". Therefore , the task can be cast as a classification problem where , given an unlabeled question - answer pair ( q i , a i ) , we seek to predict the judgement y i .",model,Extension to text sequence matching,0,98,51,23,0,model : Extension to text sequence matching,0.4434389140271493,0.5666666666666667,0.40350877192982454
text-classification,4,"Conventionally , a question q and an answer a are independently encoded by two basic CNNs to fixed - length vector representations , denoted h q and ha , respectively .",model,Extension to text sequence matching,0,99,52,24,0,model : Extension to text sequence matching,0.4479638009049774,0.5777777777777777,0.42105263157894735
text-classification,4,They are then directly employed to predict the judgement y .,model,Extension to text sequence matching,0,100,53,25,0,model : Extension to text sequence matching,0.45248868778280543,0.5888888888888889,0.43859649122807015
text-classification,4,"This strategy could be suboptimal , since no communication ( information sharing ) occurs between the questionanswer pair until the top prediction layer .",model,Extension to text sequence matching,0,101,54,26,0,model : Extension to text sequence matching,0.45701357466063347,0.6,0.45614035087719296
text-classification,4,"Intuitively , while the model is inferring the representation for a question , if the meaning of the answer is The AdaQA model can be divided into three modules : filter generation , adaptive convolution , and matching modules , as depicted schematically in .",model,Extension to text sequence matching,0,102,55,27,0,model : Extension to text sequence matching,0.46153846153846156,0.6111111111111112,0.47368421052631576
text-classification,4,"Assume there is a question - answer pair to be matched , represented by word - embedding matrices , i.e. Q ? R Tqd and A ? R Tad , where d is the embedding dimension and T q and Ta are respective sentence lengths .",model,Extension to text sequence matching,0,103,56,28,0,model : Extension to text sequence matching,0.4660633484162896,0.6222222222222222,0.49122807017543857
text-classification,4,"Assume there is a question - answer pair to be matched , represented by word - embedding matrices , i.e. Q ? R Tqd and A ? R Tad , where d is the embedding dimension and T q and Ta are respective sentence lengths .",model,Extension to text sequence matching,0,104,57,29,0,model : Extension to text sequence matching,0.47058823529411764,0.6333333333333333,0.5087719298245614
text-classification,4,"First , they are passed to two filter - generation modules , to produce two sets of filters that encapsulate features of the corresponding input sentences .",model,Extension to text sequence matching,0,105,58,30,0,model : Extension to text sequence matching,0.4751131221719457,0.6444444444444445,0.5263157894736842
text-classification,4,"Similar to the setup in Section 3.2 , we also employ a two - step process to produce the filters .",model,Extension to text sequence matching,0,106,59,31,0,model : Extension to text sequence matching,0.4796380090497738,0.6555555555555556,0.543859649122807
text-classification,4,"For a question Q , the generating process is :",model,Extension to text sequence matching,0,107,60,32,0,model : Extension to text sequence matching,0.4841628959276018,0.6666666666666666,0.5614035087719298
text-classification,4,"where CNN and DCNN denote the basic CNN unit and deconvolution layer , respectively , as discussed in Section 2.1 .",model,Extension to text sequence matching,0,108,61,33,0,model : Extension to text sequence matching,0.48868778280542985,0.6777777777777778,0.5789473684210527
text-classification,4,Parameters ? q e and ? q dare to be learned .,model,Extension to text sequence matching,0,109,62,34,0,model : Extension to text sequence matching,0.49321266968325794,0.6888888888888889,0.5964912280701754
text-classification,4,Parameters ? q e and ? q dare to be learned .,model,Extension to text sequence matching,0,110,63,35,0,model : Extension to text sequence matching,0.497737556561086,0.7,0.6140350877192983
text-classification,4,Parameters ? q e and ? q dare to be learned .,model,Extension to text sequence matching,0,111,64,36,0,model : Extension to text sequence matching,0.502262443438914,0.7111111111111111,0.631578947368421
text-classification,4,"The same process can be utilized to produce encodings z a and filters fa for the answer input , A , with parameters ? a e and ? ad , respectively .",model,Extension to text sequence matching,0,112,65,37,0,model : Extension to text sequence matching,0.5067873303167421,0.7222222222222222,0.6491228070175439
text-classification,4,"The same process can be utilized to produce encodings z a and filters fa for the answer input , A , with parameters ? a e and ? ad , respectively .",model,Extension to text sequence matching,0,113,66,38,0,model : Extension to text sequence matching,0.5113122171945701,0.7333333333333333,0.6666666666666666
text-classification,4,"The same process can be utilized to produce encodings z a and filters fa for the answer input , A , with parameters ? a e and ? ad , respectively .",model,Extension to text sequence matching,0,114,67,39,0,model : Extension to text sequence matching,0.5158371040723982,0.7444444444444445,0.6842105263157895
text-classification,4,"The two sets of filter weights are then passed to adaptive convolution modules , along with Q and A , to obtain the extracted question and answer embeddings .",model,Extension to text sequence matching,0,115,68,40,0,model : Extension to text sequence matching,0.5203619909502263,0.7555555555555555,0.7017543859649122
text-classification,4,"That is , the question embedding is convolved with the filters produced by the answer and vise versa (? q and ? a are the bias terms to be learned ) .",model,Extension to text sequence matching,0,116,69,41,0,model : Extension to text sequence matching,0.5248868778280543,0.7666666666666667,0.7192982456140351
text-classification,4,"That is , the question embedding is convolved with the filters produced by the answer and vise versa (? q and ? a are the bias terms to be learned ) .",model,Extension to text sequence matching,0,117,70,42,0,model : Extension to text sequence matching,0.5294117647058824,0.7777777777777778,0.7368421052631579
text-classification,4,The key idea is to abstract informa-tion from the answer ( or question ) that is pertinent to the corresponding question ( or answer ) .,model,Extension to text sequence matching,0,118,71,43,0,model : Extension to text sequence matching,0.5339366515837104,0.7888888888888889,0.7543859649122807
text-classification,4,"Compared to a Siamese CNN architecture , our model selectively encapsulates the most important features for judgement prediction , removing less vital information .",model,Extension to text sequence matching,0,119,72,44,0,model : Extension to text sequence matching,0.5384615384615384,0.8,0.7719298245614035
text-classification,4,"We then employ the question and answer representations h q ? Rn h , ha ? Rn h as inputs to the matching module ( where n h is the dimension of question / answer embeddings ) .",model,Extension to text sequence matching,0,120,73,45,0,model : Extension to text sequence matching,0.5429864253393665,0.8111111111111111,0.7894736842105263
text-classification,4,"We then employ the question and answer representations h q ? Rn h , ha ? Rn h as inputs to the matching module ( where n h is the dimension of question / answer embeddings ) .",model,Extension to text sequence matching,0,121,74,46,0,model : Extension to text sequence matching,0.5475113122171946,0.8222222222222222,0.8070175438596491
text-classification,4,"We then employ the question and answer representations h q ? Rn h , ha ? Rn h as inputs to the matching module ( where n h is the dimension of question / answer embeddings ) .",model,Extension to text sequence matching,0,122,75,47,0,model : Extension to text sequence matching,0.5520361990950227,0.8333333333333334,0.8245614035087719
text-classification,4,"Following , the matching function is defined as :",model,Extension to text sequence matching,0,123,76,48,0,model : Extension to text sequence matching,0.5565610859728507,0.8444444444444444,0.8421052631578947
text-classification,4,"where ? and denote an element - wise subtraction and element - wise product , respectively .",model,Extension to text sequence matching,0,124,77,49,0,model : Extension to text sequence matching,0.5610859728506787,0.8555555555555555,0.8596491228070176
text-classification,4,"where ? and denote an element - wise subtraction and element - wise product , respectively .",model,Extension to text sequence matching,0,125,78,50,0,model : Extension to text sequence matching,0.5656108597285068,0.8666666666666667,0.8771929824561403
text-classification,4,[h a ; h b ] indicates that ha and h bare stacked as column vectors .,model,Extension to text sequence matching,0,126,79,51,0,model : Extension to text sequence matching,0.5701357466063348,0.8777777777777778,0.8947368421052632
text-classification,4,"The resulting matching vector t ? R 4n h is then sent through an MLP layer ( with sigmoid activation function and parameters ? to be learned ) to model the desired conditional distribution p ( y i = 1|h q , ha ) .",model,Extension to text sequence matching,0,127,80,52,0,model : Extension to text sequence matching,0.5746606334841629,0.8888888888888888,0.9122807017543859
text-classification,4,"The resulting matching vector t ? R 4n h is then sent through an MLP layer ( with sigmoid activation function and parameters ? to be learned ) to model the desired conditional distribution p ( y i = 1|h q , ha ) .",model,Extension to text sequence matching,0,128,81,53,0,model : Extension to text sequence matching,0.579185520361991,0.9,0.9298245614035088
text-classification,4,"The resulting matching vector t ? R 4n h is then sent through an MLP layer ( with sigmoid activation function and parameters ? to be learned ) to model the desired conditional distribution p ( y i = 1|h q , ha ) .",model,Extension to text sequence matching,0,129,82,54,0,model : Extension to text sequence matching,0.583710407239819,0.9111111111111111,0.9473684210526315
text-classification,4,"Notably , we share the weights of filter generating networks for both the question and answer , so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters .",model,Extension to text sequence matching,0,130,83,55,0,model : Extension to text sequence matching,0.5882352941176471,0.9222222222222223,0.9649122807017544
text-classification,4,All three modules in AdaQA model are jointly trained end - to - end .,model,Extension to text sequence matching,0,131,84,56,0,model : Extension to text sequence matching,0.5927601809954751,0.9333333333333333,0.9824561403508771
text-classification,4,"Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks , such as paraphrase identification ( see Section 5.2 ) .",model,Extension to text sequence matching,0,132,85,57,0,model : Extension to text sequence matching,0.5972850678733032,0.9444444444444444,1.0
text-classification,4,Connections to attention mechanism,model,Connections to attention mechanism,0,133,86,1,0,model : Connections to attention mechanism,0.6018099547511312,0.9555555555555556,0.2
text-classification,4,"The adaptive context - sensitive filter generation mechanism proposed here bears close resemblance to attention mechanism ) widely adopted in the NLP community , in the sense that both methods intend to incorporate rich contextual information into text representations .",model,Connections to attention mechanism,0,134,87,2,0,model : Connections to attention mechanism,0.6063348416289592,0.9666666666666667,0.4
text-classification,4,"However , attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers , and assigns different weights to each unit according to a context vector .",model,Connections to attention mechanism,0,135,88,3,0,model : Connections to attention mechanism,0.6108597285067874,0.9777777777777777,0.6
text-classification,4,"By contrast , in our context - sensitive filter generation mechanism , the contextual information is inherently encoded into the convolutional filters , which directly interact with the input sentence during the convolution encoding operation .",model,Connections to attention mechanism,0,136,89,4,0,model : Connections to attention mechanism,0.6153846153846154,0.9888888888888889,0.8
text-classification,4,"Notably , according to our experiments , the proposed filter generation module can be readily combined with ( standard ) attention mechanisms to further enhance the modeling expressiveness of CNN encoder .",model,Connections to attention mechanism,0,137,90,5,0,model : Connections to attention mechanism,0.6199095022624435,1.0,1.0
text-classification,4,Experimental Setup,experiment,Experimental Setup,0,138,1,1,0,experiment : Experimental Setup,0.6244343891402715,1.0,1.0
text-classification,4,Datasets,dataset,dataset,0,139,1,1,0,dataset : dataset,0.6289592760180995,0.08333333333333333,0.08333333333333333
text-classification,4,We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks .,dataset,dataset,0,140,2,2,0,dataset : dataset,0.6334841628959276,0.16666666666666666,0.16666666666666666
text-classification,4,"Specifically , we consider two large - scale document classification datasets :",dataset,dataset,0,141,3,3,0,dataset : dataset,0.6380090497737556,0.25,0.25
text-classification,4,"Yelp Reviews Polarity , and DBPedia ontology datasets .",dataset,dataset,0,142,4,4,0,dataset : dataset,0.6425339366515838,0.3333333333333333,0.3333333333333333
text-classification,4,"For Yelp reviews , we seek to predict a binary label ( positive or negative ) regarding one review about a restaurant .",dataset,dataset,0,143,5,5,0,dataset : dataset,0.6470588235294118,0.4166666666666667,0.4166666666666667
text-classification,4,"DBpedia is extracted from Wikipedia by crowd - sourcing and is categorized into 14 non-overlapping ontology classes , including Company , Athlete , Natural Place , etc .",dataset,dataset,0,144,6,6,0,dataset : dataset,0.6515837104072398,0.5,0.5
text-classification,4,"We sample 15 % of the training data as the validation set , to select hyperparameters for our models and perform early stopping .",dataset,dataset,0,145,7,7,0,dataset : dataset,0.6561085972850679,0.5833333333333334,0.5833333333333334
text-classification,4,"For sentence matching , we evaluate the AdaQA model on two datasets for open - domain question answering : Wiki QA and SelQA .",dataset,dataset,0,146,8,8,0,dataset : dataset,0.6606334841628959,0.6666666666666666,0.6666666666666666
text-classification,4,"Given a question , the task is to rank the corresponding candidate answers , which , in the case of WikiQA , are sentences extracted from the summary section of a related Wikipedia article .",dataset,dataset,0,147,9,9,0,dataset : dataset,0.665158371040724,0.75,0.75
text-classification,4,"To facilitate comparison with existing results , we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset .",dataset,dataset,0,148,10,10,0,dataset : dataset,0.669683257918552,0.8333333333333334,0.8333333333333334
text-classification,4,"We also consider the task of paraphrase identification with the Quora Question Pairs dataset , with the same data splits as in .",dataset,dataset,0,149,11,11,0,dataset : dataset,0.6742081447963801,0.9166666666666666,0.9166666666666666
text-classification,4,summary of all datasets is presented in .,dataset,dataset,0,150,12,12,0,dataset : dataset,0.6787330316742082,1.0,1.0
text-classification,4,Training Details,training,training,0,151,1,1,0,training : training,0.6832579185520362,0.07142857142857142,0.07142857142857142
text-classification,4,"For the document classification experiments , we randomly initialize the word embeddings uniformly within [ ? 0.001 , 0.001 ] and update them during training .",training,training,1,152,2,2,0,training : training,0.6877828054298643,0.14285714285714285,0.14285714285714285
text-classification,4,"For the generated filters , we set the window size as h = 5 , with K = 100 feature maps ( the dimension of z is set as 100 ) .",training,training,1,153,3,3,0,training : training,0.6923076923076923,0.21428571428571427,0.21428571428571427
text-classification,4,"For direct comparison , we employ the same filter shape / size settings as in our basic CNN implementation .",training,training,0,154,4,4,0,training : training,0.6968325791855203,0.2857142857142857,0.2857142857142857
text-classification,4,"one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .",training,training,1,155,5,5,0,training : training,0.7013574660633484,0.35714285714285715,0.35714285714285715
text-classification,4,"The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .",training,training,1,156,6,6,0,training : training,0.7058823529411765,0.42857142857142855,0.42857142857142855
text-classification,4,"We observed that a larger dropout rate ( e.g. , 0.5 ) will hurt performance on document classifications and make training significantly slower .",training,training,0,157,7,7,0,training : training,0.7104072398190046,0.5,0.5
text-classification,4,"For the sentence matching tasks , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",training,training,1,158,8,8,0,training : training,0.7149321266968326,0.5714285714285714,0.5714285714285714
text-classification,4,"As for the filters , we set the window size as h = 5 , with K = 300 feature maps .",training,training,1,159,9,9,0,training : training,0.7194570135746606,0.6428571428571429,0.6428571428571429
text-classification,4,"As described in Section 3.3 , the vector t , output from the matching module , is fed to the prediction layer , implemented as a one - layer MLP followed by the sigmoid function .",training,training,0,160,10,10,0,training : training,0.7239819004524887,0.7142857142857143,0.7142857142857143
text-classification,4,"We use Adam to train the models , with a learning rate of 3 10 ?4 .",training,training,1,161,11,11,0,training : training,0.7285067873303167,0.7857142857142857,0.7857142857142857
text-classification,4,"Dropout , with a rate of 0.5 , is employed on the word embedding layer .",training,training,1,162,12,12,0,training : training,0.7330316742081447,0.8571428571428571,0.8571428571428571
text-classification,4,The hyperparameters are selected by choosing the best model on the validation set .,training,training,0,163,13,13,0,training : training,0.7375565610859729,0.9285714285714286,0.9285714285714286
text-classification,4,All models are implemented with TensorFlow and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12 GB memory .,training,training,1,164,14,14,0,training : training,0.7420814479638009,1.0,1.0
text-classification,4,Baselines,baseline,baseline,0,165,1,1,0,baseline : baseline,0.746606334841629,0.3333333333333333,0.3333333333333333
text-classification,4,"For document classification , we consider several baseline models : ( i ) ngrams , a bag - of - means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams ( up to 5 - grams ) from the training set and use their corresponding counts as features ; ( ii ) small / large word CNN : 6 layer word - based convolutional networks , with 256/1024 features at each layer , denoted as small / large , respectively ; ( iii ) deep CNN : deep convolutional neural networks with 9/17 /29 layers .",baseline,baseline,1,166,2,2,0,baseline : baseline,0.751131221719457,0.6666666666666666,0.6666666666666666
text-classification,4,"To evaluate the effectiveness of proposed AdaQA model , we compare it with several CNN - based sequence matching baselines , including Vanilla CNN , attentive pooling networks , and ABCNN ( where an attention mechanism is employed over the two sentence representations ) .",baseline,baseline,0,167,3,3,0,baseline : baseline,0.755656108597285,1.0,1.0
text-classification,4,Evaluation Metrics,evaluation,evaluation,0,168,1,1,0,evaluation : evaluation,0.7601809954751131,0.25,0.25
text-classification,4,"For document categorization and paraphrase identification tasks , we em - , are reported by , and are reported by .",evaluation,evaluation,0,169,2,2,0,evaluation : evaluation,0.7647058823529411,0.5,0.5
text-classification,4,ploy the percentage of correct predictions on the test set to evaluate and compare different models .,evaluation,evaluation,0,170,3,3,0,evaluation : evaluation,0.7692307692307693,0.75,0.75
text-classification,4,"For the answer sentence selection task , mean average precision ( MAP ) and mean reciprocal rank ( MRR ) are utilized as the corresponding evaluation metrics .",evaluation,evaluation,0,171,4,4,0,evaluation : evaluation,0.7737556561085973,1.0,1.0
text-classification,4,Experimental Results,experiment,Experimental Results,0,172,1,1,0,experiment : Experimental Results,0.7782805429864253,0.02857142857142857,1.0
text-classification,4,Document Classification,experiment,Document Classification,1,173,2,1,0,experiment : Document Classification,0.7828054298642534,0.05714285714285714,0.06666666666666667
text-classification,4,"To explicitly explore whether our ACNN model can leverage the input-aware filter weights for better sentence representation , we perform a comparison between the basic CNN and ACNN models with only a single filter , which are denoted as S - CNN , S - ACNN , respectively ( this setting may not yield best over all performance , since only a single filter is used , but it allows us to isolate the impact of adaptivity ) .",experiment,Document Classification,0,174,3,2,0,experiment : Document Classification,0.7873303167420814,0.08571428571428572,0.13333333333333333
text-classification,4,"As illustrated in , S - ACNN significantly outperforms S - CNN on both datasets , demonstrating the advantage of the filtergeneration module in our ACNN framework .",experiment,Document Classification,1,175,4,3,0,experiment : Document Classification,0.7918552036199095,0.11428571428571428,0.2
text-classification,4,"As a result , with only one convolutional filter and thus very limited modeling capacity , our S - ACNN model tends to be much more expressive than the basic CNN model , due to the flexibility of applying different filters to different sentences .",experiment,Document Classification,0,176,5,4,0,experiment : Document Classification,0.7963800904977375,0.14285714285714285,0.26666666666666666
text-classification,4,We further experiment on both ACNN and CNN models with multiple filters .,experiment,Document Classification,0,177,6,5,0,experiment : Document Classification,0.8009049773755657,0.17142857142857143,0.3333333333333333
text-classification,4,The corresponding document categorization accuracies are presented in .,experiment,Document Classification,0,178,7,6,0,experiment : Document Classification,0.8054298642533937,0.2,0.4
text-classification,4,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .",experiment,Document Classification,1,179,8,7,0,experiment : Document Classification,0.8099547511312217,0.22857142857142856,0.4666666666666667
text-classification,4,"Moreover , our method exhibits higher accuracy than n-grams , which is a very strong baseline as shown in .",experiment,Document Classification,0,180,9,8,0,experiment : Document Classification,0.8144796380090498,0.2571428571428571,0.5333333333333333
text-classification,4,We attribute the superior performance of the ACNN framework to its stronger ( adaptive ) feature - extraction ability .,experiment,Document Classification,0,181,10,9,0,experiment : Document Classification,0.8190045248868778,0.2857142857142857,0.6
text-classification,4,"Moreover , our M - ACNN also achieves slightly better performance than self - attentive sentence embeddings proposed in , which requires significant more parameters than our method .",experiment,Document Classification,1,182,11,10,0,experiment : Document Classification,0.8235294117647058,0.3142857142857143,0.6666666666666666
text-classification,4,Effect of number of filters,experiment,Document Classification,0,183,12,11,0,experiment : Document Classification,0.8280542986425339,0.34285714285714286,0.7333333333333333
text-classification,4,"To further demonstrate that the performance gains in document categorization experiments originates from the improved adaptivity of our ACNN framework , we implement the basic CNN model with different numbers of filter sizes , ranging from 1 to 1000 .",experiment,Document Classification,0,184,13,12,0,experiment : Document Classification,0.832579185520362,0.37142857142857144,0.8
text-classification,4,"As illustrated in ( a ) , when the filter size is larger than 100 , the test accuracy of the standard CNN model does not show any noticeable improvement with more filters .",experiment,Document Classification,0,185,14,13,0,experiment : Document Classification,0.8371040723981901,0.4,0.8666666666666667
text-classification,4,"More importantly , even with a filter size of 1000 , the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100 .",experiment,Document Classification,0,186,15,14,0,experiment : Document Classification,0.8416289592760181,0.42857142857142855,0.9333333333333333
text-classification,4,"Given these observations , we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature extraction of our ACNN framework .",experiment,Document Classification,0,187,16,15,0,experiment : Document Classification,0.8461538461538461,0.45714285714285713,1.0
text-classification,4,Answer Sentence Selection,experiment,Answer Sentence Selection,1,188,17,1,0,experiment : Answer Sentence Selection,0.8506787330316742,0.4857142857142857,0.05263157894736842
text-classification,4,"To elucidate the role of different parts ( modules ) in our AdaQA model , we implement several model variants for comparison : ( i ) a "" vanilla "" CNN model that independently encodes two sentence representations for matching ; ( ii ) a self - adaptive , and marked with are from .",experiment,Answer Sentence Selection,0,189,18,2,0,experiment : Answer Sentence Selection,0.8552036199095022,0.5142857142857142,0.10526315789473684
text-classification,4,"ACNN - based model where the question / answer sentence generates adaptive filters only to convolve with the input itself ; ( iii ) a one - way ACNN model where only the answer sentence representation is extracted with adaptive filters , which are generated conditioned on the question ; ( iv ) a two - way AdaQA model as described in Section 2.4 , where both sentences are adaptively encoded , with filters generated conditioned on the other sequence ; ( v ) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks ( see Section 3.4 ) , we experiment with another model variant that combines the proposed context - sensitive filter generation mechanism with the multi-perspective attention layer introduced in .",experiment,Answer Sentence Selection,0,190,19,3,0,experiment : Answer Sentence Selection,0.8597285067873304,0.5428571428571428,0.15789473684210525
text-classification,4,"show experimental results of our models on WikiQA and Se lQA datasets , along with other state - of - the - art methods .",experiment,Answer Sentence Selection,0,191,20,4,0,experiment : Answer Sentence Selection,0.8642533936651584,0.5714285714285714,0.21052631578947367
text-classification,4,"Note that the self - adaptive ACNN model variant , which generates filters only for the input itself ( without any interactions before the top matching module ) , slightly outperforms the vanilla CNN Siamese model .",experiment,Answer Sentence Selection,0,192,21,5,0,experiment : Answer Sentence Selection,0.8687782805429864,0.6,0.2631578947368421
text-classification,4,"Combined with the results in document categorization experiments , we believe that our ACNN framework , in its simplest form , can be utilized as a powerful feature extractor for transforming natural language sentences into fixed - length vectors .",experiment,Answer Sentence Selection,0,193,22,6,0,experiment : Answer Sentence Selection,0.8733031674208145,0.6285714285714286,0.3157894736842105
text-classification,4,"More importantly , our two - way AdaQA model exhibits superior results compared with the one - way variant as well as other CNN - based baseline models on the WikiQA dataset .",experiment,Answer Sentence Selection,0,194,23,7,0,experiment : Answer Sentence Selection,0.8778280542986425,0.6571428571428571,0.3684210526315789
text-classification,4,This observation indicates that the bidirectional filter gener - Model Accuracy Siamese - CNN 0.7960,experiment,Answer Sentence Selection,0,195,24,8,0,experiment : Answer Sentence Selection,0.8823529411764706,0.6857142857142857,0.42105263157894735
text-classification,4,Multi-Perspective-CNN 0 . 8138 AdaQA ( two-way ) 0.8516 AdaQA ( two-way ) + att. 0.8794 ation mechanism is strongly associated with the performance gains .,experiment,Answer Sentence Selection,0,196,25,9,0,experiment : Answer Sentence Selection,0.8868778280542986,0.7142857142857143,0.47368421052631576
text-classification,4,"While combined with the multi-perspective attention layers , adopted after the ACNN encoding layer , our two - way AdaQA model achieves even better performance .",experiment,Answer Sentence Selection,0,197,26,10,0,experiment : Answer Sentence Selection,0.8914027149321267,0.7428571428571429,0.5263157894736842
text-classification,4,"This suggests that the proposed strategy is complementary , in terms of the incorporation of rich contextual information , to the standard attention mechanism .",experiment,Answer Sentence Selection,0,198,27,11,0,experiment : Answer Sentence Selection,0.8959276018099548,0.7714285714285715,0.5789473684210527
text-classification,4,"The same trend is also observed on the SelQA dataset ( as shown in ) , which is a much larger dataset than Wiki QA .",experiment,Answer Sentence Selection,0,199,28,12,0,experiment : Answer Sentence Selection,0.9004524886877828,0.8,0.631578947368421
text-classification,4,"Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .",experiment,Answer Sentence Selection,1,200,29,13,0,experiment : Answer Sentence Selection,0.9049773755656109,0.8285714285714286,0.6842105263157895
text-classification,4,"We attribute the improvement to two potential advantages of our AdaQA model : ( i ) for the two previous baseline methods , the interaction between question and answer takes place either before or after convolution .",experiment,Answer Sentence Selection,0,201,30,14,0,experiment : Answer Sentence Selection,0.9095022624434389,0.8571428571428571,0.7368421052631579
text-classification,4,"However , in our AdaQA model , the communication between two sentences is inherent in the convolution operation , and thus can provide the abstracted features with more flexibility ; ( ii ) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer , which could enable the model to recover from initial local maxima corresponding to incorrect predictions .",experiment,Answer Sentence Selection,0,202,31,15,0,experiment : Answer Sentence Selection,0.9140271493212669,0.8857142857142857,0.7894736842105263
text-classification,4,Paragraph Identification,experiment,Answer Sentence Selection,0,203,32,16,0,experiment : Answer Sentence Selection,0.918552036199095,0.9142857142857143,0.8421052631578947
text-classification,4,"Considering that the proposed AdaQA model can be readily generalized to other text sequence matching problems , we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset .",experiment,Answer Sentence Selection,0,204,33,17,0,experiment : Answer Sentence Selection,0.9230769230769231,0.9428571428571428,0.8947368421052632
text-classification,4,"To ensure a fair comparison , we employ the same data splits as in .",experiment,Answer Sentence Selection,0,205,34,18,0,experiment : Answer Sentence Selection,0.9276018099547512,0.9714285714285714,0.9473684210526315
text-classification,4,"As illustrated in , our twoway AdaQA model again exhibits superior performances compared with basic CNN models ( as reported in ) .",experiment,Answer Sentence Selection,0,206,35,19,0,experiment : Answer Sentence Selection,0.9321266968325792,1.0,1.0
text-classification,4,Discussion,discussion,Discussion,0,207,1,1,0,discussion : Discussion,0.9366515837104072,0.14285714285714285,0.14285714285714285
text-classification,4,Reasoning ability,discussion,Discussion,0,208,2,2,0,discussion : Discussion,0.9411764705882353,0.2857142857142857,0.2857142857142857
text-classification,4,"To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model , we further categorize the questions in the WikiQA test set into 5 types containing : ' What ' , ' Where ' , ' How ' , ' When ' or ' Who ' .",discussion,Discussion,0,209,3,3,0,discussion : Discussion,0.9457013574660633,0.42857142857142855,0.42857142857142855
text-classification,4,We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types .,discussion,Discussion,0,210,4,4,0,discussion : Discussion,0.9502262443438914,0.5714285714285714,0.5714285714285714
text-classification,4,"Similar to the findings in , we observe that the ' How ' question is the hardest to answer , with the lowest MAP scores .",discussion,Discussion,0,211,5,5,0,discussion : Discussion,0.9547511312217195,0.7142857142857143,0.7142857142857143
text-classification,4,"However , our AdaQA model improves most over the basic CNN on the ' How ' type question , see ( b ) .",discussion,Discussion,0,212,6,6,0,discussion : Discussion,0.9592760180995475,0.8571428571428571,0.8571428571428571
text-classification,4,"Further comparing our results with NASM in , our AdaQA model ( with a MAP score of 0.579 ) outperforms their reported ' How ' question MAP scores ( 0.524 ) by a large margin , indicating that the adaptive convolutional filter - generation mechanism improves the model 's ability to read and reason over natural language sentences .",discussion,Discussion,0,213,7,7,0,discussion : Discussion,0.9638009049773756,1.0,1.0
text-classification,4,Filter visualization,Filter visualization,Filter visualization,0,214,1,1,0,Filter visualization : Filter visualization,0.9683257918552036,0.125,0.125
text-classification,4,"To better understand what information has been encoded into our contextsensitive filters , we visualize one of the filters for sentences within the test set ( on the DBpedia dataset ) with t- SNE .",Filter visualization,Filter visualization,0,215,2,2,0,Filter visualization : Filter visualization,0.9728506787330317,0.25,0.25
text-classification,4,The corresponding results are shown in ( c ) .,Filter visualization,Filter visualization,0,216,3,3,0,Filter visualization : Filter visualization,0.9773755656108597,0.375,0.375
text-classification,4,"It can be observed that the filters for documents with the same label ( ontology ) are grouped into clusters , indicating that for different types of document , ACNN has leveraged distinct convolutional filters for better feature extraction .",Filter visualization,Filter visualization,0,217,4,4,0,Filter visualization : Filter visualization,0.9819004524886877,0.5,0.5
text-classification,4,"We presented a context - sensitive convolutional filter - generation mechanism , introducing a meta network to adaptively produce a set of input -aware filters .",Filter visualization,Filter visualization,0,218,5,5,0,Filter visualization : Filter visualization,0.9864253393665159,0.625,0.625
text-classification,4,"In this manner , the filter weights vary from sample to sample , providing the CNN encoder network with more modeling flexibility and capacity .",Filter visualization,Filter visualization,0,219,6,6,0,Filter visualization : Filter visualization,0.9909502262443439,0.75,0.75
text-classification,4,"This framework is further generalized to model question - answer sentence pairs , leveraging a twoway feature abstraction process .",Filter visualization,Filter visualization,0,220,7,7,0,Filter visualization : Filter visualization,0.995475113122172,0.875,0.875
text-classification,4,"We evaluate our models on several document - categorization and sentence matching benchmarks , and they consistently outperform the standard CNN and attentionbased CNN baselines , demonstrating the effectiveness of our framework .",Filter visualization,Filter visualization,0,221,8,8,0,Filter visualization : Filter visualization,1.0,1.0,1.0
text-classification,5,Universal Language Model Fine - tuning for Text Classification,title,title,1,2,1,1,0,title : title,0.007936507936507936,1.0,1.0
text-classification,5,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011904761904761904,0.16666666666666666,0.16666666666666666
text-classification,5,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.015873015873015872,0.3333333333333333,0.3333333333333333
text-classification,5,"We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques thatare key for fine - tuning a language model .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01984126984126984,0.5,0.5
text-classification,5,"Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.023809523809523808,0.6666666666666666,0.6666666666666666
text-classification,5,"Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.027777777777777776,0.8333333333333334,0.8333333333333334
text-classification,5,We opensource our pretrained models and code 1 .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.031746031746031744,1.0,1.0
text-classification,5,Introduction,introduction,introduction,0,9,1,1,0,introduction : introduction,0.03571428571428571,0.037037037037037035,0.037037037037037035
text-classification,5,Inductive transfer learning has had a large impact on computer vision ( CV ) .,introduction,introduction,0,10,2,2,0,introduction : introduction,0.03968253968253968,0.07407407407407407,0.07407407407407407
text-classification,5,"Applied CV models ( including object detection , classification , and segmentation ) are rarely trained from scratch , but instead are fine - tuned from models that have been pretrained on ImageNet , MS - COCO , and other datasets .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.04365079365079365,0.1111111111111111,0.1111111111111111
text-classification,5,"Text classification is a category of Natural Language Processing ( NLP ) tasks with real - world applications such as spam , fraud , and bot detection , emergency response , and commercial document classification , such as for legal discovery .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.047619047619047616,0.14814814814814814,0.14814814814814814
text-classification,5,http://nlp.fast.ai/ulmfit.,introduction,introduction,0,13,5,5,0,introduction : introduction,0.051587301587301584,0.18518518518518517,0.18518518518518517
text-classification,5,Equal contribution .,introduction,introduction,0,14,6,6,0,introduction : introduction,0.05555555555555555,0.2222222222222222,0.2222222222222222
text-classification,5,"Jeremy focused on the algorithm development and implementation , Sebastian focused on the experiments and writing .",introduction,introduction,0,15,7,7,0,introduction : introduction,0.05952380952380952,0.25925925925925924,0.25925925925925924
text-classification,5,"While Deep Learning models have achieved state - of - the - art on many NLP tasks , these models are trained from scratch , requiring large datasets , and days to converge .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.06349206349206349,0.2962962962962963,0.2962962962962963
text-classification,5,Research in NLP focused mostly on transductive transfer .,introduction,introduction,0,17,9,9,0,introduction : introduction,0.06746031746031746,0.3333333333333333,0.3333333333333333
text-classification,5,"For inductive transfer , fine - tuning pretrained word embeddings , a simple transfer technique that only targets a model 's first layer , has had a large impact in practice and is used in most state - of - the - art models .",introduction,introduction,0,18,10,10,0,introduction : introduction,0.07142857142857142,0.37037037037037035,0.37037037037037035
text-classification,5,"Recent approaches that concatenate embeddings derived from other tasks with the input at different layers ) still train the main task model from scratch and treat pretrained embeddings as fixed parameters , limiting their usefulness .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.07539682539682539,0.4074074074074074,0.4074074074074074
text-classification,5,"In light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.07936507936507936,0.4444444444444444,0.4444444444444444
text-classification,5,"However , inductive transfer via finetuning has been unsuccessful for NLP .",introduction,introduction,0,21,13,13,0,introduction : introduction,0.08333333333333333,0.48148148148148145,0.48148148148148145
text-classification,5,"first proposed finetuning a language model ( LM ) but require millions of in - domain documents to achieve good performance , which severely limits its applicability .",introduction,introduction,0,22,14,14,0,introduction : introduction,0.0873015873015873,0.5185185185185185,0.5185185185185185
text-classification,5,We show that not the idea of LM fine - tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption .,introduction,introduction,0,23,15,15,0,introduction : introduction,0.09126984126984126,0.5555555555555556,0.5555555555555556
text-classification,5,LMs overfit to small datasets and suffered catastrophic forgetting when fine - tuned with a classifier .,introduction,introduction,0,24,16,16,0,introduction : introduction,0.09523809523809523,0.5925925925925926,0.5925925925925926
text-classification,5,"Compared to CV , NLP models are typically more shallow and thus require different fine - tuning methods .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.0992063492063492,0.6296296296296297,0.6296296296296297
text-classification,5,"We propose a new method , Universal Language Model Fine - tuning ( ULMFiT ) that addresses these issues and enables robust inductive transfer learning for any NLP task , akin to fine - tuning Image Net models :",introduction,introduction,0,26,18,18,0,introduction : introduction,0.10317460317460317,0.6666666666666666,0.6666666666666666
text-classification,5,The same 3 - layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans - fer learning approaches on six widely studied text classification tasks .,introduction,introduction,0,27,19,19,0,introduction : introduction,0.10714285714285714,0.7037037037037037,0.7037037037037037
text-classification,5,"On IMDb , with 100 labeled examples , ULMFiT matches the performance of training from scratch with 10 and - given 50 k unlabeled examples - with 100 more data .",introduction,introduction,0,28,20,20,0,introduction : introduction,0.1111111111111111,0.7407407407407407,0.7407407407407407
text-classification,5,Contributions,introduction,introduction,0,29,21,21,0,introduction : introduction,0.11507936507936507,0.7777777777777778,0.7777777777777778
text-classification,5,Our contributions are the following :,introduction,introduction,0,30,22,22,0,introduction : introduction,0.11904761904761904,0.8148148148148148,0.8148148148148148
text-classification,5,") We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .",introduction,introduction,1,31,23,23,0,introduction : introduction,0.12301587301587301,0.8518518518518519,0.8518518518518519
text-classification,5,") We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .",introduction,introduction,1,32,24,24,0,introduction : introduction,0.12698412698412698,0.8888888888888888,0.8888888888888888
text-classification,5,") We significantly outperform the state - of - the - art on six representative text classification datasets , with an error reduction of 18 - 24 % on the majority of datasets .",introduction,introduction,0,33,25,25,0,introduction : introduction,0.13095238095238096,0.9259259259259259,0.9259259259259259
text-classification,5,) We show that our method enables extremely sample - efficient transfer learning and perform an extensive ablation analysis .,introduction,introduction,0,34,26,26,0,introduction : introduction,0.1349206349206349,0.9629629629629629,0.9629629629629629
text-classification,5,) We make the pretrained models and our code available to enable wider adoption .,introduction,introduction,0,35,27,27,0,introduction : introduction,0.1388888888888889,1.0,1.0
text-classification,5,Related work,related work,Related work,0,36,1,1,0,related work : Related work,0.14285714285714285,0.0625,0.0625
text-classification,5,Transfer learning in CV Features in deep neural networks in CV have been observed to transition from general to task - specific from the first to the last layer .,related work,Related work,0,37,2,2,0,related work : Related work,0.14682539682539683,0.125,0.125
text-classification,5,"For this reason , most work in CV focuses on transferring the first layers of the model .",related work,Related work,0,38,3,3,0,related work : Related work,0.15079365079365079,0.1875,0.1875
text-classification,5,Sharif achieve state - of - theart results using features of an Image Net model as input to a simple classifier .,related work,Related work,0,39,4,4,0,related work : Related work,0.15476190476190477,0.25,0.25
text-classification,5,"In recent years , this approach has been superseded by fine - tuning either the last or several of the last layers of a pretrained model and leaving the remaining layers frozen .",related work,Related work,0,40,5,5,0,related work : Related work,0.15873015873015872,0.3125,0.3125
text-classification,5,Hypercolumns,related work,Related work,0,41,6,6,0,related work : Related work,0.1626984126984127,0.375,0.375
text-classification,5,"In NLP , only recently have methods been proposed that go beyond transferring word embeddings .",related work,Related work,0,42,7,7,0,related work : Related work,0.16666666666666666,0.4375,0.4375
text-classification,5,The prevailing approach is to pretrain embeddings that capture additional context via other tasks .,related work,Related work,0,43,8,8,0,related work : Related work,0.17063492063492064,0.5,0.5
text-classification,5,"Embeddings at different levels are then used as features , concatenated either with the word embeddings or with the inputs at intermediate layers .",related work,Related work,0,44,9,9,0,related work : Related work,0.1746031746031746,0.5625,0.5625
text-classification,5,"This method is known as hypercolumns in CV 2 and is used by , who use language modeling , paraphrasing , entailment , and Machine Translation ( MT ) respectively for pretraining .",related work,Related work,0,45,10,10,0,related work : Related work,0.17857142857142858,0.625,0.625
text-classification,5,"require engineered custom architectures , while we show state - of - the - art performance with the same basic architecture across a range of tasks .",related work,Related work,0,46,11,11,0,related work : Related work,0.18253968253968253,0.6875,0.6875
text-classification,5,"In CV , hypercolumns have been nearly entirely superseded by end - to - end fine - tuning .",related work,Related work,0,47,12,12,0,related work : Related work,0.1865079365079365,0.75,0.75
text-classification,5,Multi - task learning,related work,Related work,0,48,13,13,0,related work : Related work,0.19047619047619047,0.8125,0.8125
text-classification,5,related direction is multi-task learning ( MTL ) .,related work,Related work,0,49,14,14,0,related work : Related work,0.19444444444444445,0.875,0.875
text-classification,5,This is the approach taken by and who add a language modeling objective to the model that is trained jointly with the main task model .,related work,Related work,0,50,15,15,0,related work : Related work,0.1984126984126984,0.9375,0.9375
text-classification,5,"MTL requires the tasks to be trained from scratch every time , which makes it inefficient and often requires careful weighting of the taskspecific objective functions .",related work,Related work,0,51,16,16,0,related work : Related work,0.20238095238095238,1.0,1.0
text-classification,5,Fine - tuning,system description,Universal Language Model Fine-tuning,0,52,1,1,0,system description : Universal Language Model Fine-tuning,0.20634920634920634,0.010526315789473684,0.041666666666666664
text-classification,5,"Fine- tuning has been used successfully to transfer between similar tasks , e.g. in QA , for distantly supervised sentiment analysis , or MT domains but has been shown to fail between unrelated ones .",system description,Universal Language Model Fine-tuning,0,53,2,2,0,system description : Universal Language Model Fine-tuning,0.21031746031746032,0.021052631578947368,0.08333333333333333
text-classification,5,"also fine - tune a language model , but overfit with 10 k labeled examples and require millions of in - domain documents for good performance .",system description,Universal Language Model Fine-tuning,0,54,3,3,0,system description : Universal Language Model Fine-tuning,0.21428571428571427,0.031578947368421054,0.125
text-classification,5,"In contrast , ULMFiT leverages general - domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state - of the - art results also on small datasets .",system description,Universal Language Model Fine-tuning,0,55,4,4,0,system description : Universal Language Model Fine-tuning,0.21825396825396826,0.042105263157894736,0.16666666666666666
text-classification,5,Universal Language Model Fine- tuning,system description,Universal Language Model Fine-tuning,0,56,5,5,0,system description : Universal Language Model Fine-tuning,0.2222222222222222,0.05263157894736842,0.20833333333333334
text-classification,5,"We are interested in the most general inductive transfer learning setting for NLP ( Pan and Yang , 2010 ) :",system description,Universal Language Model Fine-tuning,0,57,6,6,0,system description : Universal Language Model Fine-tuning,0.2261904761904762,0.06315789473684211,0.25
text-classification,5,"Given a static source task T Sand any target task T T with T S = T T , we would like to improve performance on T T .",system description,Universal Language Model Fine-tuning,0,58,7,7,0,system description : Universal Language Model Fine-tuning,0.23015873015873015,0.07368421052631578,0.2916666666666667
text-classification,5,Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP :,system description,Universal Language Model Fine-tuning,0,59,8,8,0,system description : Universal Language Model Fine-tuning,0.23412698412698413,0.08421052631578947,0.3333333333333333
text-classification,5,"It captures many facets of language relevant for downstream tasks , such as long - term dependencies , hierarchical relations , and sentiment .",system description,Universal Language Model Fine-tuning,0,60,9,9,0,system description : Universal Language Model Fine-tuning,0.23809523809523808,0.09473684210526316,0.375
text-classification,5,"In contrast to tasks like and entailment , it provides data in near- unlimited quantities for most domains and languages .",system description,Universal Language Model Fine-tuning,0,61,10,10,0,system description : Universal Language Model Fine-tuning,0.24206349206349206,0.10526315789473684,0.4166666666666667
text-classification,5,"Additionally , a pretrained LM can be easily adapted to the idiosyncrasies of a target",system description,Universal Language Model Fine-tuning,0,62,11,11,0,system description : Universal Language Model Fine-tuning,0.24603174603174602,0.11578947368421053,0.4583333333333333
text-classification,5,The full LM is fine - tuned on target task data using discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( STLR ) to learn task - specific features .,system description,Universal Language Model Fine-tuning,0,63,12,12,0,system description : Universal Language Model Fine-tuning,0.25,0.12631578947368421,0.5
text-classification,5,c),system description,Universal Language Model Fine-tuning,0,64,13,13,0,system description : Universal Language Model Fine-tuning,0.25396825396825395,0.1368421052631579,0.5416666666666666
text-classification,5,"The classifier is fine - tuned on the target task using gradual unfreezing , ' Discr ' , and STLR to preserve low - level representations and adapt high - level ones ( shaded : unfreezing stages ; black : frozen ) .",system description,Universal Language Model Fine-tuning,0,65,14,14,0,system description : Universal Language Model Fine-tuning,0.25793650793650796,0.14736842105263157,0.5833333333333334
text-classification,5,"task , which we show significantly improves performance ( see Section 5 ) .",system description,Universal Language Model Fine-tuning,0,66,15,15,0,system description : Universal Language Model Fine-tuning,0.2619047619047619,0.15789473684210525,0.625
text-classification,5,"Moreover , language modeling already is a key component of existing tasks such as MT and dialogue modeling .",system description,Universal Language Model Fine-tuning,0,67,16,16,0,system description : Universal Language Model Fine-tuning,0.26587301587301587,0.16842105263157894,0.6666666666666666
text-classification,5,"Formally , language modeling induces a hypothesis space H that should be useful for many other NLP tasks .",system description,Universal Language Model Fine-tuning,0,68,17,17,0,system description : Universal Language Model Fine-tuning,0.2698412698412698,0.17894736842105263,0.7083333333333334
text-classification,5,"We propose Universal Language Model Finetuning ( ULMFiT ) , which pretrains a language model ( LM ) on a large general - domain corpus and fine - tunes it on the target task using novel techniques .",system description,Universal Language Model Fine-tuning,0,69,18,18,0,system description : Universal Language Model Fine-tuning,0.27380952380952384,0.18947368421052632,0.75
text-classification,5,The method is universal in the sense that it meets these practical criteria :,system description,Universal Language Model Fine-tuning,0,70,19,19,0,system description : Universal Language Model Fine-tuning,0.2777777777777778,0.2,0.7916666666666666
text-classification,5,") It works across tasks varying in document size , number , and label type ; 2 ) it uses a single architecture and training process ; 3 ) it requires no custom feature engineering or preprocessing ; and 4 ) it does not require additional in - domain documents or labels .",system description,Universal Language Model Fine-tuning,0,71,20,20,0,system description : Universal Language Model Fine-tuning,0.28174603174603174,0.21052631578947367,0.8333333333333334
text-classification,5,"In our experiments , we use the state - of - theart language model AWD - LSTM , a regular LSTM ( with no attention , short - cut connections , or other sophisticated additions ) with various tuned dropout hyperparameters .",system description,Universal Language Model Fine-tuning,0,72,21,21,0,system description : Universal Language Model Fine-tuning,0.2857142857142857,0.22105263157894736,0.875
text-classification,5,"Analogous to CV , we expect that downstream performance can be improved by using higherperformance language models in the future .",system description,Universal Language Model Fine-tuning,0,73,22,22,0,system description : Universal Language Model Fine-tuning,0.2896825396825397,0.23157894736842105,0.9166666666666666
text-classification,5,"ULMFiT consists of the following steps , which we show in : a) General - domain LM pretraining ( 3.1 ) ; b ) target task LM fine - tuning ( 3.2 ) ; and c ) target task classifier fine - tuning ( 3.3 ) .",system description,Universal Language Model Fine-tuning,0,74,23,23,0,system description : Universal Language Model Fine-tuning,0.29365079365079366,0.24210526315789474,0.9583333333333334
text-classification,5,We discuss these in the following sections .,system description,Universal Language Model Fine-tuning,0,75,24,24,0,system description : Universal Language Model Fine-tuning,0.2976190476190476,0.25263157894736843,1.0
text-classification,5,General - domain LM pretraining,system description,General-domain LM pretraining,0,76,25,1,0,system description : General-domain LM pretraining,0.30158730158730157,0.2631578947368421,0.16666666666666666
text-classification,5,An Image Net - like corpus for language should be large and capture general properties of language .,system description,General-domain LM pretraining,0,77,26,2,0,system description : General-domain LM pretraining,0.3055555555555556,0.2736842105263158,0.3333333333333333
text-classification,5,"We pretrain the language model on Wikitext - 103 consisting of 28,595 preprocessed Wikipedia articles and 103 million words .",system description,General-domain LM pretraining,0,78,27,3,0,system description : General-domain LM pretraining,0.30952380952380953,0.28421052631578947,0.5
text-classification,5,Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled examples .,system description,General-domain LM pretraining,0,79,28,4,0,system description : General-domain LM pretraining,0.3134920634920635,0.29473684210526313,0.6666666666666666
text-classification,5,"We leave the exploration of more diverse pretraining corpora to future work , but expect that they would boost performance .",system description,General-domain LM pretraining,0,80,29,5,0,system description : General-domain LM pretraining,0.31746031746031744,0.30526315789473685,0.8333333333333334
text-classification,5,"While this stage is the most expensive , it only needs to be performed once and improves performance and convergence of downstream models .",system description,General-domain LM pretraining,0,81,30,6,0,system description : General-domain LM pretraining,0.32142857142857145,0.3157894736842105,1.0
text-classification,5,Target task LM fine - tuning,system description,Target task LM fine-tuning,0,82,31,1,0,system description : Target task LM fine-tuning,0.3253968253968254,0.3263157894736842,0.02702702702702703
text-classification,5,"No matter how diverse the general - domain data used for pretraining is , the data of the target task will likely come from a different distribution .",system description,Target task LM fine-tuning,0,83,32,2,0,system description : Target task LM fine-tuning,0.32936507936507936,0.3368421052631579,0.05405405405405406
text-classification,5,We thus fine - tune the LM on data of the target task .,system description,Target task LM fine-tuning,0,84,33,3,0,system description : Target task LM fine-tuning,0.3333333333333333,0.3473684210526316,0.08108108108108109
text-classification,5,"Given a pretrained general - domain LM , this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data , and it allows us to train a robust LM even for small datasets .",system description,Target task LM fine-tuning,0,85,34,4,0,system description : Target task LM fine-tuning,0.3373015873015873,0.35789473684210527,0.10810810810810811
text-classification,5,"We propose discriminative fine - tuning and slanted triangular learning rates for fine - tuning the LM , which we introduce in the following .",system description,Target task LM fine-tuning,0,86,35,5,0,system description : Target task LM fine-tuning,0.3412698412698413,0.3684210526315789,0.13513513513513514
text-classification,5,Discriminative fine - tuning,system description,Target task LM fine-tuning,0,87,36,6,0,system description : Target task LM fine-tuning,0.34523809523809523,0.37894736842105264,0.16216216216216217
text-classification,5,"As different layers capture different types of information , they should be fine - tuned to different extents .",system description,Target task LM fine-tuning,0,88,37,7,0,system description : Target task LM fine-tuning,0.3492063492063492,0.3894736842105263,0.1891891891891892
text-classification,5,"To this end , we propose a novel fine - tuning method , discriminative fine - tuning 3 .",system description,Target task LM fine-tuning,0,89,38,8,0,system description : Target task LM fine-tuning,0.3531746031746032,0.4,0.21621621621621623
text-classification,5,"Instead of using the same learning rate for all layers of the model , discriminative fine - tuning allows us to tune each layer with different learning rates .",system description,Target task LM fine-tuning,0,90,39,9,0,system description : Target task LM fine-tuning,0.35714285714285715,0.4105263157894737,0.24324324324324326
text-classification,5,"For context , the regular stochastic gradient descent ( SGD ) update of a model 's parameters ? at time step t looks like the following :",system description,Target task LM fine-tuning,0,91,40,10,0,system description : Target task LM fine-tuning,0.3611111111111111,0.42105263157894735,0.2702702702702703
text-classification,5,"For context , the regular stochastic gradient descent ( SGD ) update of a model 's parameters ? at time step t looks like the following :",system description,Target task LM fine-tuning,0,92,41,11,0,system description : Target task LM fine-tuning,0.36507936507936506,0.43157894736842106,0.2972972972972973
text-classification,5,where ? is the learning rate and ? ? J ( ? ) is the gradient with regard to the model 's objective function .,system description,Target task LM fine-tuning,0,93,42,12,0,system description : Target task LM fine-tuning,0.36904761904761907,0.4421052631578947,0.32432432432432434
text-classification,5,where ? is the learning rate and ? ? J ( ? ) is the gradient with regard to the model 's objective function .,system description,Target task LM fine-tuning,0,94,43,13,0,system description : Target task LM fine-tuning,0.373015873015873,0.45263157894736844,0.35135135135135137
text-classification,5,where ? is the learning rate and ? ? J ( ? ) is the gradient with regard to the model 's objective function .,system description,Target task LM fine-tuning,0,95,44,14,0,system description : Target task LM fine-tuning,0.376984126984127,0.4631578947368421,0.3783783783783784
text-classification,5,"For discriminative fine - tuning , we split the parameters ? into {? 1 , . . . , ? L } where ? l contains the parameters of the model at the l - th layer and L is the number of layers of the model .",system description,Target task LM fine-tuning,0,96,45,15,0,system description : Target task LM fine-tuning,0.38095238095238093,0.47368421052631576,0.40540540540540543
text-classification,5,"For discriminative fine - tuning , we split the parameters ? into {? 1 , . . . , ? L } where ? l contains the parameters of the model at the l - th layer and L is the number of layers of the model .",system description,Target task LM fine-tuning,0,97,46,16,0,system description : Target task LM fine-tuning,0.38492063492063494,0.4842105263157895,0.43243243243243246
text-classification,5,"For discriminative fine - tuning , we split the parameters ? into {? 1 , . . . , ? L } where ? l contains the parameters of the model at the l - th layer and L is the number of layers of the model .",system description,Target task LM fine-tuning,0,98,47,17,0,system description : Target task LM fine-tuning,0.3888888888888889,0.49473684210526314,0.4594594594594595
text-classification,5,"For discriminative fine - tuning , we split the parameters ? into {? 1 , . . . , ? L } where ? l contains the parameters of the model at the l - th layer and L is the number of layers of the model .",system description,Target task LM fine-tuning,0,99,48,18,0,system description : Target task LM fine-tuning,0.39285714285714285,0.5052631578947369,0.4864864864864865
text-classification,5,"Similarly , we obtain {? 1 , . . . , ? L } where ? l is the learning rate of the l - th layer .",system description,Target task LM fine-tuning,0,100,49,19,0,system description : Target task LM fine-tuning,0.3968253968253968,0.5157894736842106,0.5135135135135135
text-classification,5,"Similarly , we obtain {? 1 , . . . , ? L } where ? l is the learning rate of the l - th layer .",system description,Target task LM fine-tuning,0,101,50,20,0,system description : Target task LM fine-tuning,0.4007936507936508,0.5263157894736842,0.5405405405405406
text-classification,5,"Similarly , we obtain {? 1 , . . . , ? L } where ? l is the learning rate of the l - th layer .",system description,Target task LM fine-tuning,0,102,51,21,0,system description : Target task LM fine-tuning,0.40476190476190477,0.5368421052631579,0.5675675675675675
text-classification,5,The SGD update with discriminative finetuning is then the following :,system description,Target task LM fine-tuning,0,103,52,22,0,system description : Target task LM fine-tuning,0.4087301587301587,0.5473684210526316,0.5945945945945946
text-classification,5,We empirically found it to work well to first choose the learning rate ? L of the last layer by fine - tuning only the last layer and using ? l?1 = ? l / 2.6 as the learning rate for lower layers .,system description,Target task LM fine-tuning,0,104,53,23,0,system description : Target task LM fine-tuning,0.4126984126984127,0.5578947368421052,0.6216216216216216
text-classification,5,We empirically found it to work well to first choose the learning rate ? L of the last layer by fine - tuning only the last layer and using ? l?1 = ? l / 2.6 as the learning rate for lower layers .,system description,Target task LM fine-tuning,0,105,54,24,0,system description : Target task LM fine-tuning,0.4166666666666667,0.5684210526315789,0.6486486486486487
text-classification,5,We empirically found it to work well to first choose the learning rate ? L of the last layer by fine - tuning only the last layer and using ? l?1 = ? l / 2.6 as the learning rate for lower layers .,system description,Target task LM fine-tuning,0,106,55,25,0,system description : Target task LM fine-tuning,0.42063492063492064,0.5789473684210527,0.6756756756756757
text-classification,5,Slanted triangular learning rates,system description,Target task LM fine-tuning,0,107,56,26,0,system description : Target task LM fine-tuning,0.4246031746031746,0.5894736842105263,0.7027027027027027
text-classification,5,"For adapting its parameters to task - specific features , we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters .",system description,Target task LM fine-tuning,0,108,57,27,0,system description : Target task LM fine-tuning,0.42857142857142855,0.6,0.7297297297297297
text-classification,5,Using the same learning rate ( LR ) or an annealed learning rate throughout training is not the best way to achieve this behaviour .,system description,Target task LM fine-tuning,0,109,58,28,0,system description : Target task LM fine-tuning,0.43253968253968256,0.6105263157894737,0.7567567567567568
text-classification,5,"Instead , we propose slanted triangular learning rates ( STLR ) , which first linearly increases the learning rate and then linearly decays it according to the following update schedule , which can be seen in :",system description,Target task LM fine-tuning,0,110,59,29,0,system description : Target task LM fine-tuning,0.4365079365079365,0.6210526315789474,0.7837837837837838
text-classification,5,"where T is the number of training iterations 4 , cut f rac is the fraction of iterations we increase 3 An unrelated method of the same name exists for deep Boltzmann machines",system description,Target task LM fine-tuning,0,111,60,30,0,system description : Target task LM fine-tuning,0.44047619047619047,0.631578947368421,0.8108108108108109
text-classification,5,"In other words , the number of epochs times the number of updates per epoch .",system description,Target task LM fine-tuning,0,112,61,31,0,system description : Target task LM fine-tuning,0.4444444444444444,0.6421052631578947,0.8378378378378378
text-classification,5,"the LR , cut is the iteration when we switch from increasing to decreasing the LR , p is the fraction of the number of iterations we have increased or will decrease the LR respectively , ratio specifies how much smaller the lowest LR is from the maximum LR ? max , and ? t is the learning rate at iteration t.",system description,Target task LM fine-tuning,0,113,62,32,0,system description : Target task LM fine-tuning,0.44841269841269843,0.6526315789473685,0.8648648648648649
text-classification,5,"the LR , cut is the iteration when we switch from increasing to decreasing the LR , p is the fraction of the number of iterations we have increased or will decrease the LR respectively , ratio specifies how much smaller the lowest LR is from the maximum LR ? max , and ? t is the learning rate at iteration t.",system description,Target task LM fine-tuning,0,114,63,33,0,system description : Target task LM fine-tuning,0.4523809523809524,0.6631578947368421,0.8918918918918919
text-classification,5,"We generally use cut f rac = 0.1 , ratio = 32 and ? max = 0.01 .",system description,Target task LM fine-tuning,0,115,64,34,0,system description : Target task LM fine-tuning,0.45634920634920634,0.6736842105263158,0.918918918918919
text-classification,5,"STLR modifies triangular learning rates ( Smith , 2017 ) with a short increase and along decay period , which we found key for good performance .",system description,Target task LM fine-tuning,0,116,65,35,0,system description : Target task LM fine-tuning,0.4603174603174603,0.6842105263157895,0.9459459459459459
text-classification,5,"In Section 5 , we compare against aggressive cosine annealing , a similar schedule that has recently been used to achieve state - of - the - art performance in CV .",system description,Target task LM fine-tuning,0,117,66,36,0,system description : Target task LM fine-tuning,0.4642857142857143,0.6947368421052632,0.972972972972973
text-classification,5,: The slanted triangular learning rate schedule used for ULMFiT as a function of the number of training iterations .,system description,Target task LM fine-tuning,0,118,67,37,0,system description : Target task LM fine-tuning,0.46825396825396826,0.7052631578947368,1.0
text-classification,5,Target task classifier fine - tuning,system description,Target task classifier fine-tuning,0,119,68,1,0,system description : Target task classifier fine-tuning,0.4722222222222222,0.7157894736842105,0.03571428571428571
text-classification,5,"Finally , for fine - tuning the classifier , we augment the pretrained language model with two additional linear blocks .",system description,Target task classifier fine-tuning,0,120,69,2,0,system description : Target task classifier fine-tuning,0.47619047619047616,0.7263157894736842,0.07142857142857142
text-classification,5,"Following standard practice for CV classifiers , each block uses batch normalization and dropout , with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer .",system description,Target task classifier fine-tuning,0,121,70,3,0,system description : Target task classifier fine-tuning,0.4801587301587302,0.7368421052631579,0.10714285714285714
text-classification,5,Note that the parameters in these task - specific classifier layers are the only ones thatare learned from scratch .,system description,Target task classifier fine-tuning,0,122,71,4,0,system description : Target task classifier fine-tuning,0.48412698412698413,0.7473684210526316,0.14285714285714285
text-classification,5,The first linear layer takes as the input the pooled last hidden layer states .,system description,Target task classifier fine-tuning,0,123,72,5,0,system description : Target task classifier fine-tuning,0.4880952380952381,0.7578947368421053,0.17857142857142858
text-classification,5,Concat pooling,system description,Target task classifier fine-tuning,0,124,73,6,0,system description : Target task classifier fine-tuning,0.49206349206349204,0.7684210526315789,0.21428571428571427
text-classification,5,"The signal in text classification tasks is often contained in a few words , which may occur anywhere in the document .",system description,Target task classifier fine-tuning,0,125,74,7,0,system description : Target task classifier fine-tuning,0.49603174603174605,0.7789473684210526,0.25
text-classification,5,"As input documents can consist of hundreds of words , information may get lost if we only consider the last hidden state of the model .",system description,Target task classifier fine-tuning,0,126,75,8,0,system description : Target task classifier fine-tuning,0.5,0.7894736842105263,0.2857142857142857
text-classification,5,"For this reason , we concatenate the hidden state at the last time step h T of the document with both the max - pooled and the mean - pooled representation of the hidden states over as many time steps as fit in GPU memory H = {h 1 , . . . , h T }:",system description,Target task classifier fine-tuning,0,127,76,9,0,system description : Target task classifier fine-tuning,0.503968253968254,0.8,0.32142857142857145
text-classification,5,where [ ] is concatenation .,system description,Target task classifier fine-tuning,0,128,77,10,0,system description : Target task classifier fine-tuning,0.5079365079365079,0.8105263157894737,0.35714285714285715
text-classification,5,Fine - tuning the target classifier is the most critical part of the transfer learning method .,system description,Target task classifier fine-tuning,0,129,78,11,0,system description : Target task classifier fine-tuning,0.5119047619047619,0.8210526315789474,0.39285714285714285
text-classification,5,"Overly aggressive fine - tuning will cause catastrophic forgetting , eliminating the benefit of the information captured through language modeling ; too cautious fine - tuning will lead to slow convergence ( and resultant overfitting ) .",system description,Target task classifier fine-tuning,0,130,79,12,0,system description : Target task classifier fine-tuning,0.5158730158730159,0.8315789473684211,0.42857142857142855
text-classification,5,"Besides discriminative finetuning and triangular learning rates , we propose gradual unfreezing for fine - tuning the classifier .",system description,Target task classifier fine-tuning,0,131,80,13,0,system description : Target task classifier fine-tuning,0.5198412698412699,0.8421052631578947,0.4642857142857143
text-classification,5,Gradual unfreezing,system description,Target task classifier fine-tuning,0,132,81,14,0,system description : Target task classifier fine-tuning,0.5238095238095238,0.8526315789473684,0.5
text-classification,5,"Rather than fine - tuning all layers at once , which risks catastrophic forgetting , we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge :",system description,Target task classifier fine-tuning,0,133,82,15,0,system description : Target task classifier fine-tuning,0.5277777777777778,0.8631578947368421,0.5357142857142857
text-classification,5,We first unfreeze the last layer and fine - tune all unfrozen layers for one epoch .,system description,Target task classifier fine-tuning,0,134,83,16,0,system description : Target task classifier fine-tuning,0.5317460317460317,0.8736842105263158,0.5714285714285714
text-classification,5,"We then unfreeze the next lower frozen layer and repeat , until we finetune all layers until convergence at the last iteration .",system description,Target task classifier fine-tuning,0,135,84,17,0,system description : Target task classifier fine-tuning,0.5357142857142857,0.8842105263157894,0.6071428571428571
text-classification,5,"This is similar to ' chain - thaw ' , except that we add a layer at a time to the set of ' thawed ' layers , rather than only training a single layer at a time .",system description,Target task classifier fine-tuning,0,136,85,18,0,system description : Target task classifier fine-tuning,0.5396825396825397,0.8947368421052632,0.6428571428571429
text-classification,5,"While discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing all are beneficial on their own , we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets .",system description,Target task classifier fine-tuning,0,137,86,19,0,system description : Target task classifier fine-tuning,0.5436507936507936,0.9052631578947369,0.6785714285714286
text-classification,5,BPTT for Text Classification ( BPT3C ),system description,Target task classifier fine-tuning,0,138,87,20,0,system description : Target task classifier fine-tuning,0.5476190476190477,0.9157894736842105,0.7142857142857143
text-classification,5,Language models are trained with backpropagation through time ( BPTT ) to enable gradient propagation for large input sequences .,system description,Target task classifier fine-tuning,0,139,88,21,0,system description : Target task classifier fine-tuning,0.5515873015873016,0.9263157894736842,0.75
text-classification,5,"In order to make fine - tuning a classifier for large documents feasible , we propose BPTT for Text Classification ( BPT3C ) :",system description,Target task classifier fine-tuning,0,140,89,22,0,system description : Target task classifier fine-tuning,0.5555555555555556,0.9368421052631579,0.7857142857142857
text-classification,5,We divide the document into fixedlength batches of size b.,system description,Target task classifier fine-tuning,0,141,90,23,0,system description : Target task classifier fine-tuning,0.5595238095238095,0.9473684210526315,0.8214285714285714
text-classification,5,"At the beginning of each batch , the model is initialized with the final state of the previous batch ; we keep track of the hidden states for mean and max - pooling ; gradients are back - propagated to the batches whose hidden states contributed to the final prediction .",system description,Target task classifier fine-tuning,0,142,91,24,0,system description : Target task classifier fine-tuning,0.5634920634920635,0.9578947368421052,0.8571428571428571
text-classification,5,"In practice , we use variable length backpropagation sequences .",system description,Target task classifier fine-tuning,0,143,92,25,0,system description : Target task classifier fine-tuning,0.5674603174603174,0.968421052631579,0.8928571428571429
text-classification,5,"Bidirectional language model Similar to existing work ( Peters et al. , 2017 , 2018 ) , we are not limited to fine - tuning a unidirectional language model .",system description,Target task classifier fine-tuning,0,144,93,26,0,system description : Target task classifier fine-tuning,0.5714285714285714,0.9789473684210527,0.9285714285714286
text-classification,5,"For all our experiments , we pretrain both a forward and a backward LM .",system description,Target task classifier fine-tuning,0,145,94,27,0,system description : Target task classifier fine-tuning,0.5753968253968254,0.9894736842105263,0.9642857142857143
text-classification,5,We fine - tune a classifier for each LM independently using BPT3C and average the classifier predictions .,system description,Target task classifier fine-tuning,0,146,95,28,0,system description : Target task classifier fine-tuning,0.5793650793650794,1.0,1.0
text-classification,5,Experiments,experiment,Experiments,0,147,1,1,0,experiment : Experiments,0.5833333333333334,0.3333333333333333,0.5
text-classification,5,"While our approach is equally applicable to sequence labeling tasks , we focus on text classification tasks in this work due to their important realworld applications .",experiment,Experiments,0,148,2,2,0,experiment : Experiments,0.5873015873015873,0.6666666666666666,1.0
text-classification,5,Experimental setup,experiment,Experimental setup,0,149,3,1,0,experiment : Experimental setup,0.5912698412698413,1.0,1.0
text-classification,5,Datasets and tasks,dataset,dataset,0,150,1,1,0,dataset : dataset,0.5952380952380952,0.1111111111111111,0.1111111111111111
text-classification,5,"We evaluate our method on six widely - studied datasets , with varying numbers of documents and varying document length , used by state - of - the - art text classification and transfer learning approaches as instances of three common text classification tasks : sentiment analysis , question classification , and topic classification .",dataset,dataset,0,151,2,2,0,dataset : dataset,0.5992063492063492,0.2222222222222222,0.2222222222222222
text-classification,5,We show the statistics for each dataset and task in .,dataset,dataset,0,152,3,3,0,dataset : dataset,0.6031746031746031,0.3333333333333333,0.3333333333333333
text-classification,5,TBCNN 4.0 Virtual 5.9 LSTM- CNN 3.9 ULMFiT ( ours ) 4.6 ULMFiT ( ours ) 3.6 : Test error rates ( % ) on text classification datasets used by .,dataset,dataset,0,153,4,4,0,dataset : dataset,0.6071428571428571,0.4444444444444444,0.4444444444444444
text-classification,5,Topic classification,dataset,dataset,0,154,5,5,0,dataset : dataset,0.6111111111111112,0.5555555555555556,0.5555555555555556
text-classification,5,"For topic classification , we evaluate on the large - scale AG news and DBpedia ontology datasets created by .",dataset,dataset,0,155,6,6,0,dataset : dataset,0.6150793650793651,0.6666666666666666,0.6666666666666666
text-classification,5,Pre-processing,dataset,dataset,0,156,7,7,0,dataset : dataset,0.6190476190476191,0.7777777777777778,0.7777777777777778
text-classification,5,We use the same pre-processing as in earlier work .,dataset,dataset,0,157,8,8,0,dataset : dataset,0.623015873015873,0.8888888888888888,0.8888888888888888
text-classification,5,"In addition , to allow the language model to capture aspects that might be relevant for classification , we add special tokens for upper-case words , elongation , and repetition .",dataset,dataset,0,158,9,9,0,dataset : dataset,0.626984126984127,1.0,1.0
text-classification,5,Hyperparameters,hyperparameters,Hyperparameters,0,159,1,1,0,hyperparameters : Hyperparameters,0.6309523809523809,0.1111111111111111,0.1111111111111111
text-classification,5,We are interested in a model that performs robustly across a diverse set of tasks .,hyperparameters,Hyperparameters,0,160,2,2,0,hyperparameters : Hyperparameters,0.6349206349206349,0.2222222222222222,0.2222222222222222
text-classification,5,"To this end , if not mentioned otherwise , we use the same set of hyperparameters across tasks , which we tune on the IMDb validation set .",hyperparameters,Hyperparameters,0,161,3,3,0,hyperparameters : Hyperparameters,0.6388888888888888,0.3333333333333333,0.3333333333333333
text-classification,5,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .",hyperparameters,Hyperparameters,1,162,4,4,0,hyperparameters : Hyperparameters,0.6428571428571429,0.4444444444444444,0.4444444444444444
text-classification,5,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .",hyperparameters,Hyperparameters,1,163,5,5,0,hyperparameters : Hyperparameters,0.6468253968253969,0.5555555555555556,0.5555555555555556
text-classification,5,The classifier has a hidden layer of size 50 .,hyperparameters,Hyperparameters,1,164,6,6,0,hyperparameters : Hyperparameters,0.6507936507936508,0.6666666666666666,0.6666666666666666
text-classification,5,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .",hyperparameters,Hyperparameters,1,165,7,7,0,hyperparameters : Hyperparameters,0.6547619047619048,0.7777777777777778,0.7777777777777778
text-classification,5,"We use a batch size of 64 , a base learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .",hyperparameters,Hyperparameters,1,166,8,8,0,hyperparameters : Hyperparameters,0.6587301587301587,0.8888888888888888,0.8888888888888888
text-classification,5,We otherwise use the same practices used in .,hyperparameters,Hyperparameters,0,167,9,9,0,hyperparameters : Hyperparameters,0.6626984126984127,1.0,1.0
text-classification,5,Baselines and comparison models,model,model,0,168,1,1,0,model : model,0.6666666666666666,0.25,0.25
text-classification,5,"For each task , we compare against the current state - of - theart .",model,model,0,169,2,2,0,model : model,0.6706349206349206,0.5,0.5
text-classification,5,"For the IMDb and TREC - 6 datasets , we compare against CoVe , a stateof - the - art transfer learning method for NLP .",model,model,0,170,3,3,0,model : model,0.6746031746031746,0.75,0.75
text-classification,5,"For the AG , Yelp , and DBpedia datasets , we compare against the state - of - the - art text categorization method by .",model,model,0,171,4,4,0,model : model,0.6785714285714286,1.0,1.0
text-classification,5,Results,result,Results,0,172,1,1,0,result : Results,0.6825396825396826,0.0625,0.0625
text-classification,5,"For consistency , we report all results as error rates ( lower is better ) .",result,Results,0,173,2,2,0,result : Results,0.6865079365079365,0.125,0.125
text-classification,5,We show the test error rates on the IMDb and TREC - 6 datasets used by in .,result,Results,0,174,3,3,0,result : Results,0.6904761904761905,0.1875,0.1875
text-classification,5,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .",result,Results,1,175,4,4,0,result : Results,0.6944444444444444,0.25,0.25
text-classification,5,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .",result,Results,1,176,5,5,0,result : Results,0.6984126984126984,0.3125,0.3125
text-classification,5,"This is promising as the existing stateof - the - art requires complex architectures , multiple forms of attention and sophisticated embedding schemes , while our method employs a regular LSTM with dropout .",result,Results,0,177,6,6,0,result : Results,0.7023809523809523,0.375,0.375
text-classification,5,"We note that the language model fine - tuning approach of only achieves an error of 7.64 vs. 4.6 for our method on IMDb , demonstrating the benefit of transferring knowledge from a large Image Net - like corpus using our fine - tuning techniques .",result,Results,0,178,7,7,0,result : Results,0.7063492063492064,0.4375,0.4375
text-classification,5,IMDb in particular is reflective of realworld datasets :,result,Results,0,179,8,8,0,result : Results,0.7103174603174603,0.5,0.5
text-classification,5,"It s documents are generally a few paragraphs long - similar to emails ( e.g for legal discovery ) and online comments ( e.g for community management ) ; and sentiment analysis is similar to many commercial applications , e.g. product response tracking and support email routing .",result,Results,0,180,9,9,0,result : Results,0.7142857142857143,0.5625,0.5625
text-classification,5,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .",result,Results,1,181,10,10,0,result : Results,0.7182539682539683,0.625,0.625
text-classification,5,"Nevertheless , the competitive performance on TREC - 6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences - in the case of TREC - 6to several paragraphs for IMDb .",result,Results,0,182,11,11,0,result : Results,0.7222222222222222,0.6875,0.6875
text-classification,5,"Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by , we consistently outperform their approach on both datasets .",result,Results,0,183,12,12,0,result : Results,0.7261904761904762,0.75,0.75
text-classification,5,"We show the test error rates on the larger AG , DBpedia , Yelp - bi , and Yelp - full datasets in .",result,Results,0,184,13,13,0,result : Results,0.7301587301587301,0.8125,0.8125
text-classification,5,Our method again outperforms the state - of the - art significantly .,result,Results,0,185,14,14,0,result : Results,0.7341269841269841,0.875,0.875
text-classification,5,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .",result,Results,1,186,15,15,0,result : Results,0.7380952380952381,0.9375,0.9375
text-classification,5,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .",result,Results,1,187,16,16,0,result : Results,0.7420634920634921,1.0,1.0
text-classification,5,Analysis,analysis,Analysis,0,188,1,1,0,analysis : Analysis,0.746031746031746,0.14285714285714285,0.14285714285714285
text-classification,5,"In order to assess the impact of each contribution , we perform a series of analyses and ablations .",analysis,Analysis,0,189,2,2,0,analysis : Analysis,0.75,0.2857142857142857,0.2857142857142857
text-classification,5,"We run experiments on three corpora , IMDb , TREC - 6 , and AG thatare representative of different tasks , genres , and sizes .",analysis,Analysis,0,190,3,3,0,analysis : Analysis,0.753968253968254,0.42857142857142855,0.42857142857142855
text-classification,5,"For all experiments , we split off 10 % of the training set and report error rates on this validation set with unidirectional LMs .",analysis,Analysis,0,191,4,4,0,analysis : Analysis,0.7579365079365079,0.5714285714285714,0.5714285714285714
text-classification,5,We fine - tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping .,analysis,Analysis,0,192,5,5,0,analysis : Analysis,0.7619047619047619,0.7142857142857143,0.7142857142857143
text-classification,5,Low - shot learning,analysis,Analysis,1,193,6,6,0,analysis : Analysis,0.7658730158730159,0.8571428571428571,0.8571428571428571
text-classification,5,One of the main benefits of transfer learning is being able to train a model for,analysis,Analysis,0,194,7,7,0,analysis : Analysis,0.7698412698412699,1.0,1.0
text-classification,5,Pretraining,training,Pretraining,0,195,1,1,0,training : Pretraining,0.7738095238095238,0.022222222222222223,0.1
text-classification,5,IMDb TREC - 6 AG Without pretraining 5.63 10.67 5.52 With pretraining 5.00 5.69 5.38 : Validation error rates for ULMFiT with and without pretraining .,training,Pretraining,0,196,2,2,0,training : Pretraining,0.7777777777777778,0.044444444444444446,0.2
text-classification,5,task with a small number of labels .,training,Pretraining,0,197,3,3,0,training : Pretraining,0.7817460317460317,0.06666666666666667,0.3
text-classification,5,We evaluate ULMFiT on different numbers of labeled examples in two settings : only labeled examples are used for LM fine - tuning ( 'supervised ' ) ; and all task data is available and can be used to fine - tune the LM ( ' semi-supervised ' ) .,training,Pretraining,0,198,4,4,0,training : Pretraining,0.7857142857142857,0.08888888888888889,0.4
text-classification,5,We compare ULM - FiT to training from scratch - which is necessary for hypercolumn - based approaches .,training,Pretraining,0,199,5,5,0,training : Pretraining,0.7896825396825397,0.1111111111111111,0.5
text-classification,5,"We split off balanced fractions of the training data , keep the validation set fixed , and use the same hyperparameters as before .",training,Pretraining,0,200,6,6,0,training : Pretraining,0.7936507936507936,0.13333333333333333,0.6
text-classification,5,We show the results in .,training,Pretraining,0,201,7,7,0,training : Pretraining,0.7976190476190477,0.15555555555555556,0.7
text-classification,5,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .",training,Pretraining,1,202,8,8,0,training : Pretraining,0.8015873015873016,0.17777777777777778,0.8
text-classification,5,"If we allow ULMFiT to also utilize unlabeled examples ( 50 k for IMDb , 100 k for AG ) , at 100 labeled examples , we match the performance of training from scratch with 50 and 100 more data on AG and IMDb respectively .",training,Pretraining,0,203,9,9,0,training : Pretraining,0.8055555555555556,0.2,0.9
text-classification,5,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .",training,Pretraining,1,204,10,10,0,training : Pretraining,0.8095238095238095,0.2222222222222222,1.0
text-classification,5,Impact of pretraining,training,Impact of pretraining,0,205,11,1,0,training : Impact of pretraining,0.8134920634920635,0.24444444444444444,0.125
text-classification,5,We compare using no pretraining with pretraining on WikiText - 103 in .,training,Impact of pretraining,0,206,12,2,0,training : Impact of pretraining,0.8174603174603174,0.26666666666666666,0.25
text-classification,5,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .",training,Impact of pretraining,1,207,13,3,0,training : Impact of pretraining,0.8214285714285714,0.28888888888888886,0.375
text-classification,5,"However , even for large datasets , pretraining improves performance .",training,Impact of pretraining,0,208,14,4,0,training : Impact of pretraining,0.8253968253968254,0.3111111111111111,0.5
text-classification,5,Impact of LM quality,training,Impact of pretraining,1,209,15,5,0,training : Impact of pretraining,0.8293650793650794,0.3333333333333333,0.625
text-classification,5,"In order to gauge the importance of choosing an appropriate LM , we compare a vanilla LM with the same hyperparameters without any dropout 8 with the AWD - LSTM LM with tuned dropout parameters in .",training,Impact of pretraining,0,210,16,6,0,training : Impact of pretraining,0.8333333333333334,0.35555555555555557,0.75
text-classification,5,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .",training,Impact of pretraining,1,211,17,7,0,training : Impact of pretraining,0.8373015873015873,0.37777777777777777,0.875
text-classification,5,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .",training,Impact of pretraining,1,212,18,8,0,training : Impact of pretraining,0.8412698412698413,0.4,1.0
text-classification,5,Impact of LM fine - tuning,training,Impact of LM fine-tuning,0,213,19,1,0,training : Impact of LM fine-tuning,0.8452380952380952,0.4222222222222222,0.037037037037037035
text-classification,5,"We compare no finetuning against fine - tuning the full model ( ' Full ' ) , the most commonly used fine - tuning method , with and without discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) in .",training,Impact of LM fine-tuning,0,214,20,2,0,training : Impact of LM fine-tuning,0.8492063492063492,0.4444444444444444,0.07407407407407407
text-classification,5,Fine - tuning the LM is most beneficial for larger datasets .,training,Impact of LM fine-tuning,1,215,21,3,0,training : Impact of LM fine-tuning,0.8531746031746031,0.4666666666666667,0.1111111111111111
text-classification,5,"Discr ' and ' Stlr ' improve performance across all three datasets and are necessary on the smaller TREC - 6 , where regular fine - tuning is not beneficial .",training,Impact of LM fine-tuning,0,216,22,4,0,training : Impact of LM fine-tuning,0.8571428571428571,0.4888888888888889,0.14814814814814814
text-classification,5,Impact of classifier fine - tuning,training,Impact of LM fine-tuning,0,217,23,5,0,training : Impact of LM fine-tuning,0.8611111111111112,0.5111111111111111,0.18518518518518517
text-classification,5,"We compare training from scratch , fine - tuning the full model ( ' Full ' ) , only fine - tuning the last layer ( ' Last ' ) , ' Chain - thaw ' , and gradual unfreezing ( ' Freez ' ) .",training,Impact of LM fine-tuning,0,218,24,6,0,training : Impact of LM fine-tuning,0.8650793650793651,0.5333333333333333,0.2222222222222222
text-classification,5,We furthermore assess the importance of discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) .,training,Impact of LM fine-tuning,0,219,25,7,0,training : Impact of LM fine-tuning,0.8690476190476191,0.5555555555555556,0.25925925925925924
text-classification,5,"We compare the latter to an alternative , aggressive cosine annealing schedule ( ' Cos ' ) .",training,Impact of LM fine-tuning,0,220,26,8,0,training : Impact of LM fine-tuning,0.873015873015873,0.5777777777777777,0.2962962962962963
text-classification,5,"We use a learning rate ? L = 0.01 for ' Discr ' , learning rates",training,Impact of LM fine-tuning,0,221,27,9,0,training : Impact of LM fine-tuning,0.876984126984127,0.6,0.3333333333333333
text-classification,5,8,training,Impact of LM fine-tuning,0,222,28,10,0,training : Impact of LM fine-tuning,0.8809523809523809,0.6222222222222222,0.37037037037037035
text-classification,5,"To avoid overfitting , we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier . of 0.001 and 0.0001 for the last and all other layers respectively for ' Chain - thaw ' as in , and a learning rate of 0.001 otherwise .",training,Impact of LM fine-tuning,0,223,29,11,0,training : Impact of LM fine-tuning,0.8849206349206349,0.6444444444444445,0.4074074074074074
text-classification,5,We show the results in .,training,Impact of LM fine-tuning,0,224,30,12,0,training : Impact of LM fine-tuning,0.8888888888888888,0.6666666666666666,0.4444444444444444
text-classification,5,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .",training,Impact of LM fine-tuning,1,225,31,13,0,training : Impact of LM fine-tuning,0.8928571428571429,0.6888888888888889,0.48148148148148145
text-classification,5,Freez ' provides similar performance as ' Full ' .,training,Impact of LM fine-tuning,0,226,32,14,0,training : Impact of LM fine-tuning,0.8968253968253969,0.7111111111111111,0.5185185185185185
text-classification,5,"Discr ' consistently boosts the performance of ' Full ' and ' Freez ' , except for the large AG .",training,Impact of LM fine-tuning,0,227,33,15,0,training : Impact of LM fine-tuning,0.9007936507936508,0.7333333333333333,0.5555555555555556
text-classification,5,"Cosine annealing is competitive with slanted triangular learning rates on large data , but under-performs on smaller datasets .",training,Impact of LM fine-tuning,0,228,34,16,0,training : Impact of LM fine-tuning,0.9047619047619048,0.7555555555555555,0.5925925925925926
text-classification,5,"Finally , full ULMFiT classifier fine - tuning ( bottom row ) achieves the best performance on IMDB and TREC - 6 and competitive performance on AG .",training,Impact of LM fine-tuning,0,229,35,17,0,training : Impact of LM fine-tuning,0.9087301587301587,0.7777777777777778,0.6296296296296297
text-classification,5,"Importantly , ULMFiT is the only method that shows excellent performance across the board - and is therefore the only universal method .",training,Impact of LM fine-tuning,0,230,36,18,0,training : Impact of LM fine-tuning,0.9126984126984127,0.8,0.6666666666666666
text-classification,5,Classifier fine - tuning behavior,training,Impact of LM fine-tuning,0,231,37,19,0,training : Impact of LM fine-tuning,0.9166666666666666,0.8222222222222222,0.7037037037037037
text-classification,5,"While our results demonstrate that how we fine - tune the classifier makes a significant difference , fine - tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful .",training,Impact of LM fine-tuning,0,232,38,20,0,training : Impact of LM fine-tuning,0.9206349206349206,0.8444444444444444,0.7407407407407407
text-classification,5,"To better understand the fine - tuning behavior of our model , we compare the validation error of the classifier fine - tuned with ULMFiT and ' Full ' during training in .",training,Impact of LM fine-tuning,0,233,39,21,0,training : Impact of LM fine-tuning,0.9246031746031746,0.8666666666666667,0.7777777777777778
text-classification,5,"On all datasets , fine - tuning the full model leads to the lowest error comparatively early in training , e.g. already after the first epoch on IMDb .",training,Impact of LM fine-tuning,0,234,40,22,0,training : Impact of LM fine-tuning,0.9285714285714286,0.8888888888888888,0.8148148148148148
text-classification,5,The error then increases as the model starts to overfit and knowledge captured through pretraining is lost .,training,Impact of LM fine-tuning,0,235,41,23,0,training : Impact of LM fine-tuning,0.9325396825396826,0.9111111111111111,0.8518518518518519
text-classification,5,"In contrast , ULMFiT is more stable and suffers from no such catastrophic forgetting ; performance remains similar or improves until late epochs , which shows the positive effect of the learning rate schedule .",training,Impact of LM fine-tuning,0,236,42,24,0,training : Impact of LM fine-tuning,0.9365079365079365,0.9333333333333333,0.8888888888888888
text-classification,5,Impact of bidirectionality,training,Impact of LM fine-tuning,0,237,43,25,0,training : Impact of LM fine-tuning,0.9404761904761905,0.9555555555555556,0.9259259259259259
text-classification,5,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .",training,Impact of LM fine-tuning,1,238,44,26,0,training : Impact of LM fine-tuning,0.9444444444444444,0.9777777777777777,0.9629629629629629
text-classification,5,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,training,Impact of LM fine-tuning,1,239,45,27,0,training : Impact of LM fine-tuning,0.9484126984126984,1.0,1.0
text-classification,5,Discussion and future directions,discussion,Discussion and future directions,0,240,1,1,0,discussion : Discussion and future directions,0.9523809523809523,0.125,0.125
text-classification,5,"While we have shown that ULMFiT can achieve state - of - the - art performance on widely used text classification tasks , we believe that language model fine - tuning will be particularly useful in the following settings compared to existing transfer learning approaches : a) NLP for non-English languages , where training data for supervised pretraining tasks is scarce ; b ) new NLP tasks where no state - of - the - art architecture exists ; and c) tasks with limited amounts of labeled data ( and some amounts of unlabeled data ) .",discussion,Discussion and future directions,0,241,2,2,0,discussion : Discussion and future directions,0.9563492063492064,0.25,0.25
text-classification,5,"Given that transfer learning and particularly fine - tuning for NLP is under - explored , many future directions are possible .",discussion,Discussion and future directions,0,242,3,3,0,discussion : Discussion and future directions,0.9603174603174603,0.375,0.375
text-classification,5,"One possible direction is to improve language model pretraining and fine - tuning and make them more scalable : for Image Net , predicting far fewer classes only incurs a small performance drop , while recent work shows that an alignment between source and target task label sets is important ) - focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training .",discussion,Discussion and future directions,0,243,4,4,0,discussion : Discussion and future directions,0.9642857142857143,0.5,0.5
text-classification,5,"Language modeling can also be augmented with additional tasks in a multi-task learning fashion or enriched with additional supervision , e.g. syntax - sensitive dependencies to create a model that is more general or better suited for certain downstream tasks , ideally in a weakly - supervised manner to retain its universal properties .",discussion,Discussion and future directions,0,244,5,5,0,discussion : Discussion and future directions,0.9682539682539683,0.625,0.625
text-classification,5,Another direction is to apply the method to novel tasks and models .,discussion,Discussion and future directions,0,245,6,6,0,discussion : Discussion and future directions,0.9722222222222222,0.75,0.75
text-classification,5,"While an extension to sequence labeling is straightforward , other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine - tune .",discussion,Discussion and future directions,0,246,7,7,0,discussion : Discussion and future directions,0.9761904761904762,0.875,0.875
text-classification,5,"Finally , while we have provided a series of analyses and ablations , more studies are required to better understand what knowledge a pretrained language model captures , how this changes during fine - tuning , and what information different tasks require .",discussion,Discussion and future directions,0,247,8,8,0,discussion : Discussion and future directions,0.9801587301587301,1.0,1.0
text-classification,5,Conclusion,conclusion,Conclusion,0,248,1,1,0,conclusion : Conclusion,0.9841269841269841,0.2,0.2
text-classification,5,"We have proposed ULMFiT , an effective and extremely sample - efficient transfer learning method that can be applied to any NLP task .",conclusion,Conclusion,0,249,2,2,0,conclusion : Conclusion,0.9880952380952381,0.4,0.4
text-classification,5,We have also proposed several novel fine - tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks .,conclusion,Conclusion,0,250,3,3,0,conclusion : Conclusion,0.9920634920634921,0.6,0.6
text-classification,5,Our method significantly outperformed existing transfer learning techniques and the stateof - the - art on six representative text classification tasks .,conclusion,Conclusion,0,251,4,4,0,conclusion : Conclusion,0.996031746031746,0.8,0.8
text-classification,5,We hope that our results will catalyze new developments in transfer learning for NLP .,conclusion,Conclusion,0,252,5,5,0,conclusion : Conclusion,1.0,1.0,1.0
text-classification,6,Universal Sentence Encoder,title,title,1,2,1,1,0,title : title,0.013513513513513514,1.0,1.0
text-classification,6,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.02027027027027027,0.1,0.1
text-classification,6,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.02702702702702703,0.2,0.2
text-classification,6,The models are efficient and result in accurate performance on diverse transfer tasks .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.033783783783783786,0.3,0.3
text-classification,6,Two variants of the encoding models allow for trade - offs between accuracy and compute resources .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.04054054054054054,0.4,0.4
text-classification,6,"For both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.0472972972972973,0.5,0.5
text-classification,6,Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.05405405405405406,0.6,0.6
text-classification,6,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,abstract,abstract,1,9,7,7,0,abstract : abstract,0.060810810810810814,0.7,0.7
text-classification,6,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .",abstract,abstract,1,10,8,8,0,abstract : abstract,0.06756756756756757,0.8,0.8
text-classification,6,We obtain encouraging results on Word Embedding Association Tests ( WEAT ) targeted at detecting model bias .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.07432432432432433,0.9,0.9
text-classification,6,Our pre-trained sentence encoding models are made freely available for download and on TF Hub .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.08108108108108109,1.0,1.0
text-classification,6,Introduction,introduction,introduction,0,13,1,1,0,introduction : introduction,0.08783783783783784,0.0625,0.0625
text-classification,6,Limited amounts of training data are available for many NLP tasks .,introduction,introduction,0,14,2,2,0,introduction : introduction,0.0945945945945946,0.125,0.125
text-classification,6,This presents a challenge for data hungry deep learning methods .,introduction,introduction,0,15,3,3,0,introduction : introduction,0.10135135135135136,0.1875,0.1875
text-classification,6,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .",introduction,introduction,0,16,4,4,0,introduction : introduction,0.10810810810810811,0.25,0.25
text-classification,6,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,introduction,introduction,0,17,5,5,0,introduction : introduction,0.11486486486486487,0.3125,0.3125
text-classification,6,"However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .",introduction,introduction,0,18,6,6,0,introduction : introduction,0.12162162162162163,0.375,0.375
text-classification,6,"In this paper , we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks .",introduction,introduction,0,19,7,7,0,introduction : introduction,0.12837837837837837,0.4375,0.4375
text-classification,6,We include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size .,introduction,introduction,0,20,8,8,0,introduction : introduction,0.13513513513513514,0.5,0.5
text-classification,6,We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data .,introduction,introduction,0,21,9,9,0,introduction : introduction,0.14189189189189189,0.5625,0.5625
text-classification,6,The sentence encoding models are made publicly available on TF Hub .,introduction,introduction,0,22,10,10,0,introduction : introduction,0.14864864864864866,0.625,0.625
text-classification,6,Engineering characteristics of models used for transfer learning are an important consideration .,introduction,introduction,0,23,11,11,0,introduction : introduction,0.1554054054054054,0.6875,0.6875
text-classification,6,We discuss modeling trade - offs regarding memory requirements as well as compute time on CPU and GPU .,introduction,introduction,0,24,12,12,0,introduction : introduction,0.16216216216216217,0.75,0.75
text-classification,6,Resource consumption comparisons are made for sentences of varying lengths .,introduction,introduction,0,25,13,13,0,introduction : introduction,0.16891891891891891,0.8125,0.8125
text-classification,6,"import tensorflow_hub as hub embed = hub.Module ( "" https://tfhub.dev/google/ "" "" universal- sentence - encoder / 1 "" ) embedding = embed ( [",introduction,introduction,0,26,14,14,0,introduction : introduction,0.17567567567567569,0.875,0.875
text-classification,6,"The quick brown fox jumps over the lazy dog . "" ] )",introduction,introduction,0,27,15,15,0,introduction : introduction,0.18243243243243243,0.9375,0.9375
text-classification,6,Listing 1 : Python example code for using the universal sentence encoder .,introduction,introduction,0,28,16,16,0,introduction : introduction,0.1891891891891892,1.0,1.0
text-classification,6,Model Toolkit,model,Model Toolkit,0,29,1,1,0,model : Model Toolkit,0.19594594594594594,0.03125,0.1111111111111111
text-classification,6,We make available two new models for encoding sentences into embedding vectors .,model,Model Toolkit,0,30,2,2,0,model : Model Toolkit,0.20270270270270271,0.0625,0.2222222222222222
text-classification,6,"One makes use of the transformer architecture , while the other is formulated as a deep averaging network ( DAN ) .",model,Model Toolkit,0,31,3,3,0,model : Model Toolkit,0.20945945945945946,0.09375,0.3333333333333333
text-classification,6,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,model,Model Toolkit,1,32,4,4,0,model : Model Toolkit,0.21621621621621623,0.125,0.4444444444444444
text-classification,6,The models take as input English strings and produce as output a fixed dimensional embedding representation of the string .,model,Model Toolkit,0,33,5,5,0,model : Model Toolkit,0.22297297297297297,0.15625,0.5555555555555556
text-classification,6,Listing 1 provides a minimal code snippet to convert a sentence into a tensor containing its sentence embedding .,model,Model Toolkit,0,34,6,6,0,model : Model Toolkit,0.22972972972972974,0.1875,0.6666666666666666
text-classification,6,The embedding tensor can be used directly or incorporated into larger model graphs for specific tasks .,model,Model Toolkit,0,35,7,7,0,model : Model Toolkit,0.23648648648648649,0.21875,0.7777777777777778
text-classification,6,"As illustrated in , the sentence embeddings can be trivially used to compute sentence level semantic similarity scores that achieve excellent performance on the semantic textual similarity ( STS ) Benchmark .",model,Model Toolkit,0,36,8,8,0,model : Model Toolkit,0.24324324324324326,0.25,0.8888888888888888
text-classification,6,"When included within larger models , the sentence encoding models can be fine tuned for specific tasks using gradient based updates .",model,Model Toolkit,0,37,9,9,0,model : Model Toolkit,0.25,0.28125,1.0
text-classification,6,Encoders,model,Encoders,0,38,10,1,0,model : Encoders,0.25675675675675674,0.3125,0.2
text-classification,6,We introduce the model architecture for our two encoding models in this section .,model,Encoders,0,39,11,2,0,model : Encoders,0.2635135135135135,0.34375,0.4
text-classification,6,Our two encoders have different design goals .,model,Encoders,0,40,12,3,0,model : Encoders,0.2702702702702703,0.375,0.6
text-classification,6,One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption .,model,Encoders,0,41,13,4,0,model : Encoders,0.27702702702702703,0.40625,0.8
text-classification,6,The other targets efficient inference with slightly reduced accuracy .,model,Encoders,0,42,14,5,0,model : Encoders,0.28378378378378377,0.4375,1.0
text-classification,6,Transformer,model,Transformer,0,43,15,1,0,model : Transformer,0.2905405405405405,0.46875,0.09090909090909091
text-classification,6,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,model,Transformer,1,44,16,2,0,model : Transformer,0.2972972972972973,0.5,0.18181818181818182
text-classification,6,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,model,Transformer,1,45,17,3,0,model : Transformer,0.30405405405405406,0.53125,0.2727272727272727
text-classification,6,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,model,Transformer,1,46,18,4,0,model : Transformer,0.3108108108108108,0.5625,0.36363636363636365
text-classification,6,The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding .,model,Transformer,0,47,19,5,0,model : Transformer,0.31756756756756754,0.59375,0.45454545454545453
text-classification,6,The encoding model is designed to be as general purpose as possible .,model,Transformer,1,48,20,6,0,model : Transformer,0.32432432432432434,0.625,0.5454545454545454
text-classification,6,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,model,Transformer,1,49,21,7,0,model : Transformer,0.3310810810810811,0.65625,0.6363636363636364
text-classification,6,The supported tasks include : a Skip - Thought like task for the unsupervised learning from arbitrary running text ; a conversational input - response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data .,model,Transformer,0,50,22,8,0,model : Transformer,0.33783783783783783,0.6875,0.7272727272727273
text-classification,6,The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .,model,Transformer,0,51,23,9,0,model : Transformer,0.34459459459459457,0.71875,0.8181818181818182
text-classification,6,"As will be shown in the experimental results below , the transformer based encoder achieves the best over all transfer task performance .",model,Transformer,0,52,24,10,0,model : Transformer,0.35135135135135137,0.75,0.9090909090909091
text-classification,6,"However , this comes at the cost of compute time and memory usage scaling dramatically with sentence length .",model,Transformer,0,53,25,11,0,model : Transformer,0.3581081081081081,0.78125,1.0
text-classification,6,Deep Averaging Network ( DAN ),model,Deep Averaging Network (DAN),1,54,26,1,0,model : Deep Averaging Network (DAN),0.36486486486486486,0.8125,0.14285714285714285
text-classification,6,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,model,Deep Averaging Network (DAN),1,55,27,2,0,model : Deep Averaging Network (DAN),0.3716216216216216,0.84375,0.2857142857142857
text-classification,6,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",model,Deep Averaging Network (DAN),1,56,28,3,0,model : Deep Averaging Network (DAN),0.3783783783783784,0.875,0.42857142857142855
text-classification,6,The DAN encoder is trained similarly to the Transformer based encoder .,model,Deep Averaging Network (DAN),0,57,29,4,0,model : Deep Averaging Network (DAN),0.38513513513513514,0.90625,0.5714285714285714
text-classification,6,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,model,Deep Averaging Network (DAN),1,58,30,5,0,model : Deep Averaging Network (DAN),0.3918918918918919,0.9375,0.7142857142857143
text-classification,6,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,model,Deep Averaging Network (DAN),1,59,31,6,0,model : Deep Averaging Network (DAN),0.39864864864864863,0.96875,0.8571428571428571
text-classification,6,"Similar to , our results demonstrate that DANs achieve strong baseline performance on text classification tasks .",model,Deep Averaging Network (DAN),0,60,32,7,0,model : Deep Averaging Network (DAN),0.40540540540540543,1.0,1.0
text-classification,6,Encoder Training Data,training,Encoder Training Data,0,61,1,1,0,training : Encoder Training Data,0.41216216216216217,0.047619047619047616,0.2
text-classification,6,Unsupervised training data for the sentence encoding models are drawn from a variety of web sources .,training,Encoder Training Data,0,62,2,2,0,training : Encoder Training Data,0.4189189189189189,0.09523809523809523,0.4
text-classification,6,"The sources are Wikipedia , web news , web question - answer pages and discussion forums .",training,Encoder Training Data,0,63,3,3,0,training : Encoder Training Data,0.42567567567567566,0.14285714285714285,0.6
text-classification,6,We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference ( SNLI ) corpus .,training,Encoder Training Data,0,64,4,4,0,training : Encoder Training Data,0.43243243243243246,0.19047619047619047,0.8
text-classification,6,"Similar to the findings of , we observe that training to SNLI improves transfer performance .",training,Encoder Training Data,0,65,5,5,0,training : Encoder Training Data,0.4391891891891892,0.23809523809523808,1.0
text-classification,6,Transfer Tasks,training,Transfer Tasks,0,66,6,1,0,training : Transfer Tasks,0.44594594594594594,0.2857142857142857,0.0625
text-classification,6,This section presents an overview of the data used for the transfer learning experiments and the Word Embedding Association Test ( WEAT ) data used to characterize model bias .,training,Transfer Tasks,0,67,7,2,0,training : Transfer Tasks,0.4527027027027027,0.3333333333333333,0.125
text-classification,6,"summarizes the number of samples provided by the test portion of each evaluation set and , when available , the size of the dev and training data .",training,Transfer Tasks,0,68,8,3,0,training : Transfer Tasks,0.4594594594594595,0.38095238095238093,0.1875
text-classification,6,MR : Movie review snippet sentiment on a five star scale .,training,Transfer Tasks,1,69,9,4,0,training : Transfer Tasks,0.46621621621621623,0.42857142857142855,0.25
text-classification,6,CR : Sentiment of sentences mined from customer reviews .,training,Transfer Tasks,1,70,10,5,0,training : Transfer Tasks,0.47297297297297297,0.47619047619047616,0.3125
text-classification,6,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,training,Transfer Tasks,1,71,11,6,0,training : Transfer Tasks,0.4797297297297297,0.5238095238095238,0.375
text-classification,6,MPQA : Phrase level opinion polarity from news data .,training,Transfer Tasks,1,72,12,7,0,training : Transfer Tasks,0.4864864864864865,0.5714285714285714,0.4375
text-classification,6,TREC : Fine grained question classification sourced from TREC .,training,Transfer Tasks,1,73,13,8,0,training : Transfer Tasks,0.49324324324324326,0.6190476190476191,0.5
text-classification,6,SST : Binary phrase level sentiment classification .,training,Transfer Tasks,1,74,14,9,0,training : Transfer Tasks,0.5,0.6666666666666666,0.5625
text-classification,6,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,training,Transfer Tasks,1,75,15,10,0,training : Transfer Tasks,0.5067567567567568,0.7142857142857143,0.625
text-classification,6,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,training,Transfer Tasks,1,76,16,11,0,training : Transfer Tasks,0.5135135135135135,0.7619047619047619,0.6875
text-classification,6,"For sentence classification transfer tasks , the output of the transformer and DAN sentence encoders are provided to a task specific DNN .",training,Transfer Tasks,0,77,17,12,0,training : Transfer Tasks,0.5202702702702703,0.8095238095238095,0.75
text-classification,6,"For the pairwise semantic similarity task , we directly assess the similarity of the sentence embeddings produced by our two encoders .",training,Transfer Tasks,0,78,18,13,0,training : Transfer Tasks,0.527027027027027,0.8571428571428571,0.8125
text-classification,6,"As shown Eq. 1 , we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance .",training,Transfer Tasks,0,79,19,14,0,training : Transfer Tasks,0.5337837837837838,0.9047619047619048,0.875
text-classification,6,"sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /? ( 1 )",training,Transfer Tasks,0,80,20,15,0,training : Transfer Tasks,0.5405405405405406,0.9523809523809523,0.9375
text-classification,6,"sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /? ( 1 )",training,Transfer Tasks,0,81,21,16,0,training : Transfer Tasks,0.5472972972972973,1.0,1.0
text-classification,6,Baselines,baseline,Baselines,0,82,1,1,0,baseline : Baselines,0.5540540540540541,0.16666666666666666,0.16666666666666666
text-classification,6,"For each transfer task , we include baselines that only make use of word level transfer and baselines that make use of no transfer learning at all .",baseline,Baselines,0,83,2,2,0,baseline : Baselines,0.5608108108108109,0.3333333333333333,0.3333333333333333
text-classification,6,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .",baseline,Baselines,1,84,3,3,0,baseline : Baselines,0.5675675675675675,0.5,0.5
text-classification,6,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,baseline,Baselines,1,85,4,4,0,baseline : Baselines,0.5743243243243243,0.6666666666666666,0.6666666666666666
text-classification,6,The baselines that use pretrained word embeddings allow us to contrast word versus sentence level transfer .,baseline,Baselines,0,86,5,5,0,baseline : Baselines,0.581081081081081,0.8333333333333334,0.8333333333333334
text-classification,6,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,baseline,Baselines,1,87,6,6,0,baseline : Baselines,0.5878378378378378,1.0,1.0
text-classification,6,Combined Transfer Models,model,Combined Transfer Models,0,88,1,1,0,model : Combined Transfer Models,0.5945945945945946,0.08333333333333333,0.08333333333333333
text-classification,6,We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation :,model,Combined Transfer Models,0,89,2,2,0,model : Combined Transfer Models,0.6013513513513513,0.16666666666666666,0.16666666666666666
text-classification,6,Model performance on transfer tasks .,model,Combined Transfer Models,0,90,3,3,0,model : Combined Transfer Models,0.6081081081081081,0.25,0.25
text-classification,6,USE,model,Combined Transfer Models,0,91,4,4,0,model : Combined Transfer Models,0.6148648648648649,0.3333333333333333,0.3333333333333333
text-classification,6,is the universal sentence encoder ( USE ) using Transformer .,model,Combined Transfer Models,0,92,5,5,0,model : Combined Transfer Models,0.6216216216216216,0.4166666666666667,0.4166666666666667
text-classification,6,USE,model,Combined Transfer Models,0,93,6,6,0,model : Combined Transfer Models,0.6283783783783784,0.5,0.5
text-classification,6,Dis the universal encoder DAN model .,model,Combined Transfer Models,0,94,7,7,0,model : Combined Transfer Models,0.6351351351351351,0.5833333333333334,0.5833333333333334
text-classification,6,"Models tagged with w2 v w.e. make use of pre-training word2vec skip - gram embeddings for the transfer task model , while models tagged with lrn w.e. use randomly initialized word embeddings that are learned only on the transfer task data .",model,Combined Transfer Models,0,95,8,8,0,model : Combined Transfer Models,0.6418918918918919,0.6666666666666666,0.6666666666666666
text-classification,6,Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments .,model,Combined Transfer Models,0,96,9,9,0,model : Combined Transfer Models,0.6486486486486487,0.75,0.75
text-classification,6,Pairwise similarity scores are computed directly using the sentence embeddings from the universal sentence encoder as in Eq. ( 1 ) .,model,Combined Transfer Models,0,97,10,10,0,model : Combined Transfer Models,0.6554054054054054,0.8333333333333334,0.8333333333333334
text-classification,6,to the transfer task classification layers .,model,Combined Transfer Models,0,98,11,11,0,model : Combined Transfer Models,0.6621621621621622,0.9166666666666666,0.9166666666666666
text-classification,6,"For completeness , we also explore concatenating the representations from sentence level transfer models with the baseline models that do not make use of word level transfer learning .",model,Combined Transfer Models,0,99,12,12,0,model : Combined Transfer Models,0.668918918918919,1.0,1.0
text-classification,6,Experiments,experiment,Experiments,0,100,1,1,0,experiment : Experiments,0.6756756756756757,0.1,0.1
text-classification,6,Transfer task model hyperparamaters are tuned using a combination of Vizier and light manual tuning .,experiment,Experiments,0,101,2,2,0,experiment : Experiments,0.6824324324324325,0.2,0.2
text-classification,6,"When available , model hyperparameters are tuned using task dev sets .",experiment,Experiments,0,102,3,3,0,experiment : Experiments,0.6891891891891891,0.3,0.3
text-classification,6,"Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .",experiment,Experiments,0,103,4,4,0,experiment : Experiments,0.6959459459459459,0.4,0.4
text-classification,6,Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs .,experiment,Experiments,0,104,5,5,0,experiment : Experiments,0.7027027027027027,0.5,0.5
text-classification,6,Transfer learning is critically important when training data for a target task is limited .,experiment,Experiments,0,105,6,6,0,experiment : Experiments,0.7094594594594594,0.6,0.6
text-classification,6,We explore the impact on task performance of varying the amount of training data available for the task both with and without the use of transfer learning .,experiment,Experiments,0,106,7,7,0,experiment : Experiments,0.7162162162162162,0.7,0.7
text-classification,6,"Contrasting the transformer and DAN based encoders , we demonstrate trade - offs in model complexity and the amount of data required to reach a desired level of accuracy on a task .",experiment,Experiments,0,107,8,8,0,experiment : Experiments,0.722972972972973,0.8,0.8
text-classification,6,"To assess bias in our encoding models , we evaluate the strength of various associations learned by our model on WEAT word lists .",experiment,Experiments,0,108,9,9,0,experiment : Experiments,0.7297297297297297,0.9,0.9
text-classification,6,We compare our result to those of who discovered that word embeddings could be used to reproduce human performance on implicit association tasks for both benign and potentially undesirable associations .,experiment,Experiments,0,109,10,10,0,experiment : Experiments,0.7364864864864865,1.0,1.0
text-classification,6,Results,result,Results,0,110,1,1,0,result : Results,0.7432432432432432,0.07692307692307693,0.07692307692307693
text-classification,6,Transfer task performance is summarized in Table 2 .,result,Results,0,111,2,2,0,result : Results,0.75,0.15384615384615385,0.15384615384615385
text-classification,6,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,result,Results,1,112,3,3,0,result : Results,0.7567567567567568,0.23076923076923078,0.23076923076923078
text-classification,6,"Hoewver , transfer learning using the simpler and fast DAN encoder can for some tasks perform as well or better than the more sophisticated transformer encoder .",result,Results,0,113,4,4,0,result : Results,0.7635135135135135,0.3076923076923077,0.3076923076923077
text-classification,6,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,result,Results,1,114,5,5,0,result : Results,0.7702702702702703,0.38461538461538464,0.38461538461538464
text-classification,6,The best performance on most tasks is obtained by models that make use of both sentence and word level transfer .,result,Results,0,115,6,6,0,result : Results,0.777027027027027,0.46153846153846156,0.46153846153846156
text-classification,6,illustrates transfer task performance for varying amounts of training data .,result,Results,0,116,7,7,0,result : Results,0.7837837837837838,0.5384615384615384,0.5384615384615384
text-classification,6,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .",result,Results,1,117,8,8,0,result : Results,0.7905405405405406,0.6153846153846154,0.6153846153846154
text-classification,6,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .",result,Results,1,118,9,9,0,result : Results,0.7972972972972973,0.6923076923076923,0.6923076923076923
text-classification,6,contrasts 's findings on bias within GloVe embeddings with the DAN variant of the universal encoder .,result,Results,0,119,10,10,0,result : Results,0.8040540540540541,0.7692307692307693,0.7692307692307693
text-classification,6,"Similar to GloVe , our model reproduces human associations between flowers vs. insects and pleasantness vs. unpleasantness .",result,Results,0,120,11,11,0,result : Results,0.8108108108108109,0.8461538461538461,0.8461538461538461
text-classification,6,"However , our model demonstrates weaker associations than GloVe for probes targeted at revealing at ageism , racism and sexism .",result,Results,0,121,12,12,0,result : Results,0.8175675675675675,0.9230769230769231,0.9230769230769231
text-classification,6,The differences in word association patterns can be attributed to differences in the training data composition and the mixture of tasks used to train the sentence embeddings .,result,Results,0,122,13,13,0,result : Results,0.8243243243243243,1.0,1.0
text-classification,6,Discussion,discussion,Discussion,0,123,1,1,0,discussion : Discussion,0.831081081081081,0.16666666666666666,0.16666666666666666
text-classification,6,Transfer learning leads to performance improvements on many tasks .,discussion,Discussion,0,124,2,2,0,discussion : Discussion,0.8378378378378378,0.3333333333333333,0.3333333333333333
text-classification,6,Using transfer learning is more critical when less training data is available .,discussion,Discussion,0,125,3,3,0,discussion : Discussion,0.8445945945945946,0.5,0.5
text-classification,6,"When task performance is close , the correct modeling choice should take into account engineering trade - offs regarding the memory and compute 6 Researchers and developers are strongly encouraged to independently verify whether biases in their over all model or model components impacts their use case .",discussion,Discussion,0,126,4,4,0,discussion : Discussion,0.8513513513513513,0.6666666666666666,0.6666666666666666
text-classification,6,For resources on ML fairness visit https://developers.google.com/machinelearning/fairness-overview/.,discussion,Discussion,0,127,5,5,0,discussion : Discussion,0.8581081081081081,0.8333333333333334,0.8333333333333334
text-classification,6,resource requirements introduced by the different models that could be used .,discussion,Discussion,0,128,6,6,0,discussion : Discussion,0.8648648648648649,1.0,1.0
text-classification,6,Resource Usage,Resource Usage,Resource Usage,0,129,1,1,0,Resource Usage : Resource Usage,0.8716216216216216,0.07692307692307693,0.07692307692307693
text-classification,6,This section describes memory and compute resource usage for the transformer and DAN sentence encoding models for different sentence lengths .,Resource Usage,Resource Usage,0,130,2,2,0,Resource Usage : Resource Usage,0.8783783783783784,0.15384615384615385,0.15384615384615385
text-classification,6,Figure 2 plots model resource usage against sentence length .,Resource Usage,Resource Usage,0,131,3,3,0,Resource Usage : Resource Usage,0.8851351351351351,0.23076923076923078,0.23076923076923078
text-classification,6,Compute Usage,Resource Usage,Resource Usage,0,132,4,4,0,Resource Usage : Resource Usage,0.8918918918918919,0.3076923076923077,0.3076923076923077
text-classification,6,"The transformer model time complexity is O ( n 2 ) in sentence length , while the DAN model is O ( n ) .",Resource Usage,Resource Usage,0,133,5,5,0,Resource Usage : Resource Usage,0.8986486486486487,0.38461538461538464,0.38461538461538464
text-classification,6,"As seen in ( a - b ) , for short sentences , the transformer encoding model is only moderately slower than the much simpler DAN model .",Resource Usage,Resource Usage,0,134,6,6,0,Resource Usage : Resource Usage,0.9054054054054054,0.46153846153846156,0.46153846153846156
text-classification,6,"However , compute time for transformer increases noticeably as sentence length increases .",Resource Usage,Resource Usage,0,135,7,7,0,Resource Usage : Resource Usage,0.9121621621621622,0.5384615384615384,0.5384615384615384
text-classification,6,"In contrast , the compute time for the DAN model stays nearly constant as sentence length is increased .",Resource Usage,Resource Usage,0,136,8,8,0,Resource Usage : Resource Usage,0.918918918918919,0.6153846153846154,0.6153846153846154
text-classification,6,"Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .",Resource Usage,Resource Usage,0,137,9,9,0,Resource Usage : Resource Usage,0.9256756756756757,0.6923076923076923,0.6923076923076923
text-classification,6,Memory Usage,Resource Usage,Resource Usage,0,138,10,10,0,Resource Usage : Resource Usage,0.9324324324324325,0.7692307692307693,0.7692307692307693
text-classification,6,"The transformer model space complexity also scales quadratically , O ( n 2 ) , in sentence length , while the DAN model space complexity is constant in the length of the sentence . Similar to compute usage , memory usage for the transformer model increases quickly with sentence length , while the memory usage for the DAN model remains constant .",Resource Usage,Resource Usage,0,139,11,11,0,Resource Usage : Resource Usage,0.9391891891891891,0.8461538461538461,0.8461538461538461
text-classification,6,"We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .",Resource Usage,Resource Usage,0,140,12,12,0,Resource Usage : Resource Usage,0.9459459459459459,0.9230769230769231,0.9230769230769231
text-classification,6,"Since the transformer model only needs to store unigram embeddings , for short sequences it requires nearly half as much memory as the DAN model .",Resource Usage,Resource Usage,0,141,13,13,0,Resource Usage : Resource Usage,0.9527027027027027,1.0,1.0
text-classification,6,Conclusion,conclusion,Conclusion,0,142,1,1,0,conclusion : Conclusion,0.9594594594594594,0.14285714285714285,0.14285714285714285
text-classification,6,Both the transformer and DAN based universal encoding models provide sentence level embeddings that demonstrate strong transfer performance on a number of NLP tasks .,conclusion,Conclusion,0,143,2,2,0,conclusion : Conclusion,0.9662162162162162,0.2857142857142857,0.2857142857142857
text-classification,6,The sentence level embeddings surpass the performance of transfer learning using word level embeddings alone .,conclusion,Conclusion,0,144,3,3,0,conclusion : Conclusion,0.972972972972973,0.42857142857142855,0.42857142857142855
text-classification,6,Models that make use of sentence and word level transfer achieve the best over all performance .,conclusion,Conclusion,0,145,4,4,0,conclusion : Conclusion,0.9797297297297297,0.5714285714285714,0.5714285714285714
text-classification,6,We observe that transfer learning is most helpful when limited training data is available for the transfer task .,conclusion,Conclusion,0,146,5,5,0,conclusion : Conclusion,0.9864864864864865,0.7142857142857143,0.7142857142857143
text-classification,6,The encoding models make different trade - offs regarding accuracy and model complexity that should be considered when choosing the best model for a particular application .,conclusion,Conclusion,0,147,6,6,0,conclusion : Conclusion,0.9932432432432432,0.8571428571428571,0.8571428571428571
text-classification,6,The pre-trained encoding models will be made publicly available for research and use in applications that can benefit from a better understanding of natural language .,conclusion,Conclusion,0,148,7,7,0,conclusion : Conclusion,1.0,1.0,1.0
text-classification,7,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,title,1,2,1,1,0,title : title,0.00823045267489712,1.0,1.0
text-classification,7,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.012345679012345678,0.125,0.125
text-classification,7,"In this study , we explore capsule networks with dynamic routing for text classification .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.01646090534979424,0.25,0.25
text-classification,7,"We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information or have not been successfully trained .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.0205761316872428,0.375,0.375
text-classification,7,series of experiments are conducted with capsule networks on six text classification benchmarks .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.024691358024691357,0.5,0.5
text-classification,7,"Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets , which shows the effectiveness of capsule networks for text classification .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02880658436213992,0.625,0.625
text-classification,7,We additionally show that capsule networks exhibit significant improvement when transfer single - label to multi-label text classification over the competitors .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03292181069958848,0.75,0.75
text-classification,7,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",abstract,abstract,0,9,7,7,0,abstract : abstract,0.037037037037037035,0.875,0.875
text-classification,7,Codes are publicly available at : https://github.com/andyweizhao/capsule_text_classification .,abstract,abstract,1,10,8,8,0,abstract : abstract,0.0411522633744856,1.0,1.0
text-classification,7,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.04526748971193416,0.03571428571428571,0.03571428571428571
text-classification,7,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,introduction,introduction,1,12,2,2,0,introduction : introduction,0.04938271604938271,0.07142857142857142,0.07142857142857142
text-classification,7,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.053497942386831275,0.10714285714285714,0.10714285714285714
text-classification,7,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05761316872427984,0.14285714285714285,0.14285714285714285
text-classification,7,"But it could be very difficult for a computer to predict which presidential candidate is favored by its author , or whether the author 's view in the article is more liberal or more conservative .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.06172839506172839,0.17857142857142858,0.17857142857142858
text-classification,7,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",introduction,introduction,1,16,6,6,0,introduction : introduction,0.06584362139917696,0.21428571428571427,0.21428571428571427
text-classification,7,"It is therefore not a surprise that distributed representations of words , a.k.a. word embeddings , have received great attention from NLP community addressing the question "" what "" to be modeled at the basic level .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.06995884773662552,0.25,0.25
text-classification,7,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.07407407407407407,0.2857142857142857,0.2857142857142857
text-classification,7,"common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.07818930041152264,0.32142857142857145,0.32142857142857145
text-classification,7,"Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.0823045267489712,0.35714285714285715,0.35714285714285715
text-classification,7,"Those two approaches , albeit quite different from the computational perspective , actually follow a common measure to be diagnosed regarding their answers to the "" what "" question .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.08641975308641975,0.39285714285714285,0.39285714285714285
text-classification,7,"In neural network approaches , spatial patterns aggregated at lower levels contribute to representing higher level concepts .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.09053497942386832,0.42857142857142855,0.42857142857142855
text-classification,7,"Here , they form a recursive process to articulate what to be modeled .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.09465020576131687,0.4642857142857143,0.4642857142857143
text-classification,7,"For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.09876543209876543,0.5,0.5
text-classification,7,It then hierarchically builds such pattern extraction pipelines at multiple levels .,introduction,introduction,0,25,15,15,0,introduction : introduction,0.102880658436214,0.5357142857142857,0.5357142857142857
text-classification,7,"Being a spatially sensitive model , CNN pays a price for the inefficiency of replicating feature detectors on a grid .",introduction,introduction,0,26,16,16,0,introduction : introduction,0.10699588477366255,0.5714285714285714,0.5714285714285714
text-classification,7,"As argued in , one has to choose between replicating detectors whose size grows exponentially with the number of dimensions , or increasing the volume of the labeled training set in a similar exponential way .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.1111111111111111,0.6071428571428571,0.6071428571428571
text-classification,7,"On the other hand , methods that are spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns .",introduction,introduction,0,28,18,18,0,introduction : introduction,0.11522633744855967,0.6428571428571429,0.6428571428571429
text-classification,7,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.11934156378600823,0.6785714285714286,0.6785714285714286
text-classification,7,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,introduction,introduction,0,30,20,20,0,introduction : introduction,0.12345679012345678,0.7142857142857143,0.7142857142857143
text-classification,7,recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,introduction,introduction,0,31,21,21,0,introduction : introduction,0.12757201646090535,0.75,0.75
text-classification,7,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,introduction,introduction,0,32,22,22,0,introduction : introduction,0.13168724279835392,0.7857142857142857,0.7857142857142857
text-classification,7,metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,introduction,introduction,0,33,23,23,0,introduction : introduction,0.13580246913580246,0.8214285714285714,0.8214285714285714
text-classification,7,"As an outcome , their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.13991769547325103,0.8571428571428571,0.8571428571428571
text-classification,7,"In our work , we follow a similar spirit to use this technique in modeling texts .",introduction,introduction,0,35,25,25,0,introduction : introduction,0.1440329218106996,0.8928571428571429,0.8928571428571429
text-classification,7,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",introduction,introduction,0,36,26,26,0,introduction : introduction,0.14814814814814814,0.9285714285714286,0.9285714285714286
text-classification,7,We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks .,introduction,introduction,0,37,27,27,0,introduction : introduction,0.1522633744855967,0.9642857142857143,0.9642857142857143
text-classification,7,"More importantly , we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods .",introduction,introduction,0,38,28,28,0,introduction : introduction,0.15637860082304528,1.0,1.0
text-classification,7,Our Model,model,Our Model,0,39,1,1,0,model : Our Model,0.16049382716049382,0.010638297872340425,0.2
text-classification,7,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",model,Our Model,1,40,2,2,0,model : Our Model,0.1646090534979424,0.02127659574468085,0.4
text-classification,7,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",model,Our Model,1,41,3,3,0,model : Our Model,0.16872427983539096,0.031914893617021274,0.6
text-classification,7,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",model,Our Model,1,42,4,4,0,model : Our Model,0.1728395061728395,0.0425531914893617,0.8
text-classification,7,"In the rest of this section , we elaborate the key components in detail .",model,Our Model,0,43,5,5,0,model : Our Model,0.17695473251028807,0.05319148936170213,1.0
text-classification,7,- gram Convolutional Layer,model,-gram Convolutional Layer,1,44,6,1,0,model : -gram Convolutional Layer,0.18106995884773663,0.06382978723404255,0.07142857142857142
text-classification,7,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,model,-gram Convolutional Layer,1,45,7,2,0,model : -gram Convolutional Layer,0.18518518518518517,0.07446808510638298,0.14285714285714285
text-classification,7,Suppose x ? R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,model,-gram Convolutional Layer,0,46,8,3,0,model : -gram Convolutional Layer,0.18930041152263374,0.0851063829787234,0.21428571428571427
text-classification,7,Suppose x ? R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,model,-gram Convolutional Layer,0,47,9,4,0,model : -gram Convolutional Layer,0.1934156378600823,0.09574468085106383,0.2857142857142857
text-classification,7,Let xi ? RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,model,-gram Convolutional Layer,0,48,10,5,0,model : -gram Convolutional Layer,0.19753086419753085,0.10638297872340426,0.35714285714285715
text-classification,7,Let xi ? RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,model,-gram Convolutional Layer,0,49,11,6,0,model : -gram Convolutional Layer,0.20164609053497942,0.11702127659574468,0.42857142857142855
text-classification,7,Let W a ? R K 1,model,-gram Convolutional Layer,0,50,12,7,0,model : -gram Convolutional Layer,0.205761316872428,0.1276595744680851,0.5
text-classification,7,"be the filter for the convolution operation , where K 1 is the N - gram size while sliding over a sentence for the purpose of detecting features at different positions .",model,-gram Convolutional Layer,0,51,13,8,0,model : -gram Convolutional Layer,0.20987654320987653,0.13829787234042554,0.5714285714285714
text-classification,7,"filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ? R L?K 1 + 1 , each element ma i ? R of the feature map is produced by",model,-gram Convolutional Layer,0,52,14,9,0,model : -gram Convolutional Layer,0.2139917695473251,0.14893617021276595,0.6428571428571429
text-classification,7,"filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ? R L?K 1 + 1 , each element ma i ? R of the feature map is produced by",model,-gram Convolutional Layer,0,53,15,10,0,model : -gram Convolutional Layer,0.21810699588477367,0.1595744680851064,0.7142857142857143
text-classification,7,"filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ? R L?K 1 + 1 , each element ma i ? R of the feature map is produced by",model,-gram Convolutional Layer,0,54,16,11,0,model : -gram Convolutional Layer,0.2222222222222222,0.1702127659574468,0.7857142857142857
text-classification,7,"where is element - wise multiplication , b 0 is a bias term , and f is a nonlinear activate function ( i.e. , ReLU ) .",model,-gram Convolutional Layer,0,55,17,12,0,model : -gram Convolutional Layer,0.22633744855967078,0.18085106382978725,0.8571428571428571
text-classification,7,We have described the process by which one feature is extracted from one filter .,model,-gram Convolutional Layer,0,56,18,13,0,model : -gram Convolutional Layer,0.23045267489711935,0.19148936170212766,0.9285714285714286
text-classification,7,"Hence , for a = 1 , . . . , B , totally B filters with the same N - gram size , one can generate B feature maps which can be rearranged as",model,-gram Convolutional Layer,0,57,19,14,0,model : -gram Convolutional Layer,0.2345679012345679,0.20212765957446807,1.0
text-classification,7,Primary Capsule Layer,model,Primary Capsule Layer,1,58,20,1,0,model : Primary Capsule Layer,0.23868312757201646,0.2127659574468085,0.125
text-classification,7,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,model,Primary Capsule Layer,1,59,21,2,0,model : Primary Capsule Layer,0.24279835390946503,0.22340425531914893,0.25
text-classification,7,"Suppose pi ? Rd denotes the instantiated parameters of a capsule , where d is the dimension of the capsule .",model,Primary Capsule Layer,0,60,22,3,0,model : Primary Capsule Layer,0.24691358024691357,0.23404255319148937,0.375
text-classification,7,"Suppose pi ? Rd denotes the instantiated parameters of a capsule , where d is the dimension of the capsule .",model,Primary Capsule Layer,0,61,23,4,0,model : Primary Capsule Layer,0.25102880658436216,0.24468085106382978,0.5
text-classification,7,Let W b ? R,model,Primary Capsule Layer,0,62,24,5,0,model : Primary Capsule Layer,0.2551440329218107,0.2553191489361702,0.625
text-classification,7,Bd be the filter shared in different sliding windows .,model,Primary Capsule Layer,0,63,25,6,0,model : Primary Capsule Layer,0.25925925925925924,0.26595744680851063,0.75
text-classification,7,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ? R B , then the corresponding N - gram phrases in the form of capsule are produced with",model,Primary Capsule Layer,0,64,26,7,0,model : Primary Capsule Layer,0.26337448559670784,0.2765957446808511,0.875
text-classification,7,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ? R B , then the corresponding N - gram phrases in the form of capsule are produced with",model,Primary Capsule Layer,0,65,27,8,0,model : Primary Capsule Layer,0.2674897119341564,0.2872340425531915,1.0
text-classification,7,ConvCaps Capsule,model,ConvCaps Capsule,0,66,28,1,0,model : ConvCaps Capsule,0.2716049382716049,0.2978723404255319,0.16666666666666666
text-classification,7,"Probability column - list of capsules p ? R ( L?K 1 + 1 ) d , each capsule pi ? Rd in the column - list is computed as",model,ConvCaps Capsule,0,67,29,2,0,model : ConvCaps Capsule,0.2757201646090535,0.30851063829787234,0.3333333333333333
text-classification,7,"Probability column - list of capsules p ? R ( L?K 1 + 1 ) d , each capsule pi ? Rd in the column - list is computed as",model,ConvCaps Capsule,0,68,30,3,0,model : ConvCaps Capsule,0.27983539094650206,0.3191489361702128,0.5
text-classification,7,"where g is nonlinear squash function through the entire vector , b 1 is the capsule bias term .",model,ConvCaps Capsule,0,69,31,4,0,model : ConvCaps Capsule,0.2839506172839506,0.32978723404255317,0.6666666666666666
text-classification,7,"For all C filters , the generated capsule feature maps can be rearranged as",model,ConvCaps Capsule,0,70,32,5,0,model : ConvCaps Capsule,0.2880658436213992,0.3404255319148936,0.8333333333333334
text-classification,7,where totally ( L ? K 1 + 1 ) C d-dimensional vectors are collected as capsules in P .,model,ConvCaps Capsule,0,71,33,6,0,model : ConvCaps Capsule,0.29218106995884774,0.35106382978723405,1.0
text-classification,7,Child - Parent Relationships,model,Child-Parent Relationships,0,72,34,1,0,model : Child-Parent Relationships,0.2962962962962963,0.3617021276595745,0.08333333333333333
text-classification,7,"As argued in , capsule network tries to address the representational limitation and exponential inefficiencies of convolutions with transformation matrices .",model,Child-Parent Relationships,0,73,35,2,0,model : Child-Parent Relationships,0.3004115226337449,0.3723404255319149,0.16666666666666666
text-classification,7,It allows the networks to automatically learn child - parent ( or partwhole ) relationships .,model,Child-Parent Relationships,0,74,36,3,0,model : Child-Parent Relationships,0.3045267489711934,0.3829787234042553,0.25
text-classification,7,"In text classification tasks , different sentences with the same category are supposed to have the similar topic but with different viewpoints .",model,Child-Parent Relationships,0,75,37,4,0,model : Child-Parent Relationships,0.30864197530864196,0.39361702127659576,0.3333333333333333
text-classification,7,"In this paper , we explore two different types of transformation matrices to generate prediction vector ( vote ) j|i ? Rd from it s child capsule i to the parent capsule j.",model,Child-Parent Relationships,0,76,38,5,0,model : Child-Parent Relationships,0.31275720164609055,0.40425531914893614,0.4166666666666667
text-classification,7,"In this paper , we explore two different types of transformation matrices to generate prediction vector ( vote ) j|i ? Rd from it s child capsule i to the parent capsule j.",model,Child-Parent Relationships,0,77,39,6,0,model : Child-Parent Relationships,0.3168724279835391,0.4148936170212766,0.5
text-classification,7,"The first one shares weights W t 1 ? RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",model,Child-Parent Relationships,0,78,40,7,0,model : Child-Parent Relationships,0.32098765432098764,0.425531914893617,0.5833333333333334
text-classification,7,"The first one shares weights W t 1 ? RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",model,Child-Parent Relationships,0,79,41,8,0,model : Child-Parent Relationships,0.32510288065843623,0.43617021276595747,0.6666666666666666
text-classification,7,"Formally , each corresponding vote can be computed by :",model,Child-Parent Relationships,0,80,42,9,0,model : Child-Parent Relationships,0.3292181069958848,0.44680851063829785,0.75
text-classification,7,where u i is a child - capsule in the layer below and b j|i is the capsule bias term .,model,Child-Parent Relationships,0,81,43,10,0,model : Child-Parent Relationships,0.3333333333333333,0.4574468085106383,0.8333333333333334
text-classification,7,"In the second design , we replace the shared weight matrix W t 1 j with non-shared weight matrix W t 2 i , j , where the weight matrices W t 2 ? R HN dd and H is the number of child capsules in the layer below .",model,Child-Parent Relationships,0,82,44,11,0,model : Child-Parent Relationships,0.3374485596707819,0.46808510638297873,0.9166666666666666
text-classification,7,"In the second design , we replace the shared weight matrix W t 1 j with non-shared weight matrix W t 2 i , j , where the weight matrices W t 2 ? R HN dd and H is the number of child capsules in the layer below .",model,Child-Parent Relationships,0,83,45,12,0,model : Child-Parent Relationships,0.34156378600823045,0.4787234042553192,1.0
text-classification,7,Dynamic Routing,model,Dynamic Routing,1,84,46,1,0,model : Dynamic Routing,0.345679012345679,0.48936170212765956,0.14285714285714285
text-classification,7,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,model,Dynamic Routing,1,85,47,2,0,model : Dynamic Routing,0.3497942386831276,0.5,0.2857142857142857
text-classification,7,"For each potential parent , the capsule network can increase or decrease the connection strength by dynamic routing , which is more effective than the primitive routing strategies such as max - pooling in CNN that essentially detects whether a feature is present in any position of the text , but loses spatial information about the feature .",model,Dynamic Routing,0,86,48,3,0,model : Dynamic Routing,0.35390946502057613,0.5106382978723404,0.42857142857142855
text-classification,7,We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules :,model,Dynamic Routing,0,87,49,4,0,model : Dynamic Routing,0.35802469135802467,0.5212765957446809,0.5714285714285714
text-classification,7,Orphan Category,model,Dynamic Routing,0,88,50,5,0,model : Dynamic Routing,0.36213991769547327,0.5319148936170213,0.7142857142857143
text-classification,7,"Inspired by , an additional "" orphan "" category is added to the network , which can capture the "" background "" information of the text such as stop words and the words that are unrelated to specific categories , helping the capsule network model the child - parent relationship more efficiently .",model,Dynamic Routing,0,89,51,6,0,model : Dynamic Routing,0.3662551440329218,0.5425531914893617,0.8571428571428571
text-classification,7,"Adding "" orphan "" category in the text is more effective than in image since there is no single consistent "" background "" object in images , while the stop words are consistent in texts such as predicate "" s "" , "" am "" and pronouns "" his "" , "" she "" .",model,Dynamic Routing,0,90,52,7,0,model : Dynamic Routing,0.37037037037037035,0.5531914893617021,1.0
text-classification,7,Leaky - Softmax,model,Leaky-Softmax,0,91,53,1,0,model : Leaky-Softmax,0.37448559670781895,0.5638297872340425,0.3333333333333333
text-classification,7,We explore Leaky - Softmax in the place of standard softmax while updating connection strength between the children capsules and their parents .,model,Leaky-Softmax,0,92,54,2,0,model : Leaky-Softmax,0.3786008230452675,0.574468085106383,0.6666666666666666
text-classification,7,"Despite the orphan category in the last capsule layer , we also need a light - weight method between two consecutive layers to route the noise child capsules to extra dimension without any additional parameters and computation consuming .",model,Leaky-Softmax,0,93,55,3,0,model : Leaky-Softmax,0.38271604938271603,0.5851063829787234,1.0
text-classification,7,Coefficients Amendment,model,Coefficients Amendment,0,94,56,1,0,model : Coefficients Amendment,0.3868312757201646,0.5957446808510638,0.07692307692307693
text-classification,7,We also attempt to use the probability of existence of child capsules in the layer below to iteratively amend the connection strength as Eq.6 .,model,Coefficients Amendment,0,95,57,2,0,model : Coefficients Amendment,0.39094650205761317,0.6063829787234043,0.15384615384615385
text-classification,7,"Algorithm 1 : Dynamic Routing Algorithm 1 procedure ROUTING ( j|i , j|i , r , l ) 2 Initialize the logits of coupling coefficients b j|i = 0 3 for r iterations do 4 for all capsule i in layer land capsule j in layer l + 1 :",model,Coefficients Amendment,0,96,58,3,0,model : Coefficients Amendment,0.3950617283950617,0.6170212765957447,0.23076923076923078
text-classification,7,for all capsule i in layer land capsule j in,model,Coefficients Amendment,0,97,59,4,0,model : Coefficients Amendment,0.3991769547325103,0.6276595744680851,0.3076923076923077
text-classification,7,"Given each prediction vector j|i and its probability of existence j|i , where j|i = i , each iterative coupling coefficient of connection strength c j|i is updated by",model,Coefficients Amendment,0,98,60,5,0,model : Coefficients Amendment,0.40329218106995884,0.6382978723404256,0.38461538461538464
text-classification,7,where b j|i is the logits of coupling coefficients .,model,Coefficients Amendment,0,99,61,6,0,model : Coefficients Amendment,0.4074074074074074,0.648936170212766,0.46153846153846156
text-classification,7,Each parent capsule v j in the layer above is a weighted sum over all prediction vectors j|i :,model,Coefficients Amendment,0,100,62,7,0,model : Coefficients Amendment,0.411522633744856,0.6595744680851063,0.5384615384615384
text-classification,7,"where a j is the probabilities of parent capsules , g is nonlinear squash function through the entire vector .",model,Coefficients Amendment,0,101,63,8,0,model : Coefficients Amendment,0.4156378600823045,0.6702127659574468,0.6153846153846154
text-classification,7,"Once all of the parent capsules are produced , each coupling coefficient b j|i is updated by :",model,Coefficients Amendment,0,102,64,9,0,model : Coefficients Amendment,0.41975308641975306,0.6808510638297872,0.6923076923076923
text-classification,7,"For simplicity of notation , the parent capsules and their probabilities in the layer above are denoted as v , a = Routing ( )",model,Coefficients Amendment,0,103,65,10,0,model : Coefficients Amendment,0.42386831275720166,0.6914893617021277,0.7692307692307693
text-classification,7,"where denotes all of the child capsules in the layer below , v denotes all of the parent - capsules and their probabilities a.",model,Coefficients Amendment,0,104,66,11,0,model : Coefficients Amendment,0.4279835390946502,0.7021276595744681,0.8461538461538461
text-classification,7,Our dynamic routing algorithm is summarized in Algorithm,model,Coefficients Amendment,0,105,67,12,0,model : Coefficients Amendment,0.43209876543209874,0.7127659574468085,0.9230769230769231
text-classification,7,1 .,model,Coefficients Amendment,0,106,68,13,0,model : Coefficients Amendment,0.43621399176954734,0.723404255319149,1.0
text-classification,7,Convolutional Capsule Layer,model,Convolutional Capsule Layer,1,107,69,1,0,model : Convolutional Capsule Layer,0.4403292181069959,0.7340425531914894,0.1
text-classification,7,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .",model,Convolutional Capsule Layer,1,108,70,2,0,model : Convolutional Capsule Layer,0.4444444444444444,0.7446808510638298,0.2
text-classification,7,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,model,Convolutional Capsule Layer,1,109,71,3,0,model : Convolutional Capsule Layer,0.448559670781893,0.7553191489361702,0.3
text-classification,7,Suppose W c 1 ? R Ddd and W c 2 ? R K,model,Convolutional Capsule Layer,0,110,72,4,0,model : Convolutional Capsule Layer,0.45267489711934156,0.7659574468085106,0.4
text-classification,7,Suppose W c 1 ? R Ddd and W c 2 ? R K,model,Convolutional Capsule Layer,0,111,73,5,0,model : Convolutional Capsule Layer,0.4567901234567901,0.776595744680851,0.5
text-classification,7,"CDdd denote shared and non-shared weights , respectively , where K 2 C is the number of child capsules in a local region in the layer below , Dis the number of parent capsules which the child capsules are sent to .",model,Convolutional Capsule Layer,0,112,74,6,0,model : Convolutional Capsule Layer,0.4609053497942387,0.7872340425531915,0.6
text-classification,7,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b? where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .",model,Convolutional Capsule Layer,0,113,75,7,0,model : Convolutional Capsule Layer,0.46502057613168724,0.7978723404255319,0.7
text-classification,7,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b? where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .",model,Convolutional Capsule Layer,0,114,76,8,0,model : Convolutional Capsule Layer,0.4691358024691358,0.8085106382978723,0.8
text-classification,7,"Then , we use routingby - agreement to produce parent capsules feature maps totally ( L?K 1 ? K 2 + 2 ) D d-dimensional capsules in this layer .",model,Convolutional Capsule Layer,0,115,77,9,0,model : Convolutional Capsule Layer,0.4732510288065844,0.8191489361702128,0.9
text-classification,7,"When using the non-shared weights across the child capsules , we replace the transformation matrix W c 1 j in Eq. ( 10 ) with W c 2 j .",model,Convolutional Capsule Layer,0,116,78,10,0,model : Convolutional Capsule Layer,0.4773662551440329,0.8297872340425532,1.0
text-classification,7,Fully Connected Capsule Layer,model,Fully Connected Capsule Layer,1,117,79,1,0,model : Fully Connected Capsule Layer,0.48148148148148145,0.8404255319148937,0.14285714285714285
text-classification,7,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,model,Fully Connected Capsule Layer,1,118,80,2,0,model : Fully Connected Capsule Layer,0.48559670781893005,0.851063829787234,0.2857142857142857
text-classification,7,Edd or W d 2 ? R HEdd followed by routing - by - agreement to produce final capsule v j ? Rd and its probability a j ? R for each category .,model,Fully Connected Capsule Layer,0,119,81,3,0,model : Fully Connected Capsule Layer,0.4897119341563786,0.8617021276595744,0.42857142857142855
text-classification,7,Edd or W d 2 ? R HEdd followed by routing - by - agreement to produce final capsule v j ? Rd and its probability a j ? R for each category .,model,Fully Connected Capsule Layer,0,120,82,4,0,model : Fully Connected Capsule Layer,0.49382716049382713,0.8723404255319149,0.5714285714285714
text-classification,7,Edd or W d 2 ? R HEdd followed by routing - by - agreement to produce final capsule v j ? Rd and its probability a j ? R for each category .,model,Fully Connected Capsule Layer,0,121,83,5,0,model : Fully Connected Capsule Layer,0.49794238683127573,0.8829787234042553,0.7142857142857143
text-classification,7,Edd or W d 2 ? R HEdd followed by routing - by - agreement to produce final capsule v j ? Rd and its probability a j ? R for each category .,model,Fully Connected Capsule Layer,0,122,84,6,0,model : Fully Connected Capsule Layer,0.5020576131687243,0.8936170212765957,0.8571428571428571
text-classification,7,"Here , H is the number of child capsules in the layer below , E is the number of categories plus an extra orphan category .",model,Fully Connected Capsule Layer,0,123,85,7,0,model : Fully Connected Capsule Layer,0.5061728395061729,0.9042553191489362,1.0
text-classification,7,The Architectures of Capsule Network,model,The Architectures of Capsule Network,1,124,86,1,0,model : The Architectures of Capsule Network,0.5102880658436214,0.9148936170212766,0.1111111111111111
text-classification,7,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,model,The Architectures of Capsule Network,1,125,87,2,0,model : The Architectures of Capsule Network,0.51440329218107,0.925531914893617,0.2222222222222222
text-classification,7,"Capsule - B Capsule - A starts with an embedding layer which transforms each word in the corpus to a 300 - dimensional ( V = 300 ) word vector , followed by a 3 - gram ( K 1 = 3 ) convolutional layer with 32 filters ( B = 32 ) and astride of 1 with ReLU non-linearity .",model,The Architectures of Capsule Network,0,126,88,3,0,model : The Architectures of Capsule Network,0.5185185185185185,0.9361702127659575,0.3333333333333333
text-classification,7,"All the other layers are capsule layers starting with a B d primary capsule layer with 32 filters ( C = 32 ) , followed by a 3 C d d ( K 2 = 3 ) convolutional capsule layer with 16 filters ( D = 16 ) and a fully connected capsule layer in sequence .",model,The Architectures of Capsule Network,0,127,89,4,0,model : The Architectures of Capsule Network,0.522633744855967,0.9468085106382979,0.4444444444444444
text-classification,7,Each capsule has 16 - dimensional ( d = 16 ) instantiated parameters and their length ( norm ) can describe the probability of the existence of capsules .,model,The Architectures of Capsule Network,0,128,90,5,0,model : The Architectures of Capsule Network,0.5267489711934157,0.9574468085106383,0.5555555555555556
text-classification,7,"The capsule layers are connected by the transformation matrices , and each connection is also multiplied by a routing coefficient that is dynamically computed by routing by agreement mechanism .",model,The Architectures of Capsule Network,0,129,91,6,0,model : The Architectures of Capsule Network,0.5308641975308642,0.9680851063829787,0.6666666666666666
text-classification,7,"The basic structure of Capsule - B is similar to Capsule - A except that we adopt three parallel networks with filter windows ( N ) of 3 , 4 , 5 in the N - gram convolutional layer ( see ) .",model,The Architectures of Capsule Network,0,130,92,7,0,model : The Architectures of Capsule Network,0.5349794238683128,0.9787234042553191,0.7777777777777778
text-classification,7,The final output of the fully connected capsule layer is fed into the average pooling to produce the final results .,model,The Architectures of Capsule Network,0,131,93,8,0,model : The Architectures of Capsule Network,0.5390946502057613,0.9893617021276596,0.8888888888888888
text-classification,7,"In this way , Capsule - B can learn more meaningful and comprehensive text representation .",model,The Architectures of Capsule Network,0,132,94,9,0,model : The Architectures of Capsule Network,0.5432098765432098,1.0,1.0
text-classification,7,Experimental Setup,experiment,experiment,0,133,1,1,0,experiment : experiment,0.5473251028806584,1.0,1.0
text-classification,7,Experimental Datasets,dataset,Experimental Datasets,0,134,1,1,0,dataset : Experimental Datasets,0.551440329218107,0.25,0.25
text-classification,7,"In order to evaluate the effectiveness of our model , we conduct a series of experiments on six bench - marks including : movie reviews ( MR ) , Stanford Sentiment Treebankan extension of MR ( SST - 2 ) , Subjectivity dataset ( Subj ) , TREC question dataset ( TREC ) , customer review ( CR ) , and AG 's news corpus .",dataset,Experimental Datasets,0,135,2,2,0,dataset : Experimental Datasets,0.5555555555555556,0.5,0.5
text-classification,7,"These benchmarks cover several text classification tasks such as sentiment classification , question categorization , news categorization .",dataset,Experimental Datasets,0,136,3,3,0,dataset : Experimental Datasets,0.5596707818930041,0.75,0.75
text-classification,7,The detailed statistics are presented in,dataset,Experimental Datasets,0,137,4,4,0,dataset : Experimental Datasets,0.5637860082304527,1.0,1.0
text-classification,7,Implementation Details,implementation,Implementation Details,0,138,1,1,0,implementation : Implementation Details,0.5679012345679012,0.2,0.2
text-classification,7,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",implementation,Implementation Details,1,139,2,2,0,implementation : Implementation Details,0.5720164609053497,0.4,0.4
text-classification,7,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,implementation,Implementation Details,1,140,3,3,0,implementation : Implementation Details,0.5761316872427984,0.6,0.6
text-classification,7,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,implementation,Implementation Details,1,141,4,4,0,implementation : Implementation Details,0.5802469135802469,0.8,0.8
text-classification,7,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,implementation,Implementation Details,1,142,5,5,0,implementation : Implementation Details,0.5843621399176955,1.0,1.0
text-classification,7,Baseline methods,baseline,Baseline methods,0,143,1,1,0,baseline : Baseline methods,0.588477366255144,0.5,0.5
text-classification,7,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",baseline,Baseline methods,1,144,2,2,0,baseline : Baseline methods,0.5925925925925926,1.0,1.0
text-classification,7,Experimental Results,experiment,Experimental Results,0,145,1,1,0,experiment : Experimental Results,0.5967078189300411,1.0,1.0
text-classification,7,Quantitative Evaluation,evaluation,Quantitative Evaluation,0,146,1,1,0,evaluation : Quantitative Evaluation,0.6008230452674898,0.2,0.2
text-classification,7,"In our experiments , the evaluation metric is classification accuracy .",evaluation,Quantitative Evaluation,0,147,2,2,0,evaluation : Quantitative Evaluation,0.6049382716049383,0.4,0.4
text-classification,7,We summarize the experimental results in .,evaluation,Quantitative Evaluation,0,148,3,3,0,evaluation : Quantitative Evaluation,0.6090534979423868,0.6,0.6
text-classification,7,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",evaluation,Quantitative Evaluation,1,149,4,4,0,evaluation : Quantitative Evaluation,0.6131687242798354,0.8,0.8
text-classification,7,"In particular , our model substantially and consistently outperforms",evaluation,Quantitative Evaluation,0,150,5,5,0,evaluation : Quantitative Evaluation,0.6172839506172839,1.0,1.0
text-classification,7,Ablation Study,ablation,Ablation Study,0,151,1,1,0,ablation : Ablation Study,0.6213991769547325,0.017857142857142856,0.2
text-classification,7,"To analyze the effect of varying different components of our capsule architecture for text classification , we also report the ablation test of the capsule - B model in terms of using different setups of the capsule network .",ablation,Ablation Study,0,152,2,2,0,ablation : Ablation Study,0.6255144032921811,0.03571428571428571,0.4
text-classification,7,The experimental results are summarized in .,ablation,Ablation Study,0,153,3,3,0,ablation : Ablation Study,0.6296296296296297,0.05357142857142857,0.6
text-classification,7,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",ablation,Ablation Study,1,154,4,4,0,ablation : Ablation Study,0.6337448559670782,0.07142857142857142,0.8
text-classification,7,More comprehensive comparison results are demonstrated in . 4 in Supplementary Material .,ablation,Ablation Study,0,155,5,5,0,ablation : Ablation Study,0.6378600823045267,0.08928571428571429,1.0
text-classification,7,Single - Label to Multi - Label Text Classification,ablation,Single-Label to Multi-Label Text Classification,0,156,6,1,0,ablation : Single-Label to Multi-Label Text Classification,0.6419753086419753,0.10714285714285714,0.041666666666666664
text-classification,7,Capsule network demonstrates promising performance in single - label text classification which as - signs a label from a predefined set to a text ( see ) .,ablation,Single-Label to Multi-Label Text Classification,0,157,7,2,0,ablation : Single-Label to Multi-Label Text Classification,0.6460905349794238,0.125,0.08333333333333333
text-classification,7,"Multi-label text classification is , however , a more challenging practical problem .",ablation,Single-Label to Multi-Label Text Classification,0,158,8,3,0,ablation : Single-Label to Multi-Label Text Classification,0.6502057613168725,0.14285714285714285,0.125
text-classification,7,"From singlelabel to multi-label ( with n category labels ) text classification , the label space is expanded from n to 2 n , thus more training is required to cover the whole label space .",ablation,Single-Label to Multi-Label Text Classification,0,159,9,4,0,ablation : Single-Label to Multi-Label Text Classification,0.654320987654321,0.16071428571428573,0.16666666666666666
text-classification,7,"For single - label texts , it is practically easy to collect and annotate the samples .",ablation,Single-Label to Multi-Label Text Classification,0,160,10,5,0,ablation : Single-Label to Multi-Label Text Classification,0.6584362139917695,0.17857142857142858,0.20833333333333334
text-classification,7,"However , the burden of collection and annotation for a large scale multi-label text dataset is generally extremely high .",ablation,Single-Label to Multi-Label Text Classification,0,161,11,6,0,ablation : Single-Label to Multi-Label Text Classification,0.6625514403292181,0.19642857142857142,0.25
text-classification,7,"How deep neural networks ( e.g. , CNN and LSTM ) best cope with multi-label text classification still remains a problem since obtaining large scale of multi-label dataset is a timeconsuming and expensive process .",ablation,Single-Label to Multi-Label Text Classification,0,162,12,7,0,ablation : Single-Label to Multi-Label Text Classification,0.6666666666666666,0.21428571428571427,0.2916666666666667
text-classification,7,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .",ablation,Single-Label to Multi-Label Text Classification,0,163,13,8,0,ablation : Single-Label to Multi-Label Text Classification,0.6707818930041153,0.23214285714285715,0.3333333333333333
text-classification,7,"With feature property as part of the information extracted by capsules , we may generalize the model better to multi-label text classification without an over extensive amount of labeled data .",ablation,Single-Label to Multi-Label Text Classification,0,164,14,9,0,ablation : Single-Label to Multi-Label Text Classification,0.6748971193415638,0.25,0.375
text-classification,7,The evaluation is carried on the Reuters - 21578 dataset .,ablation,Single-Label to Multi-Label Text Classification,0,165,15,10,0,ablation : Single-Label to Multi-Label Text Classification,0.6790123456790124,0.26785714285714285,0.4166666666666667
text-classification,7,"This dataset consists of 10,788 documents from the Reuters financial newswire service , where each document contains either multiple labels or a single label .",ablation,Single-Label to Multi-Label Text Classification,0,166,16,11,0,ablation : Single-Label to Multi-Label Text Classification,0.6831275720164609,0.2857142857142857,0.4583333333333333
text-classification,7,We reprocess the corpus to evaluate the capability of capsule networks of transferring from single - label to multi-label text classification .,ablation,Single-Label to Multi-Label Text Classification,0,167,17,12,0,ablation : Single-Label to Multi-Label Text Classification,0.6872427983539094,0.30357142857142855,0.5
text-classification,7,"For dev and training , we only use the single - label documents in the Reuters dev and training sets .",ablation,Single-Label to Multi-Label Text Classification,0,168,18,13,0,ablation : Single-Label to Multi-Label Text Classification,0.691358024691358,0.32142857142857145,0.5416666666666666
text-classification,7,"For testing , Reuters - Multi - label only uses the multi-label documents in testing dataset , while Reuters - Full includes all documents in test set .",ablation,Single-Label to Multi-Label Text Classification,0,169,19,14,0,ablation : Single-Label to Multi-Label Text Classification,0.6954732510288066,0.3392857142857143,0.5833333333333334
text-classification,7,The characteristics of these two datasets are described in .,ablation,Single-Label to Multi-Label Text Classification,0,170,20,15,0,ablation : Single-Label to Multi-Label Text Classification,0.6995884773662552,0.35714285714285715,0.625
text-classification,7,"Following ( Sorower , 2010 ) , we adopt Micro Averaged Precision ( Precision ) , Micro Averaged Recall ( Recall ) and Micro Averaged F1 scores ( F1 ) as the evaluation metrics for multi-label text classification .",ablation,Single-Label to Multi-Label Text Classification,0,171,21,16,0,ablation : Single-Label to Multi-Label Text Classification,0.7037037037037037,0.375,0.6666666666666666
text-classification,7,"Any of these scores are firstly computed on individual class labels and then averaged over all classes , called label - based measures .",ablation,Single-Label to Multi-Label Text Classification,0,172,22,17,0,ablation : Single-Label to Multi-Label Text Classification,0.7078189300411523,0.39285714285714285,0.7083333333333334
text-classification,7,"In addition , we also measure the Exact Match Ratio ( ER ) which considers partially correct prediction as incorrect and only counts fully correct samples .",ablation,Single-Label to Multi-Label Text Classification,0,173,23,18,0,ablation : Single-Label to Multi-Label Text Classification,0.7119341563786008,0.4107142857142857,0.75
text-classification,7,The experimental results are summarized in .,ablation,Single-Label to Multi-Label Text Classification,0,174,24,19,0,ablation : Single-Label to Multi-Label Text Classification,0.7160493827160493,0.42857142857142855,0.7916666666666666
text-classification,7,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",ablation,Single-Label to Multi-Label Text Classification,0,175,25,20,0,ablation : Single-Label to Multi-Label Text Classification,0.720164609053498,0.44642857142857145,0.8333333333333334
text-classification,7,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .",ablation,Single-Label to Multi-Label Text Classification,0,176,26,21,0,ablation : Single-Label to Multi-Label Text Classification,0.7242798353909465,0.4642857142857143,0.875
text-classification,7,This is within our expectation since the capsule network is capable of preserving the instantiated parameters of the categories trained by singlelabel documents .,ablation,Single-Label to Multi-Label Text Classification,0,177,27,22,0,ablation : Single-Label to Multi-Label Text Classification,0.7283950617283951,0.48214285714285715,0.9166666666666666
text-classification,7,The capsule network has much stronger transferring capability than the conventional deep neural networks .,ablation,Single-Label to Multi-Label Text Classification,0,178,28,23,0,ablation : Single-Label to Multi-Label Text Classification,0.7325102880658436,0.5,0.9583333333333334
text-classification,7,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",ablation,Single-Label to Multi-Label Text Classification,0,179,29,24,0,ablation : Single-Label to Multi-Label Text Classification,0.7366255144032922,0.5178571428571429,1.0
text-classification,7,Connection Strength Visualization,ablation,Connection Strength Visualization,0,180,30,1,0,ablation : Connection Strength Visualization,0.7407407407407407,0.5357142857142857,0.09090909090909091
text-classification,7,"To visualize the connection strength between capsule layers clearly , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , where the primary capsules denote N-gram phrases in the form of capsules .",ablation,Connection Strength Visualization,0,181,31,2,0,ablation : Connection Strength Visualization,0.7448559670781894,0.5535714285714286,0.18181818181818182
text-classification,7,"The connection strength shows the importance of each primary capsule for text categories , acting like a parallel attention mechanism .",ablation,Connection Strength Visualization,0,182,32,3,0,ablation : Connection Strength Visualization,0.7489711934156379,0.5714285714285714,0.2727272727272727
text-classification,7,This should allow the capsule networks to recognize multiple categories in the text even though the model is trained on singlelabel documents .,ablation,Connection Strength Visualization,0,183,33,4,0,ablation : Connection Strength Visualization,0.7530864197530864,0.5892857142857143,0.36363636363636365
text-classification,7,"Due to space reasons , we choose a multilabel document from Reuters - Multi - label test set whose category labels ( i.e. , Interest Rates and Money / Foreign Exchange ) are correctly predicted ( fully correct ) by our model with high confidence ( p > 0.8 ) to report in .",ablation,Connection Strength Visualization,0,184,34,5,0,ablation : Connection Strength Visualization,0.757201646090535,0.6071428571428571,0.45454545454545453
text-classification,7,"The categoryspecific phrases such as "" interest rates "" and "" foreign exchange "" are highlighted with red color .",ablation,Connection Strength Visualization,0,185,35,6,0,ablation : Connection Strength Visualization,0.7613168724279835,0.625,0.5454545454545454
text-classification,7,We use the tag cloud to visualize the 3 - gram phrases for Interest Rates and Money / Foreign Exchange categories .,ablation,Connection Strength Visualization,0,186,36,7,0,ablation : Connection Strength Visualization,0.7654320987654321,0.6428571428571429,0.6363636363636364
text-classification,7,"The stronger the connection strength , the bigger the font size .",ablation,Connection Strength Visualization,0,187,37,8,0,ablation : Connection Strength Visualization,0.7695473251028807,0.6607142857142857,0.7272727272727273
text-classification,7,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .",ablation,Connection Strength Visualization,0,188,38,9,0,ablation : Connection Strength Visualization,0.7736625514403292,0.6785714285714286,0.8181818181818182
text-classification,7,"The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules , as shown in To experimentally verify the convergence of the routing algorithm , we also plot learning curve to show the training loss overtime with different iterations of routing .",ablation,Connection Strength Visualization,0,189,39,10,0,ablation : Connection Strength Visualization,0.7777777777777778,0.6964285714285714,0.9090909090909091
text-classification,7,"From , we observe that the Capsule - B with 3 or 5 iterations of routing optimizes the loss faster and converges to a lower loss at the end than the capsule network with 1 iteration .",ablation,Connection Strength Visualization,0,190,40,11,0,ablation : Connection Strength Visualization,0.7818930041152263,0.7142857142857143,1.0
text-classification,7,.,ablation,MONEY RATES FIRM ON LAWSON STERLING TARGETS,0,191,41,1,0,ablation : MONEY RATES FIRM ON LAWSON STERLING TARGETS,0.7860082304526749,0.7321428571428571,0.5
text-classification,7,MONEY RATES FIRM ON LAWSON STERLING TARGETS,ablation,MONEY RATES FIRM ON LAWSON STERLING TARGETS,0,192,42,2,0,ablation : MONEY RATES FIRM ON LAWSON STERLING TARGETS,0.7901234567901234,0.75,1.0
text-classification,7,Interest Rates,ablation,Interest Rates,0,193,43,1,0,ablation : Interest Rates,0.7942386831275721,0.7678571428571429,0.08333333333333333
text-classification,7,Money / Foreign Exchange Interest rates on the London money market were slightly firmer on news U.K .,ablation,Interest Rates,0,194,44,2,0,ablation : Interest Rates,0.7983539094650206,0.7857142857142857,0.16666666666666666
text-classification,7,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",ablation,Interest Rates,0,195,45,3,0,ablation : Interest Rates,0.8024691358024691,0.8035714285714286,0.25
text-classification,7,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",ablation,Interest Rates,0,196,46,4,0,ablation : Interest Rates,0.8065843621399177,0.8214285714285714,0.3333333333333333
text-classification,7,Sterling opened 0.3 points lower in trade weighted terms at 71.3 .,ablation,Interest Rates,0,197,47,5,0,ablation : Interest Rates,0.8106995884773662,0.8392857142857143,0.4166666666666667
text-classification,7,Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates .,ablation,Interest Rates,0,198,48,6,0,ablation : Interest Rates,0.8148148148148148,0.8571428571428571,0.5
text-classification,7,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,ablation,Interest Rates,0,199,49,7,0,ablation : Interest Rates,0.8189300411522634,0.875,0.5833333333333334
text-classification,7,"Base lending rates even less likely in the near term , dealers said .",ablation,Interest Rates,0,200,50,8,0,ablation : Interest Rates,0.823045267489712,0.8928571428571429,0.6666666666666666
text-classification,7,"The feeling remains in the market , however , that fundamental factors have not really changed and that arise in U.K .",ablation,Interest Rates,0,201,51,9,0,ablation : Interest Rates,0.8271604938271605,0.9107142857142857,0.75
text-classification,7,Interest rates is not very likely .,ablation,Interest Rates,0,202,52,10,0,ablation : Interest Rates,0.831275720164609,0.9285714285714286,0.8333333333333334
text-classification,7,"The market is expected to continue at around these levels , reflecting the current 10 pct base rate level , for sometime .",ablation,Interest Rates,0,203,53,11,0,ablation : Interest Rates,0.8353909465020576,0.9464285714285714,0.9166666666666666
text-classification,7,The key three months interbank rate was 1 / 16 point firmer at 10 9 - 7 /8 pct .,ablation,Interest Rates,0,204,54,12,0,ablation : Interest Rates,0.8395061728395061,0.9642857142857143,1.0
text-classification,7,Orphan,ablation,Orphan,0,205,55,1,0,ablation : Orphan,0.8436213991769548,0.9821428571428571,0.5
text-classification,7,Mergers / Acquisitions Money / Foreign Exchange Trade Interest Rates,ablation,Orphan,0,206,56,2,0,ablation : Orphan,0.8477366255144033,1.0,1.0
text-classification,7,Related Work,related work,Related Work,0,207,1,1,0,related work : Related Work,0.8518518518518519,0.047619047619047616,0.047619047619047616
text-classification,7,"Early methods for text classification adopted the typical features such as bag - of - words , n-grams , and their TF - IDF features as input of machine learning algorithms such as support vector machine ( SVM ) , naive Bayes ( NB ) for classification .",related work,Related Work,0,208,2,2,0,related work : Related Work,0.8559670781893004,0.09523809523809523,0.09523809523809523
text-classification,7,"However , these models usually heavily relied on laborious feature engineering or massive extra linguistic resources .",related work,Related Work,0,209,3,3,0,related work : Related Work,0.8600823045267489,0.14285714285714285,0.14285714285714285
text-classification,7,Recent advances in deep neural networks and representation learning have substantially improved the performance of text classification tasks .,related work,Related Work,0,210,4,4,0,related work : Related Work,0.8641975308641975,0.19047619047619047,0.19047619047619047
text-classification,7,"The dominant approaches are recurrent neural net -works , in particular LSTMs and CNNs. reported on a series of experiments with CNNs trained on top of pre-trained word vectors for sentence - level classification tasks .",related work,Related Work,0,211,5,5,0,related work : Related Work,0.8683127572016461,0.23809523809523808,0.23809523809523808
text-classification,7,The CNN models improved upon the state of the art on 4 out of 7 tasks .,related work,Related Work,0,212,6,6,0,related work : Related Work,0.8724279835390947,0.2857142857142857,0.2857142857142857
text-classification,7,offered an empirical exploration on the use of character - level convolutional networks ( Convnets ) for text classification and the experiments showed that Convnets outperformed the traditional models .,related work,Related Work,0,213,7,7,0,related work : Related Work,0.8765432098765432,0.3333333333333333,0.3333333333333333
text-classification,7,"proposed a simple and efficient text classification method fastText , which could be trained on a billion words within ten minutes .",related work,Related Work,0,214,8,8,0,related work : Related Work,0.8806584362139918,0.38095238095238093,0.38095238095238093
text-classification,7,proposed a very deep convolutional networks ( with 29 convolutional layers ) for text classification .,related work,Related Work,0,215,9,9,0,related work : Related Work,0.8847736625514403,0.42857142857142855,0.42857142857142855
text-classification,7,generalized the LSTM to the tree - structured network topologies ( Tree - LSTM ) that achieved best results on two text classification tasks .,related work,Related Work,0,216,10,10,0,related work : Related Work,0.8888888888888888,0.47619047619047616,0.47619047619047616
text-classification,7,"Recently , a novel type of neural network is proposed using the concept of capsules to improve the representational limitations of firstly introduced the concept of "" capsules "" to address the representational limitations of CNNs and RNNs .",related work,Related Work,0,217,11,11,0,related work : Related Work,0.8930041152263375,0.5238095238095238,0.5238095238095238
text-classification,7,Capsules with transformation matrices allowed networks to automatically learn part - whole relationships .,related work,Related Work,0,218,12,12,0,related work : Related Work,0.897119341563786,0.5714285714285714,0.5714285714285714
text-classification,7,"Consequently , proposed capsule networks that replaced the scalar - output feature detectors of CNNs with vector - output capsules and max - pooling with routing - by - agreement .",related work,Related Work,0,219,13,13,0,related work : Related Work,0.9012345679012346,0.6190476190476191,0.6190476190476191
text-classification,7,The capsule network has shown its potential by achieving a state - of - the - art result on MNIST data .,related work,Related Work,0,220,14,14,0,related work : Related Work,0.9053497942386831,0.6666666666666666,0.6666666666666666
text-classification,7,"Unlike max - pooling in CNN , however , Capsule network do not throwaway information about the precise position of the entity within the region .",related work,Related Work,0,221,15,15,0,related work : Related Work,0.9094650205761317,0.7142857142857143,0.7142857142857143
text-classification,7,"For lowlevel capsules , location information is placecoded by which capsule is active .",related work,Related Work,0,222,16,16,0,related work : Related Work,0.9135802469135802,0.7619047619047619,0.7619047619047619
text-classification,7,further tested out the application of capsule networks on CIFAR data with higher dimensionality .,related work,Related Work,0,223,17,17,0,related work : Related Work,0.9176954732510288,0.8095238095238095,0.8095238095238095
text-classification,7,"proposed a new iterative routing procedure between capsule layers based on the EM algorithm , which achieves significantly better accuracy on the small NORB data set .",related work,Related Work,0,224,18,18,0,related work : Related Work,0.9218106995884774,0.8571428571428571,0.8571428571428571
text-classification,7,generalized existing routing methods within the framework of weighted kernel density estimation .,related work,Related Work,0,225,19,19,0,related work : Related Work,0.9259259259259259,0.9047619047619048,0.9047619047619048
text-classification,7,"To date , no work investigates the performance of capsule networks in NLP tasks .",related work,Related Work,0,226,20,20,0,related work : Related Work,0.9300411522633745,0.9523809523809523,0.9523809523809523
text-classification,7,This study herein takes the lead in this topic .,related work,Related Work,0,227,21,21,0,related work : Related Work,0.934156378600823,1.0,1.0
text-classification,7,Conclusion,conclusion,Conclusion,0,228,1,1,0,conclusion : Conclusion,0.9382716049382716,0.2,0.2
text-classification,7,"In this paper , we investigated capsule networks with dynamic routing for text classification .",conclusion,Conclusion,0,229,2,2,0,conclusion : Conclusion,0.9423868312757202,0.4,0.4
text-classification,7,Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the disturbance of noisy capsules .,conclusion,Conclusion,0,230,3,3,0,conclusion : Conclusion,0.9465020576131687,0.6,0.6
text-classification,7,Extensive experiments on six text classification benchmarks show the effectiveness of capsule networks in text classification .,conclusion,Conclusion,0,231,4,4,0,conclusion : Conclusion,0.9506172839506173,0.8,0.8
text-classification,7,"More importantly , capsule networks also show significant improvement when transferring single - label to multi-label text classifications over strong baseline methods .",conclusion,Conclusion,0,232,5,5,0,conclusion : Conclusion,0.9547325102880658,1.0,1.0
text-classification,7,Supplementary Material,Supplementary Material,Supplementary Material,0,233,1,1,0,Supplementary Material : Supplementary Material,0.9588477366255144,0.09090909090909091,0.1
text-classification,7,"To better demonstrate the orphan and other categories with top unigrams , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , similar to the settings in section 5.1 .",Supplementary Material,Supplementary Material,0,234,2,2,0,Supplementary Material : Supplementary Material,0.9629629629629629,0.18181818181818182,0.2
text-classification,7,"Here , the primary capsules denote uni-grams in the form of capsules .",Supplementary Material,Supplementary Material,0,235,3,3,0,Supplementary Material : Supplementary Material,0.9670781893004116,0.2727272727272727,0.3
text-classification,7,"We picked top - 20 uni-gram ( words ) from four categories ( i.e. , Orphan category , Trade category , Money Exchange category and Interest Rates category ) sorted by their connection strengths .",Supplementary Material,Supplementary Material,0,236,4,4,0,Supplementary Material : Supplementary Material,0.9711934156378601,0.36363636363636365,0.4
text-classification,7,"Money / Foreign Exchange Following is the text of a statement by the Group of Seven - the U.S. , Japan , West Germany , France , Britain , Italy and Canada - issued after a Washington meeting yesterday .",Supplementary Material,Supplementary Material,0,237,5,5,0,Supplementary Material : Supplementary Material,0.9753086419753086,0.45454545454545453,0.5
text-classification,7,. The finance ministers and central bank governors of seven major industrial countries met today .,Supplementary Material,Supplementary Material,0,238,6,6,0,Supplementary Material : Supplementary Material,0.9794238683127572,0.5454545454545454,0.6
text-classification,7,They continued the process of multilateral surveillance of their economies pursuant to the arrangements for strengthened economic policy coordination agreed at the 1986 Tokyo summit of their heads of state or government .,Supplementary Material,Supplementary Material,0,239,7,7,0,Supplementary Material : Supplementary Material,0.9835390946502057,0.6363636363636364,0.7
text-classification,7,". The ministers and governors reaffirmed the commitment to the cooperative approach agreed at the recent Paris meeting , and noted the progress achieved in implementing the undertakings embodied in the Louvre Agreement .",Supplementary Material,Supplementary Material,0,240,8,8,0,Supplementary Material : Supplementary Material,0.9876543209876543,0.7272727272727273,0.8
text-classification,7,"In this connection they welcomed the proposals just announced by the governing Liberal Democratic Party in Japan for extraordinary and urgent measures to stimulate Japan 's economy through early implementation of a large supplementary budget exceeding those of previous years , as well as unprecedented front - end loading of public works expenditures .",Supplementary Material,Supplementary Material,0,241,9,9,0,Supplementary Material : Supplementary Material,0.9917695473251029,0.8181818181818182,0.9
text-classification,7,They concluded that present and prospective progress in implementing the policy undertakings at the Louvre and in this statement provided a basis for continuing close cooperation to foster the stability of exchange rates .,Supplementary Material,Supplementary Material,0,242,10,10,0,Supplementary Material : Supplementary Material,0.9958847736625515,0.9090909090909091,1.0
text-classification,7,Index,Supplementary Material,Index,0,243,11,1,0,Supplementary Material : Index,1.0,1.0,1.0
text-classification,8,Baseline Needs More Love : On Simple Word - Embedding - Based Models and Associated Pooling Mechanisms,title,title,1,2,1,1,0,title : title,0.007434944237918215,1.0,1.0
text-classification,8,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011152416356877323,0.125,0.125
text-classification,8,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.01486988847583643,0.25,0.25
text-classification,8,"However , there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01858736059479554,0.375,0.375
text-classification,8,"In this paper , we conduct a point - by - point comparative study between Simple Word - Embeddingbased Models ( SWEMs ) , consisting of parameter - free pooling operations , relative to word - embedding - based RNN / CNN models .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.022304832713754646,0.5,0.5
text-classification,8,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.026022304832713755,0.625,0.625
text-classification,8,"Based upon this understanding , we propose two additional pooling strategies over learned word embeddings : ( i ) a max - pooling operation for improved interpretability ; and ( ii ) a hierarchical pooling operation , which preserves spatial ( n - gram ) information within text sequences .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02973977695167286,0.75,0.75
text-classification,8,"We present experiments on 17 datasets encompassing three tasks : ( i ) ( long ) document classification ; ( ii ) text sequence matching ; and ( iii ) short text tasks , including classification and tagging .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.03345724907063197,0.875,0.875
text-classification,8,The source code and datasets can be obtained from https://github.com/dinghanshen/SWEM .,abstract,abstract,1,10,8,8,0,abstract : abstract,0.03717472118959108,1.0,1.0
text-classification,8,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.040892193308550186,0.043478260869565216,0.043478260869565216
text-classification,8,"Word embeddings , learned from massive unstructured text data , are widely - adopted building blocks for Natural Language Processing ( NLP ) .",introduction,introduction,0,12,2,2,0,introduction : introduction,0.04460966542750929,0.08695652173913043,0.08695652173913043
text-classification,8,"By representing each word as a fixed - length vector , these embeddings can group semantically similar words , while implicitly encoding rich linguis - tic regularities and patterns .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.048327137546468404,0.13043478260869565,0.13043478260869565
text-classification,8,"Leveraging the word - embedding construct , many deep architectures have been proposed to model the compositionality in variable - length text sequences .",introduction,introduction,1,14,4,4,0,introduction : introduction,0.05204460966542751,0.17391304347826086,0.17391304347826086
text-classification,8,"These methods range from simple operations like addition , to more sophisticated compositional functions such as Recurrent Neural Networks ( RNNs ) , Convolutional Neural Networks ( CNNs ) and Recursive Neural Networks .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.055762081784386616,0.21739130434782608,0.21739130434782608
text-classification,8,"Models with more expressive compositional functions , e.g. , RNNs or CNNs , have demonstrated impressive results ; however , they are typically computationally expensive , due to the need to estimate hundreds of thousands , if not millions , of parameters .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.05947955390334572,0.2608695652173913,0.2608695652173913
text-classification,8,"In contrast , models with simple compositional functions often compute a sentence or document embedding by simply adding , or averaging , over the word embedding of each sequence element obtained via , e.g. , word2vec , or Glo Ve .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.06319702602230483,0.30434782608695654,0.30434782608695654
text-classification,8,"Generally , such a Simple Word - Embedding - based Model ( SWEM ) does not explicitly account for spatial , word - order information within a text sequence .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.06691449814126393,0.34782608695652173,0.34782608695652173
text-classification,8,"However , they possess the desirable property of having significantly fewer parameters , enjoying much faster training , relative to RNN - or CNN - based models .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.07063197026022305,0.391304347826087,0.391304347826087
text-classification,8,"Hence , there is a computation - vs. - expressiveness tradeoff regarding how to model the compositionality of a text sequence .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.07434944237918216,0.43478260869565216,0.43478260869565216
text-classification,8,"In this paper , we conduct an extensive experimental investigation to understand when , and why , simple pooling strategies , operated over word embeddings alone , already carry sufficient information for natural language understanding .",introduction,introduction,1,21,11,11,0,introduction : introduction,0.07806691449814127,0.4782608695652174,0.4782608695652174
text-classification,8,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with existing recurrent and convolutional networks in a pointby - point manner .",introduction,introduction,1,22,12,12,0,introduction : introduction,0.08178438661710037,0.5217391304347826,0.5217391304347826
text-classification,8,"Specifically , we consider 17 datasets , including three distinct NLP tasks : document classification ( Yahoo news , Yelp reviews , etc. ) , natural language sequence matching ( SNLI , WikiQA , etc. ) and ( short ) sentence classification / tagging ( Stanford sentiment treebank , .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.08550185873605948,0.5652173913043478,0.5652173913043478
text-classification,8,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.08921933085501858,0.6086956521739131,0.6086956521739131
text-classification,8,"In order to validate our experimental findings , we conduct additional investigations to understand to what extent the word - order information is utilized / required to make predictions on different tasks .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.09293680297397769,0.6521739130434783,0.6521739130434783
text-classification,8,"We observe that in text representation tasks , many words ( e.g. , stop words , or words thatare not related to sentiment or topic ) do not meaningfully contribute to the final predictions ( e.g. , sentiment label ) .",introduction,introduction,0,26,16,16,0,introduction : introduction,0.09665427509293681,0.6956521739130435,0.6956521739130435
text-classification,8,"Based upon this understanding , we propose to leverage a max - pooling operation directly over the word embedding matrix of a given sequence , to select its most salient features .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.10037174721189591,0.7391304347826086,0.7391304347826086
text-classification,8,"This strategy is demonstrated to extract complementary features relative to the standard averaging operation , while resulting in a more interpretable model .",introduction,introduction,0,28,18,18,0,introduction : introduction,0.10408921933085502,0.782608695652174,0.782608695652174
text-classification,8,"Inspired by a case study on sentiment analysis tasks , we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.10780669144981413,0.8260869565217391,0.8260869565217391
text-classification,8,"This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks thatare sensitive to word - order features , while maintaining the favorable properties of not having compositional parameters , thus fast training .",introduction,introduction,0,30,20,20,0,introduction : introduction,0.11152416356877323,0.8695652173913043,0.8695652173913043
text-classification,8,"Our work presents a simple yet strong baseline for text representation learning that is widely ignored in benchmarks , and highlights the general computation - vs. - expressiveness tradeoff associated with appropriately selecting compositional functions for distinct NLP problems .",introduction,introduction,0,31,21,21,0,introduction : introduction,0.11524163568773234,0.9130434782608695,0.9130434782608695
text-classification,8,"Furthermore , we quantitatively show that the word - embeddingbased text classification tasks can have the similar level of difficulty regardless of the employed models , using the subspace training to constrain the trainable parameters .",introduction,introduction,0,32,22,22,0,introduction : introduction,0.11895910780669144,0.9565217391304348,0.9565217391304348
text-classification,8,"Thus , according to Occam 's razor , simple models are preferred .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.12267657992565056,1.0,1.0
text-classification,8,Related Work,related work,Related Work,0,34,1,1,0,related work : Related Work,0.12639405204460966,0.07142857142857142,0.07142857142857142
text-classification,8,"fundamental goal in NLP is to develop expressive , yet computationally efficient compositional functions that can capture the linguistic structure of natural language sequences .",related work,Related Work,0,35,2,2,0,related work : Related Work,0.13011152416356878,0.14285714285714285,0.14285714285714285
text-classification,8,"Recently , several studies have suggested that on certain NLP applications , much simpler word - embedding - based architectures exhibit comparable or even superior performance , compared with more - sophisticated models using recurrence or convolutions .",related work,Related Work,0,36,3,3,0,related work : Related Work,0.13382899628252787,0.21428571428571427,0.21428571428571427
text-classification,8,"Although complex compositional functions are avoided in these models , additional modules , such as attention layers , are employed on top of the word embedding layer .",related work,Related Work,0,37,4,4,0,related work : Related Work,0.137546468401487,0.2857142857142857,0.2857142857142857
text-classification,8,"As a result , the specific role that the word embedding plays in these models is not emphasized ( or explicit ) , which distracts from understanding how important the word embeddings alone are to the observed superior performance .",related work,Related Work,0,38,5,5,0,related work : Related Work,0.1412639405204461,0.35714285714285715,0.35714285714285715
text-classification,8,"Moreover , several recent studies have shown empirically that the advantages of distinct compositional functions are highly dependent on the specific task .",related work,Related Work,0,39,6,6,0,related work : Related Work,0.1449814126394052,0.42857142857142855,0.42857142857142855
text-classification,8,"Therefore , it is of interest to study the practical value of the additional expressiveness , on a wide variety of NLP problems .",related work,Related Work,0,40,7,7,0,related work : Related Work,0.14869888475836432,0.5,0.5
text-classification,8,"SWEMs bear close resemblance to Deep Averaging Network ( DAN ) or fast - Text , where they show that average pooling achieves promising results on certain NLP tasks .",related work,Related Work,0,41,8,8,0,related work : Related Work,0.1524163568773234,0.5714285714285714,0.5714285714285714
text-classification,8,"However , there exist several key differences that make our work unique .",related work,Related Work,0,42,9,9,0,related work : Related Work,0.15613382899628253,0.6428571428571429,0.6428571428571429
text-classification,8,"First , we explore a series of pooling operations , rather than only average - pooling .",related work,Related Work,0,43,10,10,0,related work : Related Work,0.15985130111524162,0.7142857142857143,0.7142857142857143
text-classification,8,"Specifically , a hierarchical pooling operation is introduced to incorporate spatial information , which demonstrates superior results on sentiment analysis , relative to average pooling .",related work,Related Work,0,44,11,11,0,related work : Related Work,0.16356877323420074,0.7857142857142857,0.7857142857142857
text-classification,8,"Second , our work not only explores when simple pooling operations are enough , but also investigates the underlying reasons , i.e. , what semantic features are required for distinct NLP problems .",related work,Related Work,0,45,12,12,0,related work : Related Work,0.16728624535315986,0.8571428571428571,0.8571428571428571
text-classification,8,"Third , DAN and fast Text only focused on one or two problems at a time , thus a comprehensive study regarding the effectiveness of various compositional functions on distinct NLP tasks , e.g. , categorizing short sentence / long documents , matching natural language sentences , has heretofore been absent .",related work,Related Work,0,46,13,13,0,related work : Related Work,0.17100371747211895,0.9285714285714286,0.9285714285714286
text-classification,8,"In response , our work seeks to perform a comprehensive comparison with respect to simple - vs. - complex compositional func- tions , across a wide range of NLP problems , and reveals some general rules for rationally selecting models to tackle different tasks .",related work,Related Work,0,47,14,14,0,related work : Related Work,0.17472118959107807,1.0,1.0
text-classification,8,Models & training,system description,Models & training,0,48,1,1,0,system description : Models & training,0.17843866171003717,0.015151515151515152,0.16666666666666666
text-classification,8,"Consider a text sequence represented as X ( either a sentence or a document ) , composed of a sequence of words : {w 1 , w 2 , .... , w L } , where L is the number of tokens , i.e. , the sentence / document length .",system description,Models & training,0,49,2,2,0,system description : Models & training,0.1821561338289963,0.030303030303030304,0.3333333333333333
text-classification,8,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ? R K .",system description,Models & training,0,50,3,3,0,system description : Models & training,0.18587360594795538,0.045454545454545456,0.5
text-classification,8,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ? R K .",system description,Models & training,0,51,4,4,0,system description : Models & training,0.1895910780669145,0.06060606060606061,0.6666666666666666
text-classification,8,"The compositional function , X ? z , aims to combine word embeddings into a fixed - length sentence / document representation z .",system description,Models & training,0,52,5,5,0,system description : Models & training,0.19330855018587362,0.07575757575757576,0.8333333333333334
text-classification,8,"These representations are then used to make predictions about sequence X. Below , we describe different types of functions considered in this work .",system description,Models & training,0,53,6,6,0,system description : Models & training,0.1970260223048327,0.09090909090909091,1.0
text-classification,8,Recurrent Sequence Encoder,system description,Recurrent Sequence Encoder,0,54,7,1,0,system description : Recurrent Sequence Encoder,0.20074349442379183,0.10606060606060606,0.16666666666666666
text-classification,8,"widely adopted compositional function is defined in a recurrent manner : the model successively takes word vector v tat position t , along with the hidden unit h t?1 from the last position t ? 1 , to update the current hidden unit via",system description,Recurrent Sequence Encoder,0,55,8,2,0,system description : Recurrent Sequence Encoder,0.20446096654275092,0.12121212121212122,0.3333333333333333
text-classification,8,"widely adopted compositional function is defined in a recurrent manner : the model successively takes word vector v tat position t , along with the hidden unit h t?1 from the last position t ? 1 , to update the current hidden unit via",system description,Recurrent Sequence Encoder,0,56,9,3,0,system description : Recurrent Sequence Encoder,0.20817843866171004,0.13636363636363635,0.5
text-classification,8,"To address the issue of learning long - term dependencies , f ( ) is often defined as Long Short - Term Memory ( LSTM ) , which employs gates to control the flow of information abstracted from a sequence .",system description,Recurrent Sequence Encoder,0,57,10,4,0,system description : Recurrent Sequence Encoder,0.21189591078066913,0.15151515151515152,0.6666666666666666
text-classification,8,We omit the details of the LSTM and refer the interested readers to the work by for further explanation .,system description,Recurrent Sequence Encoder,0,58,11,5,0,system description : Recurrent Sequence Encoder,0.21561338289962825,0.16666666666666666,0.8333333333333334
text-classification,8,"Intuitively , the LSTM encodes a text sequence considering its word - order information , but yields additional compositional parameters that must be learned .",system description,Recurrent Sequence Encoder,0,59,12,6,0,system description : Recurrent Sequence Encoder,0.21933085501858737,0.18181818181818182,1.0
text-classification,8,Convolutional Sequence Encoder,system description,Convolutional Sequence Encoder,0,60,13,1,0,system description : Convolutional Sequence Encoder,0.22304832713754646,0.19696969696969696,0.16666666666666666
text-classification,8,The Convolutional Neural Network ( CNN ) architecture is another strategy extensively employed as the compositional function to encode text sequences .,system description,Convolutional Sequence Encoder,0,61,14,2,0,system description : Convolutional Sequence Encoder,0.22676579925650558,0.21212121212121213,0.3333333333333333
text-classification,8,"The convolution operation considers windows of n consecutive words within the sequence , where a set of filters ( to be learned ) are applied to these word windows to generate corresponding feature maps .",system description,Convolutional Sequence Encoder,0,62,15,3,0,system description : Convolutional Sequence Encoder,0.23048327137546468,0.22727272727272727,0.5
text-classification,8,"Subsequently , an aggregation operation ( such as max - pooling ) is used on top of the feature maps to abstract the most salient semantic features , resulting in the final representation .",system description,Convolutional Sequence Encoder,0,63,16,4,0,system description : Convolutional Sequence Encoder,0.2342007434944238,0.24242424242424243,0.6666666666666666
text-classification,8,"For most experiments , we consider a single - layer CNN text model .",system description,Convolutional Sequence Encoder,0,64,17,5,0,system description : Convolutional Sequence Encoder,0.2379182156133829,0.25757575757575757,0.8333333333333334
text-classification,8,"However , Deep CNN text models have also been developed , and are considered in a few of our experiments .",system description,Convolutional Sequence Encoder,0,65,18,6,0,system description : Convolutional Sequence Encoder,0.241635687732342,0.2727272727272727,1.0
text-classification,8,Simple Word - Embedding Model,system description,Simple Word-Embedding Model,0,66,19,1,0,system description : Simple Word-Embedding Model,0.24535315985130113,0.2878787878787879,0.020833333333333332
text-classification,8,SWEM ),system description,Simple Word-Embedding Model,0,67,20,2,0,system description : Simple Word-Embedding Model,0.24907063197026022,0.30303030303030304,0.041666666666666664
text-classification,8,"To investigate the raw modeling capacity of word embeddings , we consider a class of models with no additional compositional parameters to encode natural language sequences , termed SWEMs .",system description,Simple Word-Embedding Model,0,68,21,3,0,system description : Simple Word-Embedding Model,0.2527881040892193,0.3181818181818182,0.0625
text-classification,8,"Among them , the simplest strategy is to compute the element - wise average over word vectors for a given sequence :",system description,Simple Word-Embedding Model,0,69,22,4,0,system description : Simple Word-Embedding Model,0.25650557620817843,0.3333333333333333,0.08333333333333333
text-classification,8,"The model in can be seen as an average pooling operation , which takes the mean over each of the K dimensions for all word embeddings , resulting in a representation z with the same dimension as the embedding itself , termed here SWEM - aver .",system description,Simple Word-Embedding Model,0,70,23,5,0,system description : Simple Word-Embedding Model,0.26022304832713755,0.3484848484848485,0.10416666666666667
text-classification,8,"Intuitively , z takes the information of every sequence element into account via the addition operation .",system description,Simple Word-Embedding Model,0,71,24,6,0,system description : Simple Word-Embedding Model,0.26394052044609667,0.36363636363636365,0.125
text-classification,8,Max Pooling,system description,Simple Word-Embedding Model,0,72,25,7,0,system description : Simple Word-Embedding Model,0.26765799256505574,0.3787878787878788,0.14583333333333334
text-classification,8,"Motivated by the observation that , in general , only a small number of key words contribute to final predictions , we propose another SWEM variant , that extracts the most salient features from every word - embedding dimension , by taking the maximum value along each dimension of the word vectors .",system description,Simple Word-Embedding Model,0,73,26,8,0,system description : Simple Word-Embedding Model,0.27137546468401486,0.3939393939393939,0.16666666666666666
text-classification,8,This strategy is similar to the max - over - time pooling operation in convolutional neural networks :,system description,Simple Word-Embedding Model,0,74,27,9,0,system description : Simple Word-Embedding Model,0.275092936802974,0.4090909090909091,0.1875
text-classification,8,We denote this model variant as SWEM - max .,system description,Simple Word-Embedding Model,0,75,28,10,0,system description : Simple Word-Embedding Model,0.2788104089219331,0.42424242424242425,0.20833333333333334
text-classification,8,"Here the j - th component of z is the maximum element in the set {v 1 j , . . . , v Lj } , where v 1j is , for example , the j - th component of v 1 .",system description,Simple Word-Embedding Model,0,76,29,11,0,system description : Simple Word-Embedding Model,0.2825278810408922,0.4393939393939394,0.22916666666666666
text-classification,8,"With this pooling operation , those words thatare unimportant or unrelated to the corresponding tasks will be ignored in the encoding process ( as the components of the embedding vectors will have small amplitude ) , unlike SWEM - aver where every word contributes equally to the representation .",system description,Simple Word-Embedding Model,0,77,30,12,0,system description : Simple Word-Embedding Model,0.2862453531598513,0.45454545454545453,0.25
text-classification,8,"Considering that SWEM - aver and SWEM - max are complementary , in the sense of accounting for different types of information from text sequences , Model",system description,Simple Word-Embedding Model,0,78,31,13,0,system description : Simple Word-Embedding Model,0.2899628252788104,0.4696969696969697,0.2708333333333333
text-classification,8,"Considering that SWEM - aver and SWEM - max are complementary , in the sense of accounting for different types of information from text sequences , Model",system description,Simple Word-Embedding Model,0,79,32,14,0,system description : Simple Word-Embedding Model,0.2936802973977695,0.48484848484848486,0.2916666666666667
text-classification,8,Parameter s Speed CNN 541K 171s LSTM 1.8M 598s SWEM 61K 63s,system description,Simple Word-Embedding Model,0,80,33,15,0,system description : Simple Word-Embedding Model,0.29739776951672864,0.5,0.3125
text-classification,8,"Interestingly , for the sentiment analysis tasks , both CNN and LSTM compositional functions perform better than SWEM , suggesting that wordorder information maybe required for analyzing sentiment orientations .",system description,Simple Word-Embedding Model,1,81,34,16,0,system description : Simple Word-Embedding Model,0.30111524163568776,0.5151515151515151,0.3333333333333333
text-classification,8,"This finding is consistent with , where they hypothesize that the positional information of a word in text sequences maybe beneficial to predict sentiment .",system description,Simple Word-Embedding Model,0,82,35,17,0,system description : Simple Word-Embedding Model,0.3048327137546468,0.5303030303030303,0.3541666666666667
text-classification,8,"This is intuitively reasonable since , for instance , the phrase "" not really good "" and "" really not good "" convey different levels of negative sentiment , while being different only by their word orderings .",system description,Simple Word-Embedding Model,0,83,36,18,0,system description : Simple Word-Embedding Model,0.30855018587360594,0.5454545454545454,0.375
text-classification,8,"Contrary to SWEM , CNN and LSTM models can both capture this type of information via convolutional filters or recurrent transition functions .",system description,Simple Word-Embedding Model,0,84,37,19,0,system description : Simple Word-Embedding Model,0.31226765799256506,0.5606060606060606,0.3958333333333333
text-classification,8,"However , as suggested above , such word - order patterns maybe much less useful for predicting the topic of a document .",system description,Simple Word-Embedding Model,0,85,38,20,0,system description : Simple Word-Embedding Model,0.3159851301115242,0.5757575757575758,0.4166666666666667
text-classification,8,"This maybe attributed to the fact that word embeddings alone already provide sufficient topic information of a document , at least when the text sequences considered are relatively long .",system description,Simple Word-Embedding Model,0,86,39,21,0,system description : Simple Word-Embedding Model,0.31970260223048325,0.5909090909090909,0.4375
text-classification,8,Parameters,system description,Simple Word-Embedding Model,0,87,40,22,0,system description : Simple Word-Embedding Model,0.32342007434944237,0.6060606060606061,0.4583333333333333
text-classification,8,Complexity Sequential,system description,Simple Word-Embedding Model,0,88,41,23,0,system description : Simple Word-Embedding Model,0.3271375464684015,0.6212121212121212,0.4791666666666667
text-classification,8,"Ops we also propose a third SWEM variant , where the two abstracted features are concatenated together to form the sentence embeddings , denoted here as SWEM - concat .",system description,Simple Word-Embedding Model,0,89,42,24,0,system description : Simple Word-Embedding Model,0.3308550185873606,0.6363636363636364,0.5
text-classification,8,"For all SWEM variants , there are no additional compositional parameters to be learned .",system description,Simple Word-Embedding Model,0,90,43,25,0,system description : Simple Word-Embedding Model,0.3345724907063197,0.6515151515151515,0.5208333333333334
text-classification,8,"As a result , the models only exploit intrinsic word embedding information for predictions .",system description,Simple Word-Embedding Model,0,91,44,26,0,system description : Simple Word-Embedding Model,0.3382899628252788,0.6666666666666666,0.5416666666666666
text-classification,8,"Hierarchical Pooling Both SWEM - aver and SWEM - max do not take word - order or spatial information into consideration , which could be useful for certain NLP applications .",system description,Simple Word-Embedding Model,0,92,45,27,0,system description : Simple Word-Embedding Model,0.3420074349442379,0.6818181818181818,0.5625
text-classification,8,"So motivated , we further propose a hierarchical pooling layer .",system description,Simple Word-Embedding Model,0,93,46,28,0,system description : Simple Word-Embedding Model,0.34572490706319703,0.696969696969697,0.5833333333333334
text-classification,8,"Let v i:i+n?1 refer to the local window consisting of n consecutive words words , First , an average - pooling is performed on each local window , v i:i+n?1 .",system description,Simple Word-Embedding Model,0,94,47,29,0,system description : Simple Word-Embedding Model,0.34944237918215615,0.7121212121212122,0.6041666666666666
text-classification,8,"Let v i:i+n?1 refer to the local window consisting of n consecutive words words , First , an average - pooling is performed on each local window , v i:i+n?1 .",system description,Simple Word-Embedding Model,0,95,48,30,0,system description : Simple Word-Embedding Model,0.35315985130111527,0.7272727272727273,0.625
text-classification,8,The extracted features from all windows are further down - sampled with a global max - pooling operation on top of the representations for every window .,system description,Simple Word-Embedding Model,0,96,49,31,0,system description : Simple Word-Embedding Model,0.35687732342007433,0.7424242424242424,0.6458333333333334
text-classification,8,We call this approach SWEM - hier due to its layered pooling .,system description,Simple Word-Embedding Model,0,97,50,32,0,system description : Simple Word-Embedding Model,0.36059479553903345,0.7575757575757576,0.6666666666666666
text-classification,8,"This strategy preserves the local spatial information of a text sequence in the sense that it keeps track of how the sentence / document is constructed from individual word windows , i.e. , n-grams .",system description,Simple Word-Embedding Model,0,98,51,33,0,system description : Simple Word-Embedding Model,0.3643122676579926,0.7727272727272727,0.6875
text-classification,8,This formulation is related to bag - of - n- grams method .,system description,Simple Word-Embedding Model,0,99,52,34,0,system description : Simple Word-Embedding Model,0.3680297397769517,0.7878787878787878,0.7083333333333334
text-classification,8,"However , SWEM - hier learns fixed - length representations for the n-grams that appear in the corpus , rather than just capturing their occurrences via count features , which may potentially advantageous for prediction purposes .",system description,Simple Word-Embedding Model,0,100,53,35,0,system description : Simple Word-Embedding Model,0.37174721189591076,0.803030303030303,0.7291666666666666
text-classification,8,Parameters & Computation,system description,Simple Word-Embedding Model,0,101,54,36,0,system description : Simple Word-Embedding Model,0.3754646840148699,0.8181818181818182,0.75
text-classification,8,Comparison,system description,Simple Word-Embedding Model,0,102,55,37,0,system description : Simple Word-Embedding Model,0.379182156133829,0.8333333333333334,0.7708333333333334
text-classification,8,"We compare CNN , LSTM and SWEM wrt their parameters and computational speed .",system description,Simple Word-Embedding Model,0,103,56,38,0,system description : Simple Word-Embedding Model,0.3828996282527881,0.8484848484848485,0.7916666666666666
text-classification,8,"denotes the dimension of word embeddings , as above .",system description,Simple Word-Embedding Model,0,104,57,39,0,system description : Simple Word-Embedding Model,0.38661710037174724,0.8636363636363636,0.8125
text-classification,8,"For the CNN , we use n to denote the filter width ( assumed constant for all filters , for simplicity of analysis , but in practice variable n is commonly used ) .",system description,Simple Word-Embedding Model,0,105,58,40,0,system description : Simple Word-Embedding Model,0.3903345724907063,0.8787878787878788,0.8333333333333334
text-classification,8,We defined as the dimension of the final sequence representation .,system description,Simple Word-Embedding Model,0,106,59,41,0,system description : Simple Word-Embedding Model,0.3940520446096654,0.8939393939393939,0.8541666666666666
text-classification,8,"Specifically , d represents the dimension of hidden units or the number of filters in LSTM or CNN , respectively .",system description,Simple Word-Embedding Model,0,107,60,42,0,system description : Simple Word-Embedding Model,0.39776951672862454,0.9090909090909091,0.875
text-classification,8,We first examine the number of compositional parameters for each model .,system description,Simple Word-Embedding Model,0,108,61,43,0,system description : Simple Word-Embedding Model,0.40148698884758366,0.9242424242424242,0.8958333333333334
text-classification,8,"As shown in , both the CNN and LSTM have a large number of parameters , to model the semantic compositionality of text sequences , whereas SWEM has no such parameters .",system description,Simple Word-Embedding Model,0,109,62,44,0,system description : Simple Word-Embedding Model,0.4052044609665427,0.9393939393939394,0.9166666666666666
text-classification,8,"Similar to , we then consider the computational complexity and the minimum number of sequential operations required for each model .",system description,Simple Word-Embedding Model,0,110,63,45,0,system description : Simple Word-Embedding Model,0.40892193308550184,0.9545454545454546,0.9375
text-classification,8,SWEM tends to be more efficient than CNN and LSTM in terms of computation complexity .,system description,Simple Word-Embedding Model,0,111,64,46,0,system description : Simple Word-Embedding Model,0.41263940520446096,0.9696969696969697,0.9583333333333334
text-classification,8,"For example , considering the case where K = d , SWEM is faster than CNN or LSTM by a factor of nd or d , respectively .",system description,Simple Word-Embedding Model,0,112,65,47,0,system description : Simple Word-Embedding Model,0.4163568773234201,0.9848484848484849,0.9791666666666666
text-classification,8,"Further , the computations in SWEM are highly parallelizable , unlike LSTM that requires O ( L ) sequential steps .",system description,Simple Word-Embedding Model,0,113,66,48,0,system description : Simple Word-Embedding Model,0.4200743494423792,1.0,1.0
text-classification,8,Experiments,experiment,experiment,0,114,1,1,0,experiment : experiment,0.42379182156133827,0.058823529411764705,0.058823529411764705
text-classification,8,"We evaluate different compositional functions on a wide variety of supervised tasks , including document categorization , text sequence matching ( given a sentence pair , X 1 , X 2 , predict their relationship , y) as well as ( short ) sentence classification .",experiment,experiment,0,115,2,2,0,experiment : experiment,0.4275092936802974,0.11764705882352941,0.11764705882352941
text-classification,8,"We experiment on 17 datasets concerning natural language understanding , with corresponding data statistics summarized in the Supplementary Material .",experiment,experiment,0,116,3,3,0,experiment : experiment,0.4312267657992565,0.17647058823529413,0.17647058823529413
text-classification,8,We use Glo Ve word embeddings with K = 300 as initialization for all our models .,experiment,experiment,1,117,4,4,0,experiment : experiment,0.4349442379182156,0.23529411764705882,0.23529411764705882
text-classification,8,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",experiment,experiment,1,118,5,5,0,experiment : experiment,0.43866171003717475,0.29411764705882354,0.29411764705882354
text-classification,8,"The Glo Ve embeddings are employed in two ways to learn refined word embeddings : ( i ) directly updating each word embedding during training ; and ( ii ) training a 300 dimensional Multilayer Perceptron ( MLP ) layer with ReLU activation , with Glo Ve embeddings as input to the MLP and with output defining the refined word embeddings .",experiment,experiment,1,119,6,6,0,experiment : experiment,0.4423791821561338,0.35294117647058826,0.35294117647058826
text-classification,8,The latter approach corresponds to learning an MLP model that adapts GloVe embeddings to the dataset and task of interest .,experiment,experiment,0,120,7,7,0,experiment : experiment,0.44609665427509293,0.4117647058823529,0.4117647058823529
text-classification,8,The advantages of these two methods differ from dataset to dataset .,experiment,experiment,0,121,8,8,0,experiment : experiment,0.44981412639405205,0.47058823529411764,0.47058823529411764
text-classification,8,We choose the better strategy based on their corresponding performances on the validation set .,experiment,experiment,0,122,9,9,0,experiment : experiment,0.45353159851301117,0.5294117647058824,0.5294117647058824
text-classification,8,"The final classifier is implemented as an MLP layer with dimension selected from the set [ 100 , 300 , 500 , 1000 ] , followed by a sigmoid or softmax function , depending on the specific task .",experiment,experiment,0,123,10,10,0,experiment : experiment,0.45724907063197023,0.5882352941176471,0.5882352941176471
text-classification,8,"Adam ) is used to optimize all models , with learning rate selected from .",experiment,experiment,1,124,11,11,0,experiment : experiment,0.46096654275092935,0.6470588235294118,0.6470588235294118
text-classification,8,"Surprisingly , on topic prediction tasks , our SWEM model exhibits stronger performances , relative to both LSTM and CNN compositional architectures , this by leveraging both the average and max - pooling features from word embeddings .",experiment,experiment,1,125,12,12,0,experiment : experiment,0.4646840148698885,0.7058823529411765,0.7058823529411765
text-classification,8,"Specifically , our SWEM - concat model even outperforms a 29 - layer deep CNN model , when predicting topics .",experiment,experiment,0,126,13,13,0,experiment : experiment,0.4684014869888476,0.7647058823529411,0.7647058823529411
text-classification,8,"On the ontology classification problem ( DBpedia dataset ) , we observe the same trend , that SWEM exhibits comparable or even superior results , relative to CNN or LSTM models .",experiment,experiment,1,127,14,14,0,experiment : experiment,0.4721189591078067,0.8235294117647058,0.8235294117647058
text-classification,8,"Since there are no compositional parameters in SWEM , our models have an order of magnitude fewer parameters ( excluding embeddings ) than LSTM or CNN , and are considerably more computationally efficient .",experiment,experiment,0,128,15,15,0,experiment : experiment,0.4758364312267658,0.8823529411764706,0.8823529411764706
text-classification,8,"As illustrated in Table 4 , SWEM - concat achieves better results on Yahoo !",experiment,experiment,0,129,16,16,0,experiment : experiment,0.4795539033457249,0.9411764705882353,0.9411764705882353
text-classification,8,"Answer than CNN / LSTM , with only 61 K parameters ( one - tenth the number of LSTM parameters , or one - third the number of CNN parameters ) , while taking a fraction of the training time relative to the CNN or LSTM .",experiment,experiment,0,130,17,17,0,experiment : experiment,0.483271375464684,1.0,1.0
text-classification,8,Interpreting model predictions,model,model,0,131,1,1,0,model : model,0.48698884758364314,0.013888888888888888,0.013888888888888888
text-classification,8,"Although the proposed SWEM - max variant generally performs a slightly worse than SWEM - aver , it extracts complementary features from SWEMaver , and hence in most cases SWEM - concat exhibits the best performance among all SWEM variants .",model,model,0,132,2,2,0,model : model,0.49070631970260226,0.027777777777777776,0.027777777777777776
text-classification,8,"More importantly , we found that the word embeddings learned from SWEM - max tend to be sparse .",model,model,0,133,3,3,0,model : model,0.4944237918215613,0.041666666666666664,0.041666666666666664
text-classification,8,We trained our SWEM - max model on the Yahoo datasets ( randomly initialized ) .,model,model,0,134,4,4,0,model : model,0.49814126394052044,0.05555555555555555,0.05555555555555555
text-classification,8,"With the learned embeddings , we plot the values for each of the word embedding dimensions , for the entire vocabulary .",model,model,0,135,5,5,0,model : model,0.5018587360594795,0.06944444444444445,0.06944444444444445
text-classification,8,"As shown in , most of the values are highly concentrated around zero , indicating that the word embeddings learned are very sparse .",model,model,0,136,6,6,0,model : model,0.5055762081784386,0.08333333333333333,0.08333333333333333
text-classification,8,"On the contrary , the Glo Ve word embeddings , for the same vocabulary , are considerably denser than the embeddings learned from SWEM - max .",model,model,0,137,7,7,0,model : model,0.5092936802973977,0.09722222222222222,0.09722222222222222
text-classification,8,"This suggests that the model may only depend on a few key words , among the entire vocabulary , for predictions ( since most words do not contribute to the max - pooling operation in SWEM - max ) .",model,model,0,138,8,8,0,model : model,0.5130111524163569,0.1111111111111111,0.1111111111111111
text-classification,8,"Through the embedding , the model learns the important words for a given task ( those words with non -zero embedding components ) .",model,model,0,139,9,9,0,model : model,0.516728624535316,0.125,0.125
text-classification,8,"In this regard , the nature of max - pooling pro - cess gives rise to a more interpretable model .",model,model,0,140,10,10,0,model : model,0.5204460966542751,0.1388888888888889,0.1388888888888889
text-classification,8,"For a document , only the word with largest value in each embedding dimension is employed for the final representation .",model,model,0,141,11,11,0,model : model,0.5241635687732342,0.1527777777777778,0.1527777777777778
text-classification,8,"Thus , we suspect that semantically similar words may have large values in some shared dimensions .",model,model,0,142,12,12,0,model : model,0.5278810408921933,0.16666666666666666,0.16666666666666666
text-classification,8,"So motivated , after training the SWEM - max model on the Yahoo dataset , we selected five words with the largest values , among the entire vocabulary , for each word embedding dimension ( these words are selected preferentially in the corresponding dimension , by the max operation ) .",model,model,0,143,13,13,0,model : model,0.5315985130111525,0.18055555555555555,0.18055555555555555
text-classification,8,"As shown in , the words chosen wrt each embedding dimension are indeed highly relevant and correspond to a common topic ( the topics are inferred from words ) .",model,model,0,144,14,14,0,model : model,0.5353159851301115,0.19444444444444445,0.19444444444444445
text-classification,8,"For example , the words in the first column of are all political terms , which could be assigned to the Politics & Government topic .",model,model,0,145,15,15,0,model : model,0.5390334572490706,0.20833333333333334,0.20833333333333334
text-classification,8,Note that our model can even learn locally interpretable structure that is not explicitly indicated by the label information .,model,model,0,146,16,16,0,model : model,0.5427509293680297,0.2222222222222222,0.2222222222222222
text-classification,8,"For instance , all words in the fifth column are Chemistry - related .",model,model,0,147,17,17,0,model : model,0.5464684014869888,0.2361111111111111,0.2361111111111111
text-classification,8,"However , we do not have a chemistry label in the dataset , and regardless they should belong to the Science topic .",model,model,0,148,18,18,0,model : model,0.550185873605948,0.25,0.25
text-classification,8,Text Sequence Matching,model,model,1,149,19,19,0,model : model,0.5539033457249071,0.2638888888888889,0.2638888888888889
text-classification,8,"To gain a deeper understanding regarding the modeling capacity of word embeddings , we further investigate the problem of sentence matching , including natural language inference , answer sentence selection and paraphrase identification .",model,model,0,150,20,20,0,model : model,0.5576208178438662,0.2777777777777778,0.2777777777777778
text-classification,8,The corresponding performance metrics are shown in .,model,model,0,151,21,21,0,model : model,0.5613382899628253,0.2916666666666667,0.2916666666666667
text-classification,8,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .",model,model,1,152,22,22,0,model : model,0.5650557620817844,0.3055555555555556,0.3055555555555556
text-classification,8,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .",model,model,1,153,23,23,0,model : model,0.5687732342007435,0.3194444444444444,0.3194444444444444
text-classification,8,"As a result , with only 120K parameters , our SWEM - max achieves a test accuracy of 83.8 % , which is very competitive among state - of the - art sentence encoding - based models ( in terms of both performance and number of parameters )",model,model,0,154,24,24,0,model : model,0.5724907063197026,0.3333333333333333,0.3333333333333333
text-classification,8,1 .,model,model,0,155,25,25,0,model : model,0.5762081784386617,0.3472222222222222,0.3472222222222222
text-classification,8,"The strong results of the SWEM approach on these tasks may stem from the fact that when matching natural language sentences , it is sufficient in most cases to simply model the word - level alignments between two sequences .",model,model,0,156,26,26,0,model : model,0.5799256505576208,0.3611111111111111,0.3611111111111111
text-classification,8,"From this perspective , word - order information becomes much less useful for predicting relationship between sentences .",model,model,0,157,27,27,0,model : model,0.5836431226765799,0.375,0.375
text-classification,8,"Moreover , considering the simpler model architecture of SWEM , they could be much easier to be optimized than LSTM or CNN - based models , and thus give rise to better empirical results .",model,model,0,158,28,28,0,model : model,0.587360594795539,0.3888888888888889,0.3888888888888889
text-classification,8,Importance of word - order information,model,model,0,159,29,29,0,model : model,0.5910780669144982,0.4027777777777778,0.4027777777777778
text-classification,8,"One possible dis advantage of SWEM is that it ignores the word - order information within a text sequence , which could be potentially captured by CNN - or LSTM - based models .",model,model,0,160,30,30,0,model : model,0.5947955390334573,0.4166666666666667,0.4166666666666667
text-classification,8,"However , we empirically found that except for sentiment analysis , SWEM exhibits similar or even superior performance as the CNN or LSTM on a variety of tasks .",model,model,0,161,31,31,0,model : model,0.5985130111524164,0.4305555555555556,0.4305555555555556
text-classification,8,"In this regard , one natural question would be : how important are word - order features for these tasks ? To this end , we randomly shuffle the words for every sentence in the training set , while keeping the original word order for samples in the test set .",model,model,0,162,32,32,0,model : model,0.6022304832713755,0.4444444444444444,0.4444444444444444
text-classification,8,"In this regard , one natural question would be : how important are word - order features for these tasks ? To this end , we randomly shuffle the words for every sentence in the training set , while keeping the original word order for samples in the test set .",model,model,0,163,33,33,0,model : model,0.6059479553903345,0.4583333333333333,0.4583333333333333
text-classification,8,The motivation here is to remove the word - order features from the training set and examine how sensitive the performance on different tasks are to word - order information .,model,model,0,164,34,34,0,model : model,0.6096654275092936,0.4722222222222222,0.4722222222222222
text-classification,8,We use LSTM as the model for this purpose since it can captures wordorder information from the original training set .,model,model,0,165,35,35,0,model : model,0.6133828996282528,0.4861111111111111,0.4861111111111111
text-classification,8,The results on three distinct tasks are shown in .,model,model,0,166,36,36,0,model : model,0.6171003717472119,0.5,0.5
text-classification,8,"Somewhat surprisingly , for Yahoo and SNLI datasets , the LSTM model trained on shuffled training set shows comparable accuracies to those trained on the original dataset , indicating that word - order information does not contribute significantly on these two problems , i.e. , topic categorization and textual entailment .",model,model,0,167,37,37,0,model : model,0.620817843866171,0.5138888888888888,0.5138888888888888
text-classification,8,"However , on the Yelp polarity dataset , the results drop noticeably , further suggesting that word - order does matter for sentiment analysis ( as indicated above from a different perspective ) .",model,model,0,168,38,38,0,model : model,0.6245353159851301,0.5277777777777778,0.5277777777777778
text-classification,8,"Notably , the performance of LSTM on the Yelp dataset with a shuffled training set is very close to our results with SWEM , indicating that the main difference between LSTM and SWEM maybe due to the ability of the former to capture word - order features .",model,model,0,169,39,39,0,model : model,0.6282527881040892,0.5416666666666666,0.5416666666666666
text-classification,8,Both observations are in consistent with our experimental results in the previous section .,model,model,0,170,40,40,0,model : model,0.6319702602230484,0.5555555555555556,0.5555555555555556
text-classification,8,Case Study,model,model,0,171,41,41,0,model : model,0.6356877323420075,0.5694444444444444,0.5694444444444444
text-classification,8,"To understand what type of sentences are sensitive to word - order information , we further show those samples thatare wrongly predicted because of the shuffling of training data in .",model,model,0,172,42,42,0,model : model,0.6394052044609665,0.5833333333333334,0.5833333333333334
text-classification,8,"Taking the first sentence as an example , several words in the review are generally positive , i.e. friendly , nice , okay , great and likes .",model,model,0,173,43,43,0,model : model,0.6431226765799256,0.5972222222222222,0.5972222222222222
text-classification,8,"However , the most vital features for predicting the sentiment of this sentence could be the phrase / sentence ' is just okay ' , ' not great ' or ' makes me wonder why everyone likes ' , which can not be captured without considering word - order features .",model,model,0,174,44,44,0,model : model,0.6468401486988847,0.6111111111111112,0.6111111111111112
text-classification,8,It is worth noting the hints for predictions in this case are actually ngram phrases from the input document .,model,model,0,175,45,45,0,model : model,0.6505576208178439,0.625,0.625
text-classification,8,SWEM - hier for sentiment analysis,model,model,1,176,46,46,0,model : model,0.654275092936803,0.6388888888888888,0.6388888888888888
text-classification,8,"As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .",model,model,1,177,47,47,0,model : model,0.6579925650557621,0.6527777777777778,0.6527777777777778
text-classification,8,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :",model,model,1,178,48,48,0,model : model,0.6617100371747212,0.6666666666666666,0.6666666666666666
text-classification,8,Friendly staff and nice selection of vegetarian options .,model,model,0,179,49,49,0,model : model,0.6654275092936803,0.6805555555555556,0.6805555555555556
text-classification,8,"Food is just okay , not great .",model,model,0,180,50,50,0,model : model,0.6691449814126395,0.6944444444444444,0.6944444444444444
text-classification,8,Makes me wonder why everyone likes food fight so much .,model,model,0,181,51,51,0,model : model,0.6728624535315985,0.7083333333333334,0.7083333333333334
text-classification,8,Positive :,model,model,0,182,52,52,0,model : model,0.6765799256505576,0.7222222222222222,0.7222222222222222
text-classification,8,"The store is small , but it carries specialties thatare difficult to find in Pittsburgh .",model,model,0,183,53,53,0,model : model,0.6802973977695167,0.7361111111111112,0.7361111111111112
text-classification,8,was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights .,model,model,0,184,54,54,0,model : model,0.6840148698884758,0.75,0.75
text-classification,8,the input document .,model,model,0,185,55,55,0,model : model,0.6877323420074349,0.7638888888888888,0.7638888888888888
text-classification,8,"We hypothesize that incorporating information about the local word - order , i.e. , n-gram features , is likely to largely mitigate the limitations of the above three SWEM variants .",model,model,0,186,56,56,0,model : model,0.6914498141263941,0.7777777777777778,0.7777777777777778
text-classification,8,"Inspired by this observation , we propose using another simple pooling operation termed as hierarchical ( SWEM - hier ) , as detailed in Section 3.3 .",model,model,0,187,57,57,0,model : model,0.6951672862453532,0.7916666666666666,0.7916666666666666
text-classification,8,We evaluate this method on the two documentlevel sentiment analysis tasks and the results are shown in the last row of .,model,model,0,188,58,58,0,model : model,0.6988847583643123,0.8055555555555556,0.8055555555555556
text-classification,8,"SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .",model,model,1,189,59,59,0,model : model,0.7026022304832714,0.8194444444444444,0.8194444444444444
text-classification,8,"This indicates that the proposed hierarchical pooling operation manages to abstract spatial ( word - order ) information from the input sequence , which is beneficial for performance in sentiment analysis tasks .",model,model,0,190,60,60,0,model : model,0.7063197026022305,0.8333333333333334,0.8333333333333334
text-classification,8,Short Sentence Processing,model,model,1,191,61,61,0,model : model,0.7100371747211895,0.8472222222222222,0.8472222222222222
text-classification,8,We now consider sentence - classification tasks ( with approximately 20 words on average ) .,model,model,0,192,62,62,0,model : model,0.7137546468401487,0.8611111111111112,0.8611111111111112
text-classification,8,"We experiment on three sentiment classification datasets , i.e. , MR , SST - 1 , SST - 2 , as well as subjectivity classification ( Subj ) and question classification ( TREC ) .",model,model,0,193,63,63,0,model : model,0.7174721189591078,0.875,0.875
text-classification,8,The corresponding results are shown in .,model,model,0,194,64,64,0,model : model,0.7211895910780669,0.8888888888888888,0.8888888888888888
text-classification,8,"Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .",model,model,1,195,65,65,0,model : model,0.724907063197026,0.9027777777777778,0.9027777777777778
text-classification,8,"However , SWEM exhibits comparable performance on the other two tasks , again with much less parameters and faster training .",model,model,1,196,66,66,0,model : model,0.7286245353159851,0.9166666666666666,0.9166666666666666
text-classification,8,"Further , we investigate two sequence tagging tasks : the standard CoNLL2000 chunking and CoNLL2003 NER datasets .",model,model,0,197,67,67,0,model : model,0.7323420074349443,0.9305555555555556,0.9305555555555556
text-classification,8,"Results are shown in the Supplementary Material , where LSTM and CNN again perform better than SWEMs .",model,model,0,198,68,68,0,model : model,0.7360594795539034,0.9444444444444444,0.9444444444444444
text-classification,8,"Generally , SWEM is less effective at extracting representations from short sentences than from long documents .",model,model,0,199,69,69,0,model : model,0.7397769516728625,0.9583333333333334,0.9583333333333334
text-classification,8,"This maybe due to the fact that for a shorter text sequence , word - order features tend to be more important since the semantic information provided byword embeddings alone is relatively limited .",model,model,0,200,70,70,0,model : model,0.7434944237918215,0.9722222222222222,0.9722222222222222
text-classification,8,"Moreover , we note that the results on these relatively small datasets are highly sensitive to model regularization techniques due to the overfitting issues .",model,model,0,201,71,71,0,model : model,0.7472118959107806,0.9861111111111112,0.9861111111111112
text-classification,8,"In this regard , one interesting future direction maybe to develop specific regularization strategies for the SWEM framework , and thus make them work better on small sentence classification datasets .",model,model,0,202,72,72,0,model : model,0.7509293680297398,1.0,1.0
text-classification,8,Discussion,discussion,discussion,0,203,1,1,0,discussion : discussion,0.7546468401486989,1.0,1.0
text-classification,8,Comparison via subspace training,training,training,0,204,1,1,0,training : training,0.758364312267658,0.03571428571428571,0.03571428571428571
text-classification,8,We use subspace training to measure the model complexity in text classification problems .,training,training,0,205,2,2,0,training : training,0.7620817843866171,0.07142857142857142,0.07142857142857142
text-classification,8,"It constrains the optimization of the trainable parameters in a subspace of low dimension d , the intrinsic dimension dint defines the minimum d that yield a good solution .",training,training,0,206,3,3,0,training : training,0.7657992565055762,0.10714285714285714,0.10714285714285714
text-classification,8,"Two models are studied : the SWEM - max variant , and the CNN model including a convolutional layer followed by a FC layer .",training,training,0,207,4,4,0,training : training,0.7695167286245354,0.14285714285714285,0.14285714285714285
text-classification,8,We consider two settings :,training,training,0,208,5,5,0,training : training,0.7732342007434945,0.17857142857142858,0.17857142857142858
text-classification,8,"1 ) The word embeddings are randomly intialized , and optimized jointly with the model parameters .",training,training,0,209,6,6,0,training : training,0.7769516728624535,0.21428571428571427,0.21428571428571427
text-classification,8,We show the performance of direct and subspace training on AG News dataset in ( a ) ( b ) .,training,training,0,210,7,7,0,training : training,0.7806691449814126,0.25,0.25
text-classification,8,The two models trained via direct method share almost identical perfomrnace on training and testing .,training,training,0,211,8,8,0,training : training,0.7843866171003717,0.2857142857142857,0.2857142857142857
text-classification,8,"The subspace training yields similar accuracy with direct training for very small d , even when model parameters are not trained at all ( d = 0 ) .",training,training,0,212,9,9,0,training : training,0.7881040892193308,0.32142857142857145,0.32142857142857145
text-classification,8,"This is because the word embeddings have the full degrees of freedom to adjust to achieve good solutions , regardless of the employed models .",training,training,0,213,10,10,0,training : training,0.79182156133829,0.35714285714285715,0.35714285714285715
text-classification,8,SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best solutions .,training,training,0,214,11,11,0,training : training,0.7955390334572491,0.39285714285714285,0.39285714285714285
text-classification,8,"According to Occam 's razor , simple models are preferred , if all else are the same .",training,training,0,215,12,12,0,training : training,0.7992565055762082,0.42857142857142855,0.42857142857142855
text-classification,8,"2 ) The pre-trained GloVe are frozen for the word embeddings , and only the model parameters are optimized .",training,training,0,216,13,13,0,training : training,0.8029739776951673,0.4642857142857143,0.4642857142857143
text-classification,8,"The results on testing datasets of AG News and Yelp P. are shown in ( c ) ( d ) , respectively .",training,training,0,217,14,14,0,training : training,0.8066914498141264,0.5,0.5
text-classification,8,"SWEM shows significantly higher accuracy than CNN for a large range of low subspace dimension , indicating that SWEM is more parameter - efficient to get a decent solution .",training,training,0,218,15,15,0,training : training,0.8104089219330854,0.5357142857142857,0.5357142857142857
text-classification,8,"In ( c ) , if we set the performance threshold Model MR SST - 1 SST - 2 Subj TREC RAE 77.7 43.2 82.4 --MV-RNN 79.0 44.4 82.9 --LSTM - 46.4 84.9 --RNN 77.2 --93.7 90.2 Constituency Tree-LSTM - 51.0 88.0 -- Dynamic CNN - 48.5 86.8 - 93.0 CNN 81 as 80 % testing accuracy , SWEM exhibits a lower dint than CNN on AG News dataset .",training,training,0,219,16,16,0,training : training,0.8141263940520446,0.5714285714285714,0.5714285714285714
text-classification,8,"However , in , CNN can leverage more trainable parameters to achieve higher accuracy when dis large .",training,training,0,220,17,17,0,training : training,0.8178438661710037,0.6071428571428571,0.6071428571428571
text-classification,8,Linear classifiers,training,training,0,221,18,18,0,training : training,0.8215613382899628,0.6428571428571429,0.6428571428571429
text-classification,8,"To further investigate the quality of representations learned from SWEMs , we employ a linear classifier on top of the representations for prediction , instead of a non-linear MLP layer as in the previous section .",training,training,0,222,19,19,0,training : training,0.8252788104089219,0.6785714285714286,0.6785714285714286
text-classification,8,It turned out that utilizing a linear classifier only leads to a very small performance drop for both Yahoo !,training,training,0,223,20,20,0,training : training,0.828996282527881,0.7142857142857143,0.7142857142857143
text-classification,8,Ans. ( from 73.53 % to 73.18 % ) and Yelp P. datasets ( from 93.76 % to 93.66 % ) .,training,training,0,224,21,21,0,training : training,0.8327137546468402,0.75,0.75
text-classification,8,This observation highlights that SWEMs are able to extract robust and informative sentence representations despite their simplicity .,training,training,0,225,22,22,0,training : training,0.8364312267657993,0.7857142857142857,0.7857142857142857
text-classification,8,Extension to other languages,training,training,0,226,23,23,0,training : training,0.8401486988847584,0.8214285714285714,0.8214285714285714
text-classification,8,"We have also tried our SWEM - concat and SWE Mhier models on Sogou news corpus ( with the same experimental setup as ) , which is a Chinese dataset represented by Pinyin ( a phonetic romanization of Chinese ) .",training,training,0,227,24,24,0,training : training,0.8438661710037175,0.8571428571428571,0.8571428571428571
text-classification,8,"SWEMconcat yields an accuracy of 91.3 % , while SWEM - hier ( with a local window size of 5 ) obtains an accuracy of 96.2 % on the test set .",training,training,0,228,25,25,0,training : training,0.8475836431226765,0.8928571428571429,0.8928571428571429
text-classification,8,"Notably , the performance of SWEM - hier is comparable to the best accuracies of CNN ( 95.6 % ) and LSTM ( 95.2 % ) , as reported in .",training,training,0,229,26,26,0,training : training,0.8513011152416357,0.9285714285714286,0.9285714285714286
text-classification,8,"This indicates that hierarchical pooling is more suitable than average / max pooling for Chinese text classification , by taking spatial information into account .",training,training,0,230,27,27,0,training : training,0.8550185873605948,0.9642857142857143,0.9642857142857143
text-classification,8,It also implies that Chinese is more sensitive to local word - order features than English .,training,training,0,231,28,28,0,training : training,0.8587360594795539,1.0,1.0
text-classification,8,Conclusions,conclusion,conclusion,0,232,1,1,0,conclusion : conclusion,0.862453531598513,0.02631578947368421,0.02631578947368421
text-classification,8,"We have performed a comparative study between SWEM ( with parameter - free pooling operations ) and CNN or LSTM - based models , to represent text sequences on 17 NLP datasets .",conclusion,conclusion,0,233,2,2,0,conclusion : conclusion,0.8661710037174721,0.05263157894736842,0.05263157894736842
text-classification,8,"We further validated our experimental findings through additional exploration , and revealed some general rules for rationally selecting compositional functions for distinct problems .",conclusion,conclusion,0,234,3,3,0,conclusion : conclusion,0.8698884758364313,0.07894736842105263,0.07894736842105263
text-classification,8,Our findings regarding when ( and why ) simple pooling operations are enough for text sequence representations are summarized as follows :,conclusion,conclusion,0,235,4,4,0,conclusion : conclusion,0.8736059479553904,0.10526315789473684,0.10526315789473684
text-classification,8,"Simple pooling operations are surprisingly effective at representing longer documents ( with hundreds of words ) , while recurrent / convolutional compositional functions are most effective when constructing representations for short sentences .",conclusion,conclusion,0,236,5,5,0,conclusion : conclusion,0.8773234200743495,0.13157894736842105,0.13157894736842105
text-classification,8,Sentiment analysis tasks are more sensitive to word - order features than topic categorization tasks .,conclusion,conclusion,0,237,6,6,0,conclusion : conclusion,0.8810408921933085,0.15789473684210525,0.15789473684210525
text-classification,8,"However , a simple hierarchical pooling layer proposed here achieves comparable results to LSTM / CNN on sentiment analysis tasks .",conclusion,conclusion,0,238,7,7,0,conclusion : conclusion,0.8847583643122676,0.18421052631578946,0.18421052631578946
text-classification,8,"To match natural language sentences , e.g. , textual entailment , answer sentence selection , etc. , simple pooling operations already exhibit similar or even superior results , compared to CNN and LSTM .",conclusion,conclusion,0,239,8,8,0,conclusion : conclusion,0.8884758364312267,0.21052631578947367,0.21052631578947367
text-classification,8,"We consider a wide range of text - representationbased tasks in this paper , including document categorization , text sequence matching and ( short ) sentence classification .",conclusion,conclusion,0,240,9,9,0,conclusion : conclusion,0.8921933085501859,0.23684210526315788,0.23684210526315788
text-classification,8,"For document classification tasks , we use the same data splits in ( downloaded from https://goo.gl/QaRpr7 ) ; for short sentence classification , we employ the same training / testing data and preprocessing procedure with .",conclusion,conclusion,0,241,10,10,0,conclusion : conclusion,0.895910780669145,0.2631578947368421,0.2631578947368421
text-classification,8,"The statistics and corresponding types of these datasets are summarized in Datasets #w #c Train Types SWEM - CRF indicates that CRF is directly operated on top of the word embedding layer and make predictions for each word ( there is no contextual / word - order information before CRF layer , compared to CNN - CRF or BI - LSTM - CRF ) .",conclusion,conclusion,0,242,11,11,0,conclusion : conclusion,0.8996282527881041,0.2894736842105263,0.2894736842105263
text-classification,8,"As shown above , CNN - CRF and BI - LSTM - CRF consistently outperform SWEM - CRF on both sequence tagging tasks , although the training takes around 4 to 5 times longer ( for BI - LSTM - CRF ) than SWEM - CRF .",conclusion,conclusion,0,243,12,12,0,conclusion : conclusion,0.9033457249070632,0.3157894736842105,0.3157894736842105
text-classification,8,"This suggests that for chunking and NER , compositional functions such as LSTM or CNN are very necessary , because of the sequential ( order-sensitive ) nature of sequence tagging tasks .",conclusion,conclusion,0,244,13,13,0,conclusion : conclusion,0.9070631970260223,0.34210526315789475,0.34210526315789475
text-classification,8,"What are the key words used for predictions ? Given the sparsity of word embeddings , one natural question would be : What are those key words thatare leveraged by the model to make predictions ? To this end , after training SWEM - max on Yahoo !",conclusion,conclusion,0,245,14,14,0,conclusion : conclusion,0.9107806691449815,0.3684210526315789,0.3684210526315789
text-classification,8,"What are the key words used for predictions ? Given the sparsity of word embeddings , one natural question would be : What are those key words thatare leveraged by the model to make predictions ? To this end , after training SWEM - max on Yahoo !",conclusion,conclusion,0,246,15,15,0,conclusion : conclusion,0.9144981412639405,0.39473684210526316,0.39473684210526316
text-classification,8,"What are the key words used for predictions ? Given the sparsity of word embeddings , one natural question would be : What are those key words thatare leveraged by the model to make predictions ? To this end , after training SWEM - max on Yahoo !",conclusion,conclusion,0,247,16,16,0,conclusion : conclusion,0.9182156133828996,0.42105263157894735,0.42105263157894735
text-classification,8,"Answer dataset , we selected the top - 10 words ( with the maximum values in that dimension ) for every word embedding dimension .",conclusion,conclusion,0,248,17,17,0,conclusion : conclusion,0.9219330855018587,0.4473684210526316,0.4473684210526316
text-classification,8,The results are visualized in .,conclusion,conclusion,0,249,18,18,0,conclusion : conclusion,0.9256505576208178,0.47368421052631576,0.47368421052631576
text-classification,8,"These words are indeed very predictive since they are likely to occur in documents with a specific topic , as discussed above .",conclusion,conclusion,0,250,19,19,0,conclusion : conclusion,0.929368029739777,0.5,0.5
text-classification,8,"Another interesting observation is that the frequencies of these words are actually quite low in the training set ( e.g. colston : 320 , repubs : 255 win32 : 276 ) , considering the large size of the training set ( 1,400 K ) .",conclusion,conclusion,0,251,20,20,0,conclusion : conclusion,0.9330855018587361,0.5263157894736842,0.5263157894736842
text-classification,8,"This suggests that the model is utilizing those relatively rare , yet representative words of each topic for the final predictions .",conclusion,conclusion,0,252,21,21,0,conclusion : conclusion,0.9368029739776952,0.5526315789473685,0.5526315789473685
text-classification,8,information of a text sequence is the word embedding .,conclusion,conclusion,0,253,22,22,0,conclusion : conclusion,0.9405204460966543,0.5789473684210527,0.5789473684210527
text-classification,8,"Thus , it is of interest to see how many word embedding dimensions are needed for a SWEM architecture to perform well .",conclusion,conclusion,0,254,23,23,0,conclusion : conclusion,0.9442379182156134,0.6052631578947368,0.6052631578947368
text-classification,8,"To this end , we vary the dimension from 3 to 1000 and train a SWEMconcat model on the Yahoo dataset .",conclusion,conclusion,0,255,24,24,0,conclusion : conclusion,0.9479553903345725,0.631578947368421,0.631578947368421
text-classification,8,"For fair comparison , the word embeddings are randomly initialized in this experiment , since there are no pretrained word vectors , such as GloVe , for some dimensions we consider .",conclusion,conclusion,0,256,25,25,0,conclusion : conclusion,0.9516728624535316,0.6578947368421053,0.6578947368421053
text-classification,8,"As shown in , the model exhibits higher accuracy with larger word embedding dimensions .",conclusion,conclusion,0,257,26,26,0,conclusion : conclusion,0.9553903345724907,0.6842105263157895,0.6842105263157895
text-classification,8,"This is not surprising since with more embedding dimensions , more semantic features could be potentially encapsulated .",conclusion,conclusion,0,258,27,27,0,conclusion : conclusion,0.9591078066914498,0.7105263157894737,0.7105263157894737
text-classification,8,"However , we also observe that even with only 10 dimensions , SWEM demonstrates comparable results relative to the case with 1000 dimensions , suggesting that word embeddings are very efficient at abstracting semantic information into fixed - length vectors .",conclusion,conclusion,0,259,28,28,0,conclusion : conclusion,0.9628252788104089,0.7368421052631579,0.7368421052631579
text-classification,8,"This property indicates that we may further reduce the number of model parameters with lowerdimensional word embeddings , while still achieving competitive results .",conclusion,conclusion,0,260,29,29,0,conclusion : conclusion,0.966542750929368,0.7631578947368421,0.7631578947368421
text-classification,8,Sensitivity of compositional functions to sample size,conclusion,conclusion,0,261,30,30,0,conclusion : conclusion,0.9702602230483272,0.7894736842105263,0.7894736842105263
text-classification,8,"To explore the robustness of different compositional functions , we consider another application scenario , where we only have a limited number of training data , e.g. , when labeled data are expensive to obtain .",conclusion,conclusion,0,262,31,31,0,conclusion : conclusion,0.9739776951672863,0.8157894736842105,0.8157894736842105
text-classification,8,"To investigate this , we re-run the experiments on Yahoo and SNLI datasets , while employing increasing proportions of the original training set .",conclusion,conclusion,0,263,32,32,0,conclusion : conclusion,0.9776951672862454,0.8421052631578947,0.8421052631578947
text-classification,8,"Specifically , we use 0.1 % , 0.2 % , 0.6 % , 1.0 % , 10 % , 100 % for comparison ; the corresponding results are shown in .",conclusion,conclusion,0,264,33,33,0,conclusion : conclusion,0.9814126394052045,0.868421052631579,0.868421052631579
text-classification,8,"Surprisingly , SWEM consistently outperforms CNN and LSTM models by a large margin , on a wide range of training data proportions .",conclusion,conclusion,0,265,34,34,0,conclusion : conclusion,0.9851301115241635,0.8947368421052632,0.8947368421052632
text-classification,8,"For instance , with 0.1 % of the training samples from Yahoo dataset ( around 1.4 K labeled data ) , SWEM achieves an accuracy of 56. 10 % , which is much better than that of models with CNN ( 25.32 % ) or LSTM ( 42.37 % ) .",conclusion,conclusion,0,266,35,35,0,conclusion : conclusion,0.9888475836431226,0.9210526315789473,0.9210526315789473
text-classification,8,"On the SNLI dataset , we also noticed the same trend that the SWEM architecture result in much better accuracies , with a fraction of training data .",conclusion,conclusion,0,267,36,36,0,conclusion : conclusion,0.9925650557620818,0.9473684210526315,0.9473684210526315
text-classification,8,"This observation indicates that overfitting issues in CNN or LSTMbased models on text data mainly stems from overcomplicated compositional functions , rather than the word embedding layer .",conclusion,conclusion,0,268,37,37,0,conclusion : conclusion,0.9962825278810409,0.9736842105263158,0.9736842105263158
text-classification,8,"More importantly , SWEM tends to be afar more robust model when only limited data are available for training .",conclusion,conclusion,0,269,38,38,0,conclusion : conclusion,1.0,1.0,1.0
text-classification,9,Translations as Additional Contexts for Sentence Classification,title,title,1,2,1,1,0,title : title,0.007936507936507936,1.0,1.0
text-classification,9,abstract,abstract,abstract,0,3,1,1,0,abstract : abstract,0.011904761904761904,0.125,0.125
text-classification,9,"In sentence classification tasks , additional contexts , such as the neighboring sentences , may improve the accuracy of the classifier .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.015873015873015872,0.25,0.25
text-classification,9,"However , such contexts are domain - dependent and thus can not be used for another classification task with an inappropriate domain .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01984126984126984,0.375,0.375
text-classification,9,"In contrast , we propose the use of translated sentences as domain - free context that is always available regardless of the domain .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.023809523809523808,0.5,0.5
text-classification,9,"We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier , due to possible inaccurate translations thus producing noisy sentence vectors .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.027777777777777776,0.625,0.625
text-classification,9,"To this end , we present multiple context fixing attachment ( MCFA ) , a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.031746031746031744,0.75,0.75
text-classification,9,"We show that our method performs competitively compared to previous models , achieving best classification performance on multiple data sets .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.03571428571428571,0.875,0.875
text-classification,9,We are the first to use translations as domainfree contexts for sentence classification .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.03968253968253968,1.0,1.0
text-classification,9,Introduction,introduction,introduction,0,11,1,1,0,introduction : introduction,0.04365079365079365,0.023809523809523808,0.023809523809523808
text-classification,9,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.047619047619047616,0.047619047619047616,0.047619047619047616
text-classification,9,"This task is important as it is widely used in almost all subare as of NLP such as sentiment classification for sentiment analysis and question type classification for question answering , to name a few .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.051587301587301584,0.07142857142857142,0.07142857142857142
text-classification,9,"While past methods require feature engineering , recent methods enjoy neural - based methods to automatically encode the sentences into low - dimensional dense vectors .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05555555555555555,0.09523809523809523,0.09523809523809523
text-classification,9,"Despite the success of these methods , the major challenge in this task is that extracting features from a single sentence limits the performance .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.05952380952380952,0.11904761904761904,0.11904761904761904
text-classification,9,"To overcome this limitation , recent works attempted to augment different kinds of features to the sentence , such as the neighboring sentences and the topics of the sentences .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.06349206349206349,0.14285714285714285,0.14285714285714285
text-classification,9,"However , these methods used domain - dependent contexts thatare only effective when the domain of the task is appropriate .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.06746031746031746,0.16666666666666666,0.16666666666666666
text-classification,9,"For one thing , neighboring sentences may not be available in some tasks such as question type classification .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.07142857142857142,0.19047619047619047,0.19047619047619047
text-classification,9,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.07539682539682539,0.21428571428571427,0.21428571428571427
text-classification,9,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts thatare always available no matter what the task domain is .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.07936507936507936,0.23809523809523808,0.23809523809523808
text-classification,9,We observe two opportunities when using translations .,introduction,introduction,0,21,11,11,0,introduction : introduction,0.08333333333333333,0.2619047619047619,0.2619047619047619
text-classification,9,"First , each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.0873015873015873,0.2857142857142857,0.2857142857142857
text-classification,9,contrasts the sentence vectors of the original English sentences and their Arabictranslated sentences in the question type classification task .,introduction,introduction,0,23,13,13,0,introduction : introduction,0.09126984126984126,0.30952380952380953,0.30952380952380953
text-classification,9,yellow circle signifies a clear separation of a class .,introduction,introduction,0,24,14,14,0,introduction : introduction,0.09523809523809523,0.3333333333333333,0.3333333333333333
text-classification,9,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.0992063492063492,0.35714285714285715,0.35714285714285715
text-classification,9,"Meanwhile , location type questions ( in orange ) are better classified in English .",introduction,introduction,0,26,16,16,0,introduction : introduction,0.10317460317460317,0.38095238095238093,0.38095238095238093
text-classification,9,"Second , the original sentences may include languagespecific ambiguity , which maybe resolved when presented with its translations .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.10714285714285714,0.40476190476190477,0.40476190476190477
text-classification,9,"Consider the example English sentence "" The movie is terribly amazing "" for the sentiment classification task .",introduction,introduction,0,28,18,18,0,introduction : introduction,0.1111111111111111,0.42857142857142855,0.42857142857142855
text-classification,9,"In this case , terribly can be used in both positive and negative sense , thus introduces ambiguity in the sentence .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.11507936507936507,0.4523809523809524,0.4523809523809524
text-classification,9,"When translated to Korean , it becomes "" ? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ? "" which means "" The movie is greatly magnificent "" , removing the ambiguity .",introduction,introduction,0,30,20,20,0,introduction : introduction,0.11904761904761904,0.47619047619047616,0.47619047619047616
text-classification,9,The above two observations hold only when translations are supported for ( nearly ) arbitrary language pairs with sufficiently high quality .,introduction,introduction,0,31,21,21,0,introduction : introduction,0.12301587301587301,0.5,0.5
text-classification,9,"Thankfully , translation services ( e.g. Google Translate )",introduction,introduction,0,32,22,22,0,introduction : introduction,0.12698412698412698,0.5238095238095238,0.5238095238095238
text-classification,9,"Moreover , recent research on neural machine translation ( NMT ) improved the efficiency and even enabled zero - shot translation of models for languages with no parallel data .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.13095238095238096,0.5476190476190477,0.5476190476190477
text-classification,9,"This provides an opportunity to leverage on as many languages as possible to any domain , providing a much wider context compared to the limited contexts provided by past studies .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.1349206349206349,0.5714285714285714,0.5714285714285714
text-classification,9,"However , despite the maturity of translation , naively concatenating their vectors to the original sentence vector may introduce more noise than signals .",introduction,introduction,0,35,25,25,0,introduction : introduction,0.1388888888888889,0.5952380952380952,0.5952380952380952
text-classification,9,The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable .,introduction,introduction,0,36,26,26,0,introduction : introduction,0.14285714285714285,0.6190476190476191,0.6190476190476191
text-classification,9,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .",introduction,introduction,0,37,27,27,0,introduction : introduction,0.14682539682539683,0.6428571428571429,0.6428571428571429
text-classification,9,Suppose there are two translated sentences a and b with slight errors .,introduction,introduction,0,38,28,28,0,introduction : introduction,0.15079365079365079,0.6666666666666666,0.6666666666666666
text-classification,9,"We posit that a can be used to fix b when a is used as a context of b , and vice versa",introduction,introduction,0,39,29,29,0,introduction : introduction,0.15476190476190477,0.6904761904761905,0.6904761904761905
text-classification,9,1 .,introduction,introduction,0,40,30,30,0,introduction : introduction,0.15873015873015872,0.7142857142857143,0.7142857142857143
text-classification,9,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",introduction,introduction,0,41,31,31,0,introduction : introduction,0.1626984126984127,0.7380952380952381,0.7380952380952381
text-classification,9,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",introduction,introduction,1,42,32,32,0,introduction : introduction,0.16666666666666666,0.7619047619047619,0.7619047619047619
text-classification,9,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",introduction,introduction,1,43,33,33,0,introduction : introduction,0.17063492063492064,0.7857142857142857,0.7857142857142857
text-classification,9,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",introduction,introduction,1,44,34,34,0,introduction : introduction,0.1746031746031746,0.8095238095238095,0.8095238095238095
text-classification,9,Noises from translation may cause adverse effects to the vector itself ( e.g. when a noisy vector is directly used for the task ) and relatively to other vectors ( e.g. when a noisy vector is used to fix another noisy vector ) .,introduction,introduction,0,45,35,35,0,introduction : introduction,0.17857142857142858,0.8333333333333334,0.8333333333333334
text-classification,9,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,introduction,introduction,1,46,36,36,0,introduction : introduction,0.18253968253968253,0.8571428571428571,0.8571428571428571
text-classification,9,"b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",introduction,introduction,1,47,37,37,0,introduction : introduction,0.1865079365079365,0.8809523809523809,0.8809523809523809
text-classification,9,Listed below are the three main strengths of the MCFA attachment .,introduction,introduction,0,48,38,38,0,introduction : introduction,0.19047619047619047,0.9047619047619048,0.9047619047619048
text-classification,9,"1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",introduction,introduction,0,49,39,39,0,introduction : introduction,0.19444444444444445,0.9285714285714286,0.9285714285714286
text-classification,9,2 ) MCFA is extensible and improves the accuracy as the number of translated sentences increases .,introduction,introduction,0,50,40,40,0,introduction : introduction,0.1984126984126984,0.9523809523809523,0.9523809523809523
text-classification,9,"3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .",introduction,introduction,0,51,41,41,0,introduction : introduction,0.20238095238095238,0.9761904761904762,0.9761904761904762
text-classification,9,"Results show that a convolutional neural network ( CNN ) attached with MCFA significantly improves the classification performance of CNN , achieving state of the 1 Hereon , we mean to "" fix "" as to "" correct , repair , or alter . "" art performance over multiple data sets .",introduction,introduction,0,52,42,42,0,introduction : introduction,0.20634920634920634,1.0,1.0
text-classification,9,Preliminaries,system description,Preliminaries,0,53,1,1,0,system description : Preliminaries,0.21031746031746032,0.030303030303030304,1.0
text-classification,9,Problem : Translated Sentences as Context,system description,Problem: Translated Sentences as Context,0,54,2,1,0,system description : Problem: Translated Sentences as Context,0.21428571428571427,0.06060606060606061,0.03125
text-classification,9,"In this paper , the ultimate task that we solve is the sentence classification task where given a sentence and a list of classes , one is task to classify which class ( e.g. positive or negative sentiment ) among the list of classes does the sentence belong .",system description,Problem: Translated Sentences as Context,0,55,3,2,0,system description : Problem: Translated Sentences as Context,0.21825396825396826,0.09090909090909091,0.0625
text-classification,9,"However , the main challenge that we tackle is the task on how to utilize translated sentences as additional context in order to improve the performance of the classifier .",system description,Problem: Translated Sentences as Context,0,56,4,3,0,system description : Problem: Translated Sentences as Context,0.2222222222222222,0.12121212121212122,0.09375
text-classification,9,"Specifically , the problem states : given the original sentence s , the goal is to use t 1 , t 2 , ... , tn , or sentences in other languages which are translated from s , as additional context .",system description,Problem: Translated Sentences as Context,0,57,5,4,0,system description : Problem: Translated Sentences as Context,0.2261904761904762,0.15151515151515152,0.125
text-classification,9,Base Model : Convolutional Neural Network .,system description,Problem: Translated Sentences as Context,0,58,6,5,0,system description : Problem: Translated Sentences as Context,0.23015873015873015,0.18181818181818182,0.15625
text-classification,9,The base model used is the convolutional neural network ( CNN ) for sentences .,system description,Problem: Translated Sentences as Context,0,59,7,6,0,system description : Problem: Translated Sentences as Context,0.23412698412698413,0.21212121212121213,0.1875
text-classification,9,It is a simple variation of the original CNN for texts to be used on sentences .,system description,Problem: Translated Sentences as Context,0,60,8,7,0,system description : Problem: Translated Sentences as Context,0.23809523809523808,0.24242424242424243,0.21875
text-classification,9,Let xi ? Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,system description,Problem: Translated Sentences as Context,0,61,9,8,0,system description : Problem: Translated Sentences as Context,0.24206349206349206,0.2727272727272727,0.25
text-classification,9,Let xi ? Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,system description,Problem: Translated Sentences as Context,0,62,10,9,0,system description : Problem: Translated Sentences as Context,0.24603174603174602,0.30303030303030304,0.28125
text-classification,9,convolution operation involves applying a filter matrix W ? R hd to a window of h words and producing a new feature vector c i using the equation,system description,Problem: Translated Sentences as Context,0,63,11,10,0,system description : Problem: Translated Sentences as Context,0.25,0.3333333333333333,0.3125
text-classification,9,convolution operation involves applying a filter matrix W ? R hd to a window of h words and producing a new feature vector c i using the equation,system description,Problem: Translated Sentences as Context,0,64,12,11,0,system description : Problem: Translated Sentences as Context,0.25396825396825395,0.36363636363636365,0.34375
text-classification,9,bias vector and f ( . ) is a non-linear function .,system description,Problem: Translated Sentences as Context,0,65,13,12,0,system description : Problem: Translated Sentences as Context,0.25793650793650796,0.3939393939393939,0.375
text-classification,9,"By doing this on all possible windows of words we produce a feature map c = [ c 1 , c 2 , ... ] .",system description,Problem: Translated Sentences as Context,0,66,14,13,0,system description : Problem: Translated Sentences as Context,0.2619047619047619,0.42424242424242425,0.40625
text-classification,9,We then apply a max - over - time pooling operation over the feature map and take the maximum value as the feature vector of the filter .,system description,Problem: Translated Sentences as Context,0,67,15,14,0,system description : Problem: Translated Sentences as Context,0.26587301587301587,0.45454545454545453,0.4375
text-classification,9,We do this on all feature vectors and concatenate all the feature vectors to obtain the final feature vector v.,system description,Problem: Translated Sentences as Context,0,68,16,15,0,system description : Problem: Translated Sentences as Context,0.2698412698412698,0.48484848484848486,0.46875
text-classification,9,We can then use this vector as input features to train a classifier such as logistic regression .,system description,Problem: Translated Sentences as Context,0,69,17,16,0,system description : Problem: Translated Sentences as Context,0.27380952380952384,0.5151515151515151,0.5
text-classification,9,"We use CNN to create sentence vectors for all sentences s , t 1 , t 2 , ... , tn .",system description,Problem: Translated Sentences as Context,0,70,18,17,0,system description : Problem: Translated Sentences as Context,0.2777777777777778,0.5454545454545454,0.53125
text-classification,9,"From hereon , we refer to these vectors as v s , v t1 , v t2 , ... , v tn , respectively .",system description,Problem: Translated Sentences as Context,0,71,19,18,0,system description : Problem: Translated Sentences as Context,0.28174603174603174,0.5757575757575758,0.5625
text-classification,9,We refer to them collectively as V .,system description,Problem: Translated Sentences as Context,0,72,20,19,0,system description : Problem: Translated Sentences as Context,0.2857142857142857,0.6060606060606061,0.59375
text-classification,9,Baseline 1 : Naive Concatenation .,system description,Problem: Translated Sentences as Context,0,73,21,20,0,system description : Problem: Translated Sentences as Context,0.2896825396825397,0.6363636363636364,0.625
text-classification,9,simple method in order to use the translated sentences as additional context is to naively concatenate their vectors with the vector of the original sentence .,system description,Problem: Translated Sentences as Context,0,74,22,21,0,system description : Problem: Translated Sentences as Context,0.29365079365079366,0.6666666666666666,0.65625
text-classification,9,"That is , we create a wide vectorv = [ v s ; v t1 ; ... ; v tn ] , and use this as the input feature vector of the sentence to the classifier .",system description,Problem: Translated Sentences as Context,0,75,23,22,0,system description : Problem: Translated Sentences as Context,0.2976190476190476,0.696969696969697,0.6875
text-classification,9,This method works fine if the translated sentences are translated properly .,system description,Problem: Translated Sentences as Context,0,76,24,23,0,system description : Problem: Translated Sentences as Context,0.30158730158730157,0.7272727272727273,0.71875
text-classification,9,"However , sentences translated using machine translation models usually contain incorrect translation .",system description,Problem: Translated Sentences as Context,0,77,25,24,0,system description : Problem: Translated Sentences as Context,0.3055555555555556,0.7575757575757576,0.75
text-classification,9,"In effect , this method will have adverse effects on the over all performance of the classifier .",system description,Problem: Translated Sentences as Context,0,78,26,25,0,system description : Problem: Translated Sentences as Context,0.30952380952380953,0.7878787878787878,0.78125
text-classification,9,This will especially be very evident if the number of additional sentences increases .,system description,Problem: Translated Sentences as Context,0,79,27,26,0,system description : Problem: Translated Sentences as Context,0.3134920634920635,0.8181818181818182,0.8125
text-classification,9,Baseline 2 : L2 Regularization .,system description,Problem: Translated Sentences as Context,0,80,28,27,0,system description : Problem: Translated Sentences as Context,0.31746031746031744,0.8484848484848485,0.84375
text-classification,9,"In order to alleviate the problems above , we can use L2 regularization to automatically select useful features by weakening the appropriate weights .",system description,Problem: Translated Sentences as Context,0,81,29,28,0,system description : Problem: Translated Sentences as Context,0.32142857142857145,0.8787878787878788,0.875
text-classification,9,The main problem of this method occurs when almost all of the weights coming from the vectors of the translated sentence are weakened .,system description,Problem: Translated Sentences as Context,0,82,30,29,0,system description : Problem: Translated Sentences as Context,0.3253968253968254,0.9090909090909091,0.90625
text-classification,9,This leads to making the additional context vectors useless and to having a similar performance when there are no additional context .,system description,Problem: Translated Sentences as Context,0,83,31,30,0,system description : Problem: Translated Sentences as Context,0.32936507936507936,0.9393939393939394,0.9375
text-classification,9,"Ultimately , this method does not make use of the full potential of the additional context .",system description,Problem: Translated Sentences as Context,0,84,32,31,0,system description : Problem: Translated Sentences as Context,0.3333333333333333,0.9696969696969697,0.96875
text-classification,9,usability usability ( a ) Self and relative usability modules,system description,Problem: Translated Sentences as Context,0,85,33,32,0,system description : Problem: Translated Sentences as Context,0.3373015873015873,1.0,1.0
text-classification,9,Model,model,Model,0,86,1,1,0,model : Model,0.3412698412698413,0.018867924528301886,0.16666666666666666
text-classification,9,"To solve the problems of the baselines discussed above , we introduce an attention - based neural multiple context fixing attachment ( MCFA ) 2 , a series of modules attached to the sentence vectors V .",model,Model,0,87,2,2,0,model : Model,0.34523809523809523,0.03773584905660377,0.3333333333333333
text-classification,9,"MCFA attachment is used to fix the sentence vectors , by slightly modifying the per-dimension values of the vector , before concatenating them into the final feature vector .",model,Model,0,88,3,3,0,model : Model,0.3492063492063492,0.05660377358490566,0.5
text-classification,9,"The sentence vectors are altered using other sentence vectors as context ( e.g. v t 1 is altered using v s , v t2 , ... , v tn ) .",model,Model,0,89,4,4,0,model : Model,0.3531746031746032,0.07547169811320754,0.6666666666666666
text-classification,9,This results to moving the vectors in the same vector space .,model,Model,0,90,5,5,0,model : Model,0.35714285714285715,0.09433962264150944,0.8333333333333334
text-classification,9,The full architecture is shown in .,model,Model,0,91,6,6,0,model : Model,0.3611111111111111,0.11320754716981132,1.0
text-classification,9,Self Usability Module,model,Self Usability Module,0,92,7,1,0,model : Self Usability Module,0.36507936507936506,0.1320754716981132,0.08333333333333333
text-classification,9,"To fix a source sentence vector 3 , we use the other sentence vectors as guide to know which dimensions to fix and to what extent do we need to fix them .",model,Self Usability Module,0,93,8,2,0,model : Self Usability Module,0.36904761904761907,0.1509433962264151,0.16666666666666666
text-classification,9,"However , other vectors might also contain errors which may reflect to the fixing of the source sentence vector .",model,Self Usability Module,0,94,9,3,0,model : Self Usability Module,0.373015873015873,0.16981132075471697,0.25
text-classification,9,"In order to cope with this , we introduce self usability modules .",model,Self Usability Module,0,95,10,4,0,model : Self Usability Module,0.376984126984127,0.18867924528301888,0.3333333333333333
text-classification,9,"self usability module contains the self usability of the vector ? i ( a ) , which measures how confident sentence a is for the task at hand .",model,Self Usability Module,0,96,11,5,0,model : Self Usability Module,0.38095238095238093,0.20754716981132076,0.4166666666666667
text-classification,9,"For example , an ambiguous sentence ( e.g. "" The movie is terribly amazing "" ) may receive a low self usability , while a clear and definite sentence ( e.g. "" The movie is very good "" ) may receive a high self usability .",model,Self Usability Module,0,97,12,6,0,model : Self Usability Module,0.38492063492063494,0.22641509433962265,0.5
text-classification,9,"Mathematically , we calculate the self usability of the vector vi of sentence i , denoted as ? i ( v i ) , using the equation",model,Self Usability Module,0,98,13,7,0,model : Self Usability Module,0.3888888888888889,0.24528301886792453,0.5833333333333334
text-classification,9,is a matrix to be learned .,model,Self Usability Module,0,99,14,8,0,model : Self Usability Module,0.39285714285714285,0.2641509433962264,0.6666666666666666
text-classification,9,The produced value is a single real number from 0 to 1 .,model,Self Usability Module,0,100,15,9,0,model : Self Usability Module,0.3968253968253968,0.2830188679245283,0.75
text-classification,9,We pre-calculate the self usability of all sentence vectors vi ? V .,model,Self Usability Module,0,101,16,10,0,model : Self Usability Module,0.4007936507936508,0.3018867924528302,0.8333333333333334
text-classification,9,We pre-calculate the self usability of all sentence vectors vi ? V .,model,Self Usability Module,0,102,17,11,0,model : Self Usability Module,0.40476190476190477,0.32075471698113206,0.9166666666666666
text-classification,9,"These are used in the next module , the relative usability module .",model,Self Usability Module,0,103,18,12,0,model : Self Usability Module,0.4087301587301587,0.33962264150943394,1.0
text-classification,9,Relative Usability,model,Relative Usability Module,0,104,19,1,0,model : Relative Usability Module,0.4126984126984127,0.3584905660377358,1.0
text-classification,9,Module,model,Vector Fixing Module,0,105,20,1,0,model : Vector Fixing Module,0.4166666666666667,0.37735849056603776,0.029411764705882353
text-classification,9,"Relative usability ? r ( a , b ) measures how useful a can be when fixing b , relative to other sentences .",model,Vector Fixing Module,0,106,21,2,0,model : Vector Fixing Module,0.42063492063492064,0.39622641509433965,0.058823529411764705
text-classification,9,"There are two main differences between ? i ( a ) and ? r ( a , b ) .",model,Vector Fixing Module,0,107,22,3,0,model : Vector Fixing Module,0.4246031746031746,0.41509433962264153,0.08823529411764706
text-classification,9,"First , ? i ( a ) is calculated before a knows about b while ? r ( a , b ) is calculated when a knows about b.",model,Vector Fixing Module,0,108,23,4,0,model : Vector Fixing Module,0.42857142857142855,0.4339622641509434,0.11764705882352941
text-classification,9,"Second , ? r ( a , b ) can below even though ? i ( a ) is not .",model,Vector Fixing Module,0,109,24,5,0,model : Vector Fixing Module,0.43253968253968256,0.4528301886792453,0.14705882352941177
text-classification,9,This means that a is notable to help in fixing the wrong information in b .,model,Vector Fixing Module,0,110,25,6,0,model : Vector Fixing Module,0.4365079365079365,0.4716981132075472,0.17647058823529413
text-classification,9,"Here , we extend the additive attention module and use it as a method to calculate the relative usability of two sentences of different languages .",model,Vector Fixing Module,0,111,26,7,0,model : Vector Fixing Module,0.44047619047619047,0.49056603773584906,0.20588235294117646
text-classification,9,"To better visualize the original attention mechanism , we present the equations below .",model,Vector Fixing Module,0,112,27,8,0,model : Vector Fixing Module,0.4444444444444444,0.5094339622641509,0.23529411764705882
text-classification,9,One major challenge in using the attention mechanism in our problem is that the sentence vectors do not belong to the same vector space .,model,Vector Fixing Module,0,113,28,9,0,model : Vector Fixing Module,0.44841269841269843,0.5283018867924528,0.2647058823529412
text-classification,9,"Moreover , one characteristic of our problem is that the sentence vectors can be both a source and a context vector ( e.g. v scan be both sand ti in Equation 1 ) .",model,Vector Fixing Module,0,114,29,10,0,model : Vector Fixing Module,0.4523809523809524,0.5471698113207547,0.29411764705882354
text-classification,9,"Because of these , we can not directly use the additive attention module .",model,Vector Fixing Module,0,115,30,11,0,model : Vector Fixing Module,0.45634920634920634,0.5660377358490566,0.3235294117647059
text-classification,9,"We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ? R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",model,Vector Fixing Module,0,116,31,12,0,model : Vector Fixing Module,0.4603174603174603,0.5849056603773585,0.35294117647058826
text-classification,9,"We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ? R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",model,Vector Fixing Module,0,117,32,13,0,model : Vector Fixing Module,0.4642857142857143,0.6037735849056604,0.38235294117647056
text-classification,9,"Finally , we incorporate the self usability function ? i ( v k ) to reflect the self usability of a sentence .",model,Vector Fixing Module,0,118,33,14,0,model : Vector Fixing Module,0.46825396825396826,0.6226415094339622,0.4117647058823529
text-classification,9,"Finally , we incorporate the self usability function ? i ( v k ) to reflect the self usability of a sentence .",model,Vector Fixing Module,0,119,34,15,0,model : Vector Fixing Module,0.4722222222222222,0.6415094339622641,0.4411764705882353
text-classification,9,"Ultimately , the relative usability denoted as ? r ( v i , v j ) is calculated using the equations below , where is the multiplication of a vector and a scalar through broadcasting .",model,Vector Fixing Module,0,120,35,16,0,model : Vector Fixing Module,0.47619047619047616,0.660377358490566,0.47058823529411764
text-classification,9,Vector Fixing Module,model,Vector Fixing Module,0,121,36,17,0,model : Vector Fixing Module,0.4801587301587302,0.6792452830188679,0.5
text-classification,9,The vector fixing module applies the attention weights to the sentence vectors and creates an integrated context vector .,model,Vector Fixing Module,0,122,37,18,0,model : Vector Fixing Module,0.48412698412698413,0.6981132075471698,0.5294117647058824
text-classification,9,We then use this vector alongside with the source sentence vector to create a weighted gate vector .,model,Vector Fixing Module,0,123,38,19,0,model : Vector Fixing Module,0.4880952380952381,0.7169811320754716,0.5588235294117647
text-classification,9,The weighted gate vector is used to determine to what extent should a dimension of the source sentence vector be altered .,model,Vector Fixing Module,0,124,39,20,0,model : Vector Fixing Module,0.49206349206349204,0.7358490566037735,0.5882352941176471
text-classification,9,The common way to apply the attention weights to the context vectors and create an integrated context vector c i is to directly do weighted sum of all the context vectors .,model,Vector Fixing Module,0,125,40,21,0,model : Vector Fixing Module,0.49603174603174605,0.7547169811320755,0.6176470588235294
text-classification,9,"However , this is not possible because the context vectors are not on the same space .",model,Vector Fixing Module,0,126,41,22,0,model : Vector Fixing Module,0.5,0.7735849056603774,0.6470588235294118
text-classification,9,"Thus , we use a projection matrix U k ? R dd to linearly project the sentence vector v k to transform the sentence vectors into a common vector space .",model,Vector Fixing Module,0,127,42,23,0,model : Vector Fixing Module,0.503968253968254,0.7924528301886793,0.6764705882352942
text-classification,9,"Thus , we use a projection matrix U k ? R dd to linearly project the sentence vector v k to transform the sentence vectors into a common vector space .",model,Vector Fixing Module,0,128,43,24,0,model : Vector Fixing Module,0.5079365079365079,0.8113207547169812,0.7058823529411765
text-classification,9,The integrated context vector c i is then calculated as,model,Vector Fixing Module,0,129,44,25,0,model : Vector Fixing Module,0.5119047619047619,0.8301886792452831,0.7352941176470589
text-classification,9,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ? R 2dd is a trainable parameter and ? is the element - wise multiplication procedure .",model,Vector Fixing Module,0,130,45,26,0,model : Vector Fixing Module,0.5158730158730159,0.8490566037735849,0.7647058823529411
text-classification,9,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ? R 2dd is a trainable parameter and ? is the element - wise multiplication procedure .",model,Vector Fixing Module,0,131,46,27,0,model : Vector Fixing Module,0.5198412698412699,0.8679245283018868,0.7941176470588235
text-classification,9,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ? R 2dd is a trainable parameter and ? is the element - wise multiplication procedure .",model,Vector Fixing Module,0,132,47,28,0,model : Vector Fixing Module,0.5238095238095238,0.8867924528301887,0.8235294117647058
text-classification,9,The weighted gate vector is a vector of real numbers between 0 and 1 to modify the intensity of per-dimension values of the sentence vector .,model,Vector Fixing Module,0,133,48,29,0,model : Vector Fixing Module,0.5277777777777778,0.9056603773584906,0.8529411764705882
text-classification,9,This causes the vector to move in the same vector space towards the correct direction .,model,Vector Fixing Module,0,134,49,30,0,model : Vector Fixing Module,0.5317460317460317,0.9245283018867925,0.8823529411764706
text-classification,9,"An alternative approach to do vector correction is using a residual - style correction , where instead of multiplying agate vector , a residual vector is added to the original vector .",model,Vector Fixing Module,0,135,50,31,0,model : Vector Fixing Module,0.5357142857142857,0.9433962264150944,0.9117647058823529
text-classification,9,"However , this approach makes the correction not interpretable ; it is hard to explain what does adding a value to a specific dimension mean .",model,Vector Fixing Module,0,136,51,32,0,model : Vector Fixing Module,0.5396825396825397,0.9622641509433962,0.9411764705882353
text-classification,9,One major advantage of MCFA is that the corrections in the vectors are interpretable ; the weights in the gate vector correspond to the importance of the per-dimension features of the vector .,model,Vector Fixing Module,0,137,52,33,0,model : Vector Fixing Module,0.5436507936507936,0.9811320754716981,0.9705882352941176
text-classification,9,"The altered vector ? v s , ... , v tn are then concatenated and fed directly as an input vector to the logistic regression classifier for training .",model,Vector Fixing Module,0,138,53,34,0,model : Vector Fixing Module,0.5476190476190477,1.0,1.0
text-classification,9,Experiments,experiment,Experiments,0,139,1,1,0,experiment : Experiments,0.5515873015873016,0.03571428571428571,1.0
text-classification,9,Experimental Setting,experiment,Experimental Setting,0,140,2,1,0,experiment : Experimental Setting,0.5555555555555556,0.07142857142857142,0.037037037037037035
text-classification,9,We test our model on four different data sets as listed below and summarized in .,experiment,Experimental Setting,0,141,3,2,0,experiment : Experimental Setting,0.5595238095238095,0.10714285714285714,0.07407407407407407
text-classification,9,a ) MR 4 : Movie reviews data where the task is to classify whether the review sentence has positive or negative sentiment .,experiment,Experimental Setting,0,142,4,3,0,experiment : Experimental Setting,0.5634920634920635,0.14285714285714285,0.1111111111111111
text-classification,9,b ) SUBJ : Subjectivity data where the task is to classify whether the sentence is subjective or objective .,experiment,Experimental Setting,0,143,5,4,0,experiment : Experimental Setting,0.5674603174603174,0.17857142857142858,0.14814814814814814
text-classification,9,c ) CR 5 : Customer reviews where,experiment,Experimental Setting,0,144,6,5,0,experiment : Experimental Setting,0.5714285714285714,0.21428571428571427,0.18518518518518517
text-classification,9,The task is to classify whether the review sentence is positive or negative .,experiment,Experimental Setting,0,145,7,6,0,experiment : Experimental Setting,0.5753968253968254,0.25,0.2222222222222222
text-classification,9,d ) TREC 6 : TREC question data set the task is to classify the type of question .,experiment,Experimental Setting,0,146,8,7,0,experiment : Experimental Setting,0.5793650793650794,0.2857142857142857,0.25925925925925924
text-classification,9,All our data sets are in English .,experiment,Experimental Setting,0,147,9,8,0,experiment : Experimental Setting,0.5833333333333334,0.32142857142857145,0.2962962962962963
text-classification,9,"For the additional contexts , we use ten other languages , selected based on their diversity and their performance on prior experiments : Arabic , Finnish , French , Italian , Korean , Mongolian , Norwegian , Polish , Russian , and Ukranian .",experiment,Experimental Setting,0,148,10,9,0,experiment : Experimental Setting,0.5873015873015873,0.35714285714285715,0.3333333333333333
text-classification,9,We translate the data sets using Google Translate .,experiment,Experimental Setting,0,149,11,10,0,experiment : Experimental Setting,0.5912698412698413,0.39285714285714285,0.37037037037037035
text-classification,9,Tokenization is done using the polyglot library 7 .,experiment,Experimental Setting,0,150,12,11,0,experiment : Experimental Setting,0.5952380952380952,0.42857142857142855,0.4074074074074074
text-classification,9,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,experiment,Experimental Setting,0,151,13,12,0,experiment : Experimental Setting,0.5992063492063492,0.4642857142857143,0.4444444444444444
text-classification,9,"For N = 1 , we only show the accuracy of the best classifier for conciseness .",experiment,Experimental Setting,0,152,14,13,0,experiment : Experimental Setting,0.6031746031746031,0.5,0.48148148148148145
text-classification,9,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",experiment,Experimental Setting,1,153,15,14,0,experiment : Experimental Setting,0.6071428571428571,0.5357142857142857,0.5185185185185185
text-classification,9,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",experiment,Experimental Setting,1,154,16,15,0,experiment : Experimental Setting,0.6111111111111112,0.5714285714285714,0.5555555555555556
text-classification,9,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,experiment,Experimental Setting,1,155,17,16,0,experiment : Experimental Setting,0.6150793650793651,0.6071428571428571,0.5925925925925926
text-classification,9,"We also use an l 2 constraint of 3 , following for accurate comparisons .",experiment,Experimental Setting,0,156,18,17,0,experiment : Experimental Setting,0.6190476190476191,0.6428571428571429,0.6296296296296297
text-classification,9,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,experiment,Experimental Setting,0,157,19,18,0,experiment : Experimental Setting,0.623015873015873,0.6785714285714286,0.6666666666666666
text-classification,9,"During training , we use mini-batch size of 50 .",experiment,Experimental Setting,1,158,20,19,0,experiment : Experimental Setting,0.626984126984127,0.7142857142857143,0.7037037037037037
text-classification,9,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,experiment,Experimental Setting,1,159,21,20,0,experiment : Experimental Setting,0.6309523809523809,0.75,0.7407407407407407
text-classification,9,We perform early stopping using a random 10 % of the training set as the development set .,experiment,Experimental Setting,1,160,22,21,0,experiment : Experimental Setting,0.6349206349206349,0.7857142857142857,0.7777777777777778
text-classification,9,"We present several competing models , listed below to compare the performance of our model .",experiment,Experimental Setting,0,161,23,22,0,experiment : Experimental Setting,0.6388888888888888,0.8214285714285714,0.8148148148148148
text-classification,9,uses topics as additional contexts and changes the CNN architecture .,experiment,Experimental Setting,0,162,24,23,0,experiment : Experimental Setting,0.6428571428571429,0.8571428571428571,0.8518518518518519
text-classification,9,TopCNN uses two types of topics : word- specific topic and sentence - specific topic ; and ( D ) CNN+ B1 and CNN +,experiment,Experimental Setting,0,163,25,24,0,experiment : Experimental Setting,0.6468253968253969,0.8928571428571429,0.8888888888888888
text-classification,9,B2 are the two baselines presented in this paper .,experiment,Experimental Setting,0,164,26,25,0,experiment : Experimental Setting,0.6507936507936508,0.9285714285714286,0.9259259259259259
text-classification,9,We do not show results from RNN models because they were shown to be less effective in sentence classification in our prior experiments .,experiment,Experimental Setting,0,165,27,26,0,experiment : Experimental Setting,0.6547619047619048,0.9642857142857143,0.9629629629629629
text-classification,9,"For models with additional context , we further use an ensemble classification model using a commonly used method by averaging the class probability scores generated by the multiple variants ( in our model 's case , N = 1 and N = 10 models ) , following .",experiment,Experimental Setting,0,166,28,27,0,experiment : Experimental Setting,0.6587301587301587,1.0,1.0
text-classification,9,Results and Discussion,result,Results and Discussion,0,167,1,1,0,result : Results and Discussion,0.6626984126984127,0.04,0.04
text-classification,9,We report the classification accuracy of the competing models in .,result,Results and Discussion,0,168,2,2,0,result : Results and Discussion,0.6666666666666666,0.08,0.08
text-classification,9,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,result,Results and Discussion,1,169,3,3,0,result : Results and Discussion,0.6706349206349206,0.12,0.12
text-classification,9,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",result,Results and Discussion,1,170,4,4,0,result : Results and Discussion,0.6746031746031746,0.16,0.16
text-classification,9,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .",result,Results and Discussion,1,171,5,5,0,result : Results and Discussion,0.6785714285714286,0.2,0.2
text-classification,9,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .",result,Results and Discussion,1,172,6,6,0,result : Results and Discussion,0.6825396825396826,0.24,0.24
text-classification,9,"We emphasize that we only use the basic CNN as our sentence encoder for our experiments , yet still achieve state of the art performance :",result,Results and Discussion,0,173,7,7,0,result : Results and Discussion,0.6865079365079365,0.28,0.28
text-classification,9,Classification accuracies of competing models .,result,Results and Discussion,0,174,8,8,0,result : Results and Discussion,0.6904761904761905,0.32,0.32
text-classification,9,"refers to the additional context , N refers to the number of translations .",result,Results and Discussion,0,175,9,9,0,result : Results and Discussion,0.6944444444444444,0.36,0.36
text-classification,9,"In TopCNN , word refers to using word - specific topic while sentence refers to using sentence - specific topic .",result,Results and Discussion,0,176,10,10,0,result : Results and Discussion,0.6984126984126984,0.4,0.4
text-classification,9,Accuracies colored red are accuracies that perform worse than CNN .,result,Results and Discussion,0,177,11,11,0,result : Results and Discussion,0.7023809523809523,0.44,0.44
text-classification,9,Previous state of the art results and the results of our best model are bold - faced .,result,Results and Discussion,0,178,12,12,0,result : Results and Discussion,0.7063492063492064,0.48,0.48
text-classification,9,The winning result is underlined .,result,Results and Discussion,0,179,13,13,0,result : Results and Discussion,0.7103174603174603,0.52,0.52
text-classification,9,"The number inside the parenthesis indicates the increase from the base model , CNN . on most data sets .",result,Results and Discussion,0,180,14,14,0,result : Results and Discussion,0.7142857142857143,0.56,0.56
text-classification,9,"Hence , MCFA is successful in effectively using translations as additional context to improve the performance of the classifier .",result,Results and Discussion,0,181,15,15,0,result : Results and Discussion,0.7182539682539683,0.6,0.6
text-classification,9,"We compare our model ( CNN + MCFA ) and the baselines discussed above ( CNN + B1 , CNN + B2 ) .",result,Results and Discussion,0,182,16,16,0,result : Results and Discussion,0.7222222222222222,0.64,0.64
text-classification,9,"On all settings , our model outperforms the baselines .",result,Results and Discussion,0,183,17,17,0,result : Results and Discussion,0.7261904761904762,0.68,0.68
text-classification,9,"When N = 10 , the performance of our model increases over the performance when N = 1 , however the performance of CNN + B1 decreases when compared to the performance when N = 1 .",result,Results and Discussion,0,184,18,18,0,result : Results and Discussion,0.7301587301587301,0.72,0.72
text-classification,9,We also show the accuracies of the worst classifiers when N = 1 in .,result,Results and Discussion,0,185,19,19,0,result : Results and Discussion,0.7341269841269841,0.76,0.76
text-classification,9,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .",result,Results and Discussion,0,186,20,20,0,result : Results and Discussion,0.7380952380952381,0.8,0.8
text-classification,9,"This is resolved by CNN + B2 by applying L2 regularization , however the increase in performance is marginal .",result,Results and Discussion,0,187,21,21,0,result : Results and Discussion,0.7420634920634921,0.84,0.84
text-classification,9,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .",result,Results and Discussion,0,188,22,22,0,result : Results and Discussion,0.746031746031746,0.88,0.88
text-classification,9,"Overall , we conclude that translations are better additional contexts than topics .",result,Results and Discussion,0,189,23,23,0,result : Results and Discussion,0.75,0.92,0.92
text-classification,9,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .",result,Results and Discussion,0,190,24,24,0,result : Results and Discussion,0.753968253968254,0.96,0.96
text-classification,9,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",result,Results and Discussion,0,191,25,25,0,result : Results and Discussion,0.7579365079365079,1.0,1.0
text-classification,9,Model Interpretation,model,Model Interpretation,0,192,1,1,0,model : Model Interpretation,0.7619047619047619,0.023809523809523808,0.05
text-classification,9,We first provide examples shown in on how the self usability module determines the score of sentences .,model,Model Interpretation,0,193,2,2,0,model : Model Interpretation,0.7658730158730159,0.047619047619047616,0.1
text-classification,9,"In the first example , it is hard to classify whether the translated sentence is positive or negative , thus it is given a low self usability score .",model,Model Interpretation,0,194,3,3,0,model : Model Interpretation,0.7698412698412699,0.07142857142857142,0.15
text-classification,9,"In the second example , although the sentence contains mistranslations , these are minimal and may actually help the classifier by telling it that thirst for violence is not a attention ( negative sentence ) the mothman prophecies , which is mostly a bore , seems to exist only for its climactic setpiece . negative phrase .",model,Model Interpretation,0,195,4,4,0,model : Model Interpretation,0.7738095238095238,0.09523809523809523,0.2
text-classification,9,"Thus , it is given a high self usability score .",model,Model Interpretation,0,196,5,5,0,model : Model Interpretation,0.7777777777777778,0.11904761904761904,0.25
text-classification,9,shows two data instance examples where we show the attention weights given to the other contexts when fixing a Korean sentence .,model,Model Interpretation,0,197,6,6,0,model : Model Interpretation,0.7817460317460317,0.14285714285714285,0.3
text-classification,9,"The larger the attention weight is , the more the context is used to fix the Korean sentence .",model,Model Interpretation,0,198,7,7,0,model : Model Interpretation,0.7857142857142857,0.16666666666666666,0.35
text-classification,9,In the Original sentence : skip this turd and pick your nose instead because you 're sure to get more out of the latter experience .,model,Model Interpretation,0,199,8,8,0,model : Model Interpretation,0.7896825396825397,0.19047619047619047,0.4
text-classification,9,Korean translation :,model,Model Interpretation,0,200,9,9,0,model : Model Interpretation,0.7936507936507936,0.21428571428571427,0.45
text-classification,9,Human re-translation :,model,Model Interpretation,0,201,10,10,0,model : Model Interpretation,0.7976190476190477,0.23809523809523808,0.5
text-classification,9,"In order to get more from the latter experience , you need to skip this puddle and choose your nose .",model,Model Interpretation,0,202,11,11,0,model : Model Interpretation,0.8015873015873016,0.2619047619047619,0.55
text-classification,9,Self,model,Model Interpretation,0,203,12,12,0,model : Model Interpretation,0.8055555555555556,0.2857142857142857,0.6
text-classification,9,Usability : 0.3958 ( a ) Low self usability example Original sentence : michael moore 's latest documentary about america 's thirst for violence is his best film yet . . .,model,Model Interpretation,0,204,13,13,0,model : Model Interpretation,0.8095238095238095,0.30952380952380953,0.65
text-classification,9,Korean translation :,model,Model Interpretation,0,205,14,14,0,model : Model Interpretation,0.8134920634920635,0.3333333333333333,0.7
text-classification,9,Human re-translation :,model,Model Interpretation,0,206,15,15,0,model : Model Interpretation,0.8174603174603174,0.35714285714285715,0.75
text-classification,9,"Michael Moore 's latest American documentary "" Violent Scene "" is his best film yet . . .",model,Model Interpretation,0,207,16,16,0,model : Model Interpretation,0.8214285714285714,0.38095238095238093,0.8
text-classification,9,Self,model,Model Interpretation,0,208,17,17,0,model : Model Interpretation,0.8253968253968254,0.40476190476190477,0.85
text-classification,9,Usability : 1.0000 ( b ) High self usability example you know that ten bucks you 'd spend on a ticket ? just send it to cranky .,model,Model Interpretation,0,209,18,18,0,model : Model Interpretation,0.8293650793650794,0.42857142857142855,0.9
text-classification,9,Usability : 1.0000 ( b ) High self usability example you know that ten bucks you 'd spend on a ticket ? just send it to cranky .,model,Model Interpretation,0,210,19,19,0,model : Model Interpretation,0.8333333333333334,0.4523809523809524,0.95
text-classification,9,we do n't get paid enough to sit through crap like this .,model,Model Interpretation,0,211,20,20,0,model : Model Interpretation,0.8373015873015873,0.47619047619047616,1.0
text-classification,9,NN ( altered ),model,NN (altered),0,212,21,1,0,model : NN (altered),0.8412698412698413,0.5,0.05
text-classification,9,"after scenes of nonsense , you 'll be wistful for the testosteronecharged wizardry of jerry bruckheimer productions , especially because half past dead is like the rock on walmart budget . :",model,NN (altered),0,213,22,2,0,model : NN (altered),0.8452380952380952,0.5238095238095238,0.1
text-classification,9,"Two example sentences , from English ( first ) and Korean ( second ) vector spaces , and their nearest neighbors ( NN ) on both the unaltered and altered vector spaces .",model,NN (altered),0,214,23,3,0,model : NN (altered),0.8492063492063492,0.5476190476190477,0.15
text-classification,9,We only show the original English sentences for the Korean example for conciseness .,model,NN (altered),0,215,24,4,0,model : NN (altered),0.8531746031746031,0.5714285714285714,0.2
text-classification,9,"first example , the Korean sentence contains translation errors ; especially , the words bore and climactic setpiece were not translated and were only spelled using the Korean alphabet .",model,NN (altered),0,216,25,5,0,model : NN (altered),0.8571428571428571,0.5952380952380952,0.25
text-classification,9,"In this example , the English attention weight is larger than the Korean attention weight .",model,NN (altered),0,217,26,6,0,model : NN (altered),0.8611111111111112,0.6190476190476191,0.3
text-classification,9,"In the second example , the Korean sentence correctly translates all parts of the English sentence , except for the phrase as it does in trouble .",model,NN (altered),0,218,27,7,0,model : NN (altered),0.8650793650793651,0.6428571428571429,0.35
text-classification,9,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",model,NN (altered),0,219,28,8,0,model : NN (altered),0.8690476190476191,0.6666666666666666,0.4
text-classification,9,"Thus , the Korean attention weight is larger .",model,NN (altered),0,220,29,9,0,model : NN (altered),0.873015873015873,0.6904761904761905,0.45
text-classification,9,shows the PCA visualization of the unaltered and the altered vectors of four different languages .,model,NN (altered),0,221,30,10,0,model : NN (altered),0.876984126984127,0.7142857142857143,0.5
text-classification,9,"In the first example , the unaltered sentence vectors are mostly in the middle of the vector space , making it hard to draw a boundary between the two examples .",model,NN (altered),0,222,31,11,0,model : NN (altered),0.8809523809523809,0.7380952380952381,0.55
text-classification,9,"After the fixing , the boundary is much clearer .",model,NN (altered),0,223,32,12,0,model : NN (altered),0.8849206349206349,0.7619047619047619,0.6
text-classification,9,We also show the English sentence vectors in the second example .,model,NN (altered),0,224,33,13,0,model : NN (altered),0.8888888888888888,0.7857142857142857,0.65
text-classification,9,"Even without fixing the unaltered English sentence vectors , it is easy to distinguish both classes .",model,NN (altered),0,225,34,14,0,model : NN (altered),0.8928571428571429,0.8095238095238095,0.7
text-classification,9,"After the fix , the sentence vectors in the middle of the space are moved , making the distinction more obvious and clearer .",model,NN (altered),0,226,35,15,0,model : NN (altered),0.8968253968253969,0.8333333333333334,0.75
text-classification,9,We also provide quantitative evidence by showing that the Mahalanobis distance between the two classes in the altered vectors are significantly farther than that of the unaltered vectors .,model,NN (altered),0,227,36,16,0,model : NN (altered),0.9007936507936508,0.8571428571428571,0.8
text-classification,9,We also show two examples sentences from English and Korean vector spaces and their corresponding nearest neighbors on both the unaltered and altered vector spaces in Table 5 .,model,NN (altered),0,228,37,17,0,model : NN (altered),0.9047619047619048,0.8809523809523809,0.85
text-classification,9,"In the first example , the unaltered vector focuses on the meaning of "" wasted yours "" in the sentence , which puts it near sentences regarding wasted time or money .",model,NN (altered),0,229,38,18,0,model : NN (altered),0.9087301587301587,0.9047619047619048,0.9
text-classification,9,"After fixing , the sentence vector focuses its meaning on the slow yet worth - the - wait pace of the movie , thus moving it closer to the correct vectors .",model,NN (altered),0,230,39,19,0,model : NN (altered),0.9126984126984127,0.9285714285714286,0.95
text-classification,9,"In the second example , all three sentences have highly descriptive tones , however , the nearest neighbor on the altered space is hyperbolically negative , comparing the movie to a description unrelated to the movie itself .",model,NN (altered),0,231,40,20,0,model : NN (altered),0.9166666666666666,0.9523809523809523,1.0
text-classification,9,NN ( Unaltered ),model,NN (Unaltered),0,232,41,1,0,model : NN (Unaltered),0.9206349206349206,0.9761904761904762,0.5
text-classification,9,"in the new release of cinema paradiso , the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .",model,NN (Unaltered),0,233,42,2,0,model : NN (Unaltered),0.9246031746031746,1.0,1.0
text-classification,9,Related Work,related work,Related Work,0,234,1,1,0,related work : Related Work,0.9285714285714286,0.07142857142857142,0.07142857142857142
text-classification,9,One way to improve the performance of a sentence classifier is to introduce new context .,related work,Related Work,0,235,2,2,0,related work : Related Work,0.9325396825396826,0.14285714285714285,0.14285714285714285
text-classification,9,"Common and obvious kinds of context are the neighboring sentences of the sentence , and the document where the sentence belongs .",related work,Related Work,0,236,3,3,0,related work : Related Work,0.9365079365079365,0.21428571428571427,0.21428571428571427
text-classification,9,Topics of the words in the sentence induced by a topic model were also used as contexts .,related work,Related Work,0,237,4,4,0,related work : Related Work,0.9404761904761905,0.2857142857142857,0.2857142857142857
text-classification,9,"In this paper , we introduce yet another type of additional context , sentence translations , which to the best of our knowledge have not been used previously .",related work,Related Work,0,238,5,5,0,related work : Related Work,0.9444444444444444,0.35714285714285715,0.35714285714285715
text-classification,9,Sentence encoders trained from neural machine translation ( NMT ) systems were also used for transfer learning .,related work,Related Work,0,239,6,6,0,related work : Related Work,0.9484126984126984,0.42857142857142855,0.42857142857142855
text-classification,9,demonstrated that altered - length sentence vectors from NMT encoders outperform sentence vectors from monolingual encoders on semantic similarity tasks .,related work,Related Work,0,240,7,7,0,related work : Related Work,0.9523809523809523,0.5,0.5
text-classification,9,Recent work used representation of each word in the sentence to create a sentence representation suitable for multiple NLP tasks .,related work,Related Work,0,241,8,8,0,related work : Related Work,0.9563492063492064,0.5714285714285714,0.5714285714285714
text-classification,9,"Our work shares the commonality of using NMT for another task , but instead of using NMT to encode our sentences , we use it to translate the sentences into new contexts .",related work,Related Work,0,242,9,9,0,related work : Related Work,0.9603174603174603,0.6428571428571429,0.6428571428571429
text-classification,9,Increasing the number of data instances of the training set has also been explored to improve the performance of a classifier .,related work,Related Work,0,243,10,10,0,related work : Related Work,0.9642857142857143,0.7142857142857143,0.7142857142857143
text-classification,9,"Recent methods include the usage of thesaurus , paraphrases , among others .",related work,Related Work,0,244,11,11,0,related work : Related Work,0.9682539682539683,0.7857142857142857,0.7857142857142857
text-classification,9,These simple variation techniques are preferred because they are found to be very effective despite their simplicity .,related work,Related Work,0,245,12,12,0,related work : Related Work,0.9722222222222222,0.8571428571428571,0.8571428571428571
text-classification,9,"Our work similarly augments training data , not by adding data instances ( vertical augmentation ) , but rather by adding more context ( horizontal augmentation ) .",related work,Related Work,0,246,13,13,0,related work : Related Work,0.9761904761904762,0.9285714285714286,0.9285714285714286
text-classification,9,"Though the paraphrase of p can be alternatively used as an augmented context , this could not leverage the added semantics coming from another language , as discussed in Section 1 .",related work,Related Work,0,247,14,14,0,related work : Related Work,0.9801587301587301,1.0,1.0
text-classification,9,Conclusion,conclusion,Conclusion,0,248,1,1,0,conclusion : Conclusion,0.9841269841269841,0.2,0.2
text-classification,9,This paper investigates the use of translations as better additional contexts for sentence classification .,conclusion,Conclusion,0,249,2,2,0,conclusion : Conclusion,0.9880952380952381,0.4,0.4
text-classification,9,"To answer the problem on mistranslations , we propose multiple context fixing attachment ( MCFA ) to fix the context vectors using other context vectors .",conclusion,Conclusion,0,250,3,3,0,conclusion : Conclusion,0.9920634920634921,0.6,0.6
text-classification,9,We show that our method improves the classification performance and achieves state - of - the - art perfor - mance on multiple data sets .,conclusion,Conclusion,0,251,4,4,0,conclusion : Conclusion,0.996031746031746,0.8,0.8
text-classification,9,"In our future work , we plan to use and extend our model to other complex NLP tasks .",conclusion,Conclusion,0,252,5,5,0,conclusion : Conclusion,1.0,1.0,1.0
