topic,paper_ID,text,section,label,position,section_position,citation,length
machine-translation,8,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,title,1,2,1,0,18
machine-translation,8,abstract,abstract,0,3,1,0,1
machine-translation,8,Neural machine translation is a recently proposed approach to machine translation .,abstract,0,4,2,0,12
machine-translation,8,"Unlike the traditional statistical machine translation , the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance .",abstract,0,5,3,0,29
machine-translation,8,The models proposed recently for neural machine translation often belong to a family of encoder - decoders and encode a source sentence into a fixed - length vector from which a decoder generates a translation .,abstract,0,6,4,0,36
machine-translation,8,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .",abstract,1,7,5,0,73
machine-translation,8,"With this new approach , we achieve a translation performance comparable to the existing state - of - the - art phrase - based system on the task of English - to - French translation .",abstract,0,8,6,0,36
machine-translation,8,"Furthermore , qualitative analysis reveals that the ( soft - ) alignments found by the model agree well with our intuition .",abstract,0,9,7,0,22
machine-translation,8,introduction,introduction,0,10,1,0,1
machine-translation,8,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",introduction,1,11,2,0,18
machine-translation,8,"Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components thatare tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .",introduction,0,12,3,0,47
machine-translation,8,"Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .",introduction,0,13,4,0,43
machine-translation,8,An encoder neural network reads and encodes a source sentence into a fixed - length vector .,introduction,0,14,5,0,17
machine-translation,8,A decoder then outputs a translation from the encoded vector .,introduction,0,15,6,0,11
machine-translation,8,"The whole encoder - decoder system , which consists of the encoder and the decoder for a language pair , is jointly trained to maximize the probability of a correct translation given a source sentence .",introduction,0,16,7,0,36
machine-translation,8,A potential issue with this encoder - decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed - length vector .,introduction,0,17,8,0,35
machine-translation,8,"This may make it difficult for the neural network to cope with long sentences , especially those thatare longer than the sentences in the training corpus .",introduction,0,18,9,0,27
machine-translation,8,showed that indeed the performance of a basic encoder - decoder deteriorates rapidly as the length of an input sentence increases .,introduction,0,19,10,0,22
machine-translation,8,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .",introduction,1,20,11,0,25
machine-translation,8,"Each time the proposed model generates a word in a translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated .",introduction,1,21,12,0,35
machine-translation,8,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,introduction,1,22,13,0,25
machine-translation,8,The most important distinguishing feature of this approach from the basic encoder - decoder is that it does not attempt to encode a whole input sentence into a single fixed - length vector .,introduction,0,23,14,0,34
machine-translation,8,"Instead , it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation .",introduction,0,24,15,0,25
machine-translation,8,"This frees a neural translation model from having to squash all the information of a source sentence , regardless of its length , into a fixed - length vector .",introduction,0,25,16,0,30
machine-translation,8,We show this allows a model to cope better with long sentences .,introduction,0,26,17,0,13
machine-translation,8,"In this paper , we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder - decoder approach .",introduction,0,27,18,0,30
machine-translation,8,"The improvement is more apparent with longer sentences , but can be observed with sentences of any length .",introduction,0,28,19,0,19
machine-translation,8,"On the task of English - to - French translation , the proposed approach achieves , with a single model , a translation performance comparable , or close , to the conventional phrase - based system .",introduction,0,29,20,0,37
machine-translation,8,"Furthermore , qualitative analysis reveals that the proposed model finds a linguistically plausible ( soft - ) alignment between a source sentence and the corresponding target sentence .",introduction,0,30,21,0,28
machine-translation,8,background : neural machine translation,introduction,0,31,22,0,5
machine-translation,8,"From a probabilistic perspective , translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x , i.e. , arg max y p ( y | x ) .",introduction,0,32,23,0,39
machine-translation,8,"In neural machine translation , we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus .",introduction,0,33,24,0,24
machine-translation,8,"Once the conditional distribution is learned by a translation model , given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability .",introduction,0,34,25,0,32
machine-translation,8,"Recently , a number of papers have proposed the use of neural networks to directly learn this conditional distribution ( see , e.g. , .",introduction,0,35,26,0,25
machine-translation,8,"This neural machine translation approach typically consists of two components , the first of which encodes a source sentence x and the second decodes to a target sentence y .",introduction,0,36,27,0,30
machine-translation,8,"For instance , two recurrent neural networks ( RNN ) were used by and to encode a variable - length source sentence into a fixed - length vector and to decode the vector into a variable - length target sentence .",introduction,0,37,28,0,41
machine-translation,8,"Despite being a quite new approach , neural machine translation has already shown promising results .",introduction,0,38,29,0,16
machine-translation,8,reported that the neural machine translation based on RNNs with long shortterm memory ( LSTM ) units achieves close to the state - of - the - art performance of the conventional phrase - based machine translation system on an English - to - French translation task .,introduction,0,39,30,0,48
machine-translation,8,"1 Adding neural components to existing translation systems , for instance , to score the phrase pairs in the phrase table or to re-rank candidate translations , has allowed to surpass the previous state - of - the - art performance level .",introduction,0,40,31,0,43
machine-translation,8,rnn encoder - decoder,introduction,0,41,32,0,4
machine-translation,8,"Here , we describe briefly the underlying framework , called RNN Encoder - Decoder , proposed by and upon which we build a novel architecture that learns to align and translate simultaneously .",introduction,0,42,33,0,33
machine-translation,8,"In the Encoder - Decoder framework , an encoder reads the input sentence , a sequence of vectors x = ( x 1 , , x Tx ) , into a vector c. 2 The most common approach is to use an RNN such that",introduction,0,43,34,0,45
machine-translation,8,"( 1 ) and c = q ( {h 1 , , h Tx } ) , where ht ?",introduction,0,44,35,0,20
machine-translation,8,"Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .",introduction,0,45,36,0,23
machine-translation,8,f and q are some nonlinear functions .,introduction,0,46,37,0,8
machine-translation,8,"used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .",introduction,0,47,38,0,22
machine-translation,8,"The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.",introduction,0,48,39,0,30
machine-translation,8,"In other words , the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals :",introduction,0,49,40,0,23
machine-translation,8,"where y = y 1 , , y Ty .",introduction,0,50,41,0,10
machine-translation,8,"With an RNN , each conditional probability is modeled as",introduction,0,51,42,0,10
machine-translation,8,"where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .",introduction,0,52,43,0,28
machine-translation,8,It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .,introduction,0,53,44,0,23
machine-translation,8,LEARNING TO ALIGN AND TRANSLATE,method,0,54,1,0,5
machine-translation,8,"In this section , we propose a novel architecture for neural machine translation .",method,0,55,2,0,14
machine-translation,8,The new architecture consists of a bidirectional RNN as an encoder ( Sec. 3.2 ) and a decoder that emulates searching through a source sentence during decoding a translation ( Sec. 3.1 ) .,method,0,56,3,0,34
machine-translation,8,decoder : general description,method,0,57,4,0,4
machine-translation,8,x 1 x 2 x 3 x T :,method,0,58,5,0,9
machine-translation,8,"The graphical illustration of the proposed model trying to generate the t-th target wordy t given a source sentence ( x 1 , x 2 , . . . , x T ) .",method,0,59,6,0,34
machine-translation,8,"In a new model architecture , we define each conditional probability in Eq .",method,0,60,7,0,14
machine-translation,8,( 2 ) as :,method,0,61,8,0,5
machine-translation,8,"where s i is an RNN hidden state for time i , computed by",method,0,62,9,0,14
machine-translation,8,It should be noted that unlike the existing encoder - decoder approach ( see Eq.,method,0,63,10,0,15
machine-translation,8,"( 2 ) ) , here the probability is conditioned on a distinct context vector c i for each target wordy i .",method,0,64,11,0,23
machine-translation,8,"The context vector c i depends on a sequence of annotations ( h 1 , , h Tx ) to which an encoder maps the input sentence .",method,0,65,12,0,28
machine-translation,8,Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i - th word of the input sequence .,method,0,66,13,0,28
machine-translation,8,We explain in detail how the annotations are computed in the next section .,method,0,67,14,0,14
machine-translation,8,"The context vector c i is , then , computed as a weighted sum of these annotations hi :",method,0,68,1,0,19
machine-translation,8,the weight ?,method,0,69,2,0,3
machine-translation,8,ij of each annotation h j is computed by,method,0,70,3,0,9
machine-translation,8,"where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .",method,0,71,4,0,32
machine-translation,8,"The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .",method,0,72,5,0,37
machine-translation,8,We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system .,method,0,73,6,0,25
machine-translation,8,"Note that unlike in traditional machine translation , the alignment is not considered to be a latent variable .",method,0,74,7,0,19
machine-translation,8,"Instead , the alignment model directly computes a soft alignment , which allows the gradient of the cost function to be backpropagated through .",method,0,75,8,0,24
machine-translation,8,This gradient can be used to train the alignment model as well as the whole translation model jointly .,method,0,76,9,0,19
machine-translation,8,"We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .",method,0,77,10,0,28
machine-translation,8,let ?,method,0,78,11,0,2
machine-translation,8,"ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .",method,0,79,12,0,23
machine-translation,8,"Then , the i - th context vector c i is the expected annotation over all the annotations with probabilities ? ij .",method,0,80,13,0,23
machine-translation,8,the probability ?,method,0,81,14,0,3
machine-translation,8,"ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .",method,0,82,15,0,40
machine-translation,8,"Intuitively , this implements a mechanism of attention in the decoder .",method,0,83,16,0,12
machine-translation,8,The decoder decides parts of the source sentence to pay attention to .,method,0,84,17,0,13
machine-translation,8,"By letting the decoder have an attention mechanism , we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector .",method,0,85,18,0,31
machine-translation,8,"With this new approach the information can be spread throughout the sequence of annotations , which can be selectively retrieved by the decoder accordingly .",method,0,86,19,0,25
machine-translation,8,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,method,0,87,20,0,7
machine-translation,8,"The usual RNN , described in Eq. ( 1 ) , reads an input sequence x in order starting from the first symbol x 1 to the last one x Tx .",method,0,88,21,0,32
machine-translation,8,"However , in the proposed scheme , we would like the annotation of each word to summarize not only the preceding words , but also the following words .",method,0,89,22,0,29
machine-translation,8,"Hence , we propose to use a bidirectional RNN ( BiRNN , , which has been successfully used recently in speech recognition ( see , e.g. , .",method,0,90,23,0,28
machine-translation,8,A BiRNN consists of forward and backward RNN 's .,method,0,91,24,0,10
machine-translation,8,the forward rnn ? ?,method,0,92,25,0,5
machine-translation,8,f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (,method,0,93,26,0,26
machine-translation,8,the backward rnn,method,0,94,27,0,3
machine-translation,8,? ?,method,0,95,28,0,2
machine-translation,8,"f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (",method,0,96,29,0,26
machine-translation,8,We obtain an annotation for each word x j by concatenating the forward hidden state ? ?,method,0,97,30,0,17
machine-translation,8,h j and the backward one,method,0,98,31,0,6
machine-translation,8,"In this way , the annotation h j contains the summaries of both the preceding words and the following words .",method,0,99,32,0,21
machine-translation,8,"Due to the tendency of RNNs to better represent recent inputs , the annotation h j will be focused on the words around x j .",method,0,100,33,0,26
machine-translation,8,This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) .,method,0,101,34,0,27
machine-translation,8,See for the graphical illustration of the proposed model .,method,0,102,35,0,10
machine-translation,8,experiment settings,experiment,0,103,1,0,2
machine-translation,8,We evaluate the proposed approach on the task of English - to - French translation .,experiment,0,104,2,0,16
machine-translation,8,"We use the bilingual , parallel corpora provided by ACL WMT ' 14 .",experiment,0,105,3,0,14
machine-translation,8,3,experiment,0,106,4,0,1
machine-translation,8,"As a comparison , we also report the performance of an RNN Encoder - Decoder which was proposed recently by .",method,0,107,1,0,21
machine-translation,8,We use the same training procedures and the same dataset for both models .,method,0,108,2,0,14
machine-translation,8,4,method,0,109,3,0,1
machine-translation,8,dataset,method,0,110,4,0,1
machine-translation,8,"WMT ' 14 contains the following English - French parallel corpora : Europarl ( 61 M words ) , news commentary ( 5.5 M ) , UN ( 421M ) and two crawled corpora of 90 M and 272.5 M words respectively , totaling 850M words .",method,0,111,5,0,47
machine-translation,8,"Following the procedure described in , we reduce the size of the combined corpus to have 348M words using the data selection method by .",method,0,112,6,0,25
machine-translation,8,"We do not use any monolingual data other than the mentioned parallel corpora , although it maybe possible to use a much larger monolingual corpus to pretrain an encoder .",method,0,113,7,0,30
machine-translation,8,"We concatenate news - test - After a usual tokenization 6 , we use a shortlist of 30,000 most frequent words in each language to train our models .",method,0,114,8,0,29
machine-translation,8,Any word not included in the shortlist is mapped to a special token ( [ UNK ] ) .,method,0,115,9,0,19
machine-translation,8,"We do not apply any other special preprocessing , such as lowercasing or stemming , to the data .",method,0,116,10,0,19
machine-translation,8,models,method,0,117,11,0,1
machine-translation,8,We train two types of models .,method,1,118,12,0,7
machine-translation,8,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",method,1,119,13,0,28
machine-translation,8,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .",method,1,120,14,0,46
machine-translation,8,The encoder and decoder of the RNNencdec have 1000 hidden units each .,method,1,121,15,0,13
machine-translation,8,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,method,1,122,16,0,22
machine-translation,8,It s decoder has 1000 hidden units .,method,1,123,17,0,8
machine-translation,8,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .",method,1,124,18,0,25
machine-translation,8,We use a minibatch stochastic gradient descent ( SGD ) algorithm together with Adadelta to train each model .,method,0,125,19,0,19
machine-translation,8,Each SGD update direction is computed using a minibatch of 80 sentences .,method,0,126,20,0,13
machine-translation,8,We trained each model for approximately 5 days .,method,0,127,21,0,9
machine-translation,8,"Once a model is trained , we use a beam search to find a translation that approximately maximizes the conditional probability ( see , e.g. , .",method,0,128,22,0,27
machine-translation,8,used this approach to generate translations from their neural machine translation model .,method,0,129,23,0,13
machine-translation,8,"For more details on the architectures of the models and training procedure used in the experiments , see Appendices A and B.",method,0,130,24,0,22
machine-translation,8,results,result,0,131,1,0,1
machine-translation,8,quantitative results,result,0,132,1,0,2
machine-translation,8,In : Four sample alignments found by RNNsearch - 50 .,result,0,133,2,0,11
machine-translation,8,"The x - axis and y-axis of each plot correspond to the words in the source sentence ( English ) and the generated translation ( French ) , respectively .",result,0,134,3,0,30
machine-translation,8,Each pixel shows the weight ?,result,0,135,4,0,6
machine-translation,8,"ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .",result,0,136,5,0,35
machine-translation,8,( a ) an arbitrary sentence .,result,0,137,6,0,7
machine-translation,8,( b - d ) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set .,result,0,138,7,0,29
machine-translation,8,One of the motivations behind the proposed approach was the use of a fixed - length context vector in the basic encoder - decoder approach .,result,0,139,8,0,26
machine-translation,8,We conjectured that this limitation may make the basic encoder - decoder approach to underperform with long sentences .,result,0,140,9,0,19
machine-translation,8,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .",result,1,141,10,0,19
machine-translation,8,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .",result,1,142,11,0,23
machine-translation,8,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .",result,1,143,12,0,19
machine-translation,8,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,result,1,144,13,0,32
machine-translation,8,tokens when only the sentences having no unknown words were evaluated ( last column ) .,result,0,145,14,0,16
machine-translation,8,qualitative analysis,result,0,146,15,0,2
machine-translation,8,alignment,result,0,147,16,0,1
machine-translation,8,The proposed approach provides an intuitive way to inspect the ( soft - ) alignment between the words in a generated translation and those in a source sentence .,result,0,148,17,0,29
machine-translation,8,This is done by visualizing the annotation weights ?,result,0,149,18,0,9
machine-translation,8,"ij from Eq. , as in .",result,0,150,19,0,7
machine-translation,8,Each row of a matrix in each plot indicates the weights associated with the annotations .,result,0,151,20,0,16
machine-translation,8,From this we see which positions in the source sentence were considered more important when generating the target word .,result,0,152,21,0,20
machine-translation,8,We can see from the alignments in that the alignment of words between English and French is largely monotonic .,result,0,153,22,0,20
machine-translation,8,We see strong weights along the diagonal of each matrix .,result,0,154,23,0,11
machine-translation,8,"However , we also observe a number of non-trivial , non-monotonic alignments .",result,0,155,24,0,13
machine-translation,8,"Adjectives and nouns are typically ordered differently between French and English , and we see an example in The strength of the soft - alignment , opposed to a hard - alignment , is evident , for instance , from ].",result,0,156,25,0,41
machine-translation,8,We observe similar behaviors in all the presented cases in .,result,0,157,26,0,11
machine-translation,8,"An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths , without requiring a counter - intuitive way of mapping some words to or from nowhere ( [ NULL ] ) ( see , e.g. , Chapters 4 and 5 of .",result,0,158,27,0,52
machine-translation,8,long sentences,result,0,159,28,0,2
machine-translation,8,As clearly visible from the proposed model ( RNNsearch ) is much better than the conventional model ( RNNencdec ) at translating long sentences .,result,0,160,29,0,25
machine-translation,8,"This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .",result,0,161,30,0,40
machine-translation,8,"As an example , consider this source sentence from the test set :",result,0,162,31,0,13
machine-translation,8,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",result,0,163,32,0,41
machine-translation,8,The RNNencdec - 50 translated this sentence into :,result,0,164,33,0,9
machine-translation,8,Un privilge d'admission est le droit d'un mdecin de reconnatre un patient l'hpital ou un centre mdical d'un diagnostic ou de prendre un diagnostic en fonction de sontat de sant .,result,0,165,34,0,31
machine-translation,8,"On the other hand , the RNNsearch - 50 generated the following correct translation , preserving the whole meaning of the input sentence without omitting any details :",result,0,166,35,0,28
machine-translation,8,"Un privilge d'admission est le droit d'un mdecin d'admettre un patient un hpital ou un centre mdical pour effectuer un diagnostic ou une procdure , selon son statut de travailleur des soins de sant l'hpital .",result,0,167,36,0,36
machine-translation,8,Let us consider another sentence from the test set :,result,0,168,37,0,10
machine-translation,8,"This kind of experience is part of Disney 's efforts to "" extend the lifetime of its series and build new relationships with audiences via digital platforms thatare becoming evermore important , "" he added .",result,0,169,38,0,36
machine-translation,8,The translation by the RNNencdec - 50 is,result,0,170,39,0,8
machine-translation,8,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",result,0,171,40,0,33
machine-translation,8,"As with the previous example , the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words ( see the underlined phrase ) .",result,0,172,41,0,30
machine-translation,8,"After that point , the quality of the translation deteriorates , with basic mistakes such as the lack of a closing quotation mark .",result,0,173,42,0,24
machine-translation,8,"Again , the RNNsearch - 50 was able to translate this long sentence correctly :",result,0,174,43,0,15
machine-translation,8,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",result,0,175,44,0,43
machine-translation,8,"In conjunction with the quantitative results presented already , these qualitative observations confirm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model .",result,0,176,45,0,33
machine-translation,8,"In Appendix C , we provide a few more sample translations of long source sentences generated by the RNNencdec - 50 , RNNsearch - 50 and Google Translate along with the reference translations .",result,0,177,46,0,34
machine-translation,8,6 related work,related work,0,178,1,0,3
machine-translation,8,learning to align,related work,0,179,2,0,3
machine-translation,8,A similar approach of aligning an output symbol with an input symbol was proposed recently by in the context of handwriting synthesis .,related work,0,180,3,0,23
machine-translation,8,Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters .,related work,0,181,4,0,20
machine-translation,8,"In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .",related work,0,182,5,0,37
machine-translation,8,"More specifically , his alignment was restricted to predict the location such that the location increases monotonically .",related work,0,183,6,0,18
machine-translation,8,"The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .",related work,0,184,7,0,25
machine-translation,8,"In the context of machine translation , this is a severe limitation , as ( long - distance ) reordering is often needed to generate a grammatically correct translation ( for instance , English - to - German ) .",related work,0,185,8,0,40
machine-translation,8,"Our approach , on the other hand , requires computing the annotation weight of every word in the source sentence for each word in the translation .",related work,0,186,9,0,27
machine-translation,8,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .,related work,0,187,10,0,25
machine-translation,8,"However , this may limit the applicability of the proposed scheme to other tasks .",related work,0,188,11,0,15
machine-translation,8,NEURAL NETWORKS FOR MACHINE TRANSLATION,related work,0,189,12,0,5
machine-translation,8,"Since introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words , neural networks have widely been used in machine translation .",related work,0,190,13,0,39
machine-translation,8,"However , the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system .",related work,0,191,14,0,38
machine-translation,8,"For instance , proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase - based statistical machine translation system .",related work,0,192,15,0,40
machine-translation,8,"More recently , and reported the successful use of the neural networks as a sub-component of the existing translation system .",related work,0,193,16,0,21
machine-translation,8,"Traditionally , a neural network trained as a target - side language model has been used to rescore or rerank a list of candidate translations ( see , e.g. , .",related work,0,194,17,0,31
machine-translation,8,"Although the above approaches were shown to improve the translation performance over the stateof - the - art machine translation systems , we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks .",related work,0,195,18,0,43
machine-translation,8,The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works .,related work,0,196,19,0,20
machine-translation,8,"Rather than using a neural network as apart of the existing system , our model works on its own and generates a translation from a source sentence directly .",related work,0,197,20,0,29
machine-translation,8,conclusion,related work,0,198,21,0,1
machine-translation,8,"The conventional approach to neural machine translation , called an encoder - decoder approach , encodes a whole input sentence into a fixed - length vector from which a translation will be decoded .",related work,0,199,22,0,34
machine-translation,8,"We conjectured that the use of a fixed - length context vector is problematic for translating long sentences , based on a recent empirical study reported by and .",related work,0,200,23,0,29
machine-translation,8,"In this paper , we proposed a novel architecture that addresses this issue .",related work,0,201,24,0,14
machine-translation,8,"We extended the basic encoder - decoder by letting a model ( soft - ) search for a set of input words , or their annotations computed by an encoder , when generating each target word .",related work,0,202,25,0,37
machine-translation,8,"This frees the model from having to encode a whole source sentence into a fixed - length vector , and also lets the model focus only on information relevant to the generation of the next target word .",related work,0,203,26,0,38
machine-translation,8,This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences .,related work,0,204,27,0,23
machine-translation,8,"Unlike with the traditional machine translation systems , all of the pieces of the translation system , including the alignment mechanism , are jointly trained towards a better log-probability of producing correct translations .",related work,0,205,28,0,34
machine-translation,8,"We tested the proposed model , called RNNsearch , on the task of English - to - French translation .",related work,0,206,29,0,20
machine-translation,8,"The experiment revealed that the proposed RNNsearch outperforms the conventional encoder - decoder model ( RNNencdec ) significantly , regardless of the sentence length and that it is much more robust to the length of a source sentence .",related work,0,207,30,0,39
machine-translation,8,"From the qualitative analysis where we investigated the ( soft - ) alignment generated by the RNNsearch , we were able to conclude that the model can correctly align each target word with the relevant words , or their annotations , in the source sentence as it generated a correct translation .",related work,0,208,31,0,52
machine-translation,8,"Perhaps more importantly , the proposed approach achieved a translation performance comparable to the existing phrase - based statistical machine translation .",related work,0,209,32,0,22
machine-translation,8,"It is a striking result , considering that the proposed architecture , or the whole family of neural machine translation , has only been proposed as recently as this year .",related work,0,210,33,0,31
machine-translation,8,We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general .,related work,0,211,34,0,24
machine-translation,8,"One of challenges left for the future is to better handle unknown , or rare words .",related work,0,212,35,0,17
machine-translation,8,This will be required for the model to be more widely used and to match the performance of current state - of - the - art machine translation systems in all contexts .,related work,0,213,36,0,33
machine-translation,8,a model architecture,related work,0,214,37,0,3
machine-translation,8,a.1 architectural choices,related work,0,215,38,0,3
machine-translation,8,"The proposed scheme in Section 3 is a general framework where one can freely define , for instance , the activation functions f of recurrent neural networks ( RNN ) and the alignment model a .",related work,0,216,39,0,36
machine-translation,8,"Here , we describe the choices we made for the experiments in this paper .",related work,0,217,40,0,15
machine-translation,8,A.1.1 RECURRENT NEURAL NETWORK,related work,0,218,41,0,4
machine-translation,8,"For the activation function f of an RNN , we use the gated hidden unit recently proposed by .",related work,0,219,42,0,19
machine-translation,8,The gated hidden unit is an alternative to the conventional simple units such as an element - wise tanh .,related work,0,220,43,0,20
machine-translation,8,"This gated unit is similar to along short - term memory ( LSTM ) unit proposed earlier by , sharing with it the ability to better model and learn long - term dependencies .",related work,0,221,44,0,34
machine-translation,8,This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1 .,related work,0,222,45,0,23
machine-translation,8,These paths allow gradients to flow backward easily without suffering too much from the vanishing effect .,related work,0,223,46,0,17
machine-translation,8,"It is therefore possible to use LSTM units instead of the gated hidden unit described here , as was done in a similar context by .",related work,0,224,47,0,26
machine-translation,8,The new state s i of the RNN employing n gated hidden units 8 is computed by,related work,0,225,48,0,17
machine-translation,8,"where is an element - wise multiplication , and z i is the output of the update gates ( see below ) .",related work,0,226,49,0,23
machine-translation,8,The proposed updated states i is computed b ?,related work,0,227,50,0,9
machine-translation,8,where e ( y,related work,0,228,51,0,4
machine-translation,8,i?1 ) ?,related work,0,229,52,0,3
machine-translation,8,"R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .",related work,0,230,53,0,27
machine-translation,8,"When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ?",related work,0,231,54,0,29
machine-translation,8,r mk .,related work,0,232,55,0,3
machine-translation,8,"Whenever possible , we omit bias terms to make the equations less cluttered .",related work,0,233,56,0,14
machine-translation,8,"The update gates z i allow each hidden unit to maintain its previous activation , and the reset gates r i control how much and what information from the previous state should be reset .",related work,0,234,57,0,35
machine-translation,8,we compute them by,related work,0,235,58,0,4
machine-translation,8,where ? ( ) is a logistic sigmoid function .,related work,0,236,59,0,10
machine-translation,8,"At each step of the decoder , we compute the output probability ( Eq. ( 4 ) ) as a multi -layered function .",related work,0,237,60,0,24
machine-translation,8,We use a single hidden layer of maxout units and normalize the output probabilities ( one for each word ) with a softmax function ( see Eq. ) .,related work,0,238,61,0,29
machine-translation,8,A.1.2 ALIGNMENT MODEL,related work,0,239,62,0,3
machine-translation,8,The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .,related work,0,240,63,0,29
machine-translation,8,"In order to reduce computation , we use a singlelayer multilayer perceptron such that",related work,0,241,64,0,14
machine-translation,8,where w a ?,related work,0,242,65,0,4
machine-translation,8,"r nn , u a ?",related work,0,243,66,0,6
machine-translation,8,R n 2n and v a ?,related work,0,244,67,0,7
machine-translation,8,Rn are the weight matrices .,related work,0,245,68,0,6
machine-translation,8,since,related work,0,246,69,0,1
machine-translation,8,"U ah j does not depend on i , we can pre-compute it in advance to minimize the computational cost .",related work,0,247,70,0,21
machine-translation,8,A.2 DETAILED DESCRIPTION OF THE MODEL,related work,0,248,71,0,6
machine-translation,8,a.2.1 encoder,related work,0,249,72,0,2
machine-translation,8,"In this section , we describe in detail the architecture of the proposed model ( RNNsearch ) used in the experiments ( see .",related work,0,250,73,0,24
machine-translation,8,"From hereon , we omit all bias terms in order to increase readability .",related work,0,251,74,0,14
machine-translation,8,The model takes a source sentence of 1 - of - K coded word vectors as input,related work,0,252,75,0,17
machine-translation,8,"and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky ,",related work,0,253,76,0,34
machine-translation,8,"where K x and Ky are the vocabulary sizes of source and target languages , respectively .",related work,0,254,77,0,17
machine-translation,8,T x and Ty respectively denote the lengths of source and target sentences .,related work,0,255,78,0,14
machine-translation,8,"First , the forward states of the bidirectional recurrent neural network ( BiRNN ) are computed :",related work,0,256,79,0,17
machine-translation,8,are weight matrices .,related work,0,257,80,0,4
machine-translation,8,"m and n are the word embedding dimensionality and the number of hidden units , respectively . ? ( ) is as usual a logistic sigmoid function .",related work,0,258,81,0,28
machine-translation,8,"The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .",related work,0,259,82,0,19
machine-translation,8,"We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .",related work,0,260,83,0,19
machine-translation,8,"We concatenate the forward and backward states to to obtain the annotations ( h 1 , h 2 , , h Tx ) , where",related work,0,261,84,0,25
machine-translation,8,a.,related work,0,262,85,0,1
machine-translation,8,decoder,related work,0,263,86,0,1
machine-translation,8,The hidden state s i of the decoder given the annotations from the encoder is computed by,related work,0,264,87,0,17
machine-translation,8,E is the word embedding matrix for the target language .,related work,0,265,88,0,11
machine-translation,8,"W , W z , W r ?",related work,0,266,89,0,8
machine-translation,8,"R nm , U , U z , Ur ? R nn , and C , C z , Cr ?",related work,0,267,90,0,21
machine-translation,8,R n 2n are weights .,related work,0,268,91,0,6
machine-translation,8,"Again , m and n are the word embedding dimensionality and the number of hidden units , respectively .",related work,0,269,92,0,19
machine-translation,8,The initial hidden state s 0 is computed by,related work,0,270,93,0,9
machine-translation,8,The context vector c i are recomputed at each step by the alignment model : :,related work,0,271,94,0,16
machine-translation,8,Learning statistics and relevant information .,related work,0,272,95,0,6
machine-translation,8,Each update corresponds to updating the parameters once using a single minibatch .,related work,0,273,96,0,13
machine-translation,8,One epoch is one pass through the training set .,related work,0,274,97,0,10
machine-translation,8,NLL is the average conditional log-probabilities of the sentences in either the training set or the development set .,related work,0,275,98,0,19
machine-translation,8,Note that the lengths of the sentences differ .,related work,0,276,99,0,9
machine-translation,8,where,related work,0,277,100,0,1
machine-translation,8,and h j is the j - th annotation in the source sentence ( see Eq. ) .,related work,0,278,101,0,18
machine-translation,8,v a ?,related work,0,279,102,0,3
machine-translation,8,"rn , w a ?",related work,0,280,103,0,5
machine-translation,8,Rn n and U a ?,related work,0,281,104,0,6
machine-translation,8,Rn 2n are weight matrices .,related work,0,282,105,0,6
machine-translation,8,Note that the model becomes RNN Encoder - Decoder,related work,0,283,106,0,9
machine-translation,8,"With the decoder state s i ?1 , the context c i and the last generated wordy i ? 1 , we define the probability of a target wordy i as",related work,0,284,107,0,31
machine-translation,8,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ?",related work,0,285,108,0,37
machine-translation,8,and co ?,related work,0,286,109,0,3
machine-translation,8,R 2 l 2n are weight matrices .,related work,0,287,110,0,8
machine-translation,8,This can be understood as having a deep output with a single maxout hidden layer .,related work,0,288,111,0,16
machine-translation,8,A.2.3 MODEL SIZE,related work,0,289,112,0,3
machine-translation,8,"For all the models used in this paper , the size of a hidden layer n is 1000 , the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500 .",related work,0,290,113,0,42
machine-translation,8,The number of hidden units in the alignment model n is 1000 .,related work,0,291,114,0,13
machine-translation,8,and ? ?,related work,0,292,115,0,3
machine-translation,8,Ur as random orthogonal matrices .,related work,0,293,116,0,6
machine-translation,8,"For W a and U a , we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.001 2 .",related work,0,294,117,0,26
machine-translation,8,All the elements of Va and all the bias vectors were initialized to zero .,related work,0,295,118,0,15
machine-translation,8,Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.01 2 .,related work,0,296,119,0,20
machine-translation,8,b.2 training,related work,0,297,120,0,2
machine-translation,8,We used the stochastic gradient descent ( SGD ) algorithm .,related work,0,298,121,0,11
machine-translation,8,Adadelta was used to automatically adapt the learning rate of each parameter ( = 10 ?6 and ? = 0.95 ) .,related work,0,299,122,0,22
machine-translation,8,"We explicitly normalized the L 2 - norm of the gradient of the cost function each time to beat most a predefined threshold of 1 , when the norm was larger than the threshold .",related work,0,300,123,0,35
machine-translation,8,Each SGD update direction was computed with a minibatch of 80 sentences .,related work,0,301,124,0,13
machine-translation,8,At each update our implementation requires time proportional to the length of the longest sentence in a minibatch .,related work,0,302,125,0,19
machine-translation,8,"Hence , to minimize the waste of computation , before every 20 - th update , we retrieved 1600 sentence pairs , sorted them according to the lengths and split them into 20 minibatches .",related work,0,303,126,0,35
machine-translation,8,The training data was shuffled once before training and was traversed sequentially in this manner .,related work,0,304,127,0,16
machine-translation,8,In Tables 2 we present the statistics related to training all the models used in the experiments .,related work,0,305,128,0,18
machine-translation,8,C TRANSLATIONS OF LONG SENTENCES,related work,0,306,129,0,5
machine-translation,8,source,related work,0,307,130,0,1
machine-translation,8,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",related work,0,308,131,0,41
machine-translation,8,reference,related work,0,309,132,0,1
machine-translation,8,"Le privilge d'admission est le droit d'un mdecin , en vertu de son statut de membre soignant d'un hpital , d'admettre un patient dans un hpital ou un centre mdical afin d 'y dlivrer un diagnostic ou un traitement .",related work,0,310,133,0,40
machine-translation,8,rnnenc - 50,related work,0,311,134,0,3
machine-translation,8,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",related work,0,312,135,0,33
machine-translation,8,rnnsearch - 50,related work,0,313,136,0,3
machine-translation,8,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",related work,0,314,137,0,43
machine-translation,8,google translate,related work,0,315,138,0,2
machine-translation,8,"Ce genre d'exprience fait partie des efforts de Disney "" tendre la dure de vie de sa srie et construire de nouvelles relations avec le public par le biais des plates - formes numriques qui deviennent de plus en plus important "" , at - il ajout .",related work,0,316,139,0,48
machine-translation,8,source,related work,0,317,140,0,1
machine-translation,8,"In a press conference on Thursday , Mr Blair stated that there was nothing in this video that might constitute a "" reasonable motive "" that could lead to criminal charges being brought against the mayor .",related work,0,318,141,0,37
machine-translation,8,reference,related work,0,319,142,0,1
machine-translation,8,"En confrence de presse , jeudi , M. Blair a affirm qu'il n'y avait rien dans cette vido qui puisse constituer des "" motifs raisonnables "" pouvant mener au dpt d'une accusation criminelle contre le maire .",related work,0,320,143,0,37
machine-translation,8,rnnenc - 50,related work,0,321,144,0,3
machine-translation,8,"Lors de la confrence de presse de jeudi , M. Blair a dit qu'il n'y avait rien dans cette vido qui pourrait constituer une "" motivation raisonnable "" pouvant entraner des accusations criminelles portes contre le maire .",related work,0,322,145,0,38
machine-translation,8,rnnsearch - 50,related work,0,323,146,0,3
machine-translation,8,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait conduire des accusations criminelles contre le maire .",related work,0,324,147,0,36
machine-translation,8,google translate,related work,0,325,148,0,2
machine-translation,8,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait mener des accusations criminelles portes contre le maire . :",related work,0,326,149,0,38
machine-translation,8,The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .,related work,0,327,150,0,27
machine-translation,8,"For each source sentence , we also show the goldstandard translation .",related work,0,328,151,0,12
machine-translation,8,The translations by Google Translate were made on 27 August 2014 .,related work,0,329,152,0,12
machine-translation,8,reference,related work,0,330,153,0,1
machine-translation,8,"Ce type d'exprience entre dans le cadre des efforts de Disney pour "" tendre la dure de vie de ses sries et construire de nouvelles relations avec son public grce des plateformes numriques qui so nt de plus en plus importantes "" , a-t - il ajout .",related work,0,331,154,0,48
machine-translation,9,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,title,1,2,1,0,17
machine-translation,9,abstract,abstract,0,3,1,0,1
machine-translation,9,"Natural language processing ( NLP ) models often require a massive number of parameters for word embeddings , resulting in a large storage or memory footprint .",abstract,0,4,2,0,27
machine-translation,9,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,abstract,1,5,3,0,19
machine-translation,9,"For this purpose , we propose to construct the embeddings with few basis vectors .",abstract,0,6,4,0,15
machine-translation,9,"For each word , the composition of basis vectors is determined by a hash code .",abstract,0,7,5,0,16
machine-translation,9,"To maximize the compression rate , we adopt the multi-codebook quantization approach instead of binary coding scheme .",abstract,0,8,6,0,18
machine-translation,9,"Each code is composed of multiple discrete numbers , such as ( 3 , 2 , 1 , 8 ) , where the value of each component is limited to a fixed range .",abstract,0,9,7,0,34
machine-translation,9,We propose to directly learn the discrete codes in an end - to - end neural network by applying the Gumbel - softmax trick .,abstract,0,10,8,0,25
machine-translation,9,Experiments show the compression rate achieves 98 % in a sentiment analysis task and 94 % ? 99 % in machine translation tasks without performance loss .,abstract,0,11,9,0,27
machine-translation,9,"In both tasks , the proposed method can improve the model performance by slightly lowering the compression rate .",abstract,0,12,10,0,19
machine-translation,9,"Compared to other approaches such as character - level segmentation , the proposed method is language - independent and does not require modifications to the network architecture .",abstract,0,13,11,0,28
machine-translation,9,introduction,introduction,0,14,1,0,1
machine-translation,9,Word embeddings play an important role in neural - based natural language processing ( NLP ) models .,introduction,0,15,2,0,18
machine-translation,9,Neural word embeddings encapsulate the linguistic information of words in continuous vectors .,introduction,0,16,3,0,13
machine-translation,9,"However , as each word is assigned an independent embedding vector , the number of parameters in the embedding matrix can be huge .",introduction,0,17,4,0,24
machine-translation,9,"For example , when each embedding has 500 dimensions , the network has to hold 100M embedding parameters to represent 200K words .",introduction,0,18,5,0,23
machine-translation,9,"In practice , for a simple sentiment analysis model , the word embedding parameters account for 98.8 % of the total parameters .",introduction,0,19,6,0,23
machine-translation,9,"As only a small portion of the word embeddings is selected in the forward pass , the giant embedding matrix usually does not cause a speed issue .",introduction,0,20,7,0,28
machine-translation,9,"However , the massive number of parameters in the neural network results in a large storage or memory footprint .",introduction,0,21,8,0,20
machine-translation,9,"When other components of the neural network are also large , the model may fail to fit into GPU memory during training .",introduction,0,22,9,0,23
machine-translation,9,"Moreover , as the demand for low - latency neural computation for mobile platforms increases , some neural - based models are expected to run on mobile devices .",introduction,0,23,10,0,29
machine-translation,9,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",introduction,1,24,11,0,25
machine-translation,9,"In this study , we attempt to reduce the number of parameters used in word embeddings without hurting the model performance .",introduction,0,25,12,0,22
machine-translation,9,Neural networks are known for the significant redundancy in the connections .,introduction,0,26,13,0,12
machine-translation,9,"In this work , we further hypothesize that learning independent embeddings causes more redundancy in the embedding vectors , as the inter-similarity among words is ignored .",introduction,0,27,14,0,27
machine-translation,9,Some words are very similar regarding the semantics .,introduction,0,28,15,0,9
machine-translation,9,"For example , "" dog "" and "" dogs "" have almost the same meaning , except one is plural .",introduction,0,29,16,0,21
machine-translation,9,"To efficiently represent these two words , it is desirable to share information between the two embeddings .",introduction,0,30,17,0,18
machine-translation,9,"However , a small portion in both vectors still has to be trained independently to capture the syntactic difference .",introduction,0,31,18,0,20
machine-translation,9,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .",introduction,1,32,19,0,45
machine-translation,9,each component,introduction,0,33,20,0,2
machine-translation,9,Ci w is an integer number in .,introduction,0,34,21,0,8
machine-translation,9,"Ideally , similar words should have similar codes .",introduction,0,35,22,0,9
machine-translation,9,"For example , we may desire C dog = ( 3 , 2 , 4 , 1 ) and C dogs = ( 3 , 2 , 4 , 2 ) .",introduction,0,36,23,0,32
machine-translation,9,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .",introduction,1,37,24,0,28
machine-translation,9,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .",introduction,1,38,25,0,23
machine-translation,9,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,introduction,1,39,26,0,21
machine-translation,9,( where E i ( C i w ) is the Ci w - th codeword in the codebook E i .,introduction,0,40,27,0,22
machine-translation,9,"In this way , the number of vectors in the embedding matrix will be M K , which is usually much smaller than the vocabulary size .",introduction,0,41,28,0,27
machine-translation,9,gives an intuitive comparison between the compositional approach and the conventional approach ( assigning unique IDs ) .,introduction,0,42,29,0,18
machine-translation,9,"The codes of all the words can be stored in an integer matrix , denoted by C. Thus , the storage footprint of the embedding layer now depends on the total size of the combined codebook E and the code matrix C.",introduction,0,43,30,0,42
machine-translation,9,"Although the number of embedding vectors can be greatly reduced by using such coding approach , we want to prevent any serious degradation in performance compared to the models using normal embeddings .",introduction,0,44,31,0,33
machine-translation,9,"In other words , given a set of baseline word embeddings ? ( w ) , we wish to find a set of codes ?",introduction,0,45,32,0,25
machine-translation,9,and combined codebook that can produce the embeddings with the same effectiveness as ? ( w ) .,introduction,0,46,33,0,18
machine-translation,9,A safe and straight - forward way is to minimize the squared distance between the baseline embeddings and the composed embeddings as,introduction,0,47,34,0,22
machine-translation,9,where | V | is the vocabulary size .,introduction,0,48,35,0,9
machine-translation,9,The baseline embeddings can be a set of pre-trained vectors such as word2vec or GloVe embeddings .,introduction,0,49,36,0,17
machine-translation,9,"In Eq. 3 , the baseline embedding matrix ?",introduction,0,50,37,0,9
machine-translation,9,is approximated by M codewords selected from M codebooks .,introduction,0,51,38,0,10
machine-translation,9,The selection of codewords is controlled by the code C w .,introduction,0,52,39,0,12
machine-translation,9,"Such problem of learning compact codes with multiple codebooks is formalized and discussed in the research field of compressionbased source coding , known as product quantization and additive quantization .",introduction,0,53,40,0,30
machine-translation,9,Previous works learn compositional codes so as to enable an efficient similarity search of vectors .,introduction,0,54,41,0,16
machine-translation,9,"In this work , we utilize such codes for a different purpose , that is , constructing word embeddings with drastically fewer parameters .",introduction,1,55,42,0,24
machine-translation,9,"Due to the discreteness in the hash codes , it is usually difficult to directly optimize the objective function in Eq.",introduction,0,56,43,0,21
machine-translation,9,3 .,introduction,0,57,44,0,2
machine-translation,9,"In this paper , we propose a simple and straight - forward method to learn the codes in an end - to - end neural network .",introduction,0,58,45,0,27
machine-translation,9,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,introduction,1,59,46,0,18
machine-translation,9,"Besides the simplicity , this approach also allows one to use any arbitrary differentiable loss function , such as cosine similarity .",introduction,0,60,47,0,22
machine-translation,9,The contribution of this work can be summarized as follows :,introduction,0,61,48,0,11
machine-translation,9,We propose to utilize the compositional coding approach for constructing the word embeddings with significantly fewer parameters .,introduction,0,62,49,0,18
machine-translation,9,"In the experiments , we show that over 98 % of the embedding parameters can be eliminated in sentiment analysis task without affecting performance .",introduction,0,63,50,0,25
machine-translation,9,"In machine translation tasks , the loss - free compression rate reaches 94 % ? 99 % . We propose a direct learning approach for the codes in an end - to - end neural network , with a Gumbel - softmax layer to encourage the discreteness .",introduction,0,64,51,0,48
machine-translation,9,The neural network for learning codes will be packaged into a tool .,introduction,0,65,52,0,13
machine-translation,9,"With the learned codes and basis vectors , the computation graph for composing embeddings is fairly easy to implement , and does not require modifications to other parts in the neural network .",introduction,0,66,53,0,33
machine-translation,9,related work,related work,0,67,1,0,2
machine-translation,9,"Existing works for compressing neural networks include low - precision computation , quantization and knowledge distillation .",related work,0,68,2,0,17
machine-translation,9,"Network quantization such as HashedNet forces the weight matrix to have few real weights , with a hash function to determine the weight assignment .",related work,0,69,3,0,25
machine-translation,9,"To capture the non-uniform nature of the networks , DeepCompression groups weight values into clusters based on pre-trained weight matrices .",related work,0,70,4,0,21
machine-translation,9,The weight assignment for each value is stored in the form of Huffman codes .,related work,0,71,5,0,15
machine-translation,9,"However , as the embedding matrix is tremendously big , the number of hash codes a model need to maintain is still large even with Huffman coding .",related work,0,72,6,0,28
machine-translation,9,Network pruning works in a different way that makes a network sparse .,related work,0,73,7,0,13
machine-translation,9,Iterative pruning prunes a weight value if its absolute value is smaller than a threshold .,related work,0,74,8,0,16
machine-translation,9,The remaining network weights are retrained after pruning .,related work,0,75,9,0,9
machine-translation,9,Some recent works also apply iterative pruning to prune 80 % of the connections for neural machine translation models .,related work,0,76,10,0,20
machine-translation,9,"In this paper , we compare the proposed method with iterative pruning .",related work,0,77,11,0,13
machine-translation,9,"The problem of learning compact codes considered in this paper is closely related to learning to hash , which aims to learn the hash codes for vectors to facilitate the approximate nearest neighbor search .",related work,0,78,12,0,35
machine-translation,9,"Initiated byproduct quantization , subsequent works such as additive quantization explore the use of multiple codebooks for source coding , resulting in compositional codes .",related work,0,79,13,0,25
machine-translation,9,We also adopt the coding scheme of additive quantization for its storage efficiency .,related work,0,80,14,0,14
machine-translation,9,Previous works mainly focus on performing efficient similarity search of image descriptors .,related work,0,81,15,0,13
machine-translation,9,"In this work , we put more focus on reducing the codebook sizes and learning efficient codes to avoid performance loss .",related work,0,82,16,0,22
machine-translation,9,utilizes an improved version of product quantization to compress text classification models .,related work,0,83,17,0,13
machine-translation,9,"However , to match the baseline performance , much longer hash codes are required byproduct quantization .",related work,0,84,18,0,17
machine-translation,9,This will be detailed in Section 5.2 .,related work,0,85,19,0,8
machine-translation,9,"To learn the codebooks and code assignment , additive quantization alternatively optimizes the codebooks and the discrete codes .",related work,0,86,20,0,19
machine-translation,9,The learning of code assignment is performed by Beam Search algorithm when the codebooks are fixed .,related work,0,87,21,0,17
machine-translation,9,"In this work , we propose a straight - forward method to directly learn the code assignment and codebooks simutaneously in an end - to - end neural network .",related work,0,88,22,0,30
machine-translation,9,"Some recent works in learning to hash also utilize neural networks to produce binary codes by applying binary constrains ( e.g. , sigmoid function ) .",related work,0,89,23,0,26
machine-translation,9,"In this work , we encourage the discreteness with the Gumbel - Softmax trick for producing compositional codes .",related work,0,90,24,0,19
machine-translation,9,"As an alternative to our approach , one can also reduce the number of unique word types by forcing a character - level segmentation .",related work,0,91,25,0,25
machine-translation,9,"proposed a character - based neural language model , which applies a convolutional layer after the character embeddings .",method,0,92,1,0,19
machine-translation,9,"propose to use char-gram as input features , which are further hashed to save space .",method,0,93,2,0,16
machine-translation,9,"Generally , using characterlevel inputs requires modifications to the model architecture .",method,0,94,3,0,12
machine-translation,9,"Moreover , some Asian languages such as Japanese and Chinese retain a large vocabulary at the character level , which makes the character - based approach difficult to be applied .",method,0,95,4,0,31
machine-translation,9,"In contrast , our approach does not suffer from these limitations .",method,0,96,5,0,12
machine-translation,9,advantage of compositional codes,method,0,97,6,0,4
machine-translation,9,"In this section , we formally describe the compositional coding approach and analyze its merits for compressing word embeddings .",method,0,98,7,0,20
machine-translation,9,The coding approach follows the scheme in additive quantization .,method,0,99,8,0,10
machine-translation,9,"We represent each word w with a compact code C w that is composed of M components such that , which also indicates that M log 2 K bits are required to store each code .",method,0,100,9,0,36
machine-translation,9,"For convenience , K is selected to be a number of a multiple of 2 , so that the codes can be efficiently stored .",method,0,101,10,0,25
machine-translation,9,If we restrict each component,method,0,102,11,0,5
machine-translation,9,"Ci w to values of 0 or 1 , the code for each word C w will be a binary code .",method,0,103,12,0,22
machine-translation,9,"In this case , the code learning problem is equivalent to a matrix factorization problem with binary components .",method,0,104,13,0,19
machine-translation,9,"Forcing the compact codes to be binary numbers can be beneficial , as the learning problem is usually easier to solve in the binary case , and some existing optimization algorithms in learning to hash can be reused .",method,0,105,14,0,39
machine-translation,9,"However , the compositional coding approach produces shorter codes and is thus more storage efficient .",method,0,106,15,0,16
machine-translation,9,"As the number of basis vectors is M K regardless of the vocabulary size , the only uncertain factor contributing to the model size is the size of the hash codes , which is proportional to the vocabulary size .",method,0,107,16,0,40
machine-translation,9,"Therefore , maintaining short codes is cruicial in our work .",method,0,108,17,0,11
machine-translation,9,Suppose we wish the model to have a set of N basis vectors .,method,0,109,18,0,14
machine-translation,9,"Then in the binary case , each code will have N / 2 bits .",method,0,110,19,0,15
machine-translation,9,"For the compositional coding approach , if we can find a M K decomposition such that M K = N , then each code will have M log 2 K bits .",method,0,111,20,0,32
machine-translation,9,"For example , a binary code will have a length of 256 bits to support 512 basis vectors .",method,0,112,21,0,19
machine-translation,9,"In contrast , a 32 16 compositional coding scheme will produce codes of only 128 bits . :",method,0,113,22,0,18
machine-translation,9,Comparison of different coding approaches .,method,0,114,23,0,6
machine-translation,9,"To support N basis vectors , a binary code will have N / 2 bits and the embedding computation is a summation over N / 2 vectors .",method,0,115,24,0,28
machine-translation,9,"For the compositional approach with M codebooks and K codewords in each codebook , each code has M log 2 K bits , and the computation is a summation over M vectors .",method,0,116,25,0,33
machine-translation,9,A comparison of different coding approaches is summarized in .,method,0,117,26,0,10
machine-translation,9,We also report the number of basis vectors required to compute an embedding as a measure of computational cost .,method,0,118,27,0,20
machine-translation,9,"For the conventional approach , the number of vectors is identical to the vocabulary size and the computation is basically a single indexing operation .",method,0,119,28,0,25
machine-translation,9,"In the case of binary codes , the computation for constructing an embedding involves a summation over N / 2 basis vectors .",method,0,120,29,0,23
machine-translation,9,"For the compositional approach , the number of vectors required to construct an embedding vector is M .",method,0,121,30,0,18
machine-translation,9,Both the binary and compositional approaches have significantly fewer vectors in the embedding matrix .,method,0,122,31,0,15
machine-translation,9,The compositional coding approach provides a better balance with shorter codes and lower computational cost .,method,0,123,32,0,16
machine-translation,9,CODE LEARNING WITH GUMBEL - SOFTMAX,method,0,124,33,0,6
machine-translation,9,let ? ?,method,0,125,34,0,3
machine-translation,9,"R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",method,0,126,35,0,18
machine-translation,9,"By using the reconstruction loss as the objective function in Eq. 3 , we are actually finding an approximate matrix factorization ?",method,0,127,36,0,22
machine-translation,9,is a basis matrix for the i - th component .,method,0,128,37,0,11
machine-translation,9,Di is a | V | K code matrix in which each row is an K-dimensional one - hot vector .,method,0,129,38,0,21
machine-translation,9,If we let d i w be the one - hot vector corresponding to the code component,method,0,130,39,0,17
machine-translation,9,"Ci w for word w , the computation of the word embeddings can be reformulated as",method,0,131,40,0,16
machine-translation,9,"Therefore , the problem of learning discrete codes C w can be converted to a problem of finding a set of optimal one - hot vectors d 1 w , ... , d M wand source dictionaries A 1 , ... , AM , that minimize the reconstruction loss .",method,0,132,41,0,50
machine-translation,9,The Gumbel - softmax reparameterization trick is useful for parameterizing a discrete distribution such as the K-dimensional one - hot vectors d i win Eq.,method,0,133,42,0,25
machine-translation,9,"5 . By applying the Gumbel - softmax trick , the k - th elemement ind i w is computed as",method,0,134,43,0,21
machine-translation,9,where,method,0,135,44,0,1
machine-translation,9,Gk is a noise term that is sampled from the Gumbel distribution ? log ( ?,method,0,136,45,0,16
machine-translation,9,"log ( Uniform [ 0 , 1 ] ) ) , whereas ?",method,0,137,46,0,13
machine-translation,9,is the temperature of the softmax .,method,0,138,47,0,7
machine-translation,9,"In our model , the vector ?",method,0,139,48,0,7
machine-translation,9,i w is computed by a simple neural network with a single hidden layer as,method,0,140,49,0,15
machine-translation,9,"In our experiments , the hidden layer h w always has a size of M K /2 .",method,0,141,50,0,18
machine-translation,9,We found that a fixed temperature of ? = 1 just works well .,method,0,142,51,0,14
machine-translation,9,The Gumbel - softmax trick is applied to ?,method,0,143,52,0,9
machine-translation,9,i w to obtain d i w .,method,0,144,53,0,8
machine-translation,9,"Then , the model reconstructs the embedding E(C w ) with Eq. 5 and computes the reconstruction loss with Eq.",method,0,145,54,0,20
machine-translation,9,3 .,method,0,146,55,0,2
machine-translation,9,"The model architecture of the end - to - end neural network is illustrated in , which is effectively an auto - encoder with a Gumbel - softmax middle layer .",method,0,147,56,0,31
machine-translation,9,"The whole neural network for coding learning has five parameters ( ? , b , ? , b , A ) .",method,0,148,57,0,22
machine-translation,9,"Once the coding learning model is trained , the code C w for each word can be easily obtained by applying argmax to the one - hot vectors d 1 w , ... , d M w .",method,0,149,58,0,38
machine-translation,9,The basis vectors ( codewords ) for composing the embeddings can be found as the row vectors in the weight matrix A.,method,0,150,59,0,22
machine-translation,9,"For general NLP tasks , one can learn the compositional codes from publicly available word vectors such as GloVe vectors .",method,0,151,60,0,21
machine-translation,9,"However , for some tasks such as machine translation , the word embeddings are usually jointly learned with other parts of the neural network .",method,0,152,61,0,25
machine-translation,9,"For such tasks , one has to first train a normal model to obtain the baseline embeddings .",method,0,153,62,0,18
machine-translation,9,"Then , based on the trained embedding matrix , one can learn a set of task - specific codes .",method,0,154,63,0,20
machine-translation,9,"As the reconstructed embeddings E( C w ) are not identical to the original embeddings ? ( w ) , the model parameters other than the embedding matrix have to be retrained again .",method,0,155,64,0,34
machine-translation,9,The code learning model can not be jointly trained with the machine translation model as it takes far more iterations for the coding layer to converge to one - hot vectors .,method,0,156,65,0,32
machine-translation,9,experiments,experiment,0,157,1,0,1
machine-translation,9,"In our experiments , we focus on evaluating the maximum loss - free compression rate of word embeddings on two typical NLP tasks : sentiment analysis and machine translation .",experiment,0,158,2,0,30
machine-translation,9,We compare the model performance and the size of embedding layer with the baseline model and the iterative pruning method .,experiment,0,159,3,0,21
machine-translation,9,Please note that the sizes of other parts in the neural networks are not included in our results .,experiment,0,160,4,0,19
machine-translation,9,"For dense matrices , we report the size of dumped numpy arrays .",experiment,0,161,5,0,13
machine-translation,9,"For the sparse matrices , we report the size of dumped compressed sparse column matrices ( csc matrix ) in scipy .",experiment,0,162,6,0,22
machine-translation,9,All float numbers take 32 bits storage .,experiment,0,163,7,0,8
machine-translation,9,"We enable the "" compressed "" option when dumping the matrices , without this option , the file size is about 1.1 times bigger .",experiment,0,164,8,0,25
machine-translation,9,code learning,experiment,1,165,9,0,2
machine-translation,9,"To learn efficient compact codes for each word , our proposed method requires a set of baseline embedding vectors .",experiment,0,166,10,0,20
machine-translation,9,"For the sentiment analysis task , we learn the codes based on the publicly available GloVe vectors .",experiment,0,167,11,0,18
machine-translation,9,"For the machine translation task , we first train a normal neural machine translation ( NMT ) model to obtain task - specific word embeddings .",experiment,0,168,12,0,26
machine-translation,9,Then we learn the codes using the pre-trained embeddings .,experiment,0,169,13,0,10
machine-translation,9,We train the end - to - end network described in Section 4 to learn the codes automatically .,experiment,0,170,14,0,19
machine-translation,9,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .",experiment,1,171,15,0,19
machine-translation,9,The network parameters are optimized to minimize the reconstruction loss of the sampled embeddings .,experiment,0,172,16,0,15
machine-translation,9,"In our experiments , the batch size is set to 128 .",experiment,1,173,17,0,12
machine-translation,9,We use Adam optimizer with a fixed learning rate of 0.0001 .,experiment,1,174,18,0,12
machine-translation,9,The training is run for 200K iterations .,experiment,1,175,19,0,8
machine-translation,9,"Every 1,000 iterations , we examine the loss on a fixed validation set and save the parameters if the loss decreases .",experiment,0,176,20,0,22
machine-translation,9,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .",experiment,1,177,21,0,28
machine-translation,9,sentiment analysis,experiment,1,178,22,0,2
machine-translation,9,"Dataset : For sentiment analysis , we use a standard separation of IMDB movie review dataset , which contains 25 k reviews for training and 25 K reviews for testing purpose .",experiment,0,179,23,0,32
machine-translation,9,We lowercase and tokenize all texts with the nltk package .,experiment,0,180,24,0,11
machine-translation,9,We choose the 300 - dimensional uncased Glo Ve word vectors ( trained on 42B tokens of Common Crawl data ) as our baseline embeddings .,experiment,0,181,25,0,26
machine-translation,9,"The vocabulary for the model training contains all words appears both in the IMDB dataset and the GloVe vocabulary , which results in around 75 K words .",experiment,0,182,26,0,28
machine-translation,9,We truncate the texts of reviews to assure they are not longer than 400 words .,experiment,0,183,27,0,16
machine-translation,9,model architecture :,experiment,0,184,28,0,3
machine-translation,9,Both the baseline model and the compressed models have the same computational graph except the embedding layer .,experiment,0,185,29,0,18
machine-translation,9,The model is composed of a single LSTM layer with 150 hidden units and a softmax layer for predicting the binary label .,experiment,0,186,30,0,23
machine-translation,9,"For the baseline model , the embedding layer contains a large 75 K 300 embedding matrix initialized by GloVe embeddings .",experiment,0,187,31,0,21
machine-translation,9,"For the compressed models based on the compositional coding , the embedding layer maintains a matrix of basis vectors .",experiment,0,188,32,0,20
machine-translation,9,"Suppose we use a 32 16 coding scheme , the basis matrix will then have a shape of 512 300 , which is initialized by the concatenated weight matrices [ A 1 ; A 2 ; ... ; AM ] in the code learning model .",experiment,0,189,33,0,46
machine-translation,9,The embedding parameters for both models remain fixed during the training .,experiment,0,190,34,0,12
machine-translation,9,"For the models with network pruning , the sparse embedding matrix is finetuned during training .",experiment,0,191,35,0,16
machine-translation,9,training details :,experiment,0,192,36,0,3
machine-translation,9,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,experiment,1,193,37,0,18
machine-translation,9,"At the end of each epoch , we evaluate the loss on a small validation set .",experiment,0,194,38,0,17
machine-translation,9,The parameters with lowest validation loss are saved .,experiment,0,195,39,0,9
machine-translation,9,"Results : For different settings of the number of components M and the number of codewords K , we train the code learning network .",experiment,0,196,40,0,25
machine-translation,9,The average reconstruction loss on a fixed validation set is summarized in the left of .,experiment,0,197,41,0,16
machine-translation,9,"For reference , we also report the total size ( MB ) of the embedding layer in the right table , which includes the sizes of the basis matrix and the hash table .",experiment,0,198,42,0,34
machine-translation,9,We can see that increasing either M or K can effectively decrease the reconstruction loss .,experiment,0,199,43,0,16
machine-translation,9,"However , setting M to a large number will result in longer hash codes , thus significantly increase the size of the embedding layer .",experiment,0,200,44,0,25
machine-translation,9,"Hence , it is important to choose correct numbers for M and K to balance the performance and model size .",experiment,0,201,45,0,21
machine-translation,9,"To see how the reconstructed loss translates to the classification accuracy , we train the sentiment analysis model for different settings of code schemes and report the results in : Trade - off between the model performance and the size of embedding layer on IMDB sentiment analysis task",experiment,0,202,46,0,48
machine-translation,9,We also show the results using normalized product quantization ( NPQ ) .,experiment,0,203,47,0,13
machine-translation,9,"We quantize the filtered Glo Ve embeddings with the codes provided by the authors , and train the models based on the quantized embeddings .",experiment,0,204,48,0,25
machine-translation,9,"To make the results comparable , we report the codebook size in numpy format .",experiment,0,205,49,0,15
machine-translation,9,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",experiment,1,206,50,0,21
machine-translation,9,"In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",experiment,0,207,51,0,26
machine-translation,9,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,experiment,1,208,52,0,17
machine-translation,9,The improved model performance maybe a byproduct of the strong regularization .,experiment,0,209,53,0,12
machine-translation,9,machine translation,experiment,1,210,54,0,2
machine-translation,9,"Dataset : For machine translation tasks , we experiment on IWSLT 2014 German - to - English translation task and ASPEC English - to - Japanese translation task .",experiment,0,211,55,0,29
machine-translation,9,"The IWSLT14 training data contains 178K sentence pairs , which is a small dataset for machine translation .",experiment,0,212,56,0,18
machine-translation,9,We utilize moses toolkit to tokenize and lowercase both sides of the texts .,experiment,0,213,57,0,14
machine-translation,9,Then we concatenate all five TED / TEDx development and test corpus to form a test set containing 6750 sentence pairs .,experiment,0,214,58,0,22
machine-translation,9,We apply byte - pair encoding to transform the texts to subword level so that the vocabulary has a size of 20 K for each language .,experiment,0,215,59,0,27
machine-translation,9,"For evaluation , we report tokenized BLEU using "" multi -bleu.perl "" .",experiment,0,216,60,0,13
machine-translation,9,The ASPEC dataset contains 300M bilingual pairs in the training data with the automatically estimated quality scores provided for each pair .,experiment,0,217,61,0,22
machine-translation,9,We only use the first 150M pairs for training the models .,experiment,0,218,62,0,12
machine-translation,9,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea .,experiment,0,219,63,0,17
machine-translation,9,The vocabulary size for each language is reduced to 40K using byte - pair encoding .,experiment,0,220,64,0,16
machine-translation,9,The evaluation is performed using a standard kytea - based post -processing script for this dataset .,experiment,0,221,65,0,17
machine-translation,9,model architecture :,experiment,0,222,66,0,3
machine-translation,9,"In our preliminary experiments , we found a 32 16 coding works well for a vanilla NMT model .",experiment,0,223,67,0,19
machine-translation,9,"As it is more meaningful to test on a high - performance model , we applied several techniques to improve the performance .",experiment,0,224,68,0,23
machine-translation,9,The model has a standard bi-directional encoder composed of two LSTM layers similar to .,experiment,0,225,69,0,15
machine-translation,9,The decoder contains two LSTM layers .,experiment,0,226,70,0,7
machine-translation,9,Residual connection with a scaling factor of 1 / 2 is applied to the two decoder states to compute the outputs .,experiment,0,227,71,0,22
machine-translation,9,All LSTMs and embeddings have 256 hidden units in the IWSLT14 task and 1000 hidden units in ASPEC task .,experiment,0,228,72,0,20
machine-translation,9,The decoder states are firstly linearly transformed to 600 - dimensional vectors before computing the final softmax .,experiment,0,229,73,0,18
machine-translation,9,Dropout with a rate of 0.2 is applied everywhere except the recurrent computation .,experiment,0,230,74,0,14
machine-translation,9,"We apply Key - Value Attention to the first decoder , where the query is the sum of the feedback embedding and the previous decoder state and the keys are computed by linear transformation of encoder states .",experiment,0,231,75,0,38
machine-translation,9,training details :,experiment,0,232,76,0,3
machine-translation,9,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,experiment,1,233,77,0,17
machine-translation,9,"We evaluate the smoothed BLEU ) on a validation set composed of 50 batches every 7,000 iterations .",experiment,0,234,78,0,18
machine-translation,9,The learning rate is reduced by a factor of 10 if no improvement is observed in 3 validation runs .,experiment,0,235,79,0,20
machine-translation,9,The training ends after the learning rate is reduced three times .,experiment,0,236,80,0,12
machine-translation,9,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .",experiment,1,237,81,0,23
machine-translation,9,We firstly train a baseline NMT model to obtain the task - specific embeddings for all in - vocabulary words in both languages .,experiment,0,238,82,0,24
machine-translation,9,"Then based on these baseline embeddings , we obtain the hash codes and basis vectors by training the code learning model .",experiment,0,239,83,0,22
machine-translation,9,"Finally , the NMT models using compositional coding are retrained by plugging in the reconstructed embeddings .",experiment,0,240,84,0,17
machine-translation,9,"Note that the embedding layer is fixed in this phase , other parameters are retrained from random initial values .",experiment,0,241,85,0,20
machine-translation,9,results :,result,0,242,1,0,2
machine-translation,9,The experimental results are summarized in .,result,0,243,2,0,7
machine-translation,9,All translations are decoded by the beam search with a beam size of 5 .,result,0,244,3,0,15
machine-translation,9,The performance of iterative pruning varies between tasks .,result,0,245,4,0,9
machine-translation,9,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,result,1,246,5,0,20
machine-translation,9,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .",result,1,247,6,0,18
machine-translation,9,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .",result,1,248,7,0,28
machine-translation,9,"Similar to the sentiment analysis task , a significant performance improvement can be observed by slightly lowering the compression rate .",result,0,249,8,0,21
machine-translation,9,"Note that the sizes of NMT models are still quite large due to the big softmax layer and the recurrent layers , which are not reported in the , we show some examples of learned codes based on the 300 - dimensional uncased GloVe embeddings used in the sentiment analysis task .",result,0,250,9,0,52
machine-translation,9,We can see that the model learned to assign similar codes to the words with similar meanings .,result,0,251,10,0,18
machine-translation,9,"Such a code - sharing mechanism can significantly reduce the redundancy of the word embeddings , thus helping to achieve a high compression rate .",result,0,252,11,0,25
machine-translation,9,analysis of code efficiency,result,0,253,12,0,4
machine-translation,9,"Besides the performance , we also care about the storage efficiency of the codes .",result,0,254,13,0,15
machine-translation,9,"In the ideal situation , all codewords shall be fully utilized to convey a fraction of meaning .",result,0,255,14,0,18
machine-translation,9,"However , as the codes are category word 8 8 code 16 16 code dog 0 7 0 1 7 3 7 0 7 7 0 8 3 5 8 5 B 2 E E 0 B 0 A animal cat 7 7 0 1 7 3 7 0 7 7 2 8 B 5 8 CB 2 E E 4 B 0 A penguin 0 7 0 1 7 3 6 0 7 7 E 8 7 6 4 CF DE 3 D 8 0 A go 7 7 0 6 4 3 3 0 2 C C 8 2 C 1 1 B D 0 E 0 B 5 8 verb went 4 0 7 6 4 3 2 0 BC C 6 BC 7 5 B 8 6 E 0 D 0 4 gone 7 7 0 6 4 3 3 0 2 C C 8 0 B 1 5 B D 6 E 0 2 5 A : Examples of learned compositional codes based on Glo Ve embedding vectors automatically learned , it is possible that some codewords are abandoned during the training .",result,0,256,15,0,188
machine-translation,9,"In extreme cases , some "" dead "" codewords can be used by none of the words .",result,0,257,16,0,18
machine-translation,9,"To analyze the code efficiency , we count the number of words that contain a specific subcode in each component .",result,0,258,17,0,21
machine-translation,9,gives a visualization of the code balance for three coding schemes .,result,0,259,18,0,12
machine-translation,9,Each column shows the counts of the subcodes of a specific component .,result,0,260,19,0,13
machine-translation,9,"In our experiments , when using a 8 8 coding scheme , we found 31 % of the words have a subcode "" 0 "" for the first component , while the subcode "" 1 "" is only used by 5 % of the words .",result,0,261,20,0,46
machine-translation,9,The assignment of codes is more balanced for larger coding schemes .,result,0,262,21,0,12
machine-translation,9,"In any coding scheme , even the most unpopular codeword is used by about 1000 words .",result,0,263,22,0,17
machine-translation,9,This result indicates that the code learning model is capable of assigning codes efficiently without wasting a codeword .,result,0,264,23,0,19
machine-translation,9,The results show that any codeword is assigned to more than 1000 words without wasting .,result,0,265,24,0,16
machine-translation,9,conclusion,result,0,266,25,0,1
machine-translation,9,"In this work , we propose a novel method for reducing the number of parameters required in word embeddings .",result,0,267,26,0,20
machine-translation,9,"Instead of assigning each unique word an embedding vector , we compose the embedding vectors using a small set of basis vectors .",result,0,268,27,0,23
machine-translation,9,The selection of basis vectors is governed by the hash code of each word .,result,0,269,28,0,15
machine-translation,9,We apply the compositional coding approach to maximize the storage efficiency .,result,0,270,29,0,12
machine-translation,9,The proposed method works by eliminating the redundancy inherent in representing similar words with independent embeddings .,result,0,271,30,0,17
machine-translation,9,"In our work , we propose a simple way to directly learn the discrete codes in a neural network with Gumbel - softmax trick .",result,0,272,31,0,25
machine-translation,9,The results show that the size of the embedding layer was reduced by 98 % in IMDB sentiment analysis task and 94 % ? 99 % in machine translation tasks without affecting the performance .,result,0,273,32,0,35
machine-translation,9,Our approach achieves a high loss - free compression rate by considering the semantic inter-similarity among different words .,result,0,274,33,0,19
machine-translation,9,"In qualitative analysis , we found the learned codes of similar words are very close in Hamming space .",result,0,275,34,0,19
machine-translation,9,"As our approach maintains a dense basis matrix , it has the potential to be further compressed by applying pruning techniques to the dense matrix .",result,0,276,35,0,26
machine-translation,9,The advantage of compositional coding approach will be more significant if the size of embedding layer is dominated by the hash codes .,result,0,277,36,0,23
machine-translation,9,"Huei - Fang Yang , Kevin Lin , and Chu - Song Chen .",result,0,278,37,0,14
machine-translation,9,Supervised learning of semantics - preserving hash via deep convolutional neural networks .,result,0,279,38,0,13
machine-translation,9,"IEEE transactions on pattern analysis and machine intelligence , 2017 .",result,0,280,39,0,11
machine-translation,9,"Xiaowei Zhang , Wei Chen , Feng Wang , Shuang Xu , and Bo Xu.",result,0,281,40,0,15
machine-translation,9,Towards compact and fast neural machine translation using a combined method .,result,0,282,41,0,12
machine-translation,9,"in emnlp , 2017 .",result,0,283,42,0,5
machine-translation,9,"Aojun Zhou , Anbang Yao , Yiwen Guo , Lin Xu , and Yurong Chen .",result,0,284,43,0,16
machine-translation,9,incremental network quantization :,result,0,285,44,0,4
machine-translation,9,Towards lossless cnns with low - precision weights .,result,0,286,45,0,9
machine-translation,9,"CoRR , abs /1702.03044 , 2017 .",result,0,287,46,0,7
machine-translation,1,Neural Machine Translation in Linear Time,title,1,2,1,0,6
machine-translation,1,abstract,abstract,0,3,1,0,1
machine-translation,1,We present a novel neural network for processing sequences .,abstract,0,4,2,0,10
machine-translation,1,"The ByteNet is a one-dimensional convolutional neural network that is composed of two parts , one to encode the source sequence and the other to decode the target sequence .",abstract,0,5,3,0,30
machine-translation,1,The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences .,abstract,0,6,4,0,24
machine-translation,1,"To address the differing lengths of the source and the target , we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder .",abstract,0,7,5,0,31
machine-translation,1,The ByteNet uses dilation in the convolutional layers to increase its receptive field .,abstract,0,8,6,0,14
machine-translation,1,The resulting network has two core properties : it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization .,abstract,0,9,7,0,30
machine-translation,1,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,abstract,1,10,8,0,29
machine-translation,1,"The ByteNet also achieves state - of - the - art performance on character - to - character machine translation on the English - to - German WMT translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time .",abstract,1,11,9,1,51
machine-translation,1,We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens .,abstract,0,12,10,0,19
machine-translation,1,introduction,introduction,0,13,1,0,1
machine-translation,1,"In neural language modelling , a neural network estimates a distribution over sequences of words or characters that belong to a given language .",introduction,1,14,2,0,24
machine-translation,1,"In neural machine translation , the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language .",introduction,1,15,3,0,26
machine-translation,1,The network can bethought of as composed of two parts : a source network ( the encoder ) that encodes the source sequence into a representation and a target network ( the decoder ) that uses the representation of the source encoder to generate the target sequence .,introduction,0,16,4,0,48
machine-translation,1,"Recurrent neural networks ( RNN ) are powerful sequence models and are widely used in language modelling ) , yet they have a potential drawback .",introduction,0,17,5,0,26
machine-translation,1,RNNs have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation .,introduction,0,18,6,0,23
machine-translation,1,Forward and backward signals in a RNN also need to traverse the full distance of the serial path to reach from one token in the sequence to another .,introduction,0,19,7,0,29
machine-translation,1,"The larger the distance , the harder it is to learn the dependencies between the tokens .",introduction,0,20,8,0,17
machine-translation,1,"A number of neural architectures have been proposed for modelling translation , such as encoder - decoder networks , networks with attentional pooling and twodimensional networks .",introduction,0,21,9,0,27
machine-translation,1,"Despite the generally good performance , the proposed models ar Xiv : 1610.10099v2 [ cs. CL ] 15 Mar 2017 EOS EOS EOS | s | | t | | t| .",introduction,0,22,10,0,32
machine-translation,1,Dynamic unfolding in the ByteNet architecture .,introduction,0,23,11,0,7
machine-translation,1,"At each step the decoder is conditioned on the source representation produced by the encoder for that step , or simply on no representation for steps beyond the extended length | t | .",introduction,0,24,12,0,34
machine-translation,1,The decoding ends when the target network produces an end - of - sequence ( EOS ) symbol .,introduction,0,25,13,0,19
machine-translation,1,"either have running time that is super - linear in the length of the source and target sequences , or they process the source sequence into a constant size representation , burdening the model with a memorization step .",introduction,0,26,14,0,39
machine-translation,1,Both of these drawbacks grow more severe as the length of the sequences increases .,introduction,0,27,15,0,15
machine-translation,1,We present a family of encoder - decoder neural networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above .,introduction,0,28,16,0,29
machine-translation,1,The first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal resolution of the sequences ; this is in contrast with architectures that encode the source into a fixed - size representation .,introduction,0,29,17,0,46
machine-translation,1,The second mechanism is the dynamic unfolding mechanism that allows the network to process in a simple and efficient way source and target sequences of different lengths ( Sect. 3.2 ) .,introduction,0,30,18,0,32
machine-translation,1,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,introduction,1,31,19,0,33
machine-translation,1,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,introduction,1,32,20,0,23
machine-translation,1,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,introduction,1,33,21,0,21
machine-translation,1,The network has beneficial computational and learning properties .,introduction,1,34,22,0,9
machine-translation,1,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?",introduction,1,35,23,0,30
machine-translation,1,log d where d is the size of the desired dependency field ) .,introduction,0,36,24,0,14
machine-translation,1,The computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences ( Sect. 2 ) .,introduction,0,37,25,0,30
machine-translation,1,"From a learning perspective , the representation of the source sequence in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder .",introduction,1,38,26,0,35
machine-translation,1,"In addition , the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis - tance between the tokens .",introduction,0,39,27,0,38
machine-translation,1,Dependencies overlarge distances are connected by short paths and can be learnt more easily .,introduction,0,40,28,0,15
machine-translation,1,We apply the ByteNet model to strings of characters for character - level language modelling and character - tocharacter machine translation .,introduction,0,41,29,0,22
machine-translation,1,We evaluate the decoder network on the Hutter Prize Wikipedia task where it achieves the state - of - the - art performance of 1.31 bits / character .,introduction,0,42,30,0,29
machine-translation,1,"We further evaluate the encoderdecoder network on character - to - character machine translation on the English - to - German WMT benchmark where it achieves a state - of - the - art BLEU score of 22.85 ( 0.380 bits / character ) and 25.53 ( 0.389 bits / character ) on the 2014 and 2015 test sets , respectively .",introduction,0,43,31,0,62
machine-translation,1,"On the character - level machine translation task , ByteNet betters a comparable version of GNMT that is a state - of - the - art system .",introduction,0,44,32,0,28
machine-translation,1,"These results show that deep CNNs are simple , scalable and effective architectures for challenging linguistic processing tasks .",introduction,0,45,33,0,19
machine-translation,1,The paper is organized as follows .,introduction,0,46,34,0,7
machine-translation,1,Section 2 lays out the background and some desiderata for neural architectures underlying translation models .,introduction,0,47,35,0,16
machine-translation,1,Section 3 defines the proposed family of architectures and the specific convolutional instance ( ByteNet ) used in the experiments .,introduction,0,48,36,0,21
machine-translation,1,Section 4 analyses ByteNet as well as existing neural translation models based on the desiderata set out in Section 2 .,introduction,0,49,37,0,21
machine-translation,1,Section 5 reports the experiments on language modelling and Section 6 reports the experiments on character - to - character machine translation .,introduction,0,50,38,0,23
machine-translation,1,neural translation model,introduction,0,51,39,0,3
machine-translation,1,"Given a string s from a source language , a neural translation model estimates a distribution p ( t |s ) over strings t of a target language .",introduction,0,52,40,0,29
machine-translation,1,The distribution indicates the probability of a string t being a translation of s .,introduction,0,53,41,0,15
machine-translation,1,"A product of conditionals over the tokens in the target t = t 0 , ... , t N leads to a tractable formulation of the distribution :",introduction,0,54,42,0,28
machine-translation,1,Each conditional factor expresses complex and long - range dependencies among the source and target tokens .,introduction,0,55,43,0,17
machine-translation,1,"The strings are usually sentences of the respective languages ; the tokens are words or , as in the our case , characters .",introduction,0,56,44,0,24
machine-translation,1,The network that models p ( t | s ) is composed of two parts : a source network ( the encoder ) that processes the source string into a representation and a target network ( the decoder ) that uses the source representation to generate the target string .,introduction,0,57,45,0,50
machine-translation,1,The decoder functions as a language model for the target language .,introduction,0,58,46,0,12
machine-translation,1,A neural translation model has some basic properties .,introduction,0,59,47,0,9
machine-translation,1,The decoder is autoregressive in the target tokens and the model is sensitive to the ordering of the tokens in the source and target strings .,introduction,0,60,48,0,26
machine-translation,1,It is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary .,introduction,0,61,49,0,28
machine-translation,1,desiderata,introduction,0,62,50,0,1
machine-translation,1,"Beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture , so we aim at identifying some desiderata .",introduction,0,63,51,0,27
machine-translation,1,"First , the running time of the network should be linear in the length of the source and target strings .",introduction,0,64,52,0,21
machine-translation,1,"This ensures that the model is scalable to longer strings , which is the case when using characters as tokens .",introduction,0,65,53,0,21
machine-translation,1,The use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time .,introduction,0,66,54,0,21
machine-translation,1,"Second , the size of the source representation should be linear in the length of the source string , i.e. it should be resolution preserving , and not have constant size .",introduction,0,67,55,0,32
machine-translation,1,This is to avoid burdening the model with an additional memorization step before translation .,introduction,0,68,56,0,15
machine-translation,1,"In more general terms , the size of a representation should be proportional to the amount of information it represents or predicts .",introduction,0,69,57,0,23
machine-translation,1,"Third , the path traversed by forward and backward signals in the network ( between input and ouput tokens ) should be short .",introduction,0,70,58,0,24
machine-translation,1,Shorter paths whose length is largely decoupled from the sequence distance between the two tokens have the potential to better propagate the signals and to let the network learn long - range dependencies more easily .,introduction,0,71,59,0,36
machine-translation,1,byte net,introduction,0,72,60,0,2
machine-translation,1,We aim at building neural language and translation models that capture the desiderata set out in Sect. 2.1 .,introduction,0,73,61,0,19
machine-translation,1,The proposed ByteNet architecture is composed of a decoder that is stacked on an encoder ( Sect. 3.1 ) and generates variable - length outputs via dynamic unfolding ( Sect. 3.2 ) .,introduction,0,74,62,0,33
machine-translation,1,The decoder is a language model that is formed of one - dimensional convolutional layers that are masked ( Sect. 3.4 ) and use dilation ( Sect. 3.5 ) .,introduction,0,75,63,0,30
machine-translation,1,The encoder processes the source string into a representation and is formed of one - dimensional convolutional layers that use dilation but are not masked .,introduction,0,76,64,0,26
machine-translation,1,depicts the two networks and their combination .,introduction,0,77,65,0,8
machine-translation,1,encoder - decoder stacking,introduction,0,78,66,0,4
machine-translation,1,A notable feature of the proposed family of architectures is the way the encoder and the decoder are connected .,introduction,0,79,67,0,20
machine-translation,1,"To maximize the representational bandwidth between the encoder and the decoder , we place the decoder on top of the representation computed by the encoder .",introduction,0,80,68,0,26
machine-translation,1,This is in contrast to models that compress the source representation into a fixed - size vector or that pool over the source representation with a mechanism such as attentional pooling .,introduction,0,81,69,0,32
machine-translation,1,dynamic unfolding,introduction,0,82,70,0,2
machine-translation,1,An encoder and a decoder network that process sequences of different lengths can not be directly connected due to the different sizes of the computed representations .,introduction,0,83,71,0,27
machine-translation,1,"We circumvent this issue via a mechanism which we call dynamic unfolding , which works as follows .",introduction,0,84,72,0,18
machine-translation,1,"Given source and target sequences sand t with respective lengths | s | and | t| , one first chooses a sufficiently tight upper bound | t| on the target length | t | as a linear function of the source length | s | : |",introduction,0,85,73,0,47
machine-translation,1,"The tight upper bound | t| is chosen in such away that , on the one hand , it is greater than the actual length | t | in almost all cases and , on the other hand , it does not increase excessively the amount of computation that is required .",introduction,0,86,74,0,52
machine-translation,1,"Once a linear relationship is chosen , one designs the source encoder so that , given a source sequence of length | s | , the encoder outputs a representation of the established lengt ?",introduction,0,87,75,0,35
machine-translation,1,| t| .,introduction,0,88,76,0,3
machine-translation,1,"In our case , we let a = 1.20 and b = 0 when translating from English into German , as German sentences tend to be somewhat longer than their English counterparts .",introduction,0,89,77,0,33
machine-translation,1,"In this manner the representation produced by the encoder can be efficiently computed , while maintaining high bandwidth and being resolution - preserving .",introduction,0,90,78,0,24
machine-translation,1,"Once the encoder representation is computed , we let the decoder unfold stepby - step over the encoder representation until the decoder itself outputs an end - of - sequence symbol ; the unfolding process may freely proceed beyond the estimated length | t| of the encoder representation .",introduction,0,91,79,0,49
machine-translation,1,gives an example of dynamic unfolding .,introduction,0,92,80,0,7
machine-translation,1,input embedding tensor,introduction,0,93,81,0,3
machine-translation,1,"Given the target sequence t = t 0 , ... , tn the ByteNet decoder embeds each of the first n tokens t 0 , ... , t n?1 via a look - up table ( the n tokens t 1 , ... , tn serve as targets for the predictions ) .",introduction,0,94,82,0,53
machine-translation,1,The resulting embeddings are concatenated into a tensor of size n 2 d where d is the number of inner channels in the network .,introduction,0,95,83,0,25
machine-translation,1,masked one-dimensional,introduction,0,96,84,0,2
machine-translation,1,convolutions,introduction,0,97,85,0,1
machine-translation,1,The decoder applies masked one - dimensional convolutions ( van den to the input embedding tensor that have a masked kernel of size k.,introduction,0,98,86,0,24
machine-translation,1,The masking ensures that information from future tokens does not affect the prediction of the current token .,introduction,0,99,87,0,18
machine-translation,1,The operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2 k ?,introduction,0,100,88,0,22
machine-translation,1,1 or by padding the input map .,introduction,0,101,89,0,8
machine-translation,1,dilation,introduction,0,102,90,0,1
machine-translation,1,The masked convolutions use dilation to increase the receptive field of the target network .,introduction,0,103,91,0,15
machine-translation,1,"Dilation makes the receptive field grow exponentially in terms of the depth of the networks , as opposed to linearly .",introduction,0,104,92,0,21
machine-translation,1,We use a dilation scheme whereby the dilation rates are doubled every layer up to a maximum rater ( for our experiments r = 16 ) .,introduction,0,105,93,0,27
machine-translation,1,The scheme is repeated multiple times in the network always starting from a dilation rate of 1 ( van den .,introduction,0,106,94,0,21
machine-translation,1,residual blocks,introduction,0,107,95,0,2
machine-translation,1,"Each layer is wrapped in a residual block that contains additional convolutional layers with filters of size 1 1 . We adopt two variants of the residual blocks : one with ReLUs , which is used in the machine translation experiments , and one with Multiplicative Units , which is used in the language modelling experiments .",introduction,0,108,96,0,57
machine-translation,1,diagrams the two variants of the blocks .,introduction,0,109,97,0,8
machine-translation,1,"In both cases , we use layer normalization before the activation function , as it is well suited to sequence processing where computing the activation statistics over the following future tokens ( as would be done by batch normalization ) must be avoided .",introduction,0,110,98,0,44
machine-translation,1,"After a series of residual blocks of increased dilation , the network applies one more convolution and ReLU followed by a convolution and a final softmax layer .",introduction,0,111,99,0,28
machine-translation,1,model comparison,introduction,0,112,100,0,2
machine-translation,1,In this section we analyze the properties of various previously introduced neural translation models as well as the ByteNet family of models .,introduction,0,113,101,0,23
machine-translation,1,"For the sake of a more complete analysis , we include two recurrent ByteNet variants ( which we do not evaluate in the experiments ) .",introduction,0,114,102,0,26
machine-translation,1,recurrent bytenets,introduction,0,115,103,0,2
machine-translation,1,The ByteNet is composed of two stacked encoder and decoder networks where the decoder network dynamically adapts to the output length .,introduction,0,116,104,0,22
machine-translation,1,This way of combining the networks is not tied to the networks being strictly convolutional .,introduction,0,117,105,0,16
machine-translation,1,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks ( see ) .,introduction,0,118,106,0,23
machine-translation,1,The first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded .,introduction,0,119,107,0,19
machine-translation,1,"The second variant also replaces the convolutional encoder with a recurrent encoder , e.g. a bidirectional RNN .",introduction,0,120,108,0,18
machine-translation,1,The target RNN is then placed on top of the source RNN .,introduction,0,121,109,0,13
machine-translation,1,"Considering the latter Recurrent ByteNet , we can see that the RNN Enc - Dec network ) is a Recurrent ByteNet where all connections between source and target - except for the first one that connects s 0 and t 0 - have been severed .",introduction,0,122,110,0,46
machine-translation,1,"The Recurrent ByteNet is a generalization of the RNN Enc - Dec and , modulo the type of weight - sharing scheme , so is the convolutional ByteNet .",introduction,0,123,111,0,29
machine-translation,1,comparison of properties,introduction,0,124,112,0,3
machine-translation,1,In our comparison we consider the following neural translation models : the Recurrent Continuous Translation Model ( RCTM ) 1 and 2 ; the RNN Enc - Dec ; the RNN Enc - Dec Att with the attentional pooling mechanism of which there are a few variations ; the Grid LSTM translation model ) that uses a multi-dimensional architecture ; the Extended Neural GPU model ) that has a convolutional RNN architecture ; the ByteNet and the two Recurrent ByteNet variants .,introduction,0,125,113,0,82
machine-translation,1,Our comparison criteria reflect the desiderata set out in Sect. 2.1 .,introduction,0,126,114,0,12
machine-translation,1,We separate the first ( computation time ) desider - atum into three columns .,introduction,0,127,115,0,15
machine-translation,1,The first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by Time .,introduction,0,128,116,0,25
machine-translation,1,"The other two columns Net Sand Net T indicate , respectively , whether the source and the target network use a convolutional structure ( CNN ) or a recurrent one ( RNN ) ; a CNN structure has the advantage that it can be run in parallel along the length of the sequence .",introduction,0,129,117,0,54
machine-translation,1,"The second ( resolution preservation ) desideratum corresponds to the RP column , which indicates whether the source representation in the network is resolution preserving .",introduction,0,130,118,0,26
machine-translation,1,"Finally , the third desideratum ( short forward and backward flow paths ) is reflected by two columns .",introduction,0,131,119,0,19
machine-translation,1,The Path S column corresponds to the length in layer steps of the shortest path between a source token and any output target token .,introduction,0,132,120,0,25
machine-translation,1,"Similarly , the Path T column corresponds to the length of the shortest path between an input target token and any output target token .",introduction,0,133,121,0,25
machine-translation,1,Shorter paths lead to better forward and backward signal propagation .,introduction,0,134,122,0,11
machine-translation,1,summarizes the properties of the models .,introduction,0,135,123,0,7
machine-translation,1,"The ByteNet , the Recurrent ByteNets and the RNN Enc - Dec are the only networks that have linear running time ( up to the constant c ) .",introduction,0,136,124,0,29
machine-translation,1,"The RNN Enc - Dec , however , does not preserve the source sequence resolution , a feature that aggravates learning for long sequences such as those that appear in character - to - character machine translation .",introduction,0,137,125,0,38
machine-translation,1,"The RCTM 2 , the RNN Enc - Dec Att , the Grid LSTM and the Extended Neural GPU do preserve the resolution , but at a cost of a quadratic running time .",introduction,0,138,126,0,34
machine-translation,1,The ByteNet stands out also for its Path properties .,introduction,0,139,127,0,10
machine-translation,1,The dilated structure of the convolutions connects any two source or target tokens in the sequences byway of a small number of network layers corresponding to the depth of the source or target networks .,introduction,0,140,128,0,35
machine-translation,1,"For character sequences where learning long - range dependencies is important , paths that are sublinear in the distance are advantageous .",introduction,0,141,129,0,22
machine-translation,1,Phrase Based MT phrases phrases 20.7 24.0 RNN Enc - Dec words words 11.3 Reverse RNN Enc - Dec words words 14.0 RNN Enc - Dec,introduction,0,142,130,0,26
machine-translation,1,Att words words 20.6 RNN Enc - Dec Att words words 20.9 GNMT ( RNN Enc - Dec Att ) word - pieces word - pieces 24.61,introduction,0,143,131,0,27
machine-translation,1,rnn,introduction,0,144,132,0,1
machine-translation,1,model test,introduction,0,145,133,0,2
machine-translation,1,stacked lstm,introduction,0,146,134,0,2
machine-translation,1,1.67 GF - LSTM 1.58 Grid- LSTM 1.47 Layer - normalized LSTM 1.46 MI- LSTM 1.44 Recurrent Memory Array Structures,introduction,0,147,135,0,20
machine-translation,1,1.40 HM- LSTM 1.40 Layer,introduction,0,148,136,0,5
machine-translation,1,Norm HyperLSTM 1.38 Large Layer,introduction,0,149,137,0,5
machine-translation,1,Norm HyperLSTM 1.34 Recurrent Highway Networks 1.32 Byte,introduction,0,150,138,0,8
machine-translation,1,Net Decoder 1.31 . Negative log- likelihood results in bits / byte on the Hutter Prize Wikipedia benchmark .,introduction,0,151,139,0,19
machine-translation,1,character prediction,introduction,1,152,140,0,2
machine-translation,1,We first evaluate the ByteNet Decoder separately on a character - level language modelling benchmark .,introduction,0,153,141,0,16
machine-translation,1,"We use the Hutter Prize version of the Wikipedia dataset and follow the standard split where the first 90 million bytes are used for training , the next 5 million bytes are used for validation and the last 5 million bytes are used for testing .",introduction,0,154,142,0,46
machine-translation,1,The total number of characters in the vocabulary is 205 .,introduction,0,155,143,0,11
machine-translation,1,"The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 .",introduction,1,156,144,0,46
machine-translation,1,The masked kernel has size 3 .,introduction,1,157,145,0,7
machine-translation,1,This gives a receptive field of 315 characters .,introduction,0,158,146,0,9
machine-translation,1,The number of hidden units dis 512 .,introduction,1,159,147,0,8
machine-translation,1,For this task we use residual multiplicative blocks .,introduction,0,160,148,0,9
machine-translation,1,For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 .,introduction,1,161,149,0,20
machine-translation,1,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,introduction,1,162,150,0,19
machine-translation,1,We do not reduce the learning rate during training .,introduction,0,163,151,0,10
machine-translation,1,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .",introduction,1,164,152,0,30
machine-translation,1,lists recent results of various neural sequence models on the Wikipedia dataset .,introduction,0,165,153,0,13
machine-translation,1,All the results except for the ByteNet result are obtained using some variant of the LSTM recurrent neural network .,introduction,0,166,154,0,20
machine-translation,1,The ByteNet decoder achieves 1.31 bits / character on the test set .,introduction,1,167,155,0,13
machine-translation,1,character - level machine translation,introduction,1,168,156,0,5
machine-translation,1,We evaluate the full ByteNet on the WMT English to German translation task .,introduction,0,169,157,0,14
machine-translation,1,We use NewsTest 2013 for validation and NewsTest 2014 and 2015 for testing .,introduction,0,170,158,0,14
machine-translation,1,The English and German strings are encoded as sequences of characters ; no explicit segmentation into words or morphemes is applied to the strings .,introduction,0,171,159,0,25
machine-translation,1,The outputs of the network are strings of characters in the target language .,introduction,0,172,160,0,14
machine-translation,1,We keep 323 characters in the German vocabulary and 296 in the English vocabulary .,introduction,0,173,161,0,15
machine-translation,1,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,introduction,1,174,162,0,21
machine-translation,1,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .",introduction,1,175,163,0,30
machine-translation,1,For this task we use the residual blocks with ReLUs ( .,introduction,1,176,164,0,12
machine-translation,1,The number of hidden units dis 800 .,introduction,1,177,165,0,8
machine-translation,1,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .",introduction,1,178,166,0,26
machine-translation,1,For the optimization we use Adam with a learning rate of 0.0003 .,introduction,1,179,167,0,13
machine-translation,1,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,introduction,1,180,168,0,38
machine-translation,1,Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training .,introduction,1,181,169,0,22
machine-translation,1,We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token .,introduction,1,182,170,0,28
machine-translation,1,We use a beam of size 12 .,introduction,1,183,171,0,8
machine-translation,1,"We do not use length normalization , nor do we keep score of which parts of the source sentence have been translated .",introduction,0,184,172,0,23
machine-translation,1,and contain the results of the experiments .,introduction,0,185,173,0,8
machine-translation,1,"On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .",introduction,1,186,174,0,44
machine-translation,1,"On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date .",introduction,1,187,175,0,17
machine-translation,1,contains some of the unaltered generated translations from the ByteNet that highlight reordering and other phenomena such as transliteration .,introduction,0,188,176,0,20
machine-translation,1,The character - level aspect of the model makes post -processing unnecessary in principle .,introduction,0,189,177,0,15
machine-translation,1,We further visualize the sensitivity of the ByteNet 's predictions to specific source and target inputs using gradient - based visualization .,introduction,0,190,178,0,22
machine-translation,1,represents a heatmap of the magnitude of the gradients of the generated outputs with respect to the source and target inputs .,introduction,0,191,179,0,22
machine-translation,1,"For visual clarity , we sum the gradients for all the characters that makeup each word and normalize the values along each column .",introduction,0,192,180,0,24
machine-translation,1,"In contrast with the attentional pooling mechanism , this general technique allows us to inspect not just dependencies of the outputs on the source inputs , but also dependencies of the outputs on previous target inputs , or on any other neural network layers .",introduction,0,193,181,0,45
machine-translation,1,conclusion,introduction,0,194,182,0,1
machine-translation,1,"We have introduced the ByteNet , a neural translation model that has linear running time , decouples translation from memorization and has short signal propagation paths for tokens in sequences .",introduction,0,195,183,0,31
machine-translation,1,We have shown that the ByteNet decoder is a state - of - the - art character - level language model based on a convolutional neural network that outperforms recurrent neural language models .,introduction,0,196,184,0,34
machine-translation,1,"We have also shown that the ByteNet generalizes the RNN Enc - Dec architecture and achieves state - of - the - art results for character - to - character machine translation and excellent results in general , while maintaining linear running time complexity .",introduction,0,197,185,0,45
machine-translation,1,We have revealed the latent structure learnt by the ByteNet and found it to mirror the expected alignment between the tokens in the sentences ..,introduction,0,198,186,0,25
machine-translation,1,Magnitude of gradients of the predicted outputs with respect to the source and target inputs .,introduction,0,199,187,0,16
machine-translation,1,The gradients are summed for all the characters in a given word .,introduction,0,200,188,0,13
machine-translation,1,"In the bottom heatmap the magnitudes are nonzero on the diagonal , since the prediction of a target character depends highly on the preceding target character in the same word .",introduction,0,201,189,0,31
machine-translation,5,Tilde 's Machine Translation Systems for WMT 2018,title,1,2,1,0,8
machine-translation,5,abstract,abstract,0,3,1,0,1
machine-translation,5,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,abstract,1,4,2,0,25
machine-translation,5,"We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results .",abstract,0,5,3,0,20
machine-translation,5,"For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions .",abstract,0,6,4,0,28
machine-translation,5,The submitted systems were trained using Transformer models .,abstract,0,7,5,0,9
machine-translation,5,introduction,introduction,0,8,1,0,1
machine-translation,5,Neural machine translation ( NMT ) is a rapidly changing research area .,introduction,1,9,2,0,13
machine-translation,5,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",introduction,0,10,3,0,44
machine-translation,5,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,introduction,0,11,4,0,32
machine-translation,5,"In 2017 , multiplicative long short - term memory ( MLSTM ) units and deep GRU models were introduced in NMT .",introduction,0,12,5,0,22
machine-translation,5,"The same year , selfattentional ( Transformer ) models were introduced .",introduction,0,13,6,0,12
machine-translation,5,"Consequently , in 2018 , most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation ( WMT ) were trained using Transformer models",introduction,0,14,7,0,33
machine-translation,5,1 .,introduction,0,15,8,0,2
machine-translation,5,"However , it is already evident that the state - of - the - art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models .",introduction,0,16,9,0,48
machine-translation,5,be pushed even further in 2018 .,introduction,0,17,10,0,7
machine-translation,5,"For instance , have recently proposed RNMT + models that combine deep LSTM - based models with multi-head attention and showed that the models outperform Transformer models .",introduction,0,18,11,0,28
machine-translation,5,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .",introduction,0,19,12,0,13
machine-translation,5,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",introduction,0,20,13,0,42
machine-translation,5,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",introduction,0,21,14,0,17
machine-translation,5,The paper is further structured as follows :,introduction,0,22,15,0,8
machine-translation,5,"Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation , Section 3 describes the data used to train the NMT systems and the data pre-processing workflows , Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems , Section 5 provides automatic evaluation results , and Section 6 concludes the paper .",introduction,0,23,16,0,68
machine-translation,5,system overview,introduction,0,24,17,0,2
machine-translation,5,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .",introduction,1,25,18,0,24
machine-translation,5,The following is a list of the five MT systems submitted :,introduction,1,26,19,0,12
machine-translation,5,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,introduction,1,27,20,0,32
machine-translation,5,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,introduction,1,28,21,0,21
machine-translation,5,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,introduction,1,29,22,0,25
machine-translation,5,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",introduction,1,30,23,0,33
machine-translation,5,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,introduction,1,31,24,0,28
machine-translation,5,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,introduction,1,32,25,0,27
machine-translation,5,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,introduction,1,33,26,0,59
machine-translation,5,data,introduction,0,34,27,0,1
machine-translation,5,"Data preparation was done using one of two distinct workflows - we used the full workflow for tilde - c - nmt , tilde - nc - nmt and tilde - c - nmt - comb submissions .",introduction,0,35,28,0,38
machine-translation,5,For the tilde - c - nmt - 2 bt submission we used the light data preparation workflow .,introduction,0,36,29,0,19
machine-translation,5,full workflow,introduction,0,37,30,0,2
machine-translation,5,"First , we trained constrained system baseline models using the filtered datasets .",introduction,0,38,31,0,13
machine-translation,5,"For baseline models , we used the MLSTM and transf configurations ( see ) .",introduction,0,39,32,0,15
machine-translation,5,"Then , we used the best - performing models ( based on translation quality on the vali -dation set ) , which were the Transformer models ( see , and back - translated monolingual data .",introduction,0,40,33,0,36
machine-translation,5,"As mentioned before , for the unconstrained systems , we back - translated the monolingual data using pre-existing MLSTM - based NMT systems .",introduction,0,41,34,0,24
machine-translation,5,"Then , using the final training data ( parallel and the two synthetic corpora ) , we trained final Transformer models .",introduction,0,42,35,0,22
machine-translation,5,"For the constrained scenario , we trained multiple models ( three for each translation direction ) by experimenting with multiple model configurations .",introduction,0,43,36,0,23
machine-translation,5,"For the unconstrained scenario , we trained one model in each of the directions .",introduction,0,44,37,0,15
machine-translation,5,"In order to acquire the translations for the submissions , we performed model averaging and ensembling as follows :",introduction,0,45,38,0,19
machine-translation,5,"For the tilde - c - nmt ( constrained NMT ) systems , we performed model averaging of the best four models ( according to perplexity ) of the three different run NMT systems and deployed the averaged models in an ensemble .",introduction,0,46,39,0,43
machine-translation,5,"For the tilde - nc - nmt ( unconstrained NMT ) systems , we performed model averaging of the best four models .",introduction,0,47,40,0,23
machine-translation,5,"For the tilde - c - nmt - comb Estonian - English system , we performed majority voting ( see Section 4.3 ) of translations produced by six different runs of different constrained systems ( using best BLEU models , averaged models , ensembled averaged models , ensembled models , and larger beam search ( 10 instead of 5 ) ) .",introduction,0,48,41,0,62
machine-translation,5,data filtering,introduction,0,49,42,0,2
machine-translation,5,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",introduction,0,50,43,0,25
machine-translation,5,"The parallel corpora filtering methods remove sentence pairs that have indications of data corruption or low parallelity ( e.g. , source - target length ratio , content overlap , digit mismatch , language adherence , etc. ) issues .",introduction,0,51,44,0,39
machine-translation,5,"Contrary to Tilde 's submissions for WMT 2017 , isolated sentence pair filtering for the WMT 2018 submissions was supplemented with a maximum content overlap filter ( i.e. only one target sentence for each source sentence was preserved and vice versa based on the content overlap filter 's score for each sentence pair ) .",introduction,0,52,45,0,55
machine-translation,5,"For filtering , we required probabilistic dictionaries , which were obtained from the parallel corpora ( different dictionaries for the constrained and unconstrained scenarios ) using fast align .",introduction,0,53,46,0,29
machine-translation,5,The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,introduction,0,54,47,0,15
machine-translation,5,"During filtering , we identified that one of the corpora that were provided by the organisers contained a significant amount of data corruption .",introduction,0,55,48,0,24
machine-translation,5,it was the estonian ?,introduction,0,56,49,0,5
machine-translation,5,english paracrawl corpus,introduction,0,57,50,0,3
machine-translation,5,3 .,introduction,0,58,51,0,2
machine-translation,5,The corpus consisted of 1.30 million sentence pairs out of which 0.77 million were identified as being corrupt .,introduction,0,59,52,0,19
machine-translation,5,"To reduce the high level of noise , this corpus was filtered using stricter content overlap ( a threshold of 0.3 instead of 0.1 ) and language adherence filters ( both the language detection and the valid alphabet filters had to validate a sentence pair instead of just one of the filters ) than all other corpora .",introduction,0,60,53,0,58
machine-translation,5,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",introduction,0,61,54,0,22
machine-translation,5,"Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",introduction,0,62,55,0,18
machine-translation,5,The corpora statistics before and after filtering are provided in .,introduction,0,63,56,0,11
machine-translation,5,data pre-processing,introduction,0,64,57,0,2
machine-translation,5,All corpora were pre-processed using the parallel data pre-processing workflow from the Tilde MT platform ) that performs the following pre-processing steps :,introduction,0,65,58,0,23
machine-translation,5,"First , parallel corpora are cleaned by removing HTML and XML tags , decoding escaped symbols , normalising whitespaces and punctuation marks , replacing control characters with spaces , etc .",introduction,0,66,59,0,31
machine-translation,5,This step is performed only on the training data .,introduction,0,67,60,0,10
machine-translation,5,"Then , non-translatable entities , such as email addresses , URLs , file paths , etc. are identified and replaced with place - holders .",introduction,0,68,61,0,25
machine-translation,5,This allows reducing data sparsity where it is not needed .,introduction,0,69,62,0,11
machine-translation,5,"Then , the data are tokenised using the Tilde MT regular expression - based tokeniser .",introduction,0,70,63,0,16
machine-translation,5,The Moses truecasing script truecase .,introduction,0,71,64,0,6
machine-translation,5,perl is used to truecase the first word of every sentence .,introduction,0,72,65,0,12
machine-translation,5,"Then , tokens are split into sub - word units using byte - pair encoding ( BPE ) .",introduction,0,73,66,0,19
machine-translation,5,"For the constrained and unconstrained systems , we use BPE models consisting of 24,500 and 49,500 merging operations respectively .",introduction,0,74,67,0,20
machine-translation,5,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .",introduction,0,75,68,0,65
machine-translation,5,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",introduction,0,76,69,0,25
machine-translation,5,"The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",introduction,0,77,70,0,27
machine-translation,5,synthetic data,introduction,0,78,71,0,2
machine-translation,5,"Similarly to Tilde 's 2017 systems , we submitted systems that were trained using synthetic data :",introduction,0,79,72,0,17
machine-translation,5,"1 ) back - translated data , and 2 ) data infused with unknown token identifiers .",introduction,0,80,73,0,17
machine-translation,5,"The back - translated data allow performing domain adaptation and the second type of synthetic data allow training NMT models thatare robust to unknown phenomena ( e.g. , code - mixed content , target language words in the source text , rare or unseen words , etc . ) .",introduction,0,81,74,0,50
machine-translation,5,"To create the synthetic corpora with unknown phenomena , we extracted fast align the parallel corpora and randomly replaced one to three unambiguously ( one - to - one ) aligned content words with unknown word identifiers .",introduction,0,82,75,0,38
machine-translation,5,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",introduction,0,83,76,0,21
machine-translation,5,The back - translated data were acquired from two sources :,introduction,0,84,77,0,11
machine-translation,5,"1 ) the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",introduction,0,85,78,0,71
machine-translation,5,"In order to limit noise , the back - translated data were filtered using the same parallel data filtering methods that were described in Section 3.1.1 ( although with a higher threshold for the content overlap filter ) .",introduction,0,86,79,0,39
machine-translation,5,"Furthermore , in order to train the final systems , we also generated unknown phenomena infused data for the back - translated filtered data , thereby also almost doubling the sizes of the back - translated data .",introduction,0,87,80,0,38
machine-translation,5,The synthetic corpora statistics and the sizes of the total training data are given in .,introduction,0,88,81,0,16
machine-translation,5,light workflow,introduction,0,89,82,0,2
machine-translation,5,The light workflow was used to produce the tilde - c - nmt - 2 bt ( constrained NMT with two sets of back - translated data ) systems .,introduction,0,90,83,0,30
machine-translation,5,"First , we trained baseline models using only filtered parallel datasets ( Parallel - only in ) .",introduction,0,91,84,0,18
machine-translation,5,"Then , we back - translated the first batches of monolingual news data and trained intermediate NMT systems ( Parallel + First Back - translated ) .",introduction,0,92,85,0,27
machine-translation,5,"Finally , we used the intermediate NMT systems to backtranslate the second batches of monolingual news data and trained final NMT systems ( Parallel + Second Back - translated ) .",introduction,0,93,86,0,31
machine-translation,5,"The training progress in shows that the English - Estonian system benefits from the additional data , but the system in the other direction - not so much .",introduction,0,94,87,0,29
machine-translation,5,"For the final translations , we used a postprocessing script to replace consecutive repeating n-grams and repeating ngrams that have a preposition between them ( i.e. , victim of the victim ) with a single n-gram .",introduction,0,95,88,0,37
machine-translation,5,"This problem was more apparent in RNN - based NMT systems , but it was also noticable in our Transformer model outputs .",introduction,0,96,89,0,23
machine-translation,5,nmt,introduction,0,97,90,0,1
machine-translation,5,systems,introduction,0,98,91,0,1
machine-translation,5,"In order to train the NMT systems , we used the Nematus ( Sennrich et al. , 2017 b ) ( for MLSTM models ) and Sockeye ) ( for Transformer models ) toolkits .",introduction,0,99,92,1,35
machine-translation,5,"All models were trained until convergence ( i.e. , until an early stopping criterion was met ) .",introduction,0,100,93,0,18
machine-translation,5,Figure 1 : NMT system training progress ( BLEU scores on the validation set ) for English - Estonian ( left ) and,introduction,0,101,94,0,23
machine-translation,5,estonian - english ( right ) .,introduction,0,102,95,0,7
machine-translation,5,"Note that batch size may differ between different architectures and BLEU scores are calculated on raw ( token level ) pre-processed validation sets , therefore , the scores are slightly higher than evaluation results for the final translations !",introduction,0,103,96,0,39
machine-translation,5,Automatic Post - editing of Named Entities,introduction,0,104,97,0,7
machine-translation,5,"NMT models so far have struggled with translating rare or unseen words ( not different surface forms , but rather different words ) correctly .",introduction,0,105,98,0,25
machine-translation,5,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .",introduction,0,106,99,0,20
machine-translation,5,"In order to aid the NMT model in translating such tokens better , we extracted named entity and non-translatable token dictionaries from the parallel corpora .",introduction,0,107,100,0,26
machine-translation,5,"This was done by performing word alignment of the parallel corpora using fast align and searching ( in a language - agnostic manner ) for transliterated source - target word pairs using a similarity metric based on Levenshtein distance , which start with upper-case letters .",introduction,0,108,101,0,46
machine-translation,5,The dictionaries consist of 15.6 ( 94.7 ) thousand and 6.2 ( 149.8 ) thousand entries for the constrained ( unconstrained ) English - Estonian and Estonian - English NMT systems respectively .,introduction,0,109,102,0,33
machine-translation,5,"When the NMT systems had translated a sentence , source - to - target word alignment was extracted from the source sentence and the translation .",introduction,0,110,103,0,26
machine-translation,5,"Then named entity recognition ( based on dictionary look - up ) was performed on the source text and , if a named entity was found , the target translation was validated against the entries in the dic-tionary .",introduction,0,111,104,0,39
machine-translation,5,"In order to capture different surface forms , a stemming tool was used .",introduction,0,112,105,0,14
machine-translation,5,"If a translation was contradicting the entries in the dictionary , it was replaced with the closest matching ( by looking for the longest matching suffix ) translation from the dictionary .",introduction,0,113,106,0,32
machine-translation,5,"The automatic post-editing method for named entities has a marginal impact on translation quality , however , manual analysis showed that more named entities were corrected than ruined .",introduction,0,114,107,0,29
machine-translation,5,system combination,introduction,0,115,108,0,2
machine-translation,5,We attempted to increase the quality of existing translations by employing a voting scheme in which multiple machine translation outputs are combined to produce a single translation .,introduction,0,116,109,0,28
machine-translation,5,We used a custom implementation of the majority voting algorithm to combine six of our best - scoring outputs in the Estonian - English translation direction in the constrained scenario .,introduction,0,117,110,0,31
machine-translation,5,We did not perform the combination for English - Estonian due to lack of support for alignment extraction for Estonian in Meteor .,introduction,0,118,111,0,23
machine-translation,5,MT system translation combination happens on the sentence level .,introduction,0,119,112,0,10
machine-translation,5,The majority voting scheme assumes a single base translation hypothesis ( primary hypothesis ) which is aligned at the word level to each of the other hypotheses ( secondary hypotheses ) .,introduction,0,120,113,0,32
machine-translation,5,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,introduction,0,121,114,0,22
machine-translation,5,The table is then used to count the number of occurrences of different translations .,introduction,0,122,115,0,15
machine-translation,5,The word translations with the highest count at each position constitute the resulting combined hypothesis .,introduction,0,123,116,0,16
machine-translation,5,To acquire the necessary word alignments we used Meteor .,introduction,0,124,117,0,10
machine-translation,5,Meteor outputs were then converted to a more easily manageable form using the Jane toolkit ) ( we used an awk script distributed with Jane ) .,introduction,0,125,118,0,27
machine-translation,5,The majority voting algorithm was implemented in Python .,introduction,0,126,119,0,9
machine-translation,5,results,result,0,127,1,0,1
machine-translation,5,We performed automatic evaluation of the NMT systems using the SacreBLEU evaluation tool .,result,0,128,2,0,14
machine-translation,5,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,result,1,129,3,0,20
machine-translation,5,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .",result,1,130,4,0,19
machine-translation,5,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,result,1,131,5,0,20
machine-translation,5,"Although the unconstrained models were not trained on factored data , the datasets were 17 times larger than the constrained datasets .",result,0,132,6,0,22
machine-translation,5,"However , the difference is rather minimal and shows that the current NMT architectures may notable to learn effectively from large datasets .",result,0,133,7,0,23
machine-translation,5,The official human evaluation results ( see Table 5 ) from the WMT 2018 shared task on news translation our unconstrained scenario systems ( tilde - nc - nmt ) ranked significantly higher than any other submission for both translation directions .,result,0,134,8,0,42
machine-translation,5,"Our best constrained systems were the second highest ranked systems among all constrained scenario systems , at the same time sharing the same cluster with the highest ranked systems .",result,0,135,9,0,30
machine-translation,5,conclusion,result,0,136,10,0,1
machine-translation,5,The paper described the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,result,0,137,11,0,25
machine-translation,5,We compared Transformer models to MLSTMbased models and showed that the Transformer models outperform the older NMT architecture .,result,0,138,12,0,19
machine-translation,5,We also showed that double back - translation may improve translation quality further than single back - translation .,result,0,139,13,0,19
machine-translation,5,"In terms of model ensembling and averaging , we showed that the best results in the constrained scenario were achieved by en - :",result,0,140,14,0,24
machine-translation,5,Top three systems for the constrained ( C ) and unconstrained ( U ) scenarios according to the official results of the WMT 2018 shared task on news translation ; ordered by the direct assessment ( DA ) standardized mean score sembling different run averaged models .,result,0,141,15,0,47
machine-translation,5,"In total , seven systems were submitted by Tilde for the English ?",result,0,142,16,0,13
machine-translation,5,estonian language pair .,result,0,143,17,0,4
machine-translation,7,OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,title,0,2,1,0,15
machine-translation,7,abstract,abstract,0,3,1,0,1
machine-translation,7,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,0,4,2,0,17
machine-translation,7,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract,1,5,3,0,34
machine-translation,7,"In practice , however , there are significant algorithmic and performance challenges .",abstract,0,6,4,0,13
machine-translation,7,"In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters .",abstract,0,7,5,0,37
machine-translation,7,"We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks .",abstract,0,8,6,0,29
machine-translation,7,A trainable gating network determines a sparse combination of these experts to use for each example .,abstract,0,9,7,0,17
machine-translation,7,"We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .",abstract,0,10,8,0,32
machine-translation,7,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .,abstract,0,11,9,0,22
machine-translation,7,"On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",abstract,0,12,10,0,28
machine-translation,7,* Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency ),abstract,0,13,11,0,19
machine-translation,7,INTRODUCTION AND RELATED WORK 1 .,abstract,0,14,12,0,6
machine-translation,7,conditional computation,abstract,0,15,13,0,2
machine-translation,7,Exploiting scale in both training data and model size has been central to the success of deep learning .,abstract,1,16,14,0,19
machine-translation,7,"When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy .",abstract,0,17,15,0,24
machine-translation,7,"This has been shown in domains such as text , images , and audio .",abstract,0,18,16,0,15
machine-translation,7,"For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .",abstract,0,19,17,0,42
machine-translation,7,"Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand .",abstract,0,20,18,0,17
machine-translation,7,Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .,abstract,0,21,19,0,22
machine-translation,7,"In these schemes , large parts of a network are active or inactive on a per-example basis .",abstract,0,22,20,0,18
machine-translation,7,"The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .",abstract,0,23,21,0,14
machine-translation,7,Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions .,abstract,0,24,22,0,17
machine-translation,7,"While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .",abstract,0,25,23,0,28
machine-translation,7,We blame this on a combination of the following challenges :,abstract,0,26,24,0,11
machine-translation,7,"Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .",abstract,0,27,25,0,16
machine-translation,7,Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .,abstract,0,28,26,0,23
machine-translation,7,"Large batch sizes are critical for performance , as they amortize the costs of parameter transfers and updates .",abstract,0,29,27,0,19
machine-translation,7,Conditional computation reduces the batch sizes for the conditionally active chunks of the network .,abstract,0,30,28,0,15
machine-translation,7,Network bandwidth can be a bottleneck .,abstract,0,31,29,0,7
machine-translation,7,A cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .,abstract,0,32,30,0,21
machine-translation,7,"To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .",abstract,0,33,31,0,19
machine-translation,7,"Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .",abstract,0,34,32,0,21
machine-translation,7,"Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .",abstract,0,35,33,0,31
machine-translation,7,"Depending on the scheme , loss terms maybe necessary to achieve the desired level of sparsity per-chunk and / or per example .",abstract,0,36,34,0,23
machine-translation,7,use three such terms .,abstract,0,37,35,0,5
machine-translation,7,These issues can affect both model quality and load - balancing .,abstract,0,38,36,0,12
machine-translation,7,Model capacity is most critical for very large data sets .,abstract,0,39,37,0,11
machine-translation,7,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",abstract,0,40,38,0,21
machine-translation,7,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",abstract,0,41,39,1,29
machine-translation,7,"In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation .",abstract,0,42,40,0,24
machine-translation,7,We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets .,abstract,0,43,41,0,37
machine-translation,7,OUR APPROACH : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,method,0,44,1,0,13
machine-translation,7,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,method,1,45,2,0,32
machine-translation,7,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",method,1,46,3,0,39
machine-translation,7,All parts of the network are trained jointly by back - propagation .,method,1,47,4,0,13
machine-translation,7,"While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models .",method,0,48,5,0,30
machine-translation,7,"In particular , we apply a MoE convolutionally between stacked LSTM layers , as in .",method,0,49,6,0,16
machine-translation,7,"The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position .",method,0,50,7,0,23
machine-translation,7,The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .,method,0,51,8,0,19
machine-translation,7,"On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost .",method,0,52,9,0,23
machine-translation,7,RELATED WORK ON MIXTURES OF EXPERTS,method,0,53,10,0,6
machine-translation,7,"Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .",method,0,54,11,0,24
machine-translation,7,"Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .",method,0,55,12,0,22
machine-translation,7,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",method,0,56,13,0,24
machine-translation,7,suggest an ensemble model in the format of mixture of experts for machine translation .,method,0,57,14,0,15
machine-translation,7,The gating network is trained on a pre-trained ensemble NMT model .,method,0,58,15,0,12
machine-translation,7,The works above concern top - level mixtures of experts .,method,0,59,16,0,11
machine-translation,7,The mixture of experts is the whole model .,method,0,60,17,0,9
machine-translation,7,introduce the idea of using multiple,method,0,61,18,0,6
machine-translation,7,MoEs with their own gating networks as parts of a deep model .,method,0,62,19,0,13
machine-translation,7,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",method,0,63,20,0,23
machine-translation,7,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",method,0,64,21,0,22
machine-translation,7,Our work builds on this use of MoEs as a general purpose neural network component .,method,0,65,22,0,16
machine-translation,7,"While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",method,0,66,23,0,31
machine-translation,7,We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity .,method,0,67,24,0,19
machine-translation,7,THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER,method,0,68,1,0,10
machine-translation,7,"The Mixture - of - Experts ( MoE ) layer consists of a set of n "" expert networks "" E 1 , , E n , and a "" gating network "" G whose output is a sparse n-dimensional vector .",method,0,69,2,0,42
machine-translation,7,shows an overview of the MoE module .,method,0,70,3,0,8
machine-translation,7,"The experts are themselves neural networks , each with their own parameters .",method,0,71,4,0,13
machine-translation,7,"Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .",method,0,72,5,0,53
machine-translation,7,Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network for a given input x .,method,0,73,6,0,36
machine-translation,7,The output y of the MoE module can be written as follows :,method,0,74,7,0,13
machine-translation,7,We save computation based on the sparsity of the output of G ( x ) .,method,0,75,8,0,16
machine-translation,7,"Wherever G (x ) i = 0 , we need not compute E i ( x ) .",method,0,76,9,0,18
machine-translation,7,"In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example .",method,0,77,10,0,25
machine-translation,7,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",method,0,78,11,0,60
machine-translation,7,In the following we focus on ordinary MoEs .,method,0,79,12,0,9
machine-translation,7,We provide more details on hierarchical MoEs in Appendix B.,method,0,80,13,0,10
machine-translation,7,Our implementation is related to other models of conditional computation .,method,0,81,14,0,11
machine-translation,7,A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .,method,0,82,15,0,18
machine-translation,7,"A MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .",method,0,83,16,0,33
machine-translation,7,gating network,method,0,84,17,0,2
machine-translation,7,softmax gating :,method,0,85,18,0,3
machine-translation,7,A simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .,method,0,86,19,0,27
machine-translation,7,noisy top - k,method,0,87,20,0,4
machine-translation,7,gating :,method,0,88,21,0,2
machine-translation,7,We add two components to the Softmax gating network : sparsity and noise .,method,0,89,22,0,14
machine-translation,7,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ??",method,0,90,23,0,25
machine-translation,7,( which causes the corresponding gate values to equal 0 ) .,method,0,91,24,0,12
machine-translation,7,"The sparsity serves to save computation , as described above .",method,0,92,25,0,11
machine-translation,7,"While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .",method,0,93,26,0,30
machine-translation,7,"The noise term helps with load balancing , as will be discussed in Appendix A .",method,0,94,27,0,16
machine-translation,7,The amount of noise per component is controlled by a second trainable weight matrix W noise .,method,0,95,28,0,17
machine-translation,7,training the gating network,method,0,96,29,0,4
machine-translation,7,"We train the gating network by simple back - propagation , along with the rest of the model .",method,0,97,30,0,19
machine-translation,7,"If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .",method,0,98,31,0,28
machine-translation,7,This type of occasionally - sensitive behavior is described in with respect to noisy rectifiers .,method,0,99,32,0,16
machine-translation,7,Gradients also backpropagate through the gating network to its inputs .,method,0,100,33,0,11
machine-translation,7,Our method differs here from who use boolean gates and a REINFORCE - style approach to train the gating network .,method,0,101,34,0,21
machine-translation,7,addressing performance,method,0,102,35,0,2
machine-translation,7,challenges,method,0,103,36,0,1
machine-translation,7,the shrinking batch problem,method,0,104,37,0,4
machine-translation,7,"On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .",method,0,105,38,0,27
machine-translation,7,"If the gating network chooses k out of n experts for each example , then for a batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .",method,0,106,39,0,36
machine-translation,7,This causes a naive MoE implementation to become very inefficient as the number of experts increases .,method,0,107,40,0,17
machine-translation,7,The solution to this shrinking batch problem is to make the original batch size as large as possible .,method,0,108,41,0,19
machine-translation,7,"However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .",method,0,109,42,0,22
machine-translation,7,We propose the following techniques for increasing the batch size :,method,0,110,43,0,11
machine-translation,7,Mixing Data Parallelism and Model Parallelism :,method,0,111,44,0,7
machine-translation,7,"In a conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers .",method,0,112,45,0,33
machine-translation,7,"In our technique , these different batches run synchronously so that they can be combined for the MoE layer .",method,0,113,46,0,20
machine-translation,7,"We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert .",method,0,114,47,0,30
machine-translation,7,Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches .,method,0,115,48,0,25
machine-translation,7,The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) .,method,0,116,49,0,37
machine-translation,7,"If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .",method,0,117,50,0,30
machine-translation,7,"Thus , we achieve a factor of d improvement inexpert batch size .",method,0,118,51,0,13
machine-translation,7,"In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .",method,0,119,52,0,28
machine-translation,7,Each secondary MoE resides on one device .,method,0,120,53,0,8
machine-translation,7,This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .,method,0,121,54,0,30
machine-translation,7,"The total batch size increases , keeping the batch size per expert constant .",method,0,122,55,0,14
machine-translation,7,"The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .",method,0,123,56,0,41
machine-translation,7,It is our goal to train a trillionparameter model on a trillion - word corpus .,method,0,124,57,0,16
machine-translation,7,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .",method,0,125,58,0,26
machine-translation,7,taking advantage of convolutionality :,method,0,126,59,0,5
machine-translation,7,"In our language models , we apply the same MoE to each time step of the previous layer .",method,0,127,60,0,19
machine-translation,7,"If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch .",method,0,128,61,0,26
machine-translation,7,Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps .,method,0,129,62,0,24
machine-translation,7,Increasing Batch Size for a,method,0,130,63,0,5
machine-translation,7,recurrent moe :,method,0,131,64,0,3
machine-translation,7,We suspect that even more powerful models may involve applying a MoE recurrently .,method,0,132,65,0,14
machine-translation,7,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",method,0,133,66,0,51
machine-translation,7,"Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .",method,0,134,67,1,31
machine-translation,7,This would allow for a large increase in batch size .,method,0,135,68,0,11
machine-translation,7,network bandwidth,method,0,136,69,0,2
machine-translation,7,Another major performance concern in distributed computing is network bandwidth .,method,0,137,70,0,11
machine-translation,7,"Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .",method,0,138,71,0,35
machine-translation,7,"To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .",method,0,139,72,0,34
machine-translation,7,"For GPUs , this maybe thousands to one .",method,0,140,73,0,9
machine-translation,7,"In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units .",method,0,141,74,0,19
machine-translation,7,"Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .",method,0,142,75,0,32
machine-translation,7,"Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .",method,0,143,76,0,20
machine-translation,7,balancing expert utilization,method,0,144,77,0,3
machine-translation,7,We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts .,method,0,145,78,0,25
machine-translation,7,"This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network .",method,0,146,79,0,26
machine-translation,7,"describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .",method,0,147,80,0,21
machine-translation,7,include a soft constraint on the batch - wise average of each gate .,method,0,148,81,0,14
machine-translation,7,We take a soft constraint approach .,method,0,149,82,0,7
machine-translation,7,We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .,method,0,150,83,0,27
machine-translation,7,"We define an additional loss L importance , which is added to the over all loss function for the model .",method,0,151,84,0,21
machine-translation,7,"This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .",method,0,152,85,0,30
machine-translation,7,This additional loss encourages all experts to have equal importance .,method,0,153,86,0,11
machine-translation,7,L importance ( X ) = w importance CV ( Importance ( X ) ),method,0,154,87,0,15
machine-translation,7,2 . The number of parameters in the LSTM layers of these models vary from 2 million to 151 million .,method,0,155,88,0,21
machine-translation,7,"Quality increases greatly with parameter count , as do computational costs .",method,0,156,89,0,12
machine-translation,7,Results for these models form the top line of - right .,method,0,157,90,0,12
machine-translation,7,moe models :,method,0,158,91,0,3
machine-translation,7,Our models consist of two stacked LSTM layers with a MoE layer between them ( see ) .,method,0,159,92,0,18
machine-translation,7,We vary the sizes of the layers and the number of experts .,method,0,160,93,0,13
machine-translation,7,"For full details on model architecture , training regimen , additional baselines and results , see Appendix C .",method,0,161,94,0,19
machine-translation,7,The results of these models are shown in - left .,method,0,162,95,0,11
machine-translation,7,"The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .",method,0,163,96,0,43
machine-translation,7,"varied computation , high capacity :",method,0,164,97,0,6
machine-translation,7,"In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets .",method,0,165,98,0,32
machine-translation,7,"These models had larger LSTMs , and fewer but larger and experts .",method,0,166,99,0,13
machine-translation,7,Details can be found in Appendix C.2 .,method,0,167,100,0,8
machine-translation,7,Results of these three models form the bottom line of - right .,method,0,168,101,0,13
machine-translation,7,compares the results of these models to the best previously - published result on this dataset .,method,0,169,102,0,17
machine-translation,7,"Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation .",method,0,170,103,0,31
machine-translation,7,computational,method,0,171,104,0,1
machine-translation,7,Efficiency : We trained our models using TensorFlow on clusters containing 16 - 32 Tesla K40 GPUs .,method,0,172,105,0,18
machine-translation,7,"For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster .",method,0,173,106,0,42
machine-translation,7,"The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .",method,0,174,107,0,54
machine-translation,7,"For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total .",method,0,175,108,0,26
machine-translation,7,"For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU .",method,0,176,109,0,20
machine-translation,7,"For our low-computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism .",method,0,177,110,0,35
machine-translation,7,"Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices .",method,0,178,111,0,22
machine-translation,7,These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .,method,0,179,112,0,19
machine-translation,7,"Detailed results are in Appendix C , .",method,0,180,113,0,8
machine-translation,7,"On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in - left .",method,0,181,114,0,38
machine-translation,7,"We hypothesized that for a larger training set , even higher capacities would produce significant quality improvements .",method,0,182,115,0,18
machine-translation,7,100 BILLION WORD GOOGLE NEWS CORPUS,method,1,183,116,0,6
machine-translation,7,"We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .",method,0,184,117,0,24
machine-translation,7,"Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .",method,0,185,118,0,24
machine-translation,7,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .",method,1,186,119,0,20
machine-translation,7,This corresponds to up to 137 billion parameters in the MoE layer .,method,0,187,120,0,13
machine-translation,7,"Details on architecture , training , and results are given in Appendix D.",method,0,188,121,0,13
machine-translation,7,Results : shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) .,method,0,189,122,0,29
machine-translation,7,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .",method,1,190,123,0,47
machine-translation,7,The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets .,method,0,191,124,0,22
machine-translation,7,"Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU .",method,0,192,125,0,25
machine-translation,7,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),method,1,193,126,0,7
machine-translation,7,model architecture :,method,0,194,127,0,3
machine-translation,7,Our model was a modified version of the GNMT model described in .,method,1,195,128,0,13
machine-translation,7,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",method,1,196,129,0,26
machine-translation,7,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,method,1,197,130,0,26
machine-translation,7,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .",method,1,198,131,0,27
machine-translation,7,"Further details on model architecture , testing procedure and results can be found in Appendix E.",method,0,199,132,0,16
machine-translation,7,datasets :,method,0,200,133,0,2
machine-translation,7,We benchmarked our method on the WMT ' 14 En? Fr and En ?,method,0,201,134,0,14
machine-translation,7,"De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",method,0,202,135,0,18
machine-translation,7,"The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .",method,0,203,136,0,37
machine-translation,7,We also tested the same model on a Google 's Production English to French data . 2.79 39.22 214M 278M 6 days/96 k 80s GNMT+RL 2.96 39.92 214M 278M 6 days/96 k80s PBMT 37.0 LSTM ( 6-layer ) 31.5 LSTM ( 6-layer + PosUnk ) 33.1 DeepAtt 37.7 DeepAtt+PosUnk 39.2 5.25 24.91 214M 278M 1 day/96 k80s GNMT + RL 8.08 24.66 214M 278M 1 day/96 k80s PBMT 20.7 DeepAtt 20.6,method,0,204,137,0,71
machine-translation,7,"Results : show the results of our largest models , compared with published results .",method,0,205,138,0,15
machine-translation,7,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,method,1,206,139,0,21
machine-translation,7,"As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in .",method,0,207,140,0,28
machine-translation,7,The perplexity scores are also better .,method,0,208,141,0,7
machine-translation,7,2,method,0,209,142,0,1
machine-translation,7,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .",method,1,210,143,0,25
machine-translation,7,multilingual machine translation,method,1,211,144,0,3
machine-translation,7,results :,result,0,212,1,0,2
machine-translation,7,"Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in .",result,0,213,2,0,22
machine-translation,7,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,result,1,214,3,0,18
machine-translation,7,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",result,1,215,4,0,43
machine-translation,7,The poor performance on English ?,result,0,216,5,0,6
machine-translation,7,"Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",result,0,217,6,0,30
machine-translation,7,conclusion,result,0,218,7,0,1
machine-translation,7,This work is the first to demonstrate major wins from conditional computation in deep networks .,result,0,219,8,0,16
machine-translation,7,We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions .,result,0,220,9,0,23
machine-translation,7,"While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets .",result,0,221,10,0,22
machine-translation,7,We look forward to seeing many novel implementations and applications of conditional computation in the years to come .,result,0,222,11,0,19
machine-translation,7,APPENDICES A LOAD - BALANCING LOSS,result,0,223,12,0,6
machine-translation,7,"As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples .",result,0,224,13,0,32
machine-translation,7,"Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation .",result,0,225,14,0,24
machine-translation,7,"Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert for a batch X of inputs .",result,0,226,15,0,27
machine-translation,7,The smoothness allows us to back - propagate gradients through the estimator .,result,0,227,16,0,13
machine-translation,7,This is the purpose of the noise term in the gating function .,result,0,228,17,0,13
machine-translation,7,"We define P ( x , i ) as the probability that G (x ) i is nonzero , given a new random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements .",result,0,229,18,0,44
machine-translation,7,"To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .",result,0,230,19,0,46
machine-translation,7,The probability works out to be :,result,0,231,20,0,7
machine-translation,7,"Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .",result,0,232,21,0,21
machine-translation,7,"simplifying , we get :",result,0,233,22,0,5
machine-translation,7,where ?,result,0,234,23,0,2
machine-translation,7,is the CDF of the standard normal distribution .,result,0,235,24,0,9
machine-translation,7,"We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load .",result,0,236,25,0,32
machine-translation,7,L load ( X ) = w load CV ( Load ( X ) ),result,0,237,26,0,15
machine-translation,7,2 ( 11 ) Initial Load Imbalance :,result,0,238,27,0,8
machine-translation,7,"To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need sometime to work ) .",result,0,239,28,0,34
machine-translation,7,"To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise .",result,0,240,29,0,25
machine-translation,7,experiments :,experiment,0,241,1,0,2
machine-translation,7,"We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load .",experiment,0,242,2,0,31
machine-translation,7,"We trained each model for 10 epochs , then measured perplexity on the test set .",experiment,0,243,3,0,16
machine-translation,7,"We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load .",experiment,0,244,4,0,29
machine-translation,7,This last value is significant for load balancing purposes on distributed hardware .,experiment,0,245,5,0,13
machine-translation,7,All of these metrics were averaged over several training batches .,experiment,0,246,6,0,11
machine-translation,7,results :,result,0,247,1,0,2
machine-translation,7,results are reported in .,result,0,248,1,0,5
machine-translation,7,"All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse .",result,0,249,2,0,25
machine-translation,7,Models with higher values of w load had lower loads on the most overloaded expert .,result,0,250,3,0,16
machine-translation,7,B HIERACHICAL MIXTURE OF EXPERTS,result,0,251,4,0,5
machine-translation,7,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",result,0,252,5,0,60
machine-translation,7,3,result,0,253,6,0,1
machine-translation,7,"If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by , and the expert networks by ( E 0 , 0 , E 0 , 1 ..E a , b ) .",result,0,254,7,0,50
machine-translation,7,The output of the MoE is given by :,result,0,255,8,0,9
machine-translation,7,Our metrics of expert utilization change to the following :,result,0,256,9,0,10
machine-translation,7,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,result,0,257,10,0,22
machine-translation,7,X ( i ) denotes the subset of X for which G primary ( x ) i >,result,0,258,11,0,18
machine-translation,7,0 .,result,0,259,12,0,2
machine-translation,7,"It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above .",result,0,260,13,0,45
machine-translation,7,C 1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS,result,0,261,14,0,10
machine-translation,7,C.1 8- MILLION - OPERATIONS - PER - TIMESTEP MODELS,result,0,262,15,0,10
machine-translation,7,model architecture :,result,0,263,16,0,3
machine-translation,7,"Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer , a MoE layer , a second LSTM layer , and a softmax layer .",result,0,264,17,0,38
machine-translation,7,"The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 .",result,0,265,18,0,32
machine-translation,7,"For every layer other than the softmax , we apply drouput to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 ? DropP rob ) .",result,0,266,19,0,34
machine-translation,7,"After dropout , the output of the previous layer is added to the layer output .",result,0,267,20,0,16
machine-translation,7,This residual connection encourages gradient flow .,result,0,268,21,0,7
machine-translation,7,"For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .",result,0,269,22,0,24
machine-translation,7,We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers .,result,0,270,23,0,35
machine-translation,7,"Thus , each example is processed by exactly 4 experts for a total of 4M ops / timestep .",result,0,271,24,0,19
machine-translation,7,The two LSTM layers contribute 2M ops / timestep each for the desired total of 8 M .,result,0,272,25,0,18
machine-translation,7,computationally - matched baselines :,result,0,273,26,0,5
machine-translation,7,"The MoE - 4 model does not employ sparsity , since all 4 experts are always used .",result,0,274,27,0,18
machine-translation,7,"In addition , we trained four more computationally - matched baseline models with no sparsity :",result,0,275,28,0,16
machine-translation,7,moe - 1 - wide :,result,0,276,29,0,6
machine-translation,7,"The MoE layer consists of a single "" expert "" containing one ReLU - activated hidden layer of size 4096 .",result,0,277,30,0,21
machine-translation,7,moe - 1 - deep :,result,0,278,31,0,6
machine-translation,7,"The MoE layer consists of a single "" expert "" containing four ReLU - activated hidden layers , each with size 1024 .",result,0,279,32,0,23
machine-translation,7,4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers .,result,0,280,33,0,18
machine-translation,7,lstm - 2048-512 :,result,0,281,34,0,4
machine-translation,7,The model contains one 2048 - unit LSTM layer ( and no MoE ) .,result,0,282,35,0,15
machine-translation,7,The output of the LSTM is projected down to 512 dimensions .,result,0,283,36,0,12
machine-translation,7,The next timestep of the LSTM receives the projected output .,result,0,284,37,0,11
machine-translation,7,This is identical to one of the models published in .,result,0,285,38,0,11
machine-translation,7,"We re-ran it to account for differences in training regimen , and obtained results very similar to the published ones .",result,0,286,39,0,21
machine-translation,7,training :,result,0,287,40,0,2
machine-translation,7,The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 .,result,0,288,41,0,20
machine-translation,7,"Each batch consisted of a set of sentences totaling roughly 300,000 words .",result,0,289,42,0,13
machine-translation,7,"In the interest of time , we limited training to 10 epochs , ( 27,000 steps ) .",result,0,290,43,0,18
machine-translation,7,"Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) .",result,0,291,44,0,36
machine-translation,7,We used the Adam optimizer .,result,0,292,45,0,6
machine-translation,7,"The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number .",result,0,293,46,0,33
machine-translation,7,The Softmax output layer was trained efficiently using importance sampling similarly to the models in .,result,0,294,47,0,16
machine-translation,7,"For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 .",result,0,295,48,0,23
machine-translation,7,"To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A.",result,0,296,49,0,25
machine-translation,7,results :,result,0,297,1,0,2
machine-translation,7,"We evaluate our model using perplexity on the holdout dataset , used by .",result,0,298,2,0,14
machine-translation,7,We follow the standard procedure and sum over all the words including the end of sentence symbol .,result,0,299,3,0,18
machine-translation,7,results are reported in .,result,0,300,1,0,5
machine-translation,7,"For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .",result,0,301,2,0,29
machine-translation,7,We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .,result,0,302,3,0,17
machine-translation,7,"First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass .",result,0,303,4,0,25
machine-translation,7,"Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage :",result,0,304,5,0,16
machine-translation,7,The Adam optimizer keeps first and second moment estimates of the perparameter gradients .,result,0,305,6,0,14
machine-translation,7,This triples the required memory .,result,0,306,7,0,6
machine-translation,7,"To avoid keeping a first - moment estimator , we set ?",result,0,307,8,0,12
machine-translation,7,"1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .",result,0,308,9,0,22
machine-translation,7,"For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix .",result,0,309,10,0,34
machine-translation,7,"At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one .",result,0,310,11,0,27
machine-translation,7,This technique could similarly be applied to Adagrad .,result,0,311,12,0,9
machine-translation,7,results :,result,0,312,1,0,2
machine-translation,7,We evaluate our model using perplexity on a holdout dataset .,result,0,313,2,0,11
machine-translation,7,results are reported in .,result,0,314,1,0,5
machine-translation,7,Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model .,result,0,315,2,0,25
machine-translation,7,It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models .,result,0,316,3,0,27
machine-translation,7,"This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs .",result,0,317,4,0,34
machine-translation,7,"For comparison , we include results for a computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing .",result,0,318,5,0,31
machine-translation,7,4,result,0,319,6,0,1
machine-translation,7,E MACHINE TRANSLATION - EXPERIMENTAL DETAILS,result,0,320,7,0,6
machine-translation,7,Model Architecture for Single Language,result,0,321,8,0,5
machine-translation,7,pair moe models :,result,0,322,9,0,4
machine-translation,7,Our model is a modified version of the GNMT model described in .,result,0,323,10,0,13
machine-translation,7,"To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",result,0,324,11,0,26
machine-translation,7,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,result,0,325,12,0,26
machine-translation,7,"We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .",result,0,326,13,0,27
machine-translation,7,All of the layers in our model have input and output dimensionality of 512 .,result,0,327,14,0,15
machine-translation,7,"Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .",result,0,328,15,0,16
machine-translation,7,We add residual connections around all LSTM and MoE layers to encourage gradient flow .,result,0,329,16,0,15
machine-translation,7,"Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as "" wordpieces "" )",result,0,330,17,0,23
machine-translation,7,"( Schuster & Nakajima , 2012 ) for inputs and outputs in our system .",result,0,331,18,1,15
machine-translation,7,We use a shared source and target vocabulary of 32 K wordpieces .,result,0,332,19,0,13
machine-translation,7,We also used the same beam search technique as proposed in Model Architecture for Multilingual MoE Model :,result,0,333,20,0,18
machine-translation,7,"We used the same model architecture as for the single - language - pair models , with the following exceptions :",result,0,334,21,0,21
machine-translation,7,"We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .",result,0,335,22,0,42
machine-translation,7,Each expert has a larger hidden layer of size 8192 .,result,0,336,23,0,11
machine-translation,7,"This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .",result,0,337,24,0,28
machine-translation,7,training :,result,0,338,25,0,2
machine-translation,7,We trained our networks using the Adam optimizer .,result,0,339,26,0,9
machine-translation,7,"The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number .",result,0,340,27,0,41
machine-translation,7,"For the single - language - pair models , similarly to , we applied dropout to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 .",result,0,341,28,0,33
machine-translation,7,Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 .,result,0,342,29,0,18
machine-translation,7,Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU .,result,0,343,30,0,17
machine-translation,7,"To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A.",result,0,344,31,0,25
machine-translation,7,Metrics : We evaluated our models using the perplexity and the standard BLEU score metric .,result,0,345,32,0,16
machine-translation,7,"We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .",result,0,346,33,0,31
machine-translation,7,Results : and 4 in Section 5.3 show comparisons of our results to other published methods .,result,0,347,34,0,17
machine-translation,7,shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .,result,0,348,35,0,28
machine-translation,7,"As can be seen from the as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve .",result,0,349,36,0,27
machine-translation,7,"We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in .",result,0,350,37,0,22
machine-translation,7,"For example , one expert is used when the indefinite article "" a "" introduces the direct object in a verb phrase indicating importance or leadership .",result,0,351,38,0,27
machine-translation,7,f strictly balanced gating,result,0,352,39,0,4
machine-translation,7,"Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size .",result,0,353,40,0,39
machine-translation,7,"To accommodate this , we used a different gating function which we describe below .",result,0,354,41,0,15
machine-translation,7,Recall that we define the softmax gating function to be :,result,0,355,42,0,11
machine-translation,7,sparse gating ( alternate formulation ) :,result,0,356,43,0,7
machine-translation,7,"To obtain a sparse gating vector , we multiply G ?",result,0,357,44,0,11
machine-translation,7,( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .,result,0,358,45,0,23
machine-translation,7,"The mask itself is a function of G ? ( x ) and specifies which experts are assigned to each input example : M batchwise ( X , m ) j , i = 1 if X j, i is in the top m values for to expert i 0 otherwise",result,0,359,46,0,51
machine-translation,7,"As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .",result,0,360,47,0,37
machine-translation,7,Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask .,result,0,361,48,0,23
machine-translation,7,We use the following mask at inference time :,result,0,362,49,0,9
machine-translation,7,"To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .",result,0,363,50,0,28
machine-translation,7,"L batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ?",result,0,364,51,0,26
machine-translation,7,"M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",result,0,365,52,0,21
machine-translation,7,g attention function,result,0,366,53,0,3
machine-translation,7,"The attention mechanism described in GNMT involves a learned "" Attention Function "" A ( x i , y j ) which takes a "" source vector "" x i and a "" target vector "" y j , and must be computed for every source time step i and target time step j .",result,0,367,54,0,55
machine-translation,7,"In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n.",result,0,368,55,0,21
machine-translation,7,It can be expressed as :,result,0,369,56,0,6
machine-translation,7,Where U and Ware trainable weight matrices and V is a trainable weight vector .,result,0,370,57,0,15
machine-translation,7,"For performance reasons , in our models , we used a slightly different attention function :",result,0,371,58,0,16
machine-translation,7,"With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications .",result,0,372,59,0,27
machine-translation,7,We found little difference in quality between the two functions .,result,0,373,60,0,11
machine-translation,4,Unsupervised Neural Machine Translation with Weight Sharing,title,0,2,1,0,7
machine-translation,4,abstract,abstract,0,3,1,0,1
machine-translation,4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,abstract,1,4,2,0,27
machine-translation,4,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",abstract,1,5,3,0,53
machine-translation,4,"To address this issue , we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high - level representations of the input sentences .",abstract,0,6,4,0,33
machine-translation,4,"Besides , two different generative adversarial networks ( GANs ) , namely the local GAN and global GAN , are proposed to enhance the cross - language translation .",abstract,0,7,5,0,29
machine-translation,4,"With this new approach , we achieve significant improvements on English - German , English - French and Chinese - to - English translation tasks .",abstract,0,8,6,0,26
machine-translation,4,introduction,introduction,0,9,1,0,1
machine-translation,4,"Neural machine translation , directly applying a single neural network to transform the source sentence into the target sentence , has now reached impressive performance .",introduction,0,10,2,0,26
machine-translation,4,The NMT typically consists of two sub neural networks .,introduction,0,11,3,0,10
machine-translation,4,"The encoder network reads and encodes the source sentence into a 1 Feng Wang is the corresponding author of this paper context vector , and the decoder network generates the target sentence iteratively based on the context vector .",introduction,0,12,4,0,39
machine-translation,4,NMT can be studied in supervised and unsupervised learning settings .,introduction,0,13,5,0,11
machine-translation,4,"In the supervised setting , bilingual corpora is available for training the NMT model .",introduction,0,14,6,0,15
machine-translation,4,"In the unsupervised setting , we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages .",introduction,0,15,7,0,33
machine-translation,4,"Due to lack of alignment information , the unsupervised NMT is considered more challenging .",introduction,0,16,8,0,15
machine-translation,4,"However , this task is very promising , since the monolingual corpora is usually easy to be collected .",introduction,0,17,9,0,19
machine-translation,4,"Motivated by recent success in unsupervised cross - lingual embeddings , the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared - latent space .",introduction,0,18,10,0,43
machine-translation,4,"Following this assumption , use a single encoder and a single decoder for both the source and target languages .",introduction,0,19,11,0,20
machine-translation,4,"The encoder and decoder , acting as a standard auto - encoder ( AE ) , are trained to reconstruct the inputs .",introduction,0,20,12,0,23
machine-translation,4,And utilize a shared encoder but two independent decoders .,introduction,0,21,13,0,10
machine-translation,4,"With some good performance , they share a glaring defect , i.e. , only one encoder is shared by the source and target languages .",introduction,0,22,14,0,25
machine-translation,4,"Although the shared encoder is vital for mapping sentences from different languages into the shared - latent space , it is weak in keeping the uniqueness and internal characteristics of each language , such as the style , terminology and sentence structure .",introduction,0,23,15,0,43
machine-translation,4,"Since each language has its own characteristics , the source and target languages should be encoded and learned independently .",introduction,0,24,16,0,20
machine-translation,4,"Therefore , we conjecture that the shared encoder maybe a factor limit - ing the potential translation performance .",introduction,0,25,17,0,19
machine-translation,4,"In order to address this issue , we extend the encoder - shared model , i.e. , the model with one shared encoder , by leveraging two independent encoders with each for one language .",introduction,1,26,18,0,35
machine-translation,4,"Similarly , two independent decoders are utilized .",introduction,1,27,19,0,8
machine-translation,4,"For each language , the encoder and its corresponding decoder perform an AE , where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations .",introduction,1,28,20,0,37
machine-translation,4,"To map the latent representations from different languages to a shared - latent space , we propose the weightsharing constraint to the two AEs .",introduction,1,29,21,0,25
machine-translation,4,"Specifically , we share the weights of the last few layers of two encoders thatare responsible for extracting highlevel representations of input sentences .",introduction,0,30,22,0,24
machine-translation,4,"Similarly , we share the weights of the first few layers of two decoders .",introduction,0,31,23,0,15
machine-translation,4,"To enforce the shared - latent space , the word embeddings are used as a reinforced encoding component in our encoders .",introduction,0,32,24,0,22
machine-translation,4,"For cross - language translation , we utilize the backtranslation following .",introduction,1,33,25,0,12
machine-translation,4,"Additionally , two different generative adversarial networks ( GAN ) , namely the local and global GAN , are proposed to further improve the cross - language translation .",introduction,1,34,26,0,29
machine-translation,4,"We utilize the local GAN to constrain the source and target latent representations to have the same distribution , whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation .",introduction,1,35,27,0,42
machine-translation,4,"We apply the global GAN to finetune the corresponding generator , i.e. , the composition of the encoder and decoder of the other language , where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 .",introduction,1,36,28,0,53
machine-translation,4,"In summary , we mainly make the following contributions :",introduction,0,37,29,0,10
machine-translation,4,"We propose the weight - sharing constraint to unsupervised NMT , enabling the model to utilize an independent encoder for each language .",introduction,0,38,30,0,23
machine-translation,4,"To enforce the shared - latent space , we also propose the embedding - reinforced encoders and two different GANs for our model .",introduction,0,39,31,0,24
machine-translation,4,We conduct extensive experiments on The code that we utilized to train and evaluate our models can be found at https://github.com/ZhenYangIACAS/unsupervised-NMT,introduction,0,40,32,0,21
machine-translation,4,"English - German , English - French and Chinese - to - English translation tasks .",introduction,0,41,33,0,16
machine-translation,4,Experimental results show that the proposed approach consistently achieves great success .,introduction,0,42,34,0,12
machine-translation,4,"Last but not least , we introduce the directional self - attention to model temporal order information for the proposed model .",introduction,0,43,35,0,22
machine-translation,4,Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self - attention layers of NMT .,introduction,0,44,36,0,24
machine-translation,4,related work,related work,0,45,1,0,2
machine-translation,4,Several approaches have been proposed to train NMT models without direct parallel corpora .,related work,0,46,2,0,14
machine-translation,4,The scenario that has been widely investigated is one where two languages have little parallel data between them but are well connected by one pivot language .,related work,0,47,3,0,27
machine-translation,4,The most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language ( Saha et al .,related work,0,48,4,1,33
machine-translation,4,The two works mentioned above both use a single shared encoder to guarantee the shared latent space .,related work,0,49,5,0,18
machine-translation,4,"However , a concomitant defect is that the shared encoder is weak in keeping the uniqueness of each language .",related work,0,50,6,0,20
machine-translation,4,"Our work also belongs to this more ambitious scenario , and to the best of our knowledge , we are one among the first endeavors to investigate how to train an NMT model with monolingual corpora only .",related work,0,51,7,0,38
machine-translation,4,is the translation in reversed direction .,related work,0,52,8,0,7
machine-translation,4,D l is utilized to assess whether the hidden representation of the encoder is from the source or target language .,related work,0,53,9,0,21
machine-translation,4,D g 1 and D g 2 are used to evaluate whether the translated sentences are realistic for each language respectively .,related work,0,54,10,0,22
machine-translation,4,Z represents the shared - latent space .,related work,0,55,11,0,8
machine-translation,4,3,related work,0,56,12,0,1
machine-translation,4,the approach,related work,0,57,13,0,2
machine-translation,4,model architecture,related work,0,58,14,0,2
machine-translation,4,"The model architecture , as illustrated in figure 1 , is based on the AE and GAN .",related work,0,59,15,0,18
machine-translation,4,"It consists of seven sub networks : including two encoders Enc sand Enc t , two decoders Dec sand Dec t , the local discriminator D l , and the global discriminators D g 1 and D g 2 .",related work,0,60,16,0,40
machine-translation,4,"For the encoder and decoder , we follow the newly emerged Transformer .",related work,0,61,17,0,13
machine-translation,4,"Specifically , the encoder is composed of a stack of four identical layers",related work,0,62,18,0,13
machine-translation,4,2 .,related work,0,63,19,0,2
machine-translation,4,Each layer consists of a multi-head self - attention and a simple position - wise fully connected feed - forward network .,related work,0,64,20,0,22
machine-translation,4,The decoder is also composed of four identical layers .,related work,0,65,21,0,10
machine-translation,4,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sublayer , which performs multi-head attention over the output of the encoder stack .",related work,0,66,22,0,30
machine-translation,4,"For more details about the multi-head self - attention layer , we refer the reader to .",related work,0,67,23,0,17
machine-translation,4,We implement the local discriminator as a multi -layer perceptron and implement the global discriminator based on the convolutional neural network ( CNN ) .,related work,0,68,24,0,25
machine-translation,4,Several ways exist to interpret the roles of the sub networks are summarised in table,related work,0,69,25,0,15
machine-translation,4,1 .,related work,0,70,26,0,2
machine-translation,4,"The proposed system has several striking components , which are critical either for the system to be trained in an 2 The layer number is selected according to our preliminary experiment , which is presented in appendix A. unsupervised manner or for improving the translation performance .",related work,0,71,27,0,47
machine-translation,4,networks,related work,0,72,28,0,1
machine-translation,4,Roles : Interpretation of the roles for the subnetworks in the proposed system .,related work,0,73,29,0,14
machine-translation,4,directional self - attention,related work,0,74,30,0,4
machine-translation,4,"Compared to recurrent neural network , a dis advantage of the simple self - attention mechanism is that the temporal order information is lost .",related work,0,75,31,0,25
machine-translation,4,"Although the Transformer applies the positional encoding to the sequence before processed by the self - attention , how to model temporal order information within an attention is still an open question .",related work,0,76,32,0,33
machine-translation,4,"Following , we build the encoders in our model on the directional self - attention which utilizes the positional masks to encode temporal order information into attention output .",related work,0,77,33,0,29
machine-translation,4,"More concretely , two positional masks , namely the forward mask M f and backward mask Mb , are calculated as :",related work,0,78,34,0,22
machine-translation,4,"With the forward mask M f , the later token only makes attention connections to the early tokens in the sequence , and vice versa with the backward mask .",related work,0,79,35,0,30
machine-translation,4,"Similar to , we utilize a self - attention network to process the input sequence in forward direction .",related work,0,80,36,0,19
machine-translation,4,"The output of this layer is taken by an upper self - attention network as input , processed in the reverse direction .",related work,0,81,37,0,23
machine-translation,4,weight sharing,related work,0,82,38,0,2
machine-translation,4,"Based on the shared - latent space assumption , we apply the weight sharing constraint to relate the two AEs .",related work,0,83,39,0,21
machine-translation,4,"Specifically , we share the weights of the last few layers of the Enc sand Enc t , which are responsible for extracting high - level representations of the input sentences .",related work,0,84,40,0,32
machine-translation,4,"Similarly , we also share the first few layers of the Dec sand Dec t , which are expected to decode high - level representations thatare vital for reconstructing the input sentences .",related work,0,85,41,0,33
machine-translation,4,"Compared to which use the fully shared encoder , we only share partial weights for the encoders and decoders .",related work,0,86,42,0,20
machine-translation,4,"In the proposed model , the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language , such as the terminology , style , and sentence structure .",related work,0,87,43,0,40
machine-translation,4,The shared weights are utilized to map the hidden features extracted by the independent weights to the shared - latent space .,related work,0,88,44,0,22
machine-translation,4,embedding reinforced encoder,related work,0,89,45,0,3
machine-translation,4,We use pretrained cross - lingual embeddings in the encoders thatare kept fixed during training .,related work,0,90,46,0,16
machine-translation,4,And the fixed embeddings are used as a reinforced encoding component in our encoder .,related work,0,91,47,0,15
machine-translation,4,"Formally , given the input sequence embedding vectors E = {e 1 , . . . , e t } and the initial output sequence of the encoder stack H = {h 1 , . . . , ht } , we compute H r as :",related work,0,92,48,0,47
machine-translation,4,"where H r is the final output sequence of the encoder which will be attended by the decoder ( In Transformer , H is the final output of the encoder ) , g is agate unit and computed as :",related work,0,93,49,0,40
machine-translation,4,"where W 1 , W 2 and bare trainable parameters and they are shared by the two encoders .",related work,0,94,50,0,19
machine-translation,4,The motivation behind is twofold .,related work,0,95,51,0,6
machine-translation,4,"Firstly , taking the fixed cross - lingual embedding as the other encoding component is helpful to reinforce the sharedlatent space .",related work,0,96,52,0,22
machine-translation,4,"Additionally , from the point of multichannel encoders , providing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suiting its own linguistic structure .",related work,0,97,53,0,36
machine-translation,4,unsupervised training,related work,0,98,54,0,2
machine-translation,4,"Based on the architecture proposed above , we train the NMT model with the monolingual corpora only using the following four strategies :",related work,0,99,55,0,23
machine-translation,4,denoising auto - encoding,related work,0,100,56,0,4
machine-translation,4,"Firstly , we train the two AEs to reconstruct their inputs respectively .",related work,0,101,57,0,13
machine-translation,4,"In this form , each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language .",related work,0,102,58,0,32
machine-translation,4,"Nevertheless , without any constraint , the AE quickly learns to merely copy every word one by one , without capturing any internal structure of the language involved .",related work,0,103,59,0,29
machine-translation,4,"To address this problem , we utilize the same strategy of denoising AE and add some noise to the input sentences .",related work,0,104,60,0,22
machine-translation,4,"To this end , we shuffle the input sentences randomly .",related work,0,105,61,0,11
machine-translation,4,"Specifically , we apply a random permutation ?",related work,0,106,62,0,8
machine-translation,4,"to the input sentence , verifying the condition :",related work,0,107,63,0,9
machine-translation,4,"where n is the length of the input sentence , steps is the global steps the model has been updated , k and s are the tunable parameters which can beset by users beforehand .",related work,0,108,64,0,35
machine-translation,4,"This way , the system needs to learn some useful structure of the involved languages to be able to recover the correct word order .",related work,0,109,65,0,25
machine-translation,4,"In practice , we set k = 2 and s = 100000 .",related work,0,110,66,0,13
machine-translation,4,back - translation,related work,0,111,67,0,3
machine-translation,4,"In spite of denoising autoencoding , the training procedure still involves a single language at each time , without considering our final goal of mapping an input sentence from the source / target language to the target / source language .",related work,0,112,68,0,41
machine-translation,4,"For the cross language training , we utilize the back - translation approach for our unsupervised training procedure .",related work,0,113,69,0,19
machine-translation,4,Back - translation has shown its great effectiveness on improving NMT model with monolingual data and has been widely investigated by .,related work,0,114,70,0,22
machine-translation,4,"In our approach , given an input sentence in a given language , we apply the corresponding encoder and the decoder of the other language to translate it to the other language 3 .",related work,0,115,71,0,34
machine-translation,4,"By combining the translation with its original sentence , we get a pseudo - parallel corpus which is utilized to train the model to reconstruct the original sentence from its translation .",related work,0,116,72,0,32
machine-translation,4,local gan,related work,0,117,73,0,2
machine-translation,4,"Although the weight sharing constraint is vital for the shared - latent space assumption , it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code .",related work,0,118,74,0,36
machine-translation,4,"To further enforce the shared - latent space , we train a discriminative neural network , referred to as the local discriminator , to classify between the encoding of source sentences and the encoding of target sentences .",related work,0,119,75,0,38
machine-translation,4,"The local discriminator , implemented as a multilayer perceptron with two hidden layers of size 256 , takes the output of the encoder , i.e. , H r calculated as equation 3 , as input , and produces a binary prediction about the language of the input sentence .",related work,0,120,76,0,49
machine-translation,4,The local discriminator is trained to predict the language by minimizing the following crossentropy loss :,related work,0,121,77,0,16
machine-translation,4,where ?,related work,0,122,78,0,2
machine-translation,4,"D l represents the parameters of the local discriminator and f ? {s , t}.",related work,0,123,79,0,15
machine-translation,4,The encoders are trained to fool the local discriminator :,related work,0,124,80,0,10
machine-translation,4,where ?,related work,0,125,81,0,2
machine-translation,4,encs and ?,related work,0,126,82,0,3
machine-translation,4,Enct are the parameters of the two encoders .,related work,0,127,83,0,9
machine-translation,4,global gan,related work,0,128,84,0,2
machine-translation,4,"We apply the global GANs to fine tune the whole model so that the model is able to generate sentences undistinguishable from the true data , i.e. , sentences in the training corpus .",related work,0,129,85,0,34
machine-translation,4,"Different from the local GANs which updates the parameters of the encoders locally , the global GANs are utilized to update the whole parameters of the proposed model , including the parameters of encoders and decoders .",related work,0,130,86,0,37
machine-translation,4,The proposed model has two global GANs : GAN g 1 and GAN g 2 .,related work,0,131,87,0,16
machine-translation,4,"In GAN g 1 , the Enc t and Dec s act as the generator , which generates the sentencex t 4 from x t .",related work,0,132,88,0,26
machine-translation,4,"The D g 1 , implemented based on CNN , assesses whether the generated sentencex t is the true target - language sentence or the generated sentence .",related work,0,133,89,0,28
machine-translation,4,"The global discriminator aims to distinguish among the true sentences and generated sentences , and it is trained to minimize its classification error rate .",related work,0,134,90,0,25
machine-translation,4,"During training , the D g 1 feeds back its assessment to finetune the encoder Enc t and decoder Dec s .",related work,0,135,91,0,22
machine-translation,4,"Since the machine translation is a sequence generation problem , following , we leverage policy gradient reinforcement training to back - propagate the assessment .",related work,0,136,92,0,25
machine-translation,4,We apply a similar processing to GAN g2 ( The details about the architecture of the global discriminator and the training procedure of the global GANs can be seen in appendix B and C ) .,related work,0,137,93,0,36
machine-translation,4,There are two stages in the proposed unsupervised training .,related work,0,138,94,0,10
machine-translation,4,"In the first stage , we train the proposed model with denoising auto - encoding , backtranslation and the local GANs , until no improvement is achieved on the development set .",related work,0,139,95,0,32
machine-translation,4,"Specifically , we perform one batch of denoising autoencoding for the source and target languages , one batch of back - translation for the two languages , and another batch of local GAN for the two languages .",related work,0,140,96,0,38
machine-translation,4,"In the second stage , we fine tune the proposed model with the global GANs .",related work,0,141,97,0,16
machine-translation,4,experiments and results,experiment,0,142,1,0,3
machine-translation,4,"We evaluate the proposed approach on English - German , English - French and Chinese - to - English translation tasks",experiment,0,143,2,0,21
machine-translation,4,5 .,experiment,0,144,3,0,2
machine-translation,4,"We firstly describe the datasets , pre-processing and model hyper - parameters we used , then we introduce the baseline systems , and finally we present our experimental results .",experiment,0,145,4,0,30
machine-translation,4,data sets and preprocessing,experiment,0,146,5,0,4
machine-translation,4,"In English - German and English - French translation , we make our experiments comparable with previous work by using the datasets from the WMT 2014 and WMT 2016 shared tasks respectively .",experiment,0,147,6,0,33
machine-translation,4,"For Chinese - to - English translation , we use the datasets from LDC , which has been widely utilized by previous works .",experiment,0,148,7,0,24
machine-translation,4,"WMT14 English - French Similar to , we use the full training set of 36M sentence pairs and we lower - case them and remove sentences longer than 50 words , resulting in a parallel corpus of about 30M pairs of sentences .",experiment,0,149,8,0,43
machine-translation,4,"To guarantee no exact correspondence between the source and target monolingual sets , we build monolingual corpora by selecting English sentences from 15M random pairs , and selecting the French sentences from the complementary set .",experiment,0,150,9,0,36
machine-translation,4,"Sentences are encoded with byte - pair encoding , which has an English vocabulary of about 32000 tokens , and French vocabulary of about 33000 tokens .",experiment,0,151,10,0,27
machine-translation,4,We report results on newstest2014 .,experiment,0,152,11,0,6
machine-translation,4,wmt16 english - german,experiment,0,153,12,0,4
machine-translation,4,"We follow the same procedure mentioned above to create monolingual training corpora for English - German translation , and we get two monolingual training data of 1.8 M sentences each .",experiment,0,154,13,0,31
machine-translation,4,The two languages share a vocabulary of about 32000 tokens .,experiment,0,155,14,0,11
machine-translation,4,We report results on newstest2016 .,experiment,0,156,15,0,6
machine-translation,4,"LDC Chinese - English For Chinese - to - English translation , our training data consists of 1.6 M sentence pairs randomly extracted from LDC corpora",experiment,0,157,16,0,26
machine-translation,4,6 .,experiment,0,158,17,0,2
machine-translation,4,"Since the data set is not big enough , we just build the monolingual data set by randomly shuffling the Chinese and English sentences respectively .",experiment,0,159,18,0,26
machine-translation,4,"In spite of the fact that some correspondence between examples in these two monolingual sets may exist , we never utilize this alignment information in our training procedure ( see Section 3.2 ) .",experiment,0,160,19,0,34
machine-translation,4,Both the Chinese and English sentences are encoded with byte - pair encoding .,experiment,0,161,20,0,14
machine-translation,4,"We get an English vocabulary of about 34000 tokens , and Chinese vocabulary of about 38000 tokens .",experiment,0,162,21,0,18
machine-translation,4,The results are reported on N IST 02 .,experiment,0,163,22,0,9
machine-translation,4,"Since the proposed system relies on the pretrained cross - lingual embeddings , we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2 vec .",experiment,0,164,23,0,33
machine-translation,4,"We then apply the public implementation 7 of the method proposed by to map these 6 LDC2002L27 , LDC2002T01 , LDC2002E18 , LDC2003E07 , LDC2004T08 , LDC2004E12 , LDC2005T10 7 https://github.com/artetxem/vecmap",experiment,0,165,24,0,31
machine-translation,4,embeddings to a shared - latent space 8 .,experiment,0,166,25,0,9
machine-translation,4,Model Hyper - parameters and Evaluation,experiment,0,167,26,0,6
machine-translation,4,"Following the base model in , we set the dimension of word embedding as 512 , dropout rate as 0.1 and the head number as 8 .",experiment,1,168,27,0,27
machine-translation,4,We use beam search with a beam size of 4 and length penalty ? = 0.6 .,experiment,1,169,28,0,17
machine-translation,4,The model is implemented in TensorFlow and trained on up to four K80 GPUs synchronously in a multi - GPU setup on a single machine .,experiment,1,170,29,0,26
machine-translation,4,"For model selection , we stop training when the model achieves no improvement for the tenth evaluation on the development set , which is comprised of 3000 source and target sentences extracted randomly from the monolingual training corpora .",experiment,0,171,30,0,39
machine-translation,4,"Following ( Lample et al. , 2017 ) , we translate the source sentences to the target language , and then translate the resulting sentences back to the source language .",experiment,0,172,31,1,31
machine-translation,4,The quality of the model is then evaluated by computing the BLEU score over the original inputs and their reconstructions via this two - step translation process .,experiment,0,173,32,0,28
machine-translation,4,"The performance is finally averaged over two directions , i.e. , from source to target and from target to source .",experiment,0,174,33,0,21
machine-translation,4,BLEU is utilized as the evaluation metric .,experiment,0,175,34,0,8
machine-translation,4,"For Chinese - to - English , we apply the script mteval - v11 b. pl to evaluate the translation performance .",experiment,0,176,35,0,22
machine-translation,4,"For English - German and English - French , we evaluate the translation performance with the script multi-belu.pl 9 .",experiment,0,177,36,0,20
machine-translation,4,baseline systems,experiment,0,178,37,0,2
machine-translation,4,Word - by - word translation ( WBW ) The first baseline we consider is a system that performs word - by - word translations using the inferred bilingual dictionary .,experiment,1,179,38,0,31
machine-translation,4,"Specifically , it translates a sentence word - by - word , replacing each word with its nearest neighbor in the other language .",experiment,0,180,39,0,24
machine-translation,4,lample et al .,experiment,1,181,40,1,4
machine-translation,4,The second baseline is a previous work that uses the same training and testing sets with this paper .,experiment,0,182,41,0,19
machine-translation,4,"Their model belongs to the standard attention - based encoder - decoder framework , which implements the encoder using a bidirectional long short term memory network ( LSTM ) and implements the decoder using a sim - 8 The configuration we used to run these open - source toolkits can be found in appendix D 9 https://github.com/mosessmt/mosesdecoder/blob/617e8c8/scripts/generic/multibleu.perl;mteval-v11b.pl",experiment,0,183,42,0,57
machine-translation,4,en - de de - en en - fr fr- en zh - en are copied directly from their paper .,experiment,0,184,43,0,21
machine-translation,4,"We do not present the results of ( Artetxe et al. , 2017 b ) since we use different training sets .",experiment,0,185,44,1,22
machine-translation,4,ple forward lstm .,experiment,0,186,45,0,4
machine-translation,4,They apply one single encoder and decoder for the source and target languages .,experiment,0,187,46,0,14
machine-translation,4,supervised training,experiment,1,188,47,0,2
machine-translation,4,"We finally consider exactly the same model as ours , but trained using the standard cross - entropy loss on the original parallel sentences .",experiment,0,189,48,0,25
machine-translation,4,This model can be viewed as an upper bound for the proposed unsupervised model .,experiment,0,190,49,0,15
machine-translation,4,results and analysis,result,0,191,1,0,3
machine-translation,4,Number of weight - sharing layers,result,1,192,2,0,6
machine-translation,4,We firstly investigate how the number of weightsharing layers affects the translation performance .,result,0,193,3,0,14
machine-translation,4,"In this experiment , we vary the number of weightsharing layers in the AEs from 0 to 4 .",result,0,194,4,0,19
machine-translation,4,"Sharing one layer in AEs means sharing one layer for the encoders and in the meanwhile , sharing one layer for the decoders .",result,0,195,5,0,24
machine-translation,4,"The BLEU scores of English - to - German , English - to - French and Chinese - to - English translation tasks are reported in figure",result,0,196,6,0,27
machine-translation,4,2 . Each curve corresponds to a different translation task and the x - axis denotes the number of weight - sharing layers for the AEs .,result,0,197,7,0,27
machine-translation,4,We find that the number of weight - sharing layers shows much effect on the translation performance .,result,0,198,8,0,18
machine-translation,4,And the best translation performance is achieved when only one layer is shared in our system .,result,1,199,9,0,17
machine-translation,4,"When all of the four layers are shared , i.e. , only one shared encoder is utilized , we get poor translation performance in all of the three translation tasks .",result,1,200,10,0,31
machine-translation,4,This verifies our conjecture that the shared encoder is detrimental to the performance of unsupervised NMT especially for the translation tasks on distant language pairs .,result,0,201,11,0,26
machine-translation,4,"More concretely , for the related language pair translation , i.e. , English - to - French , the encoder - shared model achieves - 0.53 BLEU points decline than the best model where only one layer is shared .",result,0,202,12,0,40
machine-translation,4,"For the more distant language pair English - to - German , the encoder - shared model achieves more significant decline , i.e. , - 0.85 BLEU points decline .",result,0,203,13,0,30
machine-translation,4,"And for the most distant language pair Chinese - to - English , the decline is as large as - 1.66 BLEU points .",result,0,204,14,0,24
machine-translation,4,"We explain this as that the more distant the language pair is , the more different characteristics they have .",result,0,205,15,0,20
machine-translation,4,And the shared encoder is weak in keeping the unique characteristic of each language .,result,0,206,16,0,15
machine-translation,4,"Additionally , we also notice that using two completely independent encoders , i.e. , setting the number of weight - sharing layers as 0 , results in poor translation performance too .",result,0,207,17,0,32
machine-translation,4,This confirms our intuition that the shared layers are vital to map the source and target latent representations to a shared - latent space .,result,0,208,18,0,25
machine-translation,4,"In the rest of our experiments , we set the number of weightsharing layer as 1 . model only trained with monolingual data effectively learns to use the context information and the internal structure of each language .",result,0,209,19,0,38
machine-translation,4,"Compared to the work of ( Lample et al. , 2017 ) , our model also achieves up to + 1.92 BLEU points improvement on English - to - French translation task .",result,0,210,20,1,33
machine-translation,4,We believe that the unsupervised NMT is very promising .,result,0,211,21,0,10
machine-translation,4,"However , there is still a large room for improvement compared to the supervised upper bound .",result,0,212,22,0,17
machine-translation,4,The gap between the supervised and unsupervised model is as large as 12.3 - 25.5 BLEU points depending on the language pair and translation direction .,result,0,213,23,0,26
machine-translation,4,translation results,result,0,214,1,0,2
machine-translation,4,ablation study,result,0,215,2,0,2
machine-translation,4,"To understand the importance of different components of the proposed system , we perform an ablation study by training multiple versions of our model with some missing components : the local GANs , the global GANs , the directional self - attention , the weight - sharing , the embeddingreinforced encoders , etc .",result,0,216,3,0,54
machine-translation,4,Results are reported in table 3 .,result,0,217,4,0,7
machine-translation,4,"We do not test the the importance of the auto - encoding , back - translation and the pre-trained embeddings because they have been widely tested in .",result,0,218,5,0,28
machine-translation,4,shows that the best performance is obtained with the simultaneous use of all the tested elements .,result,0,219,6,0,17
machine-translation,4,"The most critical component is the weight - sharing constraint , which is vital to map sentences of different languages to the shared - latent space .",result,1,220,7,0,27
machine-translation,4,The embedding - reinforced encoder also brings some improvement on all of the translation tasks .,result,1,221,8,0,16
machine-translation,4,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .",result,1,222,9,0,18
machine-translation,4,This indicates that it deserves more efforts to investigate the temporal order information in self - attention mechanism .,result,0,223,10,0,19
machine-translation,4,The GANs also significantly improve the translation performance of our system .,result,1,224,11,0,12
machine-translation,4,"Specifically , the global GANs achieve improvement up to + 0.78 BLEU points on English - to - French translation and the local GANs also obtain improvement up to + 0.57 BLEU points on English - to - French translation .",result,0,225,12,0,41
machine-translation,4,This reveals that the proposed model benefits a lot from the crossdomain loss defined by GANs .,result,0,226,13,0,17
machine-translation,4,conclusion and future work,result,0,227,14,0,4
machine-translation,4,The models proposed recently for unsupervised NMT use a single encoder to map sentences from different languages to a shared - latent space .,result,0,228,15,0,24
machine-translation,4,We conjecture that the shared encoder is problematic for keeping the unique and inherent characteristic of each language .,result,0,229,16,0,19
machine-translation,4,"In this paper , we propose the weight - sharing constraint in unsupervised NMT to address this issue .",result,0,230,17,0,19
machine-translation,4,"To enhance the cross - language translation performance , we also propose the embedding - reinforced encoders , local GAN and global GAN into the proposed system .",result,0,231,18,0,28
machine-translation,4,"Additionally , the directional self - attention is introduced to model the temporal order information for our system .",result,0,232,19,0,19
machine-translation,4,"We test the proposed model on English - German , English - French and Chinese - to - English translation tasks .",result,0,233,20,0,22
machine-translation,4,The experimental results reveal that our approach achieves significant improvement and verify our conjecture that the shared encoder is really a bottleneck for improving the unsupervised NMT .,result,0,234,21,0,28
machine-translation,4,The ablation study shows that each component of our system achieves some improvement for the final translation performance .,result,0,235,22,0,19
machine-translation,4,Unsupervised NMT opens exciting opportunities for the future research .,result,0,236,23,0,10
machine-translation,4,"However , there is still a large room for improvement compared to the supervised NMT .",result,0,237,24,0,16
machine-translation,4,"In the future , we would like to investigate how to utilize the monolingual data more effectively , such as incorporating the language model and syntactic information into unsupervised NMT .",result,0,238,25,0,31
machine-translation,4,"Besides , we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model .",result,0,239,26,0,22
machine-translation,6,FRAGE : Frequency - Agnostic Word Representation,title,1,2,1,0,7
machine-translation,6,abstract,abstract,0,3,1,0,1
machine-translation,6,Continuous word representation ( aka word embedding ) is a basic building block in many neural network - based models used in natural language processing tasks .,abstract,0,4,2,0,27
machine-translation,6,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .",abstract,1,5,3,0,78
machine-translation,6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",abstract,0,6,4,0,23
machine-translation,6,"In this paper , we develop a neat , simple yet effective way to learn FRequency - AGnostic word Embedding ( FRAGE ) using adversarial training .",abstract,0,7,5,0,27
machine-translation,6,"We conducted comprehensive studies on ten datasets across four natural language processing tasks , including word similarity , language modeling , machine translation and text classification .",abstract,0,8,6,0,27
machine-translation,6,"Results show that with FRAGE , we achieve higher performance than the baselines in all tasks .",abstract,0,9,7,0,17
machine-translation,6,introduction,introduction,0,10,1,0,1
machine-translation,6,"Word embeddings , which are distributed and continuous vector representations for word tokens , have been one of the basic building blocks for many neural network - based models used in natural language processing ( NLP ) tasks , such as language modeling , text classification and machine translation .",introduction,0,11,2,0,50
machine-translation,6,"Different from classic one - hot representation , the learned word embeddings contain semantic information which can measure the semantic similarity between words , and can also be transferred into other learning tasks .",introduction,0,12,3,0,34
machine-translation,6,"In deep learning approaches for NLP tasks , word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters .",introduction,0,13,4,0,28
machine-translation,6,"As the inputs of the neural network , word embeddings carryall the information of words that will be further processed by the network , and the quality of embeddings is critical and highly impacts the final performance of the learning task .",introduction,0,14,5,0,42
machine-translation,6,"Unfortunately , we find the word embeddings learned by many deep learning approaches are far from perfect .",introduction,0,15,6,0,18
machine-translation,6,"As shown in ( a ) and 1 ( b ) , in the embedding space learned by word2 vec model , the nearest neighbors of word "" Peking "" includes "" quickest "" , "" multicellular "" , and "" epigenetic "" , which are not semantically similar , while semantically related words such as "" Beijing "" and "" China "" are far from it .",introduction,0,16,7,0,68
machine-translation,6,Similar phenomena are observed from the word embeddings learned from translation tasks .,introduction,0,17,8,0,13
machine-translation,6,"With a careful study , we find a more general problem which is rooted in low - frequency words in the text corpus .",introduction,0,18,9,0,24
machine-translation,6,"Without any confusion , we also call high - frequency words as popular words and call low - frequency words as rare words .",introduction,0,19,10,0,24
machine-translation,6,"As is well known , the frequency distribution of words roughly follows a simple mathematical form known as Zipf 's law .",introduction,0,20,11,0,22
machine-translation,6,"When the size of a text corpus grows , the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words .",introduction,0,21,12,0,34
machine-translation,6,"Interestingly , the learned embeddings of rare words and popular words behave differently .",introduction,1,22,13,0,14
machine-translation,6,"In the embedding space , a popular word usually has semantically related neighbors , while a rare word usually does not .",introduction,0,23,14,0,22
machine-translation,6,"Moreover , the nearest neighbors of more than 85 % rare words are rare words .",introduction,0,24,15,0,16
machine-translation,6,Word embeddings encode frequency information .,introduction,0,25,16,0,6
machine-translation,6,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .",introduction,1,26,17,0,29
machine-translation,6,Such a phenomenon is also observed in .,introduction,0,27,18,0,8
machine-translation,6,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,introduction,1,28,19,0,18
machine-translation,6,"First , such embeddings will affect the semantic understanding of words .",introduction,0,29,20,0,12
machine-translation,6,We observe more than half of the rare words are nouns or variants of popular words .,introduction,0,30,21,0,17
machine-translation,6,Those rare words should have similar meanings or share the same topics with popular words .,introduction,0,31,22,0,16
machine-translation,6,"Second , the neighbors of a large number of rare words are semantically unrelated rare words .",introduction,0,32,23,0,17
machine-translation,6,"To some extent , those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding .",introduction,0,33,24,0,25
machine-translation,6,It will consequently limit the performance of down - stream tasks using the embeddings .,introduction,0,34,25,0,15
machine-translation,6,"For example , in text classification , it can not be well guaranteed that the label of a sentence does not change when you replace one popular / rare word in the sentence by its rare / popular alternatives .",introduction,0,35,26,0,40
machine-translation,6,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .",introduction,1,36,27,0,26
machine-translation,6,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",introduction,1,37,28,0,50
machine-translation,6,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .",introduction,1,38,29,0,37
machine-translation,6,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .",introduction,1,39,30,0,25
machine-translation,6,"Consequently , rare words lie in the same region as and are mixed with popular words in the embedding space .",introduction,0,40,31,0,21
machine-translation,6,Then FRAGE will catch better semantic information and help the task - specific model to perform better .,introduction,0,41,32,0,18
machine-translation,6,"We conduct experiments on four types of NLP tasks , including three word similarity tasks , two language modeling tasks , three sentiment classification tasks and two machine translation tasks to test our method .",introduction,0,42,33,0,35
machine-translation,6,"In all tasks , FRAGE outperforms the baselines .",introduction,0,43,34,0,9
machine-translation,6,"Specifically , in language modeling and machine translation , we achieve better performance than the state - of - the - art results on PTB , WT2 and WMT14 English - German datasets .",introduction,0,44,35,0,34
machine-translation,6,background,introduction,0,45,36,0,1
machine-translation,6,word representation,introduction,0,46,37,0,2
machine-translation,6,"Words are the basic units of natural languages , and distributed word representations ( i.e. , word embeddings ) are the basic units of many models in NLP tasks including language modeling and machine translation .",introduction,0,47,38,0,36
machine-translation,6,It has been demonstrated that word representations learned from one task can be transferred to other tasks and achieve competitive performance .,introduction,0,48,39,0,22
machine-translation,6,"While word embeddings play an important role in neural network - based models in NLP and achieve great success , one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences .",introduction,0,49,40,0,42
machine-translation,6,develops a novel way to split word into sub- word units which is widely used in neural machine translation .,introduction,0,50,41,0,20
machine-translation,6,"However , the low - frequency sub- word units are still difficult to train : provides a comprehensive study which shows that the rare ( sub ) words are usually under-estimated in neural machine translation : during inference step , the model tends to choose popular words over their rare alternatives .",introduction,0,51,42,0,52
machine-translation,6,adversarial training,introduction,0,52,43,0,2
machine-translation,6,"The basic idea of our work to address the above problem is adversarial training , in which two or more models learn together by pursuing competing goals .",introduction,0,53,44,0,28
machine-translation,6,"A representative example of adversarial training is Generative Adversarial Networks ( GANs ) for image generation , in which a discriminator and a generator compete with each other : the generator aims to generate images similar to the natural ones , and the discriminator aims to detect the generated ones from the natural ones .",introduction,0,54,45,0,55
machine-translation,6,"Recently , adversarial training has been successfully applied to NLP tasks .",introduction,0,55,46,0,12
machine-translation,6,introduce an additional discriminator to differentiate the semantics learned from different languages in non-parallel bilingual data .,introduction,0,56,47,0,17
machine-translation,6,develops a discriminator to classify whether a sentence is created by human or generated by a model .,introduction,0,57,48,0,18
machine-translation,6,Our proposed method is under the adversarial training framework but not exactly the conventional generator - discriminator approach since there is no generator in our scenario .,introduction,0,58,49,0,27
machine-translation,6,"For an NLP task and its neural network model ( including word embeddings ) , we introduce a discriminator to differentiate embeddings of popular words and rare words ; while the NN model aims to fool the discriminator and minimize the task - specific loss simultaneously .",introduction,0,59,50,0,47
machine-translation,6,Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the negative effects of domain shift between training and testing .,introduction,0,60,51,0,25
machine-translation,6,"The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing ; instead , we aim to improve the effectiveness of word embeddings and consequently improve the performance of end - to - end NLP tasks .",introduction,0,61,52,0,48
machine-translation,6,empirical study,introduction,0,62,53,0,2
machine-translation,6,"In this section , we study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English - German translation task using Transformer .",introduction,0,63,54,0,38
machine-translation,6,The implementation details can be found in the supplementary material ( part A ) .,introduction,0,64,55,0,15
machine-translation,6,experimental design,experiment,0,65,1,0,2
machine-translation,6,"In both tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words ( roughly speaking , we set a word as a rare word if it s relative frequency is lower than 10 ? 6 in WMT14 dataset and 10 ? 7 in Google News dataset ) .",experiment,0,66,2,0,61
machine-translation,6,We have tried other thresholds such as 10 % or 25 % and found the observations are similar .,experiment,0,67,3,0,19
machine-translation,6,We study whether the semantic relationship between two words is reasonable .,experiment,0,68,4,0,12
machine-translation,6,"To achieve this , we randomly sampled some rare / popular words and checked the embeddings trained from different tasks .",experiment,0,69,5,0,21
machine-translation,6,"For each sampled word , we determined its nearest neighbors based on the cosine similarity between its embeddings and others '.",experiment,0,70,6,0,21
machine-translation,6,We also manually chose words which are semantically similar to it .,experiment,0,71,7,0,12
machine-translation,6,"For simplicity , for each word , we call the nearest words predicted from the embeddings as model - predicted neighbors , and call our chosen words as semantic neighbors .",experiment,0,72,8,0,31
machine-translation,6,observation,experiment,0,73,9,0,1
machine-translation,6,"To visualize word embeddings , we reduce their dimensionalities by SVD and plot two cases in .",experiment,0,74,10,0,17
machine-translation,6,More cases and other studies without dimensionality reduction can be found in the supplementary material ( part C ) .,experiment,0,75,11,0,20
machine-translation,6,We find that the embeddings trained from different tasks share some common patterns .,experiment,0,76,12,0,14
machine-translation,6,"For both tasks , more than 90 % of model - predicted neighbors of rare words are rare words .",experiment,0,77,13,0,20
machine-translation,6,"For each rare word , the model - predicted neighbor is usually not semantically related to this word , and semantic neighbors we chose are faraway from it in the embedding space .",experiment,0,78,14,0,33
machine-translation,6,"In contrast , the model - predicted neighbors of popular words are very reasonable .",experiment,0,79,15,0,15
machine-translation,6,"As the patterns in rare words are different from that of popular words , we further check the whole embedding matrix to make a general understanding .",experiment,0,80,16,0,27
machine-translation,6,We also visualize the word embeddings using SVD by keeping the two directions with top - 2 largest eigenvalues as in and plot them in,experiment,0,81,17,0,25
machine-translation,6,input tokens word embeddings,experiment,0,82,18,0,4
machine-translation,6,task - specific outputs,experiment,0,83,19,0,4
machine-translation,6,task - specific,experiment,0,84,20,0,3
machine-translation,6,model,experiment,0,85,21,0,1
machine-translation,6,loss,experiment,0,86,22,0,1
machine-translation,6,rare / popular labels discriminator,experiment,0,87,23,0,5
machine-translation,6,loss predict predict :,experiment,0,88,24,0,4
machine-translation,6,"The proposed learning framework includes a task - specific predictor and a discriminator , whose function is to classify rare and popular words .",experiment,0,89,25,0,24
machine-translation,6,Both modules use word embeddings as the input .,experiment,0,90,26,0,9
machine-translation,6,"a certain degree : the rare words and popular words lie in different regions after this linear projection , and thus they occupy different regions in the original embedding space .",experiment,0,91,27,0,31
machine-translation,6,This strange phenomenon is also observed in other learned embeddings ( e.g. CBOW and GLOVE ) and mentioned in .,experiment,0,92,28,0,20
machine-translation,6,explanation,experiment,0,93,29,0,1
machine-translation,6,"From the empirical study above , we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason .",experiment,0,94,30,0,30
machine-translation,6,We simply take word2vec as an example which is trained by stochastic gradient descent .,experiment,0,95,31,0,15
machine-translation,6,"During training , the sample rate of a popular word is high and the embedding of a popular word updates frequently .",experiment,0,96,32,0,22
machine-translation,6,"For a rare word , the sample rate is low and its embedding rarely updates .",experiment,0,97,33,0,16
machine-translation,6,"According to our study , on average , the moving distance of the embedding for a popular word is twice longer than that of a rare word during training .",experiment,0,98,34,0,30
machine-translation,6,"As all word embeddings are usually initialized around the origin with a small variance , we observe in the final model , the embeddings of rare words are still around the origin and the popular words have moved faraway .",experiment,0,99,35,0,40
machine-translation,6,discussion,experiment,0,100,36,0,1
machine-translation,6,We have strong evidence that the current phenomena are problematic .,experiment,0,101,37,0,11
machine-translation,6,"First , according to our study , in both tasks , more than half of the rare words are nouns , e.g. , company names , city names .",experiment,0,102,38,0,29
machine-translation,6,"They may share some similar topics to popular entities , e.g. , big companies and cities ; around 10 % percent of rare words include a hyphen ( which is usually used to join popular words ) , and over 30 % rare words are different PoSs of popular words .",experiment,0,103,39,0,51
machine-translation,6,These words should have mixed or similar semantics to some popular words .,experiment,0,104,40,0,13
machine-translation,6,"These facts show that rare words and popular words should lie in the same region of the embedding space , which is different from what we observed .",experiment,0,105,41,0,28
machine-translation,6,"Second , as we can see from the cases , for rare words , model - predicted neighbors are usually not semantically related words but frequency - related words ( rare words ) .",experiment,0,106,42,0,34
machine-translation,6,"This shows , for rare words , the embeddings encode more frequency information than semantic information .",experiment,0,107,43,0,17
machine-translation,6,"It is not good to use such word embeddings into semantic understanding tasks , e.g. , text classification , language modeling , language understanding and translation .",experiment,0,108,44,0,27
machine-translation,6,our method,method,0,109,1,0,2
machine-translation,6,"In this section , we present our method to improve word representations .",method,0,110,2,0,13
machine-translation,6,"As we have a strong prior that many rare words should share the same region in the embedding space as popular words , the basic idea of our algorithm is to train the word embeddings in an adversarial framework :",method,0,111,3,0,40
machine-translation,6,We introduce a discriminator to categorize word embeddings into two classes : popular ones or rare ones .,method,0,112,4,0,18
machine-translation,6,We hope the learned word embeddings not only minimize the task - specific training loss but also fool the discriminator .,method,0,113,5,0,21
machine-translation,6,"By doing so , the frequency information is removed from the embedding and we call our method frequency - agnostic word embedding ( FRAGE ) .",method,0,114,6,0,26
machine-translation,6,We first define some notations and then introduce our algorithm .,method,0,115,7,0,11
machine-translation,6,"We develop three types of notations : embeddings , task - specific parameters / loss , and discriminator parameters / loss .",method,0,116,8,0,22
machine-translation,6,denote ? emb ?,method,0,117,9,0,4
machine-translation,6,"R d|V | as the word embedding matrix to be learned , where d is the dimension of the embedding vectors and | V | is the vocabulary size .",method,0,118,10,0,30
machine-translation,6,Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words .,method,0,119,11,0,24
machine-translation,6,then the embedding matrix ?,method,0,120,12,0,5
machine-translation,6,emb can be divided into two parts : ?,method,0,121,13,0,9
machine-translation,6,emb pop for popular words and ?,method,0,122,14,0,7
machine-translation,6,emb rare for rare words .,method,0,123,15,0,6
machine-translation,6,let ?,method,0,124,16,0,2
machine-translation,6,emb w denote the embedding of word w .,method,0,125,17,0,9
machine-translation,6,let ?,method,0,126,18,0,2
machine-translation,6,model denote all the other task - specific parameters except word embeddings .,method,0,127,19,0,13
machine-translation,6,"For instance , for language modeling , ?",method,0,128,20,0,8
machine-translation,6,"model is the parameters of the RNN or LSTM ; for neural machine translation , ?",method,0,129,21,0,16
machine-translation,6,"model is the parameters of the encoder , attention module and decoder .",method,0,130,22,0,13
machine-translation,6,"Let L T ( S ; ? model , ? emb ) denote the task - specific loss over a dataset S. Taking language modeling as an example , the loss L T ( S ; ? model , ? emb ) is defined as the negative log likelihood of the data :",method,0,131,23,0,53
machine-translation,6,where y is a sentence .,method,0,132,24,0,6
machine-translation,6,let f ?,method,0,133,25,0,3
machine-translation,6,D denote a discriminator with parameters ?,method,0,134,26,0,7
machine-translation,6,"D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",method,0,135,27,0,28
machine-translation,6,"Let L D ( V ; ? D , ? emb ) denote the loss of the discriminator :",method,0,136,28,0,19
machine-translation,6,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ?",method,0,137,29,0,19
machine-translation,6,model and ? emb ) and the discriminator ( ?,method,0,138,30,0,10
machine-translation,6,d ) as below :,method,0,139,31,0,5
machine-translation,6,where ?,method,0,140,32,0,2
machine-translation,6,is a coefficient to trade off the two loss terms .,method,0,141,33,0,11
machine-translation,6,We can see that when the model parameter ?,method,0,142,34,0,9
machine-translation,6,model and the embedding ?,method,0,143,35,0,5
machine-translation,6,"emb are fixed , the optimization of the discriminator ?",method,0,144,36,0,10
machine-translation,6,d becomes,method,0,145,37,0,2
machine-translation,6,which is to minimize the classification error of popular and rare words .,method,0,146,38,0,13
machine-translation,6,when the discriminator ?,method,0,147,39,0,4
machine-translation,6,"Dis fixed , the optimization of ?",method,0,148,40,0,7
machine-translation,6,model and ?,method,0,149,41,0,3
machine-translation,6,emb becomes,method,0,150,42,0,2
machine-translation,6,"i.e. , to optimize the task performance as well as fooling the discriminator .",method,0,151,43,0,14
machine-translation,6,we train ?,method,0,152,44,0,3
machine-translation,6,"model , ?",method,0,153,45,0,3
machine-translation,6,emb and ?,method,0,154,46,0,3
machine-translation,6,D iteratively by stochastic gradient descent or its variants .,method,0,155,47,0,10
machine-translation,6,The general training process is shown in Algorithm 1 .,method,0,156,48,0,10
machine-translation,6,experiment,experiment,0,157,1,0,1
machine-translation,6,"We test our method on a wide range of tasks , including word similarity , language modeling , machine translation and text classification .",experiment,0,158,2,0,24
machine-translation,6,"For each task , we choose the state - of - the - art architecture together with the state - of - the - art training method as our baseline .",experiment,0,159,3,0,31
machine-translation,6,sample a minibatch ?,experiment,0,160,4,0,4
machine-translation,6,from s.,experiment,0,161,5,0,2
machine-translation,6,4 :,experiment,0,162,6,0,2
machine-translation,6,Sample a minibatchV = V pop ?,experiment,0,163,7,0,7
machine-translation,6,v rare from v .,experiment,0,164,8,0,5
machine-translation,6,5 :,experiment,0,165,9,0,2
machine-translation,6,"update ? model , ?",experiment,0,166,10,0,5
machine-translation,6,emb by gradient descent according to Eqn. ( 5 ) with data ?.,experiment,0,167,11,0,13
machine-translation,6,update ?,experiment,0,168,12,0,2
machine-translation,6,D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,experiment,0,169,13,0,14
machine-translation,6,"7 : until Converge 8 : Output : ? model , ? emb , ?",experiment,0,170,14,0,15
machine-translation,6,d .,experiment,0,171,15,0,2
machine-translation,6,"For fair comparisons , for each task , our method shares the same model architecture as the baseline .",experiment,0,172,16,0,19
machine-translation,6,The only difference is that we use the original task - specific loss function with an additional adversarial loss as in Eqn..,experiment,0,173,17,0,22
machine-translation,6,"Due to space limitations , we put dataset description , model description , hyperparameter configuration into supplementary material ( part A ) .",experiment,0,174,18,0,23
machine-translation,6,settings,experiment,0,175,19,0,1
machine-translation,6,We conduct experiments on the following tasks .,experiment,0,176,20,0,8
machine-translation,6,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .",experiment,1,177,21,0,47
machine-translation,6,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .",experiment,1,178,22,0,20
machine-translation,6,"We test the baseline and our method on three datasets : RG65 , WS and RW .",experiment,1,179,23,0,17
machine-translation,6,The RW dataset is a dataset for the evaluation of rare words .,experiment,0,180,24,0,13
machine-translation,6,"Following common practice , we use cosine distance while computing the similarity between two word embeddings .",experiment,0,181,25,0,17
machine-translation,6,Language Modeling is a basic task in natural language processing .,experiment,1,182,26,0,11
machine-translation,6,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,experiment,1,183,27,0,20
machine-translation,6,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",experiment,1,184,28,0,22
machine-translation,6,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .",experiment,1,185,29,0,34
machine-translation,6,Machine Translation is a popular task in both deep learning and natural language processing .,experiment,1,186,30,0,15
machine-translation,6,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .",experiment,1,187,31,0,25
machine-translation,6,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .",experiment,1,188,32,0,23
machine-translation,6,We use transformer_base and transformer_big configurations following tensor2 tensor .,experiment,0,189,33,0,10
machine-translation,6,Text Classification is a conventional machine learning task and is evaluated by accuracy .,experiment,1,190,34,0,14
machine-translation,6,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .",experiment,1,191,35,0,40
machine-translation,6,"In all tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words , which is the same as our empirical study .",experiment,0,192,36,0,35
machine-translation,6,"For all the tasks except word embedding , we use full - batch gradient descent to update the discriminator .",experiment,0,193,37,0,20
machine-translation,6,"For word embedding , mini- batch stochastic gradient descent is used to update the discriminator with a batch size 3000 , since the vocabulary size is large .",experiment,0,194,38,0,28
machine-translation,6,"For language modeling and machine translation tasks , we use logistic regression as the discriminator .",experiment,0,195,39,0,16
machine-translation,6,"For other tasks , we find using a shallow neural network with 5 https://github.com/tensorflow/models/blob/master/tutorials/embedding",experiment,0,196,40,0,14
machine-translation,6,6 http://mattmahoney.net/dc/textdata.html,experiment,0,197,41,0,2
machine-translation,6,7 https://github.com/salesforce/awd-lstm-lm,experiment,0,198,42,0,2
machine-translation,6,8 https://github.com/zihangdai/mos,experiment,0,199,43,0,2
machine-translation,6,9 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,experiment,0,200,44,0,2
machine-translation,6,"To improve the training for imbalanced labeled data , a common method is to adjust loss function by reweighting the training samples ; To regularize the parameter space , a common method is to use l 2 regularization .",experiment,0,201,45,0,39
machine-translation,6,We tested these methods in machine translation and found the performance is not good .,experiment,0,202,46,0,15
machine-translation,6,Detailed analysis is provided in the supplementary material ( part B ) .,experiment,0,203,47,0,13
machine-translation,6,https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size .,experiment,0,204,48,0,24
machine-translation,6,"In all tasks , we set the hyper - parameter ? to 0.1 .",experiment,0,205,49,0,14
machine-translation,6,We list other hyper - parameters related to different task - specific models in the supplementary material ( part A ) .,experiment,0,206,50,0,22
machine-translation,6,"In this subsection , we provide the experimental results of all tasks .",experiment,0,207,51,0,13
machine-translation,6,"For simplicity , we use "" with FRAGE "" as our proposed method in the tables .",experiment,0,208,52,0,17
machine-translation,6,results,result,0,209,1,0,1
machine-translation,6,rg65,result,0,210,2,0,1
machine-translation,6,word similarity,result,0,211,3,0,2
machine-translation,6,The results on three word similarity tasks are listed in .,result,0,212,4,0,11
machine-translation,6,""" Paras "" denotes the number of model parameters .",result,0,213,5,0,10
machine-translation,6,language modeling,result,1,214,6,0,2
machine-translation,6,The results of language modeling on PTB and WT2 datasets are presented in .,result,0,215,7,0,14
machine-translation,6,"We test our model and the baselines at several checkpoints used in the baseline papers : without finetune , with finetune , with post -process ( continuous cache pointer or dynamic evaluation ) .",result,0,216,8,0,34
machine-translation,6,"In all these settings , our method outperforms the two baselines .",result,1,217,9,0,12
machine-translation,6,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .",result,1,218,10,0,30
machine-translation,6,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .",result,1,219,11,0,16
machine-translation,6,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .",result,1,220,12,0,26
machine-translation,6,machine translation,result,1,221,13,0,2
machine-translation,6,The results of neural machine translation on WMT14 English - German and IWSLT14 German - English tasks are shown in .,result,0,222,14,0,21
machine-translation,6,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,result,1,223,15,0,39
machine-translation,6,"task , respectively .",result,0,224,16,0,4
machine-translation,6,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,result,1,225,17,0,19
machine-translation,6,These results show improving word embeddings can achieve better results in more complicated tasks and larger datasets .,result,0,226,18,0,18
machine-translation,6,text classification,result,1,227,19,0,2
machine-translation,6,The results are listed in .,result,0,228,20,0,6
machine-translation,6,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,result,1,229,21,0,14
machine-translation,6,"As a summary , our experiments on four different tasks with 10 datasets verify the effectiveness of our method .",result,0,230,22,0,20
machine-translation,6,"We provide some case studies and visualizations of our method in the supplementary material ( part C ) , which show that the semantic similarities are reasonable and the popular / rare words are better mixed together in the embedding space .",result,0,231,23,0,42
machine-translation,6,conclusion,result,0,232,24,0,1
machine-translation,6,"In this paper , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of high - frequency and low - frequency words lie in different subregions of the embedding space .",result,0,233,25,0,39
machine-translation,6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",result,0,234,26,0,23
machine-translation,6,"We propose a neat , simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks .",result,0,235,27,0,26
machine-translation,6,We will explore several directions in the future .,result,0,236,28,0,9
machine-translation,6,"First , we will investigate the theoretical aspects of word embedding learning and our adversarial training method .",result,0,237,29,0,18
machine-translation,6,"Second , we will study more applications which have the similar problem even beyond NLP .",result,0,238,30,0,16
machine-translation,6,A Experimental settings A.1 Dataset Description,result,0,239,31,0,6
machine-translation,6,"For word similarity , we use three test datasets .",result,0,240,32,0,10
machine-translation,6,"WordSim-353 ( WS ) dataset consists of 353 pairs of commonly used verbs and nouns ; The rare - words ( RW ) dataset contains rarely used words ; The RG65 dataset contains 65 word pairs , and the similarity values in the dataset are the means of judgments made by 51 subjects .",result,0,241,33,0,54
machine-translation,6,"For language modeling tasks , we use Penn Treebank dataset and WikiText - 2 dataset .",result,0,242,34,0,16
machine-translation,6,The details of the datasets are provided in . :,result,0,243,35,0,10
machine-translation,6,"Statistics of the Penn Treebank , and WikiText - 2 dataset used in language modeling .",result,0,244,36,0,16
machine-translation,6,The out of vocabulary ( OOV ) words will be replaced by < unk > during training and testing .,result,0,245,37,0,20
machine-translation,6,"For machine translation , we use WMT14 English - German and IWSLT14 German - English datasets .",result,0,246,38,0,17
machine-translation,6,The training set of WMT14 English - German task consists of 4.5 M sentence pairs .,result,0,247,39,0,16
machine-translation,6,Source and target tokens are processed into 37 K shared sub - word units based on byte - pair encoding ( BPE ) .,result,0,248,40,0,24
machine-translation,6,We use the concatenation of newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set following all previous works .,result,0,249,41,0,24
machine-translation,6,IWSLT14 German - English dataset contains 160K training sentence pairs and 7K validation sentence pairs .,result,0,250,42,0,16
machine-translation,6,Tokens are processed using BPE and eventually we obtain a shared vocabulary of about 32 K tokens .,result,0,251,43,0,18
machine-translation,6,"We use the concatenation of dev2010 , tst2010 , tst2011 and tst 2011 as the test set , which is widely adopted in .",result,0,252,44,0,24
machine-translation,6,"For text classification tasks , we use three datasets : AG 's News , IMDB and 20 NG .",result,0,253,45,0,19
machine-translation,6,"AG 's news corpus is a news article corpus with categorized articles from more than 2,000 news .",result,0,254,46,0,18
machine-translation,6,IMDB movie review dataset is a sentiment classification dataset .,result,0,255,47,0,10
machine-translation,6,It consists of movie review comments with binary sentiment labels .,result,0,256,48,0,11
machine-translation,6,"20 Newsgroups is an email collection dataset , in which the emails are categorized into 20 different groups .",result,0,257,49,0,19
machine-translation,6,"We use the bydate version and select 4 major categories ( comp , politics , rec , and religion ) following . :",result,0,258,50,0,23
machine-translation,6,Detailed statistics about text classification datasets .,result,0,259,51,0,7
machine-translation,6,A.2 Hyper - parameter configurations,result,0,260,52,0,5
machine-translation,6,The hyper -parameters used for AWD - LSTM with / without MoS in language modeling experiment is shown in .,result,0,261,53,0,20
machine-translation,6,"For machine translation tasks , we choose Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 , and follow the learning rate schedule in .",result,0,262,54,0,33
machine-translation,6,"For evaluation , we use the case - sensitive tokenized BLEU score for WMT14 English - German and case - insensitive tokenized BLEU score for IWSLT14 German - English .",result,0,263,55,0,30
machine-translation,6,The hyper - parameters used in machine translation task are summarized in .,result,0,264,56,0,13
machine-translation,6,The hyper - parameters used in word embedding task are summarized in .,result,0,265,57,0,13
machine-translation,6,"For all text classification tasks , we use convolutional kernel with size 2 , 3 , 5 .",result,0,266,58,0,18
machine-translation,6,"We implement batch normalization and shortcut connection , and use Adam optimizer with ? 1 = 0.9 , ? 2 = 0.99 , ? = 10 ?8 .: Hyper- parameter used for word embedding training .",result,0,267,59,0,36
machine-translation,6,awd - lstm + mos,result,0,268,60,0,5
machine-translation,6,a.3 models description,result,0,269,61,0,3
machine-translation,6,We use task - specific baseline models .,result,0,270,62,0,8
machine-translation,6,"In language modeling , AWD - LSTM is a weight - dropped LSTM which uses Drop Connect on hidden - to - hidden weights as a means of recurrent regularization .",result,0,271,63,0,31
machine-translation,6,"The model is trained by NT - ASGD , which is a variant of the averaged stochastic gradient method .",result,0,272,64,0,20
machine-translation,6,"The training process has two steps , in the second step , the model is finetuned using another configuration of NT - ASGD .",result,0,273,65,0,24
machine-translation,6,AWD - LSTM - MoS uses the Mixture of Softmaxes structure to the vanilla AWD - LSTM and achieves the state - of - the - art result on PTB and WT2 .,result,0,274,66,0,33
machine-translation,6,"For machine translation , Transformer is a recently developed architecture in which the selfattention network is used during encoding and decoding step .",result,0,275,67,0,23
machine-translation,6,"It achieves the best performances on several machine translation tasks , e.g.",result,0,276,68,0,12
machine-translation,6,"WMT14 English - German , WMT14 English - French datasets .",result,0,277,69,0,11
machine-translation,6,Word2vec is one of the pioneer works on using deep learning to NLP tasks .,result,0,278,70,0,15
machine-translation,6,"Based on the co-occurrence of words , it produces distributed representations of words ( word embeddings ) .",result,0,279,71,0,18
machine-translation,6,"RCNN contains both recurrent and convolutional layers to catch the key components in texts , and is widely used in text classification tasks .",result,0,280,72,0,24
machine-translation,6,b additional comparisons,result,0,281,73,0,3
machine-translation,6,"We compare some other simple methods with ours on machine translation tasks , which include reweighting method and l 2 regularization ( weight decay ) .",result,0,282,74,0,26
machine-translation,6,results are listed in .,result,0,283,1,0,5
machine-translation,6,"We notice that those simple methods do notwork for the tasks , even have negative effects . : BLEU scores on test set of the WMT14 English - German task and IWSLT14 German - English task .",result,0,284,2,0,37
machine-translation,6,wmt,result,0,285,3,0,1
machine-translation,6,"Our method is denoted as "" FRAGE "" , "" Reweighting "" denotes reweighting the loss of each word by reciprocal of its frequency , and "" Weight Decay "" denotes putting weight decay rate ( 0.2 ) on embeddings .",result,0,286,4,0,41
machine-translation,6,C Case Study on Original Models and Qualitative Analysis of Our Method,result,0,287,5,0,12
machine-translation,6,We provide more word similarity cases in to justify our statement in Section 3 .,result,0,288,6,0,15
machine-translation,6,We also present the effectiveness of our method by showcase and embedding visualizations .,result,0,289,7,0,14
machine-translation,6,"From the cases and visualizations in and , we find the word similarities are improved and popular / rare words are better mixed together .",result,0,290,8,0,25
machine-translation,6,"( a ) ( b ) : These figures show that , in different tasks , the embeddings of rare and popular words are better mixed together after applying our method .",result,0,291,9,0,32
machine-translation,2,Attention Is All You Need,title,0,2,1,0,5
machine-translation,2,abstract,abstract,0,3,1,0,1
machine-translation,2,The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .,abstract,0,4,2,0,22
machine-translation,2,The best performing models also connect the encoder and decoder through an attention mechanism .,abstract,0,5,3,0,15
machine-translation,2,"We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .",abstract,1,6,4,0,24
machine-translation,2,Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train .,abstract,0,7,5,0,26
machine-translation,2,"Our model achieves 28.4 BLEU on the WMT 2014 Englishto - German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .",abstract,0,8,6,0,30
machine-translation,2,"On the WMT 2014 English - to - French translation task , our model establishes a new single - model state - of - the - art BLEU score of 41.8 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .",abstract,0,9,7,0,55
machine-translation,2,We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data .,abstract,0,10,8,0,26
machine-translation,2,* equal contribution .,abstract,0,11,9,0,4
machine-translation,2,listing order is random .,abstract,0,12,10,0,5
machine-translation,2,Jakob proposed replacing RNNs with self - attention and started the effort to evaluate this idea .,abstract,0,13,11,0,17
machine-translation,2,"Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .",abstract,0,14,12,0,24
machine-translation,2,"Noam proposed scaled dot-product attention , multi-head attention and the parameter - free position representation and became the other person involved in nearly every detail .",abstract,0,15,13,0,26
machine-translation,2,"Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .",abstract,0,16,14,0,18
machine-translation,2,"Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .",abstract,0,17,15,0,21
machine-translation,2,"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .",abstract,0,18,16,0,29
machine-translation,2,Work performed while at Google Brain .,abstract,0,19,17,0,7
machine-translation,2,introduction,introduction,0,20,1,0,1
machine-translation,2,"Recurrent neural networks , long short - term memory and gated recurrent neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation .",introduction,0,21,2,0,41
machine-translation,2,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,introduction,1,22,3,0,19
machine-translation,2,Recurrent models typically factor computation along the symbol positions of the input and output sequences .,introduction,0,23,4,0,16
machine-translation,2,"Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t.",introduction,0,24,5,0,34
machine-translation,2,"This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .",introduction,0,25,6,0,26
machine-translation,2,"Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter .",introduction,0,26,7,0,27
machine-translation,2,"The fundamental constraint of sequential computation , however , remains .",introduction,0,27,8,0,11
machine-translation,2,"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences .",introduction,0,28,9,0,34
machine-translation,2,"In all but a few cases , however , such attention mechanisms are used in conjunction with a recurrent network .",introduction,0,29,10,0,21
machine-translation,2,"In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .",introduction,1,30,11,0,30
machine-translation,2,The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .,introduction,0,31,12,0,33
machine-translation,2,background,introduction,0,32,13,0,1
machine-translation,2,"The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU , ByteNet and ConvS2S , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .",introduction,0,33,14,0,44
machine-translation,2,"In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .",introduction,0,34,15,0,34
machine-translation,2,This makes it more difficult to learn dependencies between distant positions .,introduction,0,35,16,0,12
machine-translation,2,"In the Transformer this is reduced to a constant number of operations , albeit at the cost of reduced effective resolution due to averaging attention - weighted positions , an effect we counteract with Multi - Head Attention as described in section 3.2 .",introduction,0,36,17,0,44
machine-translation,2,"Self - attention , sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .",introduction,0,37,18,0,28
machine-translation,2,"Self - attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task - independent sentence representations .",introduction,0,38,19,0,29
machine-translation,2,End - to - end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple - language question answering and language modeling tasks .,introduction,0,39,20,0,36
machine-translation,2,"To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self - attention to compute representations of its input and output without using sequencealigned RNNs or convolution .",introduction,0,40,21,0,37
machine-translation,2,"In the following sections , we will describe the Transformer , motivate self - attention and discuss its advantages over models such as and .",introduction,0,41,22,0,25
machine-translation,2,model architecture,introduction,0,42,23,0,2
machine-translation,2,Most competitive neural sequence transduction models have an encoder - decoder structure .,introduction,0,43,24,0,13
machine-translation,2,"Here , the encoder maps an input sequence of symbol representations ( x 1 , ... , x n ) to a sequence of continuous representations z = ( z 1 , ... , z n ) .",introduction,0,44,25,0,38
machine-translation,2,"Given z , the decoder then generates an output sequence ( y 1 , ... , y m ) of symbols one element at a time .",introduction,0,45,26,0,27
machine-translation,2,"At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next .",introduction,0,46,27,0,21
machine-translation,2,"The Transformer follows this over all architecture using stacked self - attention and point - wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of",introduction,0,47,28,0,35
machine-translation,2,encoder and decoder stacks,introduction,1,48,29,0,4
machine-translation,2,encoder :,introduction,1,49,30,0,2
machine-translation,2,The encoder is composed of a stack of N = 6 identical layers .,introduction,1,50,31,0,14
machine-translation,2,Each layer has two sub-layers .,introduction,1,51,32,0,6
machine-translation,2,"The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .",introduction,1,52,33,0,25
machine-translation,2,"We employ a residual connection around each of the two sub-layers , followed by layer normalization .",introduction,1,53,34,0,17
machine-translation,2,"That is , the output of each sub - layer is LayerNorm ( x + Sublayer ( x ) ) , where Sublayer ( x ) is the function implemented by the sub - layer itself .",introduction,0,54,35,0,37
machine-translation,2,"To facilitate these residual connections , all sub- layers in the model , as well as the embedding layers , produce outputs of dimension d model = 512 .",introduction,0,55,36,0,29
machine-translation,2,decoder :,introduction,1,56,37,0,2
machine-translation,2,The decoder is also composed of a stack of N = 6 identical layers .,introduction,1,57,38,0,15
machine-translation,2,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .",introduction,1,58,39,0,32
machine-translation,2,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .",introduction,1,59,40,0,20
machine-translation,2,We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,introduction,1,60,41,0,23
machine-translation,2,"This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i.",introduction,0,61,42,0,35
machine-translation,2,attention,introduction,1,62,43,0,1
machine-translation,2,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",introduction,1,63,44,0,36
machine-translation,2,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .",introduction,1,64,45,0,33
machine-translation,2,scaled dot - product attention,introduction,1,65,46,0,5
machine-translation,2,"We call our particular attention "" Scaled Dot -Product Attention "" ) .",introduction,0,66,47,0,13
machine-translation,2,"The input consists of queries and keys of dimension d k , and values of dimension d v .",introduction,0,67,48,0,19
machine-translation,2,"We compute the dot products of the query with all keys , divide each by ? d k , and apply a softmax function to obtain the weights on the values .",introduction,0,68,49,0,32
machine-translation,2,"In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .",introduction,0,69,50,0,22
machine-translation,2,The keys and values are also packed together into matrices K and V .,introduction,0,70,51,0,14
machine-translation,2,We compute the matrix of outputs as :,introduction,0,71,52,0,8
machine-translation,2,"The two most commonly used attention functions are additive attention , and dot-product ( multiplicative ) attention .",introduction,0,72,53,0,18
machine-translation,2,"Dot-product attention is identical to our algorithm , except for the scaling factor of 1",introduction,0,73,54,0,15
machine-translation,2,Additive attention computes the compatibility function using a feed - forward network with a single hidden layer .,introduction,0,74,55,0,18
machine-translation,2,"While the two are similar in theoretical complexity , dot-product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code .",introduction,0,75,56,0,34
machine-translation,2,"While for small values of d k the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of d k.",introduction,0,76,57,0,27
machine-translation,2,"We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients",introduction,0,77,58,0,30
machine-translation,2,4 .,introduction,0,78,59,0,2
machine-translation,2,"To counteract this effect , we scale the dot products by 1",introduction,0,79,60,0,12
machine-translation,2,multi - head attention,introduction,1,80,61,0,4
machine-translation,2,"Instead of performing a single attention function with d model - dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to d k , d k and d v dimensions , respectively .",introduction,0,81,62,0,52
machine-translation,2,"On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding d v - dimensional output values .",introduction,0,82,63,0,29
machine-translation,2,"These are concatenated and once again projected , resulting in the final values , as depicted in .",introduction,0,83,64,0,18
machine-translation,2,Multi - head attention allows the model to jointly attend to information from different representation subspaces at different positions .,introduction,0,84,65,0,20
machine-translation,2,"With a single attention head , averaging inhibits this .",introduction,0,85,66,0,10
machine-translation,2,Where the projections are parameter matrices,introduction,0,86,67,0,6
machine-translation,2,"In this work we employ h = 8 parallel attention layers , or heads .",introduction,0,87,68,0,15
machine-translation,2,For each of these we use,introduction,0,88,69,0,6
machine-translation,2,"Due to the reduced dimension of each head , the total computational cost is similar to that of single - head attention with full dimensionality .",introduction,0,89,70,0,26
machine-translation,2,Applications of Attention in our Model,introduction,0,90,71,0,6
machine-translation,2,The Transformer uses multi-head attention in three different ways :,introduction,0,91,72,0,10
machine-translation,2,"In "" encoder - decoder attention "" layers , the queries come from the previous decoder layer , and the memory keys and values come from the output of the encoder .",introduction,0,92,73,0,32
machine-translation,2,This allows every position in the decoder to attend over all positions in the input sequence .,introduction,0,93,74,0,17
machine-translation,2,This mimics the typical encoder - decoder attention mechanisms in sequence - to - sequence models such as . The encoder contains self - attention layers .,introduction,0,94,75,0,27
machine-translation,2,"In a self - attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .",introduction,0,95,76,0,34
machine-translation,2,Each position in the encoder can attend to all positions in the previous layer of the encoder .,introduction,0,96,77,0,18
machine-translation,2,"Similarly , self - attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .",introduction,0,97,78,0,30
machine-translation,2,We need to prevent leftward information flow in the decoder to preserve the auto - regressive property .,introduction,0,98,79,0,18
machine-translation,2,We implement this inside of scaled dot-product attention by masking out ( setting to ?? ) all values in the input of the softmax which correspond to illegal connections .,introduction,0,99,80,0,30
machine-translation,2,see.,introduction,0,100,81,0,1
machine-translation,2,Position - wise Feed - Forward Networks,introduction,1,101,82,0,7
machine-translation,2,"In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .",introduction,1,102,83,0,36
machine-translation,2,This consists of two linear transformations with a ReLU activation in between .,introduction,1,103,84,0,13
machine-translation,2,"While the linear transformations are the same across different positions , they use different parameters from layer to layer .",introduction,0,104,85,0,20
machine-translation,2,Another way of describing this is as two convolutions with kernel size,introduction,0,105,86,0,12
machine-translation,2,1 .,introduction,0,106,87,0,2
machine-translation,2,"The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .",introduction,0,107,88,0,22
machine-translation,2,embeddings and softmax,introduction,1,108,89,0,3
machine-translation,2,"Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .",introduction,1,109,90,0,26
machine-translation,2,We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .,introduction,1,110,91,0,23
machine-translation,2,"In our model , we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation , similar to .",introduction,0,111,92,0,24
machine-translation,2,"In the embedding layers , we multiply those weights by ? d model .",introduction,0,112,93,0,14
machine-translation,2,positional encoding,introduction,1,113,94,0,2
machine-translation,2,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .",introduction,1,114,95,0,56
machine-translation,2,"n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self - attention .",introduction,0,115,96,0,32
machine-translation,2,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,introduction,0,116,97,0,10
machine-translation,2,tokens in the sequence .,introduction,1,117,98,0,5
machine-translation,2,"To this end , we add "" positional encodings "" to the input embeddings at the bottoms of the encoder and decoder stacks .",introduction,1,118,99,0,24
machine-translation,2,"The positional encodings have the same dimension d model as the embeddings , so that the two can be summed .",introduction,0,119,100,0,21
machine-translation,2,"There are many choices of positional encodings , learned and fixed .",introduction,0,120,101,0,12
machine-translation,2,"In this work , we use sine and cosine functions of different frequencies : where pos is the position and i is the dimension .",introduction,0,121,102,0,25
machine-translation,2,"That is , each dimension of the positional encoding corresponds to a sinusoid .",introduction,0,122,103,0,14
machine-translation,2,The wavelengths form a geometric progression from 2 ? to 10000 2 ?.,introduction,0,123,104,0,13
machine-translation,2,"We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PE pos+k can be represented as a linear function of PE pos .",introduction,0,124,105,0,41
machine-translation,2,"We also experimented with using learned positional embeddings instead , and found that the two versions produced nearly identical results ( see row ( E ) ) .",introduction,0,125,106,0,28
machine-translation,2,We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .,introduction,0,126,107,0,24
machine-translation,2,why self - attention,introduction,0,127,108,0,4
machine-translation,2,"In this section we compare various aspects of self - attention layers to the recurrent and convolutional layers commonly used for mapping one variable - length sequence of symbol representations ( x 1 , ... , x n ) to another sequence of equal length ( z 1 , ... , z n ) , with x i , z i ?",introduction,0,128,109,0,62
machine-translation,2,"Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .",introduction,0,129,110,0,16
machine-translation,2,Motivating our use of self - attention we consider three desiderata .,introduction,0,130,111,0,12
machine-translation,2,One is the total computational complexity per layer .,introduction,0,131,112,0,9
machine-translation,2,"Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .",introduction,0,132,113,0,22
machine-translation,2,The third is the path length between long - range dependencies in the network .,introduction,0,133,114,0,15
machine-translation,2,Learning long - range dependencies is a key challenge in many sequence transduction tasks .,introduction,0,134,115,0,15
machine-translation,2,One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .,introduction,0,135,116,0,27
machine-translation,2,"The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long - range dependencies .",introduction,0,136,117,0,27
machine-translation,2,Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types .,introduction,0,137,118,0,24
machine-translation,2,"As noted in , a self - attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O ( n ) sequential operations .",introduction,0,138,119,0,33
machine-translation,2,"In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece and byte - pair representations .",introduction,0,139,120,0,62
machine-translation,2,"To improve computational performance for tasks involving very long sequences , self - attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position .",introduction,0,140,121,0,35
machine-translation,2,This would increase the maximum path length to O ( n / r ) .,introduction,0,141,122,0,15
machine-translation,2,We plan to investigate this approach further in future work .,introduction,0,142,123,0,11
machine-translation,2,A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions .,introduction,0,143,124,0,21
machine-translation,2,"Doing so requires a stack of O ( n / k ) convolutional layers in the case of contiguous kernels , or O ( log k ( n ) ) in the case of dilated convolutions , increasing the length of the longest paths between any two positions in the network .",introduction,0,144,125,0,52
machine-translation,2,"Convolutional layers are generally more expensive than recurrent layers , by a factor of k.",introduction,0,145,126,0,15
machine-translation,2,"Separable convolutions , however , decrease the complexity considerably , to O ( k n d + n d 2 ) .",introduction,0,146,127,0,22
machine-translation,2,"Even with k = n , however , the complexity of a separable convolution is equal to the combination of a self - attention layer and a point - wise feed - forward layer , the approach we take in our model .",introduction,0,147,128,0,43
machine-translation,2,"As side benefit , self - attention could yield more interpretable models .",introduction,0,148,129,0,13
machine-translation,2,We inspect attention distributions from our models and present and discuss examples in the appendix .,introduction,0,149,130,0,16
machine-translation,2,"Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .",introduction,0,150,131,0,29
machine-translation,2,training,introduction,0,151,132,0,1
machine-translation,2,This section describes the training regime for our models .,introduction,0,152,133,0,10
machine-translation,2,training data and batching,introduction,0,153,134,0,4
machine-translation,2,We trained on the standard WMT 2014 English - German dataset consisting of about 4.5 million sentence pairs .,introduction,0,154,135,0,19
machine-translation,2,"Sentences were encoded using byte - pair encoding , which has a shared sourcetarget vocabulary of about 37000 tokens .",introduction,0,155,136,0,20
machine-translation,2,"For English - French , we used the significantly larger WMT 2014 English - French dataset consisting of 36M sentences and split tokens into a 32000 word - piece vocabulary .",introduction,0,156,137,0,31
machine-translation,2,Sentence pairs were batched together by approximate sequence length .,introduction,0,157,138,0,10
machine-translation,2,Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .,introduction,0,158,139,0,19
machine-translation,2,hardware and schedule,introduction,1,159,140,0,3
machine-translation,2,We trained our models on one machine with 8 NVIDIA P100 GPUs .,introduction,1,160,141,0,13
machine-translation,2,"For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .",introduction,0,161,142,0,20
machine-translation,2,"We trained the base models for a total of 100,000 steps or 12 hours .",introduction,1,162,143,0,15
machine-translation,2,"For our big models , ( described on the bottom line of table 3 ) , step time was 1.0 seconds .",introduction,0,163,144,0,22
machine-translation,2,"The big models were trained for 300,000 steps ( 3.5 days ) .",introduction,1,164,145,0,13
machine-translation,2,optimizer,introduction,1,165,146,0,1
machine-translation,2,"We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .",introduction,1,166,147,0,20
machine-translation,2,"We varied the learning rate over the course of training , according to the formula :",introduction,0,167,148,0,16
machine-translation,2,"This corresponds to increasing the learning rate linearly for the first warmup_steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .",introduction,0,168,149,0,30
machine-translation,2,we used warmup_steps = 4000 .,introduction,1,169,150,0,6
machine-translation,2,regularization,introduction,1,170,151,0,1
machine-translation,2,We employ three types of regularization during training :,introduction,0,171,152,0,9
machine-translation,2,residual dropout,introduction,1,172,153,0,2
machine-translation,2,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .",introduction,1,173,154,0,25
machine-translation,2,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .",introduction,1,174,155,0,24
machine-translation,2,"For the base model , we use a rate of P drop = 0.1 .",introduction,0,175,156,0,15
machine-translation,2,label smoothing,introduction,1,176,157,0,2
machine-translation,2,"During training , we employed label smoothing of value ls = 0.1 .",introduction,1,177,158,0,13
machine-translation,2,"This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .",introduction,0,178,159,0,20
machine-translation,2,results,result,0,179,1,0,1
machine-translation,2,machine translation,result,1,180,2,0,2
machine-translation,2,"On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing a new state - of - the - art BLEU score of 28.4 .",result,1,181,3,0,54
machine-translation,2,The configuration of this model is listed in the bottom line of .,result,0,182,4,0,13
machine-translation,2,Training took 3.5 days on 8 P100 GPUs .,result,0,183,5,0,9
machine-translation,2,"Even our base model surpasses all previously published models and ensembles , at a fraction of the training cost of any of the competitive models .",result,0,184,6,0,26
machine-translation,2,"On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .",result,1,185,7,0,52
machine-translation,2,"The Transformer ( big ) model trained for English - to - French used dropout rate P drop = 0.1 , instead of 0.3 .",result,0,186,8,0,25
machine-translation,2,"For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 - minute intervals .",result,0,187,9,0,27
machine-translation,2,"For the big models , we averaged the last 20 checkpoints .",result,0,188,10,0,12
machine-translation,2,We used beam search with a beam size of 4 and length penalty ? = 0.6 .,result,0,189,11,0,17
machine-translation,2,These hyperparameters were chosen after experimentation on the development set .,result,0,190,12,0,11
machine-translation,2,"We set the maximum output length during inference to input length + 50 , but terminate early when possible .",result,0,191,13,0,20
machine-translation,2,summarizes our results and compares our translation quality and training costs to other model architectures from the literature .,result,0,192,14,0,19
machine-translation,2,"We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single - precision floating - point capacity of each GPU 5 .",result,0,193,15,0,43
machine-translation,2,model variations,result,0,194,16,0,2
machine-translation,2,"To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English - to - German translation on the development set , newstest2013 .",result,0,195,17,0,39
machine-translation,2,"We used beam search as described in the previous section , but no checkpoint averaging .",result,0,196,18,0,16
machine-translation,2,We present these results in .,result,0,197,19,0,6
machine-translation,2,"In rows ( A ) , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .",result,0,198,20,0,34
machine-translation,2,"While single - head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .",result,0,199,21,0,23
machine-translation,2,"In rows ( B ) , we observe that reducing the attention key size d k hurts model quality .",result,0,200,22,0,20
machine-translation,2,This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product maybe beneficial .,result,0,201,23,0,21
machine-translation,2,"We further observe in rows ( C ) and ( D ) that , as expected , bigger models are better , and dropout is very helpful in avoiding over-fitting .",result,0,202,24,0,31
machine-translation,2,"In row ( E ) we replace our sinusoidal positional encoding with learned positional embeddings , and observe nearly identical results to the base model .",result,0,203,25,0,26
machine-translation,2,english constituency parsing,result,1,204,26,0,3
machine-translation,2,To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing .,result,0,205,27,0,18
machine-translation,2,This task presents specific challenges : the output is subject to strong structural constraints and is significantly longer than the input .,result,0,206,28,0,22
machine-translation,2,"Furthermore , RNN sequence - to - sequence models have not been able to attain state - of - the - art results in small - data regimes .",result,0,207,29,0,29
machine-translation,2,"We trained a 4 - layer transformer with d model = 1024 on the Wall Street Journal ( WSJ ) portion of the Penn Treebank , about 40 K training sentences .",result,0,208,30,0,32
machine-translation,2,"We also trained it in a semi-supervised setting , using the larger high - confidence and BerkleyParser corpora from with approximately 17M sentences .",result,0,209,31,0,24
machine-translation,2,We used a vocabulary of 16 K tokens for the WSJ only setting and a vocabulary of 32 K tokens for the semi-supervised setting .,result,0,210,32,0,25
machine-translation,2,"We performed only a small number of experiments to select the dropout , both attention and residual ( section 5.4 ) , learning rates and beam size on the Section 22 development set , all other parameters remained unchanged from the English - to - German base translation model .",result,0,211,33,0,50
machine-translation,2,"During inference , we increased the maximum output length to input length + 300 .",result,0,212,34,0,15
machine-translation,2,We used a beam size of 21 and ? = 0.3 for both WSJ only and the semi-supervised setting .,result,0,213,35,0,20
machine-translation,2,"Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .",result,1,214,36,0,37
machine-translation,2,"In contrast to RNN sequence - to - sequence models , the Transformer outperforms the Berkeley - Parser even when training only on the WSJ training set of 40K sentences .",result,0,215,37,0,31
machine-translation,2,conclusion,result,0,216,38,0,1
machine-translation,2,"In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder - decoder architectures with multi-headed self - attention .",result,0,217,39,0,37
machine-translation,2,"For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .",result,0,218,40,0,20
machine-translation,2,"On both WMT 2014 English - to - German and WMT 2014 English - to - French translation tasks , we achieve a new state of the art .",result,0,219,41,0,29
machine-translation,2,In the former task our best model outperforms even all previously reported ensembles .,result,0,220,42,0,14
machine-translation,2,We are excited about the future of attention - based models and plan to apply them to other tasks .,result,0,221,43,0,20
machine-translation,2,"We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .",result,0,222,44,0,39
machine-translation,2,Making generation less sequential is another research goals of ours .,result,0,223,45,0,11
machine-translation,2,The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.,result,0,224,46,0,15
machine-translation,3,Deep Recurrent Models with Fast - Forward Connections for Neural Machine Translation,title,0,2,1,0,12
machine-translation,3,abstract,abstract,0,3,1,0,1
machine-translation,3,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,abstract,1,4,2,0,27
machine-translation,3,"However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system .",abstract,0,5,3,0,29
machine-translation,3,"In this work , we introduce a new type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers .",abstract,0,6,4,0,41
machine-translation,3,Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 .,abstract,0,7,5,0,21
machine-translation,3,"On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points .",abstract,0,8,6,0,33
machine-translation,3,This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points .,abstract,0,9,7,0,30
machine-translation,3,We can still achieve BLEU = 36.3 even without using an attention mechanism .,abstract,0,10,8,0,14
machine-translation,3,"After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 .",abstract,0,11,9,0,26
machine-translation,3,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,abstract,0,12,10,0,19
machine-translation,3,introduction,introduction,0,13,1,0,1
machine-translation,3,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,introduction,0,14,2,0,25
machine-translation,3,"Unlike conventional statistical machine translation ( SMT ) systems which consist of multiple separately tuned components , NMT models encode the source sequence into continuous representation space and generate the target sequence in an end - to - end fashon .",introduction,0,15,3,0,41
machine-translation,3,"Moreover , NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation .",introduction,0,16,4,0,25
machine-translation,3,"In general , there are two types of NMT topologies : the encoder - decoder network and the attention network .",introduction,0,17,5,0,21
machine-translation,3,The encoder - decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword .,introduction,0,18,6,0,26
machine-translation,3,The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words .,introduction,0,19,7,0,28
machine-translation,3,Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems .,introduction,0,20,8,0,19
machine-translation,3,"However , a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT ' 14 English - to - French task .",introduction,0,21,9,0,35
machine-translation,3,The best BLEU score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0 .,introduction,0,22,10,0,22
machine-translation,3,We focus on improving the single model perfor - mance by increasing the model depth .,introduction,0,23,11,0,16
machine-translation,3,Deep topology has been proven to outperform the shallow architecture in computer vision .,introduction,0,24,12,0,14
machine-translation,3,In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers .,introduction,0,25,13,0,25
machine-translation,3,"But in NMT , the biggest depth used successfully is only six .",introduction,0,26,14,0,13
machine-translation,3,We attribute this problem to the properties of the Long Short - Term Memory ( LSTM ) which is widely used in NMT .,introduction,0,27,15,0,24
machine-translation,3,"In the LSTM , there are more non-linear activations than in convolution layers .",introduction,0,28,16,0,14
machine-translation,3,"These activations significantly decrease the magnitude of the gradient in the deep topology , especially when the gradient propagates in recurrent form .",introduction,0,29,17,0,23
machine-translation,3,"There are also many efforts to increase the depth of the LSTM such as the work by , where the shortcuts do not avoid the nonlinear and recurrent computation .",introduction,0,30,18,0,30
machine-translation,3,"In this work , we introduce a new type of linear connections for multi - layer recurrent networks .",introduction,1,31,19,0,19
machine-translation,3,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .",introduction,1,32,20,0,25
machine-translation,3,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .",introduction,1,33,21,0,17
machine-translation,3,This topology can be used for both the encoder - decoder network and the attention network .,introduction,0,34,22,0,17
machine-translation,3,"On the WMT ' 14 Englishto - French task , this is the deepest NMT topology that has ever been investigated .",introduction,0,35,23,0,22
machine-translation,3,"With our deep attention model , the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 BLEU points .",introduction,0,36,24,0,27
machine-translation,3,This is also the first time on this task that a single NMT model achieves state - of - the - art performance and outperforms the best conventional SMT system with an improvement of 0.7 .,introduction,0,37,25,0,36
machine-translation,3,"Even without using the attention mechanism , we can still achieve 36.3 with a single model .",introduction,0,38,26,0,17
machine-translation,3,"After model ensembling and unknown word processing , the BLEU score can be further improved to 40.4 .",introduction,0,39,27,0,18
machine-translation,3,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",introduction,0,40,28,0,18
machine-translation,3,"As a reference , previous work showed that oracle rescoring of the 1000 - best sequences generated by the SMT model can achieve the BLEU score of about 45 .",introduction,0,41,29,0,30
machine-translation,3,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,introduction,0,42,30,0,19
machine-translation,3,neural machine translation,introduction,0,43,31,0,3
machine-translation,3,"Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models .",introduction,0,44,32,0,43
machine-translation,3,"In this task , the likelihood p ( y | x , ? ) of the target sequence will be maximized with parameter ?",introduction,0,45,33,0,24
machine-translation,3,to learn :,introduction,0,46,34,0,3
machine-translation,3,where y 0:j?1 is the sub sequence from y 0 toy j?1 . y 0 and y m + 1 denote the start mark and end mark of target sequence respectively .,introduction,0,47,35,0,32
machine-translation,3,"The process can be explicitly split into an encoding part , a decoding part and the interface between these two parts .",introduction,0,48,36,0,22
machine-translation,3,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .",introduction,0,49,37,0,30
machine-translation,3,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,introduction,0,50,38,0,21
machine-translation,3,"At the decoding step , the target sequence is generated from the representation c.",introduction,0,51,39,0,14
machine-translation,3,"Recently , there have been two types of NMT models which are different in the interface part .",introduction,0,52,40,0,18
machine-translation,3,"In the encoder - decoder model , a single vector extracted from e is used as the representation .",introduction,0,53,41,0,19
machine-translation,3,"In the attention model , c is dynamically obtained according to the relationship between the target sequence and the source sequence .",introduction,0,54,42,0,22
machine-translation,3,"The recurrent neural network ( RNN ) , or its specific form the LSTM , is generally used as the basic unit of the encoding and decoding part .",introduction,0,55,43,0,29
machine-translation,3,"However , the topology of most of the existing models is shallow .",introduction,0,56,44,0,13
machine-translation,3,"In the attention network , the encoding part and the decoding part have only one LSTM layer respectively .",introduction,0,57,45,0,19
machine-translation,3,"In the encoder - decoder network , researchers have used at most six LSTM layers .",introduction,0,58,46,0,16
machine-translation,3,"Because machine translation is a difficult problem , we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence .",introduction,0,59,47,0,31
machine-translation,3,"In this work , we focus on enhancing the complexity of the encoding / decoding architecture by increasing the model depth .",introduction,0,60,48,0,22
machine-translation,3,Deep neural models have been studied in a wide range of problems .,introduction,0,61,49,0,13
machine-translation,3,"In computer vision , models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years .",introduction,0,62,50,0,24
machine-translation,3,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path .,introduction,0,63,51,0,17
machine-translation,3,"Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",introduction,0,64,52,0,22
machine-translation,3,"Because of the existence of many more nonlinear activations and the recurrent computation , gradient values are not stable and are generally smaller .",introduction,0,65,53,0,24
machine-translation,3,"Following the same spirit for convolutional networks , a lot of effort has also been spent on training deep LSTM networks .",introduction,0,66,54,0,22
machine-translation,3,"introduced depth - gated shortcuts , connecting LSTM cells at adjacent layers , to provide a fast way to propagate the gradients .",introduction,0,67,55,0,23
machine-translation,3,They validated the modification of these shortcuts on an MT task and a language modeling task .,introduction,0,68,56,0,17
machine-translation,3,"However , the best score was obtained using models with three layers .",introduction,0,69,57,0,13
machine-translation,3,"Similarly , proposed a two dimensional structure for the LSTM .",introduction,0,70,58,0,11
machine-translation,3,Their structure decreases the number of nonlinear activations and path length .,introduction,0,71,59,0,12
machine-translation,3,"However , the gradient propagation still relies on the recurrent computation .",introduction,0,72,60,0,12
machine-translation,3,"The investigations were also made on question - answering to encode the questions , whereat most two LSTM layers were stacked .",introduction,0,73,61,0,22
machine-translation,3,"Based on the above considerations , we propose new connections to facilitate gradient propagation in the following section .",introduction,0,74,62,0,19
machine-translation,3,deep topology,introduction,0,75,63,0,2
machine-translation,3,We build the deep LSTM network with the new proposed linear connections .,introduction,0,76,64,0,13
machine-translation,3,The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation .,introduction,0,77,65,0,22
machine-translation,3,We call these connections fastforward connections .,introduction,0,78,66,0,7
machine-translation,3,"Within the deep topology , we also introduce an interleaved bi-directional architecture to stack the LSTM layers .",introduction,0,79,67,0,18
machine-translation,3,network,introduction,0,80,68,0,1
machine-translation,3,Our entire deep neural network is shown in .,introduction,0,81,69,0,9
machine-translation,3,"This topology can be divided into three parts : the encoder part ( P -E ) on the left , the decoder part ( P - D ) on the right and the interface between these two parts ( P - I ) which extracts the representation of the source sequence .",introduction,0,82,70,0,52
machine-translation,3,"We have two instantiations of this topology : Deep - ED and Deep - Att , which correspond to the extension of the encoder - decoder network and the attention network respectively .",introduction,0,83,71,0,33
machine-translation,3,Our main innovation is the novel scheme for connecting adjacent recurrent layers .,introduction,0,84,72,0,13
machine-translation,3,We will start with the basic RNN model for the sake of clarity .,introduction,0,85,73,0,14
machine-translation,3,recurrent layer :,introduction,0,86,74,0,3
machine-translation,3,"When an input sequence {x 1 , . . . , x m } is given to a recurrent layer , the output ht at each time step t can be computed as ( see )",introduction,0,87,75,0,36
machine-translation,3,where the bias parameter is not included for simplicity .,introduction,0,88,76,0,10
machine-translation,3,We use a red circle and a blue empty square to denote an input and a hidden state .,introduction,0,89,77,0,19
machine-translation,3,"A blue square with a "" - "" denotes the previous hidden state .",introduction,0,90,78,0,14
machine-translation,3,A dotted line means that the hidden state is used recurrently .,introduction,0,91,79,0,12
machine-translation,3,This computation can be equivalently split into two consecutive steps :,introduction,0,92,80,0,11
machine-translation,3,"Feed-Forward computation : ft = W f x t . Left part in ) . "" f "" block .",introduction,0,93,81,0,20
machine-translation,3,"Recurrent computation : RNN ( f t , h t?1 ) .",introduction,0,94,82,0,12
machine-translation,3,Right part and the sum operation ( + ) followed by activation in .,introduction,0,95,83,0,14
machine-translation,3,""" r "" block .",introduction,0,96,84,0,5
machine-translation,3,"For a deep topology with stacked recurrent layers , the input of each block "" f "" at recurrent layer k ( denoted by f k ) is usually the output of block "" r "" at its previous recurrent layer k ?",introduction,0,97,85,0,43
machine-translation,3,1 ( denoted by h k?1 ) .,introduction,0,98,86,0,8
machine-translation,3,"In our work , we add fast - forward connections ( F - F connections ) which connect two feed - forward computation blocks "" f "" of adjacent recurrent layers .",introduction,0,99,87,0,32
machine-translation,3,"It means that each block "" f "" at recurrent layer k takes both the outputs of block "" f "" and block "" r "" at its previous layer as input ( ) .",introduction,0,100,88,0,35
machine-translation,3,F - F connections are denoted by dashed red lines in and .,introduction,0,101,89,0,13
machine-translation,3,The path of F - F connections contains neither nonlinear activations nor recurrent computation .,introduction,0,102,90,0,15
machine-translation,3,"It provides a fast path for information to propagate , so we call this path fast - forward connections .",introduction,0,103,91,0,20
machine-translation,3,"Additionally , in order to learn more temporal dependencies , the sequences can be processed in different directions at each pair of adjacent recurrent layers .",introduction,0,104,92,0,26
machine-translation,3,This is quantitatively expressed in Eq. 3 :,introduction,0,105,93,0,8
machine-translation,3,The opposite directions are marked by the direction term ( ? 1 ) k .,introduction,0,106,94,0,15
machine-translation,3,"At the first recurrent layer , the block "" f "" takes x t as the input .",introduction,0,107,95,0,18
machine-translation,3,"[ , ] denotes the concatenation of vectors .",introduction,0,108,96,0,9
machine-translation,3,this is shown in .,introduction,0,109,97,0,5
machine-translation,3,The two changes are summarized here :,introduction,0,110,98,0,7
machine-translation,3,We add a connection between f kt and f k ?1 t .,introduction,0,111,99,0,13
machine-translation,3,"Without f k ?1 t , our model will be reduced to the traditional stacked model .",introduction,0,112,100,0,17
machine-translation,3,We alternate the RNN direction at different layers k with the direction term ( ? 1 ) k .,introduction,0,113,101,0,19
machine-translation,3,"If we fix the direction term to ? 1 , all layers work in the forward direction .",introduction,0,114,102,0,18
machine-translation,3,lstm layer :,introduction,0,115,103,0,3
machine-translation,3,"In our experiments , instead of an RNN , a specific type of recurrent layer called LSTM ) is used .",introduction,0,116,104,0,21
machine-translation,3,The LSTM is structurally more complex than the basic RNN in Eq .,introduction,0,117,105,0,13
machine-translation,3,"2 . We define the computation of the LSTM as a function which maps the input f and its state - output pair ( h , s ) at the previous time step to the current stateoutput pair .",introduction,0,118,106,0,39
machine-translation,3,"The exact computations for ( h t , st ) = LSTM ( f t , h t?1 , s t?1 ) are the following :",introduction,0,119,107,0,26
machine-translation,3,"is the concatenation of four vectors of equal size , means element - wise multiplication , ?",introduction,0,120,108,0,17
machine-translation,3,"i is the input activation function , ?",introduction,0,121,109,0,8
machine-translation,3,"o is the output activation function , ?",introduction,0,122,110,0,8
machine-translation,3,"g is the activation function for gates , and W r , ? ? , ? ? , and ? ?",introduction,0,123,111,0,21
machine-translation,3,are the parameters of the LSTM .,introduction,0,124,112,0,7
machine-translation,3,It is slightly different from the standard notation in that we do not have a matrix to multiply with the input fin our notation .,introduction,0,125,113,0,25
machine-translation,3,"With this notation , we can write down the computations for our deep bi-directional LSTM model with F - F connections :",introduction,0,126,114,0,22
machine-translation,3,where x t is the input to the deep bi-directional LSTM model .,introduction,0,127,115,0,13
machine-translation,3,"For the encoder , x t is the embedding of the t th word in the source sentence .",introduction,0,128,116,0,19
machine-translation,3,For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t.,introduction,0,129,117,0,27
machine-translation,3,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",introduction,0,130,118,0,19
machine-translation,3,"6 . Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",introduction,0,131,119,0,38
machine-translation,3,The use of Half ( ) is to reduce the parameter size and does not affect the performance .,introduction,0,132,120,0,19
machine-translation,3,"We observed noticeable performance degradation when using only the first third of the elements of "" f "" .",introduction,0,133,121,0,19
machine-translation,3,"With the F - F connections , we build a fast channel to propagate the gradients in the deep topology .",introduction,0,134,122,0,21
machine-translation,3,F - F connections can accelerate the model convergence and while improving the performance .,introduction,0,135,123,0,15
machine-translation,3,A similar idea was also used in .,introduction,0,136,124,0,8
machine-translation,3,Encoder : The LSTM layers are stacked following Eq. 5 .,introduction,0,137,125,0,11
machine-translation,3,We call this type of encoder interleaved bidirectional encoder .,introduction,0,138,126,0,10
machine-translation,3,"In addition , there are two similar columns ( a 1 and a 2 ) in the encoder part .",introduction,0,139,127,0,20
machine-translation,3,Each column consists of n e stacked LSTM layers .,introduction,0,140,128,0,10
machine-translation,3,There is no connection between the two columns .,introduction,0,141,129,0,9
machine-translation,3,The first layers of the two columns process the word representations of the source sequence in different directions .,introduction,0,142,130,0,19
machine-translation,3,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",introduction,0,143,131,0,17
machine-translation,3,The group size is the same as the length of the input sequence .,introduction,0,144,132,0,14
machine-translation,3,Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,introduction,0,145,133,0,24
machine-translation,3,"In our work , as a consequence of the introduced F - F connections , we have 4 output vectors ( h For Deep - Att , we do not need the above two operations .",introduction,0,146,134,0,36
machine-translation,3,"We only concatenate the 4 output vectors at each time step to obtain e t , and a soft attention mechanism is used to calculate the final representation ct from e t .",introduction,0,147,135,0,33
machine-translation,3,e t is summarized as :,introduction,0,148,136,0,6
machine-translation,3,Note that the vector dimensionality off is four times larger than that of h ( see Eq. 4 ) .,introduction,0,149,137,0,20
machine-translation,3,ct is summarized as :,introduction,0,150,138,0,5
machine-translation,3,?,introduction,0,151,139,0,1
machine-translation,3,"t,t is the normalized attention weight computed by :",introduction,0,152,140,0,9
machine-translation,3,"the concatenated vector e t to a vector with 1 / 4 dimension size , denoted by the ( fully connected ) block "" fc "" in .",introduction,0,153,141,0,28
machine-translation,3,decoder :,introduction,0,154,142,0,2
machine-translation,3,The decoder follows Eq. 5 and Eq. 6 with fixed direction term ? 1 .,introduction,0,155,143,0,15
machine-translation,3,"At the first layer , we use the following x t :",introduction,0,156,144,0,12
machine-translation,3,y t?1 is the target word embedding at the previous time step and y 0 is zero .,introduction,0,157,145,0,18
machine-translation,3,There is a single column of n d stacked LSTM layers .,introduction,0,158,146,0,12
machine-translation,3,We also use the F - F connections like those in the encoder and all layers are in the forward direction .,introduction,0,159,147,0,22
machine-translation,3,"Note that at the last LSTM layer , we only use ht to make the prediction with a softmax layer .",introduction,0,160,148,0,21
machine-translation,3,"Although the network is deep , the training technique is straightforward .",introduction,0,161,149,0,12
machine-translation,3,We will describe this in the next part .,introduction,0,162,150,0,9
machine-translation,3,training technique,introduction,0,163,151,0,2
machine-translation,3,We take the parallel data as the only input without using any monolingual data for either word representation pre-training or language modeling .,introduction,0,164,152,0,23
machine-translation,3,"Because of the deep bi-directional structure , we do not need to reverse the sequence order as .",introduction,0,165,153,0,18
machine-translation,3,"The deep topology brings difficulties for the model training , especially when first order methods such as stochastic gradient descent ( SGD ) are used .",introduction,0,166,154,0,26
machine-translation,3,The parameters should be properly initialized and the converging process can be slow .,introduction,0,167,155,0,14
machine-translation,3,"We tried several optimization techniques such as AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman and and .",introduction,0,168,156,1,20
machine-translation,3,We found that all of them were able to speedup the process a lot compared to simple SGD while no significant performance difference was observed among them .,introduction,0,169,157,0,28
machine-translation,3,"In this work , we chose Adam for model training and do not present a detailed comparison with other optimization methods .",introduction,0,170,158,0,22
machine-translation,3,Dropout is also used to avoid over-fitting .,introduction,0,171,159,0,8
machine-translation,3,It is utilized on the LSTM nodes h kt ( See Eq. 5 ) with a ratio of pd for both the encoder and decoder .,introduction,0,172,160,0,26
machine-translation,3,"During the whole model training process , we keep all hyper parameters fixed without any intermediate interruption .",introduction,0,173,161,0,18
machine-translation,3,The hyper parameters are selected according to the performance on the development set .,introduction,0,174,162,0,14
machine-translation,3,"For such a deep and large network , it is not easy to determine the tuning strategy and this will be considered in future work .",introduction,0,175,163,0,26
machine-translation,3,generation,introduction,0,176,164,0,1
machine-translation,3,We use the common left - to - right beam - search method for sequence generation .,introduction,0,177,165,0,17
machine-translation,3,"At each time step t , the wordy t can be predicted by :",introduction,0,178,166,0,14
machine-translation,3,where ?,introduction,0,179,167,0,2
machine-translation,3,t is the predicted target word .?,introduction,0,180,168,0,7
machine-translation,3,0:t ? 1 is the generated sequence from time step 0 tot ?,introduction,0,181,169,0,13
machine-translation,3,"1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",introduction,0,182,170,0,26
machine-translation,3,"The hypotheses are ranked by the total likelihood of the generated sequence , although normalized likelihood is used in some works .",introduction,0,183,171,0,22
machine-translation,3,experiments,experiment,0,184,1,0,1
machine-translation,3,We evaluate our method mainly on the widely used WMT ' 14 English - to - French translation task .,experiment,0,185,2,0,20
machine-translation,3,"In order to validate our model on more difficult language pairs , we also provide results on the WMT ' 14 English - to - German translation task .",experiment,0,186,3,0,29
machine-translation,3,Our models are implemented in the PADDLE ( PArallel Distributed Deep LEarning ) platform .,experiment,0,187,4,0,15
machine-translation,3,data sets,experiment,0,188,5,0,2
machine-translation,3,"For both tasks , we use the full WMT ' 14 parallel corpus as our training data .",experiment,0,189,6,0,18
machine-translation,3,The detailed data sets are listed below :,experiment,0,190,7,0,8
machine-translation,3,"English - to - French : Europarl v7 , Common Crawl , UN , News Commentary , Gigaword",experiment,0,191,8,0,18
machine-translation,3,"English - to - German : Europarl v7 , Common Crawl , News Commentary",experiment,0,192,9,0,14
machine-translation,3,"In total , the English - to - French corpus includes 36 million sentence pairs , and the English - to - German corpus includes 4.5 million sentence pairs .",experiment,0,193,10,0,30
machine-translation,3,"The news - test - 2012 and news - test - 2013 are concatenated as our development set , and the news - test - 2014 is the test set .",experiment,0,194,11,0,31
machine-translation,3,Our data partition is consistent with previous works on NMT to ensure fair comparison .,experiment,0,195,12,0,15
machine-translation,3,"For the source language , we select the most frequent 200K words as the input vocabulary .",experiment,0,196,13,0,17
machine-translation,3,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,experiment,0,197,14,0,25
machine-translation,3,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",experiment,0,198,15,0,22
machine-translation,3,Out - of - vocabulary words are replaced with the unknown symbol unk .,experiment,0,199,16,0,14
machine-translation,3,"For complete comparison to previous work on the Englishto - French task , we also show the results with a smaller vocabulary of 30K input words and 30 K output words on the sub train set with selected 12M parallel sequences .",experiment,0,200,17,0,42
machine-translation,3,model settings,experiment,0,201,18,0,2
machine-translation,3,"We have two models as described above , named Deep - ED and Deep - Att.",experiment,0,202,19,0,16
machine-translation,3,Both models have exactly the same configuration and layer size except the interface part P - I.,experiment,0,203,20,0,17
machine-translation,3,We use 256 dimensional word embeddings for both the source and target languages .,experiment,1,204,21,0,14
machine-translation,3,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .",experiment,1,205,22,0,25
machine-translation,3,The output layer size is the same as the size of the target vocabulary .,experiment,0,206,23,0,15
machine-translation,3,The dimension of ct is 5120 and 1280 for Deep - ED and Deep - Att respectively .,experiment,0,207,24,0,18
machine-translation,3,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",experiment,1,208,25,0,23
machine-translation,3,Our network is narrow on word embeddings and LSTM layers .,experiment,0,209,26,0,11
machine-translation,3,"Note that in previous work , 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used .",experiment,0,210,27,0,18
machine-translation,3,We also tried larger scale models but did not obtain further improvements .,experiment,0,211,28,0,13
machine-translation,3,optimization,experiment,0,212,29,0,1
machine-translation,3,"Note that each LSTM layer includes two parts as described in Eq. 3 , feed - forward computation and recurrent computation .",experiment,0,213,30,0,22
machine-translation,3,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",experiment,1,214,31,0,45
machine-translation,3,Word embeddings and the softmax layer also use this learning rate l f .,experiment,0,215,32,0,14
machine-translation,3,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,experiment,0,216,33,0,17
machine-translation,3,"Because of the large model size , we use strong L 2 regularization to constrain the parameter matrix v in the following way :",experiment,0,217,34,0,24
machine-translation,3,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",experiment,0,218,35,0,21
machine-translation,3,The two embedding layers are not regularized .,experiment,0,219,36,0,8
machine-translation,3,All the other layers have the same r = 2 .,experiment,0,220,37,0,11
machine-translation,3,The parameters of the recurrent computation part are initialized to zero .,experiment,0,221,38,0,12
machine-translation,3,All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07 .,experiment,0,222,39,0,15
machine-translation,3,A detailed guide for setting hyperparameters can be found in .,experiment,0,223,40,0,11
machine-translation,3,The dropout ratio pd is 0.1 .,experiment,1,224,41,0,7
machine-translation,3,"In each batch , there are 500 ? 800 sequences in our work .",experiment,0,225,42,0,14
machine-translation,3,The exact number depends on the sequence lengths and model size .,experiment,0,226,43,0,12
machine-translation,3,We also find that larger batch size results in better convergence although the improvement is not large .,experiment,0,227,44,0,18
machine-translation,3,"However , the largest batch size is constrained by the GPU memory .",experiment,0,228,45,0,13
machine-translation,3,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,experiment,1,229,46,0,32
machine-translation,3,It takes nearly 1.5 days for each pass .,experiment,0,230,47,0,9
machine-translation,3,One thing we want to emphasize here is that our deep model is not sensitive to these settings .,experiment,0,231,48,0,19
machine-translation,3,Small variation does not affect the final performance .,experiment,0,232,49,0,9
machine-translation,3,results,result,0,233,1,0,1
machine-translation,3,We evaluate the same way as previous NMT works .,result,0,234,2,0,10
machine-translation,3,All reported BLEU scores are computed with the multi-bleu. perl 1 script which is also used in the above works .,result,0,235,3,0,21
machine-translation,3,The results are for tokenized and case sensitive evaluation .,result,0,236,4,0,10
machine-translation,3,single models,result,1,237,5,0,2
machine-translation,3,English - to - French : First we list our single model results on the English - to - French task in Tab .,result,1,238,6,0,24
machine-translation,3,1 . In the first block we show the results with the full corpus .,result,0,239,7,0,15
machine-translation,3,The previous best single NMT encoderdecoder model ( Enc - Dec ) with six layers achieves BLEU = 31.5 .,result,0,240,8,0,20
machine-translation,3,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .",result,1,241,9,0,24
machine-translation,3,"This result is even better than the ensemble result of eight Enc - Dec models , which is 35.6 .",result,0,242,10,0,20
machine-translation,3,"This shows that , in addition to the convolutional layers for computer vision , deep topologies can also work for LSTM layers .",result,0,243,11,0,23
machine-translation,3,"For Deep - Att , the performance is further improved to 37.7 .",result,1,244,12,0,13
machine-translation,3,We also list the previous state - of - the - art performance from a conventional SMT system with the BLEU of 37.0 .,result,0,245,13,0,24
machine-translation,3,This is the first time that a single NMT model trained in an end - to - end form beats the best conventional system on this task .,result,0,246,14,0,28
machine-translation,3,We also show the results on the smaller data set with 12M sentence pairs and 30 K vocabulary in the second block .,result,0,247,15,0,23
machine-translation,3,"The two attention models , RNNsearch and RNNsearch - LV , achieve BLEU scores of 28.5 and 32.7 respectively .",result,0,248,16,0,20
machine-translation,3,Note that RNNsearch - LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch .,result,0,249,17,0,21
machine-translation,3,We obtain BLEU = 35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points .,result,0,250,18,0,17
machine-translation,3,The SMT result from is also listed and falls behind our model by 2.6 BLEU points .,result,0,251,19,0,17
machine-translation,3,methods,method,0,252,1,0,1
machine-translation,3,Data Voc BLEU RNNsearch 4.5M 50K 16.5 RNNsearch-LV 4.5M 500K 16.9 SMT 4.5 M Full 20.7 Deep - Att ( Ours ) 4.5M 160K 20.6 : English - to - German task : BLEU scores of single neural models .,method,0,253,2,0,40
machine-translation,3,We also list the conventional SMT system for comparison .,method,0,254,3,0,10
machine-translation,3,post processing,method,0,255,4,0,2
machine-translation,3,Two post processing techniques are used to improve the performance further on the English - to - French task .,method,0,256,5,0,20
machine-translation,3,"First , three Deep - Att models are built for ensemble results .",method,0,257,6,0,13
machine-translation,3,"They are initialized with different random parameters ; in addition , the training corpus for these models is shuffled with different random seeds .",method,0,258,7,0,24
machine-translation,3,We sum over the predicted probabilities of the target words and normalize the final distribution to generate the next word .,method,0,259,8,0,21
machine-translation,3,It is shown in Tab .,method,0,260,9,0,6
machine-translation,3,8 that the model ensemble can improve the performance further to 38.9 .,method,0,261,10,0,13
machine-translation,3,"In and there are eight models for the best scores , but we only use three models and we do not obtain further gain from more models . : BLEU scores of different models .",method,0,262,11,0,35
machine-translation,3,The first two blocks are our results of two single models and models with post processing .,method,0,263,12,0,17
machine-translation,3,In the last block we list two baselines of the best conventional SMT system and NMT system .,method,0,264,13,0,18
machine-translation,3,"Second , we recover the unknown words in the generated sequences with the Positional Unknown ( Pos Unk ) model introduced in .",method,0,265,14,0,23
machine-translation,3,The full parallel corpus is used to obtain the word mappings .,method,0,266,15,0,12
machine-translation,3,"We find this method provides an additional 1.5 BLEU points , which is consistent with the conclusion in .",method,0,267,16,0,19
machine-translation,3,We obtain the new BLEU score of 39.2 with a single Deep - Att model .,method,0,268,17,0,16
machine-translation,3,"For the ensemble models of Deep - Att , the BLEU score rises to 40.4 .",method,0,269,18,0,16
machine-translation,3,"In the last two lines , we list the conventional SMT model and the previous best neural models based system Enc - Dec for comparison .",method,0,270,19,0,26
machine-translation,3,We find our best score outperforms the previous best score by nearly 3 points .,method,0,271,20,0,15
machine-translation,3,analysis,method,0,272,21,0,1
machine-translation,3,length,method,0,273,22,0,1
machine-translation,3,"On the English - to - French task , we analyze the effect of the source sentence length on our models as shown in .",method,0,274,23,0,25
machine-translation,3,"Here we show five curves : our Deep - Att single model , our Deep - Att ensemble model , our Deep - ED model , a previously proposed Enc - Dec model with four layers and an SMT model .",method,0,275,24,0,41
machine-translation,3,We find our Deep - Att model works better than the previous two models ( Enc - Dec and SMT ) on nearly all sentence lengths .,method,0,276,25,0,27
machine-translation,3,"It is also shown that for very long sequences with length over 70 words , the performance of our Deep - Att does not degrade , when compared to another NMT model Enc - Dec.",method,0,277,26,0,35
machine-translation,3,"Our Deep - ED also has much better performance than the shallow Enc - Dec model on nearly all lengths , although for long sequences it degrades and starts to fall behind Deep - Att .",method,0,278,27,0,36
machine-translation,3,unknown words,method,0,279,28,0,2
machine-translation,3,Next we look into the detail of the effect of unknown words on the English - to - French task .,method,0,280,29,0,21
machine-translation,3,We select the subset without unknown words on target sentences from the original test set .,method,0,281,30,0,16
machine-translation,3,There are 1705 such sentences ( 56.8 % ) .,method,0,282,31,0,10
machine-translation,3,We compute the BLEU scores on this subset and the results are shown in Tab .,method,0,283,32,0,16
machine-translation,3,9 . We also list the results from SMT model the score 37.7 on the full test set .,method,0,284,33,0,19
machine-translation,3,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",method,0,285,34,0,23
machine-translation,3,This suggests that the difficulty on this subset is not much different from that on the full set .,method,0,286,35,0,19
machine-translation,3,We therefore attribute the larger gap for Deep - att to the existence of unknown words .,method,0,287,36,0,17
machine-translation,3,We also compute the BLEU score on the subset of the ensemble model and obtain 41.4 .,method,0,288,37,0,17
machine-translation,3,"As a reference related to human performance , in , it has been tested that the BLEU score of oracle re-scoring the LIUM 1000 - best results is 45 .",method,0,289,38,0,30
machine-translation,3,over-fitting,method,0,290,39,0,1
machine-translation,3,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .",method,0,291,40,0,19
machine-translation,3,"However , our experimental results suggest that deep models are less prone to the problem of over-fitting .",method,0,292,41,0,18
machine-translation,3,"In , we show three results from models with a different depth on the English - to - French task .",method,0,293,42,0,21
machine-translation,3,"These three models are evaluated by token error rate , which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input .",method,0,294,43,0,30
machine-translation,3,The curve with square marks corresponds to Deep - Att with n e = 9 and n d = 7 .,method,0,295,44,0,21
machine-translation,3,The curve with circle marks corresponds ton e = 5 and n d = 3 .,method,0,296,45,0,16
machine-translation,3,The curve with triangle marks corresponds ton e = 1 and n d = 1 .,method,0,297,46,0,16
machine-translation,3,We find that the deep model has better performance on the test set when the token error rate is the same as that of the shallow models on the training set .,method,0,298,47,0,32
machine-translation,3,"This shows that , with decreased token error rate , the deep model is more advantageous in avoiding the over - fitting phenomenon .",method,0,299,48,0,24
machine-translation,3,"We only plot the early training stage curves because , during the late training stage , the curves are not smooth .",method,0,300,49,0,22
machine-translation,3,conclusion,method,0,301,50,0,1
machine-translation,3,"With the introduction of fast - forward connections to the deep LSTM network , we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom .",method,0,302,51,0,38
machine-translation,3,"On this path , gradients decay much slower compared to the standard deep network .",method,0,303,52,0,15
machine-translation,3,This enables us to build the deep topology of NMT models .,method,0,304,53,0,12
machine-translation,3,We trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on the WMT ' 14 English - to - French translation task .,method,0,305,54,0,29
machine-translation,3,This is the deepest topology that has been investigated in the NMT are a on this task .,method,0,306,55,0,18
machine-translation,3,"We showed that our Deep - Att exhibits 6.2 BLEU points improvement over the previous best single model , achieving a 37.7 BLEU score .",method,0,307,56,0,25
machine-translation,3,This single end - toend NMT model outperforms the best conventional SMT system and achieves a state - of - the - art performance .,method,0,308,57,0,25
machine-translation,3,"After utilizing unknown word processing and model ensemble of three models , we obtained a BLEU score of 40.4 , an improvement of 2.9 BLEU points over the previous best result .",method,0,309,58,0,32
machine-translation,3,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",method,0,310,59,0,18
machine-translation,3,Our model is also validated on the more difficult English - to - German task .,method,0,311,60,0,16
machine-translation,3,Our model is also efficient in sequence generation .,method,0,312,61,0,9
machine-translation,3,"The best results from both a single model and model ensemble are obtained with a beam size of 3 , much smaller than previous NMT systems where beam size is about 12",method,0,313,62,0,32
machine-translation,0,Learning Phrase Representations using RNN Encoder - Decoder for Statistical Machine Translation,title,1,2,1,0,12
machine-translation,0,abstract,abstract,0,3,1,0,1
machine-translation,0,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) .",abstract,0,4,2,0,27
machine-translation,0,"One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols .",abstract,0,5,3,0,25
machine-translation,0,The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .,abstract,0,6,4,0,25
machine-translation,0,The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model .,abstract,0,7,5,0,40
machine-translation,0,"Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",abstract,0,8,6,0,19
machine-translation,0,introduction,introduction,0,9,1,0,1
machine-translation,0,"Deep neural networks have shown great success in various applications such as objection recognition ( see , e.g. , ) and speech recognition ( see , e.g. , ) .",introduction,0,10,2,0,30
machine-translation,0,"Furthermore , many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing ( NLP ) .",introduction,0,11,3,0,26
machine-translation,0,"These include , but are not limited to , language modeling , paraphrase detection and word embedding extraction .",introduction,0,12,4,0,19
machine-translation,0,"In the field of statistical machine translation ( SMT ) , deep neural networks have begun to show promising results .",introduction,0,13,5,0,21
machine-translation,0,summarizes a successful usage of feedforward neural networks in the framework of phrase - based SMT system .,introduction,0,14,6,0,18
machine-translation,0,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .",introduction,1,15,7,0,36
machine-translation,0,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .",introduction,1,16,8,0,37
machine-translation,0,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .",introduction,1,17,9,0,32
machine-translation,0,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,introduction,1,18,10,0,20
machine-translation,0,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .",introduction,1,19,11,0,25
machine-translation,0,The proposed RNN Encoder - Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French .,introduction,0,20,12,0,24
machine-translation,0,We train the model to learn the translation probability of an English phrase to a corresponding French phrase .,introduction,0,21,13,0,19
machine-translation,0,The model is then used as apart of a standard phrase - based SMT system by scoring each phrase pair in the phrase table .,introduction,0,22,14,0,25
machine-translation,0,The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder - Decoder improves the translation performance .,introduction,0,23,15,0,22
machine-translation,0,We qualitatively analyze the trained RNN Encoder - Decoder by comparing its phrase scores with those given by the existing translation model .,introduction,0,24,16,0,23
machine-translation,0,"The qualitative analysis shows that the RNN Encoder - Decoder is better at capturing the linguistic regularities in the phrase table , indirectly explaining the quantitative improvements in the over all translation performance .",introduction,0,25,17,0,34
machine-translation,0,The further analysis of the model reveals that the RNN Encoder - Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase .,introduction,0,26,18,0,33
machine-translation,0,"A recurrent neural network ( RNN ) is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = ( x 1 , . . . , x T ) .",introduction,0,27,19,0,42
machine-translation,0,"At each time step t , the hidden state ht of the RNN is updated by",introduction,0,28,20,0,16
machine-translation,0,where f is a non-linear activation function .,introduction,0,29,21,0,8
machine-translation,0,f maybe as simple as an elementwise logistic sigmoid function and as complex as along short - term memory ( LSTM ) unit ) .,introduction,0,30,22,0,25
machine-translation,0,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence .,introduction,0,31,23,0,22
machine-translation,0,"In that case , the output at each timestep t is the conditional distribution p ( x t | x t?1 , . . . , x 1 ) .",introduction,0,32,24,0,30
machine-translation,0,"For example , a multinomial distribution ( 1 - of - K coding ) can be output using a softmax activation function",introduction,0,33,25,0,22
machine-translation,0,"for all possible symbols j = 1 , . . . , K , where w j are the rows of a weight matrix W. By combining these probabilities , we can compute the probability of the sequence x using",introduction,0,34,26,0,40
machine-translation,0,"From this learned distribution , it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step .",introduction,0,35,27,0,23
machine-translation,0,rnn encoder - decoder,introduction,0,36,28,0,4
machine-translation,0,The RNN Encoder - Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder .,introduction,0,37,29,0,25
machine-translation,0,"The input matrix between each input symbol x t and the hidden unit is approximated with two lower - rank matrices , and the output matrix is approximated similarly .",introduction,0,38,30,0,30
machine-translation,0,"We used rank - 100 matrices , equivalent to learning an embedding of dimension 100 for each word .",introduction,0,39,31,0,19
machine-translation,0,The activation function used forh in Eq. ( 8 ) is a hyperbolic tangent function .,introduction,0,40,32,0,16
machine-translation,0,"The computation from the hidden state in the decoder to the output is implemented as a deep neural network ( Pascanu et al. , 2014 ) with a single intermediate layer having 500 maxout units each pooling 2 inputs .",introduction,0,41,33,1,40
machine-translation,0,"All the weight parameters in the RNN Encoder - Decoder were initialized by sampling from an isotropic zero-mean ( white ) Gaussian distribution with its standard deviation fixed to 0.01 , except for the recurrent weight parameters .",introduction,0,42,34,0,38
machine-translation,0,"For the recurrent weight matrices , we first sampled from a white Gaussian distribution and used its left singular vectors matrix , following .",introduction,0,43,35,0,24
machine-translation,0,"We used Adadelta and stochastic gradient descent to train the RNN Encoder - Decoder with hyperparameters = 10 ?6 and ? = 0.95 ( Zeiler , 2012 ) .",introduction,0,44,36,1,29
machine-translation,0,"At each update , we used 64 randomly selected phrase pairs from a phrase table ( which was created from 348 M words ) .",introduction,0,45,37,0,25
machine-translation,0,The model was trained for approximately three days .,introduction,0,46,38,0,9
machine-translation,0,Details of the architecture used in the experiments are explained in more depth in the supplementary material .,introduction,0,47,39,0,18
machine-translation,0,Hidden Unit that Adaptively Remembers and Forgets,introduction,0,48,40,0,7
machine-translation,0,"In addition to a novel model architecture , we also propose a new type of hidden unit ( f in Eq .",introduction,0,49,41,0,22
machine-translation,0,( 1 ) ) that has been motivated by the LSTM unit but is much simpler to compute and implement .,introduction,0,50,42,0,21
machine-translation,0,1 shows the graphical depiction of the proposed hidden unit .,introduction,0,51,43,0,11
machine-translation,0,Let us describe how the activation of the j - th hidden unit is computed .,introduction,0,52,44,0,16
machine-translation,0,"First , the reset gate r j is computed by",introduction,0,53,45,0,10
machine-translation,0,where ?,introduction,0,54,46,0,2
machine-translation,0,"is the logistic sigmoid function , and [. ] j denotes the j - th element of a vector .",introduction,0,55,47,0,20
machine-translation,0,"x and h t?1 are the input and the previous hidden state , respectively .",introduction,0,56,48,0,15
machine-translation,0,W rand Ur are weight matrices which are learned .,introduction,0,57,49,0,10
machine-translation,0,"Similarly , the update gate z j is computed by",introduction,0,58,50,0,10
machine-translation,0,The actual activation of the proposed unit h j is then computed by,introduction,0,59,51,0,13
machine-translation,0,wher ?,introduction,0,60,52,0,2
machine-translation,0,h,introduction,0,61,53,0,1
machine-translation,0,"In this formulation , when the reset gate is close to 0 , the hidden state is forced to ignore the previous hidden state and reset with the current input only .",introduction,0,62,54,0,32
machine-translation,0,"This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future , thus , allowing a more compact representation .",introduction,0,63,55,0,29
machine-translation,0,"On the other hand , the update gate controls how much information from the previous hidden state will carryover to the current hidden state .",introduction,0,64,56,0,25
machine-translation,0,This acts similarly to the memory cell in the LSTM network and helps the RNN to remember longterm information .,introduction,0,65,57,0,20
machine-translation,0,"Furthermore , this maybe considered an adaptive variant of a leaky - integration unit .",introduction,0,66,58,0,15
machine-translation,0,"As each hidden unit has separate reset and update gates , each hidden unit will learn to capture dependencies over different time scales .",introduction,0,67,59,0,24
machine-translation,0,"Those units that learn to capture short - term dependencies will tend to have reset gates thatare frequently active , but those that capture longer - term dependencies will have update gates thatare mostly active .",introduction,0,68,60,0,36
machine-translation,0,"In our preliminary experiments , we found that it is crucial to use this new unit with gating units .",introduction,0,69,61,0,20
machine-translation,0,We were notable to get meaningful result with an oft - used tanh unit without any gating .,introduction,0,70,62,0,18
machine-translation,0,statistical machine translation,introduction,0,71,63,0,3
machine-translation,0,"In a commonly used statistical machine translation system ( SMT ) , the goal of the system ( decoder , specifically ) is to find a translation f given a source sentence e , which maximizes",introduction,0,72,64,0,36
machine-translation,0,"where the first term at the right hand side is called translation model and the latter language model ( see , e.g. , ) .",introduction,0,73,65,0,25
machine-translation,0,"In practice , however , most SMT systems model log p ( f | e ) as a loglinear model with additional features and corre - sponding weights : where f n and w n are the n - th feature and weight , respectively .",introduction,0,74,66,0,46
machine-translation,0,Z ( e ) is a normalization constant that does not depend on the weights .,introduction,0,75,67,0,16
machine-translation,0,The weights are often optimized to maximize the BLEU score on a development set .,introduction,0,76,68,0,15
machine-translation,0,"In the phrase - based SMT framework introduced in and , the translation model log p ( e | f ) is factorized into the translation probabilities of matching phrases in the source and target sentences .",introduction,0,77,69,0,37
machine-translation,0,2,introduction,0,78,70,0,1
machine-translation,0,These probabilities are once again considered additional features in the log - linear model ( see Eq. ) and are weighted accordingly to maximize the BLEU score .,introduction,0,79,71,0,28
machine-translation,0,"Since the neural net language model was proposed in , neural networks have been used widely in SMT systems .",introduction,0,80,72,0,20
machine-translation,0,"In many cases , neural networks have been used to rescore translation hypotheses ( n- best lists ) ( see , e.g. , ) .",introduction,0,81,73,0,25
machine-translation,0,"Recently , however , there has been interest in training neural networks to score the translated sentence ( or phrase pairs ) using a representation of the source sentence as an additional input .",introduction,0,82,74,0,34
machine-translation,0,"see , e.g. , , and .",introduction,0,83,75,0,7
machine-translation,0,Scoring Phrase Pairs with RNN Encoder - Decoder,method,0,84,1,0,8
machine-translation,0,Here we propose to train the RNN Encoder - Decoder ( see Sec. 2.2 ) on a table of phrase pairs and use its scores as additional features in the loglinear model in Eq. ( 9 ) when tuning the SMT decoder .,method,0,85,2,0,43
machine-translation,0,"When we train the RNN Encoder - Decoder , we ignore the ( normalized ) frequencies of each phrase pair in the original corpora .",method,0,86,3,0,25
machine-translation,0,This measure was taken in order ( 1 ) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and ( 2 ) to ensure that the RNN Encoder - Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences .,method,0,87,4,0,57
machine-translation,0,One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase pairs in the original corpus .,method,0,88,5,0,29
machine-translation,0,"With a fixed capacity of the RNN Encoder - Decoder , we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities , i.e. , distinguishing between plausible and implausible translations , or learning the "" manifold "" ( region of probability concentration ) of plausible translations .",method,0,89,6,0,55
machine-translation,0,"Once the RNN Encoder - Decoder is trained , we add a new score for each phrase pair to the existing phrase table .",method,0,90,7,0,24
machine-translation,0,This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation .,method,0,91,8,0,19
machine-translation,0,"As Schwenk pointed out in , it is possible to completely replace the existing phrase table with the proposed RNN Encoder - Decoder .",method,0,92,9,0,24
machine-translation,0,"In that case , for a given source phrase , the RNN Encoder - Decoder will need to generate a list of ( good ) target phrases .",method,0,93,10,0,28
machine-translation,0,"This requires , however , an expensive sampling procedure to be performed repeatedly .",method,0,94,11,0,14
machine-translation,0,"In this paper , thus , we only consider rescoring the phrase pairs in the phrase table .",method,0,95,12,0,18
machine-translation,0,Related Approaches : Neural Networks in Machine Translation,method,0,96,13,0,8
machine-translation,0,"Before presenting the empirical results , we discuss a number of recent works that have proposed to use neural networks in the context of SMT .",method,0,97,14,0,26
machine-translation,0,Schwenk in proposed a similar approach of scoring phrase pairs .,method,0,98,15,0,11
machine-translation,0,"Instead of the RNN - based neural network , he used a feedforward neural network that has fixed - size inputs ( 7 words in his case , with zero - padding for shorter phrases ) and fixed - size outputs ( 7 words in the target language ) .",method,0,99,16,0,50
machine-translation,0,"When it is used specifically for scoring phrases for the SMT system , the maximum phrase length is often chosen to be small .",method,0,100,17,0,24
machine-translation,0,"However , as the length of phrases increases or as we apply neural networks to other variable - length sequence data , it is important that the neural network can handle variable - length input and output .",method,0,101,18,0,38
machine-translation,0,The proposed RNN Encoder - Decoder is well - suited for these applications .,method,0,102,1,0,14
machine-translation,0,"Similar to , Devlin et al. proposed to use a feedforward neural network to model a translation model , however , by predicting one word in a target phrase at a time .",method,0,103,2,1,33
machine-translation,0,"They reported an impressive improvement , but their approach still requires the maximum length of the input phrase ( or context words ) to be fixed a priori .",method,0,104,3,0,29
machine-translation,0,"Although it is not exactly a neural network they train , the authors of proposed to learn a bilingual embedding of words / phrases .",method,0,105,4,0,25
machine-translation,0,They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system .,method,0,106,5,0,30
machine-translation,0,"In , a feedforward neural network was trained to learn a mapping from a bag - of - words representation of an input phrase to an output phrase .",method,0,107,6,0,29
machine-translation,0,"This is closely related to both the proposed RNN Encoder - Decoder and the model proposed in , except that their input representation of a phrase is a bag - of - words .",method,0,108,7,0,34
machine-translation,0,A similar approach of using bag - of - words representations was proposed in as well .,method,0,109,8,0,17
machine-translation,0,"Earlier , a similar encoder - decoder model using two recursive neural networks was proposed in ) , but their model was restricted to a monolingual setting , i.e. the model reconstructs an input sentence .",method,0,110,9,0,36
machine-translation,0,"More recently , another encoder - decoder model using an RNN was proposed in , where the decoder is conditioned on a representation of either a source sentence or a source context .",method,0,111,10,0,33
machine-translation,0,One important difference between the proposed RNN Encoder - Decoder and the approaches in and is that the order of the words in source and target phrases is taken into account .,method,0,112,11,0,32
machine-translation,0,"The RNN Encoder - Decoder naturally distinguishes between sequences that have the same words but in a different order , whereas the aforementioned approaches effectively ignore order information .",method,0,113,12,0,29
machine-translation,0,The closest approach related to the proposed RNN Encoder - Decoder is the Recurrent Continuous Translation Model ( Model 2 ) proposed in .,method,0,114,13,0,24
machine-translation,0,"In their paper , they proposed a similar model that consists of an encoder and decoder .",method,0,115,14,0,17
machine-translation,0,The difference with our model is that they used a convolutional n-gram model ( CGM ) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder .,method,0,116,15,0,35
machine-translation,0,"They , however , evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations .",method,0,117,16,0,28
machine-translation,0,experiments,experiment,0,118,1,0,1
machine-translation,0,We evaluate our approach on the English / French translation task of the WMT ' 14 workshop .,experiment,0,119,2,0,18
machine-translation,0,data and baseline system,experiment,0,120,3,0,4
machine-translation,0,Large amounts of resources are available to build an English / French SMT system in the framework of the WMT ' 14 translation task .,experiment,0,121,4,0,25
machine-translation,0,"The bilingual corpora include Europarl ( 61M words ) , news commentary ( 5.5 M ) , UN ( 421 M ) , and two crawled corpora of 90 M and 780M words respectively .",experiment,0,122,5,0,35
machine-translation,0,The last two corpora are quite noisy .,experiment,0,123,6,0,8
machine-translation,0,"To train the French language model , about 712M words of crawled newspaper material is available in addition to the target side of the bitexts .",experiment,0,124,7,0,26
machine-translation,0,All the word counts refer to French words after tokenization .,experiment,0,125,8,0,11
machine-translation,0,"It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .",experiment,0,126,9,0,35
machine-translation,0,"Instead , one should focus on the most relevant subset of the data for a given task .",experiment,0,127,10,0,18
machine-translation,0,"We have done so by applying the data selection method proposed in , and its extension to bitexts .",experiment,0,128,11,0,19
machine-translation,0,By these means we selected a subset of 418 M words out of more than 2G words for language modeling and a subset of 348 M out of 850 M words for training the RNN Encoder - Decoder .,experiment,0,129,12,0,39
machine-translation,0,"We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT , and newstest2014 as our test set .",experiment,0,130,13,0,24
machine-translation,0,Each set has more than 70 thousand words and a single reference translation .,experiment,0,131,14,0,14
machine-translation,0,"For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",experiment,0,132,15,0,33
machine-translation,0,This covers approximately 93 % of the dataset .,experiment,0,133,16,0,9
machine-translation,0,All the out - of - vocabulary words were mapped to a special token ( [ UNK ] ) .,experiment,0,134,17,0,20
machine-translation,0,The baseline phrase - based SMT system was built using Moses with default settings .,experiment,1,135,18,0,15
machine-translation,0,"This system achieves a BLEU score of 30.64 and 33.3 on the development and test sets , respectively ( see Table 1 ) .",experiment,0,136,19,0,24
machine-translation,0,neural language model,experiment,0,137,20,0,3
machine-translation,0,"In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder - Decoder , we also tried a more traditional approach of using a neural network for learning a target language model ( CSLM ) .",experiment,0,138,21,0,40
machine-translation,0,"Especially , the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder - Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys - tem add up or are redundant .",experiment,0,139,22,0,48
machine-translation,0,We trained the CSLM model on 7 - grams from the target corpus .,experiment,0,140,23,0,14
machine-translation,0,"Each input word was projected into the embedding space R 512 , and they were concatenated to form a 3072 dimensional vector .",experiment,0,141,24,0,23
machine-translation,0,The concatenated vector was fed through two rectified layers ( of size 1536 and 1024 ) .,experiment,0,142,25,0,17
machine-translation,0,The output layer was a simple softmax layer ( see Eq. ) .,experiment,0,143,26,0,13
machine-translation,0,"All the weight parameters were initialized uniformly between ? 0.01 and 0.01 , and the model was trained until the validation perplexity did not improve for 10 epochs .",experiment,0,144,27,0,29
machine-translation,0,"After training , the language model achieved a perplexity of 45.80 .",experiment,0,145,28,0,12
machine-translation,0,The validation set was a random selection of 0.1 % of the corpus .,experiment,0,146,29,0,14
machine-translation,0,"The model was used to score partial translations during the decoding process , which generally leads to higher gains in BLEU score than n-best list rescoring .",experiment,0,147,30,0,27
machine-translation,0,To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder .,experiment,0,148,31,0,27
machine-translation,0,"Only when the buffer is full , or a stack is about to be pruned , the n-grams are scored by the CSLM .",experiment,0,149,32,0,24
machine-translation,0,This allows us to perform fast matrixmatrix multiplication on GPU using Theano .,experiment,0,150,33,0,13
machine-translation,0,quantitative analysis,experiment,0,151,34,0,2
machine-translation,0,We tried the following combinations : :,experiment,0,152,35,0,7
machine-translation,0,The top scoring target phrases for a small set of source phrases according to the translation model ( direct translation probability ) and by the RNN Encoder - Decoder .,experiment,0,153,36,0,30
machine-translation,0,Source phrases were randomly selected from phrases with 4 or more words .,experiment,0,154,37,0,13
machine-translation,0,?,experiment,0,155,38,0,1
machine-translation,0,denotes an incomplete ( partial ) character .,experiment,0,156,39,0,8
machine-translation,0,r is a Cyrillic letter ghe .,experiment,0,157,40,0,7
machine-translation,0,The results are presented in .,experiment,0,158,41,0,6
machine-translation,0,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .",experiment,1,159,42,0,18
machine-translation,0,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,experiment,1,160,43,0,21
machine-translation,0,This suggests that the contributions of the CSLM and the RNN Encoder - Decoder are not too correlated and that one can expect better results by improving each method independently .,experiment,0,161,44,0,31
machine-translation,0,"Furthermore , we tried penalizing the number of words thatare unknown to the neural networks ( i.e. words which are not in the shortlist ) .",experiment,0,162,45,0,26
machine-translation,0,We do so by simply adding the number of unknown words as an additional feature the loglinear model in Eq. ( 9 ) .,experiment,0,163,46,0,24
machine-translation,0,3,experiment,0,164,47,0,1
machine-translation,0,"However , in this case we 3 To understand the effect of the penalty , consider the set of all words in the 15,000 large shortlist , SL .",experiment,0,165,48,0,29
machine-translation,0,all words x i / ?,experiment,0,166,49,0,6
machine-translation,0,SL are replaced by a special token [ UNK ] before being scored by the neural networks .,experiment,0,167,50,0,18
machine-translation,0,"Hence , the conditional probability of any xi t / ?",experiment,0,168,51,0,11
machine-translation,0,SL is actually given by the model as,experiment,0,169,52,0,8
machine-translation,0,"where x <t is a shorthand notation for xt ? 1 , . . . , x 1 .",experiment,0,170,53,0,19
machine-translation,0,"were notable to achieve better performance on the test set , but only on the development set .",experiment,0,171,54,0,18
machine-translation,0,qualitative analysis,experiment,0,172,55,0,2
machine-translation,0,"In order to understand where the performance improvement comes from , we analyze the phrase pair scores computed by the RNN Encoder - Decoder against the corresponding p ( f | e ) from the translation model .",experiment,0,173,56,0,38
machine-translation,0,"Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus , we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases .",experiment,0,174,57,0,37
machine-translation,0,"Also , as we mentioned earlier in Sec. 3.1 , we further expect the RNN Encoder - Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus .",experiment,0,175,58,0,47
machine-translation,0,We focus on those pairs whose source phrase is long ( more than 3 words per source phrase ) and,experiment,0,176,59,0,20
machine-translation,0,"As a result , the probability of words not in the shortlist is always overestimated .",experiment,0,177,60,0,16
machine-translation,0,"It is possible to address this issue by backing off to an existing model that contain non-shortlisted words ( see ) In this paper , however , we opt for introducing a word penalty instead , which counteracts the word probability overestimation .",experiment,0,178,61,0,43
machine-translation,0,frequent .,experiment,0,179,62,0,2
machine-translation,0,"For each such source phrase , we look at the target phrases that have been scored high either by the translation probability p ( f | e ) or by the RNN Encoder - Decoder .",experiment,0,180,63,0,36
machine-translation,0,"Similarly , we perform the same procedure with those pairs whose source phrase is long but rare in the corpus .",experiment,0,181,64,0,21
machine-translation,0,lists the top - 3 target phrases per source phrase favored either by the translation model or by the RNN Encoder - Decoder .,experiment,0,182,65,0,24
machine-translation,0,The source phrases were randomly chosen among long ones having more than 4 or 5 words .,experiment,0,183,66,0,17
machine-translation,0,"In most cases , the choices of the target phrases by the RNN Encoder - Decoder are closer to actual or literal translations .",experiment,0,184,67,0,24
machine-translation,0,We can observe that the RNN Encoder - Decoder prefers shorter phrases in general .,experiment,0,185,68,0,15
machine-translation,0,"Interestingly , many phrase pairs were scored similarly by both the translation model and the RNN Encoder - Decoder , but there were as many other phrase pairs that were scored radically different ( see ) .",experiment,0,186,69,0,37
machine-translation,0,"This could arise from the proposed approach of training the RNN Encoder - Decoder on a set of unique phrase pairs , discouraging the RNN Encoder - Decoder from learning simply the frequencies of the phrase pairs from the corpus , as explained earlier . , we show for each of the source phrases in , the generated samples from the RNN Encoder - Decoder .",experiment,0,187,70,0,66
machine-translation,0,"For each source phrase , we generated 50 samples and show the top - five phrases accordingly to their scores .",experiment,0,188,71,0,21
machine-translation,0,We can see that the RNN Encoder - Decoder is able to propose well - formed target phrases without looking at the actual phrase table .,experiment,0,189,72,0,26
machine-translation,0,"Importantly , the generated phrases do not overlap completely with the target phrases from the phrase table .",experiment,0,190,73,0,18
machine-translation,0,This encourages us to further investigate the possibility of replacing the whole or apart of the phrase table with the proposed RNN Encoder - Decoder in the future .,experiment,0,191,74,0,29
machine-translation,0,"furthermore , in",experiment,0,192,75,0,3
machine-translation,0,word and phrase representations,experiment,0,193,76,0,4
machine-translation,0,"Since the proposed RNN Encoder - Decoder is not specifically designed only for the task of machine translation , here we briefly look at the properties of the trained model .",experiment,0,194,77,0,31
machine-translation,0,"It has been known for sometime that continuous space language models using neural networks are able to learn semantically meaningful embeddings ( See , e.g. , ) .",experiment,0,195,78,0,28
machine-translation,0,"Since the proposed RNN Encoder - Decoder also projects to and maps back from a sequence of words into a continuous space vector , we expect to see a similar property with the proposed model as well .",experiment,0,196,79,0,38
machine-translation,0,The left plot in shows the 2 - D embedding of the words using the word embedding matrix learned by the RNN Encoder - Decoder .,experiment,0,197,80,0,26
machine-translation,0,The projection was done by the recently proposed Barnes - Hut - SNE .,experiment,0,198,81,0,14
machine-translation,0,We can clearly see that semantically similar words are clustered with each other ( see the zoomed - in plots in .,experiment,0,199,82,0,22
machine-translation,0,The proposed RNN Encoder - Decoder naturally generates a continuous - space representation of a phrase .,experiment,0,200,83,0,17
machine-translation,0,The representation ( c in ) in this case is a 1000 - dimensional vector .,experiment,0,201,84,0,16
machine-translation,0,"Similarly to the word representations , we visualize the representations of the phrases that consists of four or more words using the Barnes - Hut - SNE in .",experiment,0,202,85,0,29
machine-translation,0,"From the visualization , it is clear that the RNN Encoder - Decoder captures both semantic and syntactic structures of the phrases .",experiment,0,203,86,0,23
machine-translation,0,"For instance , in the bottom - left plot , most of the phrases are about the duration of time , while those phrases thatare syntactically similar are clustered together .",experiment,0,204,87,0,31
machine-translation,0,The bottom - right plot shows the cluster of phrases thatare semantically similar ( countries or regions ) .,experiment,0,205,88,0,19
machine-translation,0,"On the other hand , the top - right plot shows the phrases thatare syntactically similar .",experiment,0,206,89,0,17
machine-translation,0,conclusion,experiment,0,207,90,0,1
machine-translation,0,"In this paper , we proposed a new neural network architecture , called an RNN Encoder - Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence , possibly from a different set , of an arbitrary length .",experiment,0,208,91,0,47
machine-translation,0,The proposed RNN Encoder - Decoder is able to either score a pair of sequences ( in terms of a conditional probability ) or generate a target sequence given a source sequence .,experiment,0,209,92,0,33
machine-translation,0,"Along with the new architecture , we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading / generating a sequence .",experiment,0,210,93,0,39
machine-translation,0,"We evaluated the proposed model with the task of statistical machine translation , where we used the RNN Encoder - Decoder to score each phrase pair in the phrase table .",experiment,0,211,94,0,31
machine-translation,0,"Qualitatively , we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder - Decoder is able to propose well - formed target phrases .",experiment,0,212,95,0,40
machine-translation,0,The scores by the RNN Encoder - Decoder were found to improve the over all translation performance in terms of BLEU scores .,experiment,0,213,96,0,23
machine-translation,0,"Also , we found that the contribution by the RNN Encoder - Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system , so that we can improve further the performance by using , for instance , the RNN Encoder - Decoder and the neural net language model together .",experiment,0,214,97,0,56
machine-translation,0,Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level .,experiment,0,215,98,0,29
machine-translation,0,This suggests that there maybe more natural language related applications that may benefit from the proposed RNN Encoder - Decoder .,experiment,0,216,99,0,21
machine-translation,0,The proposed architecture has large potential for further improvement and analysis .,experiment,0,217,100,0,12
machine-translation,0,"One approach that was not investigated here is to replace the whole , or apart of the phrase table by letting the RNN Encoder - Decoder propose target phrases .",experiment,0,218,101,0,30
machine-translation,0,"Also , noting that the proposed model is not limited to being used with written language , it will bean important future research to apply the proposed architecture to other applications such as speech transcription .",experiment,0,219,102,0,36
text-classification,8,Baseline Needs More Love : On Simple Word - Embedding - Based Models and Associated Pooling Mechanisms,title,1,2,1,0,17
text-classification,8,abstract,abstract,0,3,1,0,1
text-classification,8,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .",abstract,1,4,2,0,25
text-classification,8,"However , there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions .",abstract,0,5,3,0,18
text-classification,8,"In this paper , we conduct a point - by - point comparative study between Simple Word - Embeddingbased Models ( SWEMs ) , consisting of parameter - free pooling operations , relative to word - embedding - based RNN / CNN models .",abstract,0,6,4,0,44
text-classification,8,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",abstract,0,7,5,0,16
text-classification,8,"Based upon this understanding , we propose two additional pooling strategies over learned word embeddings : ( i ) a max - pooling operation for improved interpretability ; and ( ii ) a hierarchical pooling operation , which preserves spatial ( n - gram ) information within text sequences .",abstract,0,8,6,0,50
text-classification,8,"We present experiments on 17 datasets encompassing three tasks : ( i ) ( long ) document classification ; ( ii ) text sequence matching ; and ( iii ) short text tasks , including classification and tagging .",abstract,0,9,7,0,39
text-classification,8,The source code and datasets can be obtained from https://github.com/dinghanshen/SWEM .,abstract,1,10,8,0,11
text-classification,8,introduction,introduction,0,11,1,0,1
text-classification,8,"Word embeddings , learned from massive unstructured text data , are widely - adopted building blocks for Natural Language Processing ( NLP ) .",introduction,0,12,2,0,24
text-classification,8,"By representing each word as a fixed - length vector , these embeddings can group semantically similar words , while implicitly encoding rich linguis - tic regularities and patterns .",introduction,0,13,3,0,30
text-classification,8,"Leveraging the word - embedding construct , many deep architectures have been proposed to model the compositionality in variable - length text sequences .",introduction,1,14,4,0,24
text-classification,8,"These methods range from simple operations like addition , to more sophisticated compositional functions such as Recurrent Neural Networks ( RNNs ) , Convolutional Neural Networks ( CNNs ) and Recursive Neural Networks .",introduction,0,15,5,0,34
text-classification,8,"Models with more expressive compositional functions , e.g. , RNNs or CNNs , have demonstrated impressive results ; however , they are typically computationally expensive , due to the need to estimate hundreds of thousands , if not millions , of parameters .",introduction,0,16,6,0,43
text-classification,8,"In contrast , models with simple compositional functions often compute a sentence or document embedding by simply adding , or averaging , over the word embedding of each sequence element obtained via , e.g. , word2vec , or Glo Ve .",introduction,0,17,7,0,41
text-classification,8,"Generally , such a Simple Word - Embedding - based Model ( SWEM ) does not explicitly account for spatial , word - order information within a text sequence .",introduction,0,18,8,0,30
text-classification,8,"However , they possess the desirable property of having significantly fewer parameters , enjoying much faster training , relative to RNN - or CNN - based models .",introduction,0,19,9,0,28
text-classification,8,"Hence , there is a computation - vs. - expressiveness tradeoff regarding how to model the compositionality of a text sequence .",introduction,0,20,10,0,22
text-classification,8,"In this paper , we conduct an extensive experimental investigation to understand when , and why , simple pooling strategies , operated over word embeddings alone , already carry sufficient information for natural language understanding .",introduction,1,21,11,0,36
text-classification,8,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with existing recurrent and convolutional networks in a pointby - point manner .",introduction,1,22,12,0,37
text-classification,8,"Specifically , we consider 17 datasets , including three distinct NLP tasks : document classification ( Yahoo news , Yelp reviews , etc. ) , natural language sequence matching ( SNLI , WikiQA , etc. ) and ( short ) sentence classification / tagging ( Stanford sentiment treebank , .",introduction,0,23,13,0,50
text-classification,8,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",introduction,0,24,14,0,16
text-classification,8,"In order to validate our experimental findings , we conduct additional investigations to understand to what extent the word - order information is utilized / required to make predictions on different tasks .",introduction,0,25,15,0,33
text-classification,8,"We observe that in text representation tasks , many words ( e.g. , stop words , or words thatare not related to sentiment or topic ) do not meaningfully contribute to the final predictions ( e.g. , sentiment label ) .",introduction,0,26,16,0,41
text-classification,8,"Based upon this understanding , we propose to leverage a max - pooling operation directly over the word embedding matrix of a given sequence , to select its most salient features .",introduction,0,27,17,0,32
text-classification,8,"This strategy is demonstrated to extract complementary features relative to the standard averaging operation , while resulting in a more interpretable model .",introduction,0,28,18,0,23
text-classification,8,"Inspired by a case study on sentiment analysis tasks , we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations .",introduction,0,29,19,0,29
text-classification,8,"This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks thatare sensitive to word - order features , while maintaining the favorable properties of not having compositional parameters , thus fast training .",introduction,0,30,20,0,38
text-classification,8,"Our work presents a simple yet strong baseline for text representation learning that is widely ignored in benchmarks , and highlights the general computation - vs. - expressiveness tradeoff associated with appropriately selecting compositional functions for distinct NLP problems .",introduction,0,31,21,0,40
text-classification,8,"Furthermore , we quantitatively show that the word - embeddingbased text classification tasks can have the similar level of difficulty regardless of the employed models , using the subspace training to constrain the trainable parameters .",introduction,0,32,22,0,36
text-classification,8,"Thus , according to Occam 's razor , simple models are preferred .",introduction,0,33,23,0,13
text-classification,8,related work,related work,0,34,1,0,2
text-classification,8,"A fundamental goal in NLP is to develop expressive , yet computationally efficient compositional functions that can capture the linguistic structure of natural language sequences .",related work,0,35,2,0,26
text-classification,8,"Recently , several studies have suggested that on certain NLP applications , much simpler word - embedding - based architectures exhibit comparable or even superior performance , compared with more - sophisticated models using recurrence or convolutions .",related work,0,36,3,0,38
text-classification,8,"Although complex compositional functions are avoided in these models , additional modules , such as attention layers , are employed on top of the word embedding layer .",related work,0,37,4,0,28
text-classification,8,"As a result , the specific role that the word embedding plays in these models is not emphasized ( or explicit ) , which distracts from understanding how important the word embeddings alone are to the observed superior performance .",related work,0,38,5,0,40
text-classification,8,"Moreover , several recent studies have shown empirically that the advantages of distinct compositional functions are highly dependent on the specific task .",related work,0,39,6,0,23
text-classification,8,"Therefore , it is of interest to study the practical value of the additional expressiveness , on a wide variety of NLP problems .",related work,0,40,7,0,24
text-classification,8,"SWEMs bear close resemblance to Deep Averaging Network ( DAN ) or fast - Text , where they show that average pooling achieves promising results on certain NLP tasks .",related work,0,41,8,0,30
text-classification,8,"However , there exist several key differences that make our work unique .",related work,0,42,9,0,13
text-classification,8,"First , we explore a series of pooling operations , rather than only average - pooling .",related work,0,43,10,0,17
text-classification,8,"Specifically , a hierarchical pooling operation is introduced to incorporate spatial information , which demonstrates superior results on sentiment analysis , relative to average pooling .",related work,0,44,11,0,26
text-classification,8,"Second , our work not only explores when simple pooling operations are enough , but also investigates the underlying reasons , i.e. , what semantic features are required for distinct NLP problems .",related work,0,45,12,0,33
text-classification,8,"Third , DAN and fast Text only focused on one or two problems at a time , thus a comprehensive study regarding the effectiveness of various compositional functions on distinct NLP tasks , e.g. , categorizing short sentence / long documents , matching natural language sentences , has heretofore been absent .",related work,0,46,13,0,52
text-classification,8,"In response , our work seeks to perform a comprehensive comparison with respect to simple - vs. - complex compositional func- tions , across a wide range of NLP problems , and reveals some general rules for rationally selecting models to tackle different tasks .",related work,0,47,14,0,45
text-classification,8,models & training,related work,0,48,15,0,3
text-classification,8,"Consider a text sequence represented as X ( either a sentence or a document ) , composed of a sequence of words : {w 1 , w 2 , .... , w L } , where L is the number of tokens , i.e. , the sentence / document length .",related work,0,49,16,0,51
text-classification,8,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ?",related work,0,50,17,0,25
text-classification,8,r k .,related work,0,51,18,0,3
text-classification,8,"The compositional function , X ? z , aims to combine word embeddings into a fixed - length sentence / document representation z .",related work,0,52,19,0,24
text-classification,8,"These representations are then used to make predictions about sequence X. Below , we describe different types of functions considered in this work .",related work,0,53,20,0,24
text-classification,8,recurrent sequence encoder,related work,0,54,21,0,3
text-classification,8,"A widely adopted compositional function is defined in a recurrent manner : the model successively takes word vector v tat position t , along with the hidden unit h t?1 from the last position t ?",related work,0,55,22,0,36
text-classification,8,"1 , to update the current hidden unit via",related work,0,56,23,0,9
text-classification,8,"To address the issue of learning long - term dependencies , f ( ) is often defined as Long Short - Term Memory ( LSTM ) , which employs gates to control the flow of information abstracted from a sequence .",related work,0,57,24,0,41
text-classification,8,We omit the details of the LSTM and refer the interested readers to the work by for further explanation .,related work,0,58,25,0,20
text-classification,8,"Intuitively , the LSTM encodes a text sequence considering its word - order information , but yields additional compositional parameters that must be learned .",related work,0,59,26,0,25
text-classification,8,convolutional sequence encoder,related work,0,60,27,0,3
text-classification,8,The Convolutional Neural Network ( CNN ) architecture is another strategy extensively employed as the compositional function to encode text sequences .,related work,0,61,28,0,22
text-classification,8,"The convolution operation considers windows of n consecutive words within the sequence , where a set of filters ( to be learned ) are applied to these word windows to generate corresponding feature maps .",related work,0,62,29,0,35
text-classification,8,"Subsequently , an aggregation operation ( such as max - pooling ) is used on top of the feature maps to abstract the most salient semantic features , resulting in the final representation .",related work,0,63,30,0,34
text-classification,8,"For most experiments , we consider a single - layer CNN text model .",related work,0,64,31,0,14
text-classification,8,"However , Deep CNN text models have also been developed , and are considered in a few of our experiments .",related work,0,65,32,0,21
text-classification,8,simple word - embedding model,related work,0,66,33,0,5
text-classification,8,( swem ),related work,0,67,34,0,3
text-classification,8,"To investigate the raw modeling capacity of word embeddings , we consider a class of models with no additional compositional parameters to encode natural language sequences , termed SWEMs .",related work,0,68,35,0,30
text-classification,8,"Among them , the simplest strategy is to compute the element - wise average over word vectors for a given sequence :",related work,0,69,36,0,22
text-classification,8,"The model in can be seen as an average pooling operation , which takes the mean over each of the K dimensions for all word embeddings , resulting in a representation z with the same dimension as the embedding itself , termed here SWEM - aver .",related work,0,70,37,0,47
text-classification,8,"Intuitively , z takes the information of every sequence element into account via the addition operation .",related work,0,71,38,0,17
text-classification,8,max pooling,related work,0,72,39,0,2
text-classification,8,"Motivated by the observation that , in general , only a small number of key words contribute to final predictions , we propose another SWEM variant , that extracts the most salient features from every word - embedding dimension , by taking the maximum value along each dimension of the word vectors .",related work,0,73,40,0,53
text-classification,8,This strategy is similar to the max - over - time pooling operation in convolutional neural networks :,related work,0,74,41,0,18
text-classification,8,We denote this model variant as SWEM - max .,related work,0,75,42,0,10
text-classification,8,"Here the j - th component of z is the maximum element in the set {v 1 j , . . . , v Lj } , where v 1j is , for example , the j - th component of v 1 .",related work,0,76,43,0,44
text-classification,8,"With this pooling operation , those words thatare unimportant or unrelated to the corresponding tasks will be ignored in the encoding process ( as the components of the embedding vectors will have small amplitude ) , unlike SWEM - aver where every word contributes equally to the representation .",related work,0,77,44,0,49
text-classification,8,"Considering that SWEM - aver and SWEM - max are complementary , in the sense of accounting for different types of information from text sequences ,",related work,0,78,45,0,26
text-classification,8,model,related work,0,79,46,0,1
text-classification,8,Parameter s Speed CNN 541K 171s LSTM 1.8M 598s SWEM 61K 63s,related work,0,80,47,0,12
text-classification,8,"Interestingly , for the sentiment analysis tasks , both CNN and LSTM compositional functions perform better than SWEM , suggesting that wordorder information maybe required for analyzing sentiment orientations .",related work,1,81,48,0,30
text-classification,8,"This finding is consistent with , where they hypothesize that the positional information of a word in text sequences maybe beneficial to predict sentiment .",related work,0,82,49,0,25
text-classification,8,"This is intuitively reasonable since , for instance , the phrase "" not really good "" and "" really not good "" convey different levels of negative sentiment , while being different only by their word orderings .",related work,0,83,50,0,38
text-classification,8,"Contrary to SWEM , CNN and LSTM models can both capture this type of information via convolutional filters or recurrent transition functions .",related work,0,84,51,0,23
text-classification,8,"However , as suggested above , such word - order patterns maybe much less useful for predicting the topic of a document .",related work,0,85,52,0,23
text-classification,8,"This maybe attributed to the fact that word embeddings alone already provide sufficient topic information of a document , at least when the text sequences considered are relatively long .",related work,0,86,53,0,30
text-classification,8,parameters,related work,0,87,54,0,1
text-classification,8,complexity sequential,related work,0,88,55,0,2
text-classification,8,"Ops we also propose a third SWEM variant , where the two abstracted features are concatenated together to form the sentence embeddings , denoted here as SWEM - concat .",related work,0,89,56,0,30
text-classification,8,"For all SWEM variants , there are no additional compositional parameters to be learned .",related work,0,90,57,0,15
text-classification,8,"As a result , the models only exploit intrinsic word embedding information for predictions .",related work,0,91,58,0,15
text-classification,8,"Hierarchical Pooling Both SWEM - aver and SWEM - max do not take word - order or spatial information into consideration , which could be useful for certain NLP applications .",related work,0,92,59,0,31
text-classification,8,"So motivated , we further propose a hierarchical pooling layer .",related work,0,93,60,0,11
text-classification,8,"Let v i:i+n?1 refer to the local window consisting of n consecutive words words ,",related work,0,94,61,0,15
text-classification,8,"First , an average - pooling is performed on each local window , v i:i+n?1 .",related work,0,95,62,0,16
text-classification,8,The extracted features from all windows are further down - sampled with a global max - pooling operation on top of the representations for every window .,related work,0,96,63,0,27
text-classification,8,We call this approach SWEM - hier due to its layered pooling .,related work,0,97,64,0,13
text-classification,8,"This strategy preserves the local spatial information of a text sequence in the sense that it keeps track of how the sentence / document is constructed from individual word windows , i.e. , n-grams .",related work,0,98,65,0,35
text-classification,8,This formulation is related to bag - of - n- grams method .,related work,0,99,66,0,13
text-classification,8,"However , SWEM - hier learns fixed - length representations for the n-grams that appear in the corpus , rather than just capturing their occurrences via count features , which may potentially advantageous for prediction purposes .",related work,0,100,67,0,37
text-classification,8,parameters & computation,related work,0,101,68,0,3
text-classification,8,comparison,related work,0,102,69,0,1
text-classification,8,"We compare CNN , LSTM and SWEM wrt their parameters and computational speed .",related work,0,103,70,0,14
text-classification,8,"K denotes the dimension of word embeddings , as above .",related work,0,104,71,0,11
text-classification,8,"For the CNN , we use n to denote the filter width ( assumed constant for all filters , for simplicity of analysis , but in practice variable n is commonly used ) .",related work,0,105,72,0,34
text-classification,8,We defined as the dimension of the final sequence representation .,related work,0,106,73,0,11
text-classification,8,"Specifically , d represents the dimension of hidden units or the number of filters in LSTM or CNN , respectively .",related work,0,107,74,0,21
text-classification,8,We first examine the number of compositional parameters for each model .,related work,0,108,75,0,12
text-classification,8,"As shown in , both the CNN and LSTM have a large number of parameters , to model the semantic compositionality of text sequences , whereas SWEM has no such parameters .",related work,0,109,76,0,32
text-classification,8,"Similar to , we then consider the computational complexity and the minimum number of sequential operations required for each model .",related work,0,110,77,0,21
text-classification,8,SWEM tends to be more efficient than CNN and LSTM in terms of computation complexity .,related work,0,111,78,0,16
text-classification,8,"For example , considering the case where K = d , SWEM is faster than CNN or LSTM by a factor of nd or d , respectively .",related work,0,112,79,0,28
text-classification,8,"Further , the computations in SWEM are highly parallelizable , unlike LSTM that requires O ( L ) sequential steps .",related work,0,113,80,0,21
text-classification,8,experiments,experiment,0,114,1,0,1
text-classification,8,"We evaluate different compositional functions on a wide variety of supervised tasks , including document categorization , text sequence matching ( given a sentence pair , X 1 , X 2 , predict their relationship , y) as well as ( short ) sentence classification .",experiment,0,115,2,0,46
text-classification,8,"We experiment on 17 datasets concerning natural language understanding , with corresponding data statistics summarized in the Supplementary Material .",experiment,0,116,3,0,20
text-classification,8,We use Glo Ve word embeddings with K = 300 as initialization for all our models .,experiment,1,117,4,0,17
text-classification,8,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",experiment,1,118,5,0,24
text-classification,8,"The Glo Ve embeddings are employed in two ways to learn refined word embeddings : ( i ) directly updating each word embedding during training ; and ( ii ) training a 300 dimensional Multilayer Perceptron ( MLP ) layer with ReLU activation , with Glo Ve embeddings as input to the MLP and with output defining the refined word embeddings .",experiment,1,119,6,0,62
text-classification,8,The latter approach corresponds to learning an MLP model that adapts GloVe embeddings to the dataset and task of interest .,experiment,0,120,7,0,21
text-classification,8,The advantages of these two methods differ from dataset to dataset .,experiment,0,121,8,0,12
text-classification,8,We choose the better strategy based on their corresponding performances on the validation set .,experiment,0,122,9,0,15
text-classification,8,"The final classifier is implemented as an MLP layer with dimension selected from the set [ 100 , 300 , 500 , 1000 ] , followed by a sigmoid or softmax function , depending on the specific task .",experiment,0,123,10,0,39
text-classification,8,"Adam ) is used to optimize all models , with learning rate selected from .",experiment,1,124,11,0,15
text-classification,8,"Surprisingly , on topic prediction tasks , our SWEM model exhibits stronger performances , relative to both LSTM and CNN compositional architectures , this by leveraging both the average and max - pooling features from word embeddings .",experiment,1,125,12,0,38
text-classification,8,"Specifically , our SWEM - concat model even outperforms a 29 - layer deep CNN model , when predicting topics .",experiment,0,126,13,0,21
text-classification,8,"On the ontology classification problem ( DBpedia dataset ) , we observe the same trend , that SWEM exhibits comparable or even superior results , relative to CNN or LSTM models .",experiment,1,127,14,0,32
text-classification,8,"Since there are no compositional parameters in SWEM , our models have an order of magnitude fewer parameters ( excluding embeddings ) than LSTM or CNN , and are considerably more computationally efficient .",experiment,0,128,15,0,34
text-classification,8,"As illustrated in Table 4 , SWEM - concat achieves better results on Yahoo !",experiment,0,129,16,0,15
text-classification,8,"Answer than CNN / LSTM , with only 61 K parameters ( one - tenth the number of LSTM parameters , or one - third the number of CNN parameters ) , while taking a fraction of the training time relative to the CNN or LSTM .",experiment,0,130,17,0,47
text-classification,8,interpreting model predictions,experiment,0,131,18,0,3
text-classification,8,"Although the proposed SWEM - max variant generally performs a slightly worse than SWEM - aver , it extracts complementary features from SWEMaver , and hence in most cases SWEM - concat exhibits the best performance among all SWEM variants .",experiment,0,132,19,0,41
text-classification,8,"More importantly , we found that the word embeddings learned from SWEM - max tend to be sparse .",experiment,0,133,20,0,19
text-classification,8,We trained our SWEM - max model on the Yahoo datasets ( randomly initialized ) .,experiment,0,134,21,0,16
text-classification,8,"With the learned embeddings , we plot the values for each of the word embedding dimensions , for the entire vocabulary .",experiment,0,135,22,0,22
text-classification,8,"As shown in , most of the values are highly concentrated around zero , indicating that the word embeddings learned are very sparse .",experiment,0,136,23,0,24
text-classification,8,"On the contrary , the Glo Ve word embeddings , for the same vocabulary , are considerably denser than the embeddings learned from SWEM - max .",experiment,0,137,24,0,27
text-classification,8,"This suggests that the model may only depend on a few key words , among the entire vocabulary , for predictions ( since most words do not contribute to the max - pooling operation in SWEM - max ) .",experiment,0,138,25,0,40
text-classification,8,"Through the embedding , the model learns the important words for a given task ( those words with non -zero embedding components ) .",experiment,0,139,26,0,24
text-classification,8,"In this regard , the nature of max - pooling pro - cess gives rise to a more interpretable model .",experiment,0,140,27,0,21
text-classification,8,"For a document , only the word with largest value in each embedding dimension is employed for the final representation .",experiment,0,141,28,0,21
text-classification,8,"Thus , we suspect that semantically similar words may have large values in some shared dimensions .",experiment,0,142,29,0,17
text-classification,8,"So motivated , after training the SWEM - max model on the Yahoo dataset , we selected five words with the largest values , among the entire vocabulary , for each word embedding dimension ( these words are selected preferentially in the corresponding dimension , by the max operation ) .",experiment,0,143,30,0,51
text-classification,8,"As shown in , the words chosen wrt each embedding dimension are indeed highly relevant and correspond to a common topic ( the topics are inferred from words ) .",experiment,0,144,31,0,30
text-classification,8,"For example , the words in the first column of are all political terms , which could be assigned to the Politics & Government topic .",experiment,0,145,32,0,26
text-classification,8,Note that our model can even learn locally interpretable structure that is not explicitly indicated by the label information .,experiment,0,146,33,0,20
text-classification,8,"For instance , all words in the fifth column are Chemistry - related .",experiment,0,147,34,0,14
text-classification,8,"However , we do not have a chemistry label in the dataset , and regardless they should belong to the Science topic .",experiment,0,148,35,0,23
text-classification,8,text sequence matching,experiment,1,149,36,0,3
text-classification,8,"To gain a deeper understanding regarding the modeling capacity of word embeddings , we further investigate the problem of sentence matching , including natural language inference , answer sentence selection and paraphrase identification .",experiment,0,150,37,0,34
text-classification,8,The corresponding performance metrics are shown in .,experiment,0,151,38,0,8
text-classification,8,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .",experiment,1,152,39,0,28
text-classification,8,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .",experiment,1,153,40,1,49
text-classification,8,"As a result , with only 120K parameters , our SWEM - max achieves a test accuracy of 83.8 % , which is very competitive among state - of the - art sentence encoding - based models ( in terms of both performance and number of parameters )",experiment,0,154,41,0,48
text-classification,8,1 .,experiment,0,155,42,0,2
text-classification,8,"The strong results of the SWEM approach on these tasks may stem from the fact that when matching natural language sentences , it is sufficient in most cases to simply model the word - level alignments between two sequences .",experiment,0,156,43,0,40
text-classification,8,"From this perspective , word - order information becomes much less useful for predicting relationship between sentences .",experiment,0,157,44,0,18
text-classification,8,"Moreover , considering the simpler model architecture of SWEM , they could be much easier to be optimized than LSTM or CNN - based models , and thus give rise to better empirical results .",experiment,0,158,45,0,35
text-classification,8,Importance of word - order information,experiment,0,159,46,0,6
text-classification,8,"One possible dis advantage of SWEM is that it ignores the word - order information within a text sequence , which could be potentially captured by CNN - or LSTM - based models .",experiment,0,160,47,0,34
text-classification,8,"However , we empirically found that except for sentiment analysis , SWEM exhibits similar or even superior performance as the CNN or LSTM on a variety of tasks .",experiment,0,161,48,0,29
text-classification,8,"In this regard , one natural question would be : how important are word - order features for these tasks ?",experiment,0,162,49,0,21
text-classification,8,"To this end , we randomly shuffle the words for every sentence in the training set , while keeping the original word order for samples in the test set .",experiment,0,163,50,0,30
text-classification,8,The motivation here is to remove the word - order features from the training set and examine how sensitive the performance on different tasks are to word - order information .,experiment,0,164,51,0,31
text-classification,8,We use LSTM as the model for this purpose since it can captures wordorder information from the original training set .,experiment,0,165,52,0,21
text-classification,8,The results on three distinct tasks are shown in .,experiment,0,166,53,0,10
text-classification,8,"Somewhat surprisingly , for Yahoo and SNLI datasets , the LSTM model trained on shuffled training set shows comparable accuracies to those trained on the original dataset , indicating that word - order information does not contribute significantly on these two problems , i.e. , topic categorization and textual entailment .",experiment,0,167,54,0,51
text-classification,8,"However , on the Yelp polarity dataset , the results drop noticeably , further suggesting that word - order does matter for sentiment analysis ( as indicated above from a different perspective ) .",experiment,0,168,55,0,34
text-classification,8,"Notably , the performance of LSTM on the Yelp dataset with a shuffled training set is very close to our results with SWEM , indicating that the main difference between LSTM and SWEM maybe due to the ability of the former to capture word - order features .",experiment,0,169,56,0,48
text-classification,8,Both observations are in consistent with our experimental results in the previous section .,experiment,0,170,57,0,14
text-classification,8,case study,experiment,0,171,58,0,2
text-classification,8,"To understand what type of sentences are sensitive to word - order information , we further show those samples thatare wrongly predicted because of the shuffling of training data in .",experiment,0,172,59,0,31
text-classification,8,"Taking the first sentence as an example , several words in the review are generally positive , i.e. friendly , nice , okay , great and likes .",experiment,0,173,60,0,28
text-classification,8,"However , the most vital features for predicting the sentiment of this sentence could be the phrase / sentence ' is just okay ' , ' not great ' or ' makes me wonder why everyone likes ' , which can not be captured without considering word - order features .",experiment,0,174,61,0,51
text-classification,8,It is worth noting the hints for predictions in this case are actually ngram phrases from the input document .,experiment,0,175,62,0,20
text-classification,8,SWEM - hier for sentiment analysis,experiment,1,176,63,0,6
text-classification,8,"As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .",experiment,1,177,64,0,19
text-classification,8,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :",experiment,1,178,65,0,26
text-classification,8,Friendly staff and nice selection of vegetarian options .,experiment,0,179,66,0,9
text-classification,8,"Food is just okay , not great .",experiment,0,180,67,0,8
text-classification,8,Makes me wonder why everyone likes food fight so much .,experiment,0,181,68,0,11
text-classification,8,positive :,experiment,0,182,69,0,2
text-classification,8,"The store is small , but it carries specialties thatare difficult to find in Pittsburgh .",experiment,0,183,70,0,16
text-classification,8,I was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights .,experiment,0,184,71,0,16
text-classification,8,the input document .,experiment,0,185,72,0,4
text-classification,8,"We hypothesize that incorporating information about the local word - order , i.e. , n-gram features , is likely to largely mitigate the limitations of the above three SWEM variants .",experiment,0,186,73,0,31
text-classification,8,"Inspired by this observation , we propose using another simple pooling operation termed as hierarchical ( SWEM - hier ) , as detailed in Section 3.3 .",experiment,0,187,74,0,27
text-classification,8,We evaluate this method on the two documentlevel sentiment analysis tasks and the results are shown in the last row of .,experiment,0,188,75,0,22
text-classification,8,"SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .",experiment,1,189,76,0,26
text-classification,8,"This indicates that the proposed hierarchical pooling operation manages to abstract spatial ( word - order ) information from the input sequence , which is beneficial for performance in sentiment analysis tasks .",experiment,0,190,77,0,33
text-classification,8,short sentence processing,experiment,1,191,78,0,3
text-classification,8,We now consider sentence - classification tasks ( with approximately 20 words on average ) .,experiment,0,192,79,0,16
text-classification,8,"We experiment on three sentiment classification datasets , i.e. , MR , SST - 1 , SST - 2 , as well as subjectivity classification ( Subj ) and question classification ( TREC ) .",experiment,0,193,80,0,35
text-classification,8,The corresponding results are shown in .,experiment,0,194,81,0,7
text-classification,8,"Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .",experiment,1,195,82,0,28
text-classification,8,"However , SWEM exhibits comparable performance on the other two tasks , again with much less parameters and faster training .",experiment,1,196,83,0,21
text-classification,8,"Further , we investigate two sequence tagging tasks : the standard CoNLL2000 chunking and CoNLL2003 NER datasets .",experiment,0,197,84,0,18
text-classification,8,"Results are shown in the Supplementary Material , where LSTM and CNN again perform better than SWEMs .",experiment,0,198,85,0,18
text-classification,8,"Generally , SWEM is less effective at extracting representations from short sentences than from long documents .",experiment,0,199,86,0,17
text-classification,8,"This maybe due to the fact that for a shorter text sequence , word - order features tend to be more important since the semantic information provided byword embeddings alone is relatively limited .",experiment,0,200,87,0,34
text-classification,8,"Moreover , we note that the results on these relatively small datasets are highly sensitive to model regularization techniques due to the overfitting issues .",experiment,0,201,88,0,25
text-classification,8,"In this regard , one interesting future direction maybe to develop specific regularization strategies for the SWEM framework , and thus make them work better on small sentence classification datasets .",experiment,0,202,89,0,31
text-classification,8,discussion,experiment,0,203,90,0,1
text-classification,8,comparison via subspace training,experiment,0,204,91,0,4
text-classification,8,We use subspace training to measure the model complexity in text classification problems .,experiment,0,205,92,0,14
text-classification,8,"It constrains the optimization of the trainable parameters in a subspace of low dimension d , the intrinsic dimension dint defines the minimum d that yield a good solution .",experiment,0,206,93,0,30
text-classification,8,"Two models are studied : the SWEM - max variant , and the CNN model including a convolutional layer followed by a FC layer .",experiment,0,207,94,0,25
text-classification,8,we consider two settings :,experiment,0,208,95,0,5
text-classification,8,"( 1 ) The word embeddings are randomly intialized , and optimized jointly with the model parameters .",experiment,0,209,96,0,18
text-classification,8,We show the performance of direct and subspace training on AG News dataset in ( a ) ( b ) .,experiment,0,210,97,0,21
text-classification,8,The two models trained via direct method share almost identical perfomrnace on training and testing .,experiment,0,211,98,0,16
text-classification,8,"The subspace training yields similar accuracy with direct training for very small d , even when model parameters are not trained at all ( d = 0 ) .",experiment,0,212,99,0,29
text-classification,8,"This is because the word embeddings have the full degrees of freedom to adjust to achieve good solutions , regardless of the employed models .",experiment,0,213,100,0,25
text-classification,8,SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best solutions .,experiment,0,214,101,0,19
text-classification,8,"According to Occam 's razor , simple models are preferred , if all else are the same .",experiment,0,215,102,0,18
text-classification,8,"( 2 ) The pre-trained GloVe are frozen for the word embeddings , and only the model parameters are optimized .",experiment,0,216,103,0,21
text-classification,8,"The results on testing datasets of AG News and Yelp P. are shown in ( c ) ( d ) , respectively .",experiment,0,217,104,0,23
text-classification,8,"SWEM shows significantly higher accuracy than CNN for a large range of low subspace dimension , indicating that SWEM is more parameter - efficient to get a decent solution .",experiment,0,218,105,0,30
text-classification,8,"In ( c ) , if we set the performance threshold Model MR SST - 1 SST - 2 Subj TREC RAE 77.7 43.2 82.4 --MV-RNN 79.0 44.4 82.9 --LSTM - 46.4 84.9 --RNN 77.2 --93.7 90.2 Constituency Tree-LSTM - 51.0 88.0 -- Dynamic CNN - 48.5 86.8 - 93.0 CNN 81 as 80 % testing accuracy , SWEM exhibits a lower dint than CNN on AG News dataset .",experiment,0,219,106,0,70
text-classification,8,"However , in , CNN can leverage more trainable parameters to achieve higher accuracy when dis large .",experiment,0,220,107,0,18
text-classification,8,linear classifiers,experiment,0,221,108,0,2
text-classification,8,"To further investigate the quality of representations learned from SWEMs , we employ a linear classifier on top of the representations for prediction , instead of a non-linear MLP layer as in the previous section .",experiment,0,222,109,0,36
text-classification,8,It turned out that utilizing a linear classifier only leads to a very small performance drop for both Yahoo !,experiment,0,223,110,0,20
text-classification,8,Ans. ( from 73.53 % to 73.18 % ) and Yelp P. datasets ( from 93.76 % to 93.66 % ) .,experiment,0,224,111,0,22
text-classification,8,This observation highlights that SWEMs are able to extract robust and informative sentence representations despite their simplicity .,experiment,0,225,112,0,18
text-classification,8,extension to other languages,experiment,0,226,113,0,4
text-classification,8,"We have also tried our SWEM - concat and SWE Mhier models on Sogou news corpus ( with the same experimental setup as ) , which is a Chinese dataset represented by Pinyin ( a phonetic romanization of Chinese ) .",experiment,0,227,114,0,41
text-classification,8,"SWEMconcat yields an accuracy of 91.3 % , while SWEM - hier ( with a local window size of 5 ) obtains an accuracy of 96.2 % on the test set .",experiment,0,228,115,0,32
text-classification,8,"Notably , the performance of SWEM - hier is comparable to the best accuracies of CNN ( 95.6 % ) and LSTM ( 95.2 % ) , as reported in .",experiment,0,229,116,0,31
text-classification,8,"This indicates that hierarchical pooling is more suitable than average / max pooling for Chinese text classification , by taking spatial information into account .",experiment,0,230,117,0,25
text-classification,8,It also implies that Chinese is more sensitive to local word - order features than English .,experiment,0,231,118,0,17
text-classification,8,conclusions,experiment,0,232,119,0,1
text-classification,8,"We have performed a comparative study between SWEM ( with parameter - free pooling operations ) and CNN or LSTM - based models , to represent text sequences on 17 NLP datasets .",experiment,0,233,120,0,33
text-classification,8,"We further validated our experimental findings through additional exploration , and revealed some general rules for rationally selecting compositional functions for distinct problems .",experiment,0,234,121,0,24
text-classification,8,Our findings regarding when ( and why ) simple pooling operations are enough for text sequence representations are summarized as follows :,experiment,0,235,122,0,22
text-classification,8,"Simple pooling operations are surprisingly effective at representing longer documents ( with hundreds of words ) , while recurrent / convolutional compositional functions are most effective when constructing representations for short sentences .",experiment,0,236,123,0,33
text-classification,8,Sentiment analysis tasks are more sensitive to word - order features than topic categorization tasks .,experiment,0,237,124,0,16
text-classification,8,"However , a simple hierarchical pooling layer proposed here achieves comparable results to LSTM / CNN on sentiment analysis tasks .",experiment,0,238,125,0,21
text-classification,8,"To match natural language sentences , e.g. , textual entailment , answer sentence selection , etc. , simple pooling operations already exhibit similar or even superior results , compared to CNN and LSTM .",experiment,0,239,126,0,34
text-classification,8,"We consider a wide range of text - representationbased tasks in this paper , including document categorization , text sequence matching and ( short ) sentence classification .",experiment,0,240,127,0,28
text-classification,8,"For document classification tasks , we use the same data splits in ( downloaded from https://goo.gl/QaRpr7 ) ; for short sentence classification , we employ the same training / testing data and preprocessing procedure with .",experiment,0,241,128,0,36
text-classification,8,"The statistics and corresponding types of these datasets are summarized in Datasets #w #c Train Types SWEM - CRF indicates that CRF is directly operated on top of the word embedding layer and make predictions for each word ( there is no contextual / word - order information before CRF layer , compared to CNN - CRF or BI - LSTM - CRF ) .",experiment,0,242,129,0,65
text-classification,8,"As shown above , CNN - CRF and BI - LSTM - CRF consistently outperform SWEM - CRF on both sequence tagging tasks , although the training takes around 4 to 5 times longer ( for BI - LSTM - CRF ) than SWEM - CRF .",experiment,0,243,130,0,47
text-classification,8,"This suggests that for chunking and NER , compositional functions such as LSTM or CNN are very necessary , because of the sequential ( order-sensitive ) nature of sequence tagging tasks .",experiment,0,244,131,0,32
text-classification,8,What are the key words used for predictions ?,experiment,0,245,132,0,9
text-classification,8,"Given the sparsity of word embeddings , one natural question would be : What are those key words thatare leveraged by the model to make predictions ?",experiment,0,246,133,0,27
text-classification,8,"To this end , after training SWEM - max on Yahoo !",experiment,0,247,134,0,12
text-classification,8,"Answer dataset , we selected the top - 10 words ( with the maximum values in that dimension ) for every word embedding dimension .",experiment,0,248,135,0,25
text-classification,8,The results are visualized in .,experiment,0,249,136,0,6
text-classification,8,"These words are indeed very predictive since they are likely to occur in documents with a specific topic , as discussed above .",experiment,0,250,137,0,23
text-classification,8,"Another interesting observation is that the frequencies of these words are actually quite low in the training set ( e.g. colston : 320 , repubs : 255 win32 : 276 ) , considering the large size of the training set ( 1,400 K ) .",experiment,0,251,138,0,45
text-classification,8,"This suggests that the model is utilizing those relatively rare , yet representative words of each topic for the final predictions .",experiment,0,252,139,0,22
text-classification,8,information of a text sequence is the word embedding .,experiment,0,253,140,0,10
text-classification,8,"Thus , it is of interest to see how many word embedding dimensions are needed for a SWEM architecture to perform well .",experiment,0,254,141,0,23
text-classification,8,"To this end , we vary the dimension from 3 to 1000 and train a SWEMconcat model on the Yahoo dataset .",experiment,0,255,142,0,22
text-classification,8,"For fair comparison , the word embeddings are randomly initialized in this experiment , since there are no pretrained word vectors , such as GloVe , for some dimensions we consider .",experiment,0,256,143,0,32
text-classification,8,"As shown in , the model exhibits higher accuracy with larger word embedding dimensions .",experiment,0,257,144,0,15
text-classification,8,"This is not surprising since with more embedding dimensions , more semantic features could be potentially encapsulated .",experiment,0,258,145,0,18
text-classification,8,"However , we also observe that even with only 10 dimensions , SWEM demonstrates comparable results relative to the case with 1000 dimensions , suggesting that word embeddings are very efficient at abstracting semantic information into fixed - length vectors .",experiment,0,259,146,0,41
text-classification,8,"This property indicates that we may further reduce the number of model parameters with lowerdimensional word embeddings , while still achieving competitive results .",experiment,0,260,147,0,24
text-classification,8,Sensitivity of compositional functions to sample size,experiment,0,261,148,0,7
text-classification,8,"To explore the robustness of different compositional functions , we consider another application scenario , where we only have a limited number of training data , e.g. , when labeled data are expensive to obtain .",experiment,0,262,149,0,36
text-classification,8,"To investigate this , we re-run the experiments on Yahoo and SNLI datasets , while employing increasing proportions of the original training set .",experiment,0,263,150,0,24
text-classification,8,"Specifically , we use 0.1 % , 0.2 % , 0.6 % , 1.0 % , 10 % , 100 % for comparison ; the corresponding results are shown in .",experiment,0,264,151,0,31
text-classification,8,"Surprisingly , SWEM consistently outperforms CNN and LSTM models by a large margin , on a wide range of training data proportions .",experiment,0,265,152,0,23
text-classification,8,"For instance , with 0.1 % of the training samples from Yahoo dataset ( around 1.4 K labeled data ) , SWEM achieves an accuracy of 56. 10 % , which is much better than that of models with CNN ( 25.32 % ) or LSTM ( 42.37 % ) .",experiment,0,266,153,0,51
text-classification,8,"On the SNLI dataset , we also noticed the same trend that the SWEM architecture result in much better accuracies , with a fraction of training data .",experiment,0,267,154,0,28
text-classification,8,"This observation indicates that overfitting issues in CNN or LSTMbased models on text data mainly stems from overcomplicated compositional functions , rather than the word embedding layer .",experiment,0,268,155,0,28
text-classification,8,"More importantly , SWEM tends to be afar more robust model when only limited data are available for training .",experiment,0,269,156,0,20
text-classification,9,Translations as Additional Contexts for Sentence Classification,title,1,2,1,0,7
text-classification,9,abstract,abstract,0,3,1,0,1
text-classification,9,"In sentence classification tasks , additional contexts , such as the neighboring sentences , may improve the accuracy of the classifier .",abstract,0,4,2,0,22
text-classification,9,"However , such contexts are domain - dependent and thus can not be used for another classification task with an inappropriate domain .",abstract,0,5,3,0,23
text-classification,9,"In contrast , we propose the use of translated sentences as domain - free context that is always available regardless of the domain .",abstract,0,6,4,0,24
text-classification,9,"We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier , due to possible inaccurate translations thus producing noisy sentence vectors .",abstract,0,7,5,0,32
text-classification,9,"To this end , we present multiple context fixing attachment ( MCFA ) , a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context .",abstract,0,8,6,0,38
text-classification,9,"We show that our method performs competitively compared to previous models , achieving best classification performance on multiple data sets .",abstract,0,9,7,0,21
text-classification,9,We are the first to use translations as domainfree contexts for sentence classification .,abstract,0,10,8,0,14
text-classification,9,introduction,introduction,0,11,1,0,1
text-classification,9,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .",introduction,1,12,2,0,50
text-classification,9,"This task is important as it is widely used in almost all subare as of NLP such as sentiment classification for sentiment analysis and question type classification for question answering , to name a few .",introduction,0,13,3,0,36
text-classification,9,"While past methods require feature engineering , recent methods enjoy neural - based methods to automatically encode the sentences into low - dimensional dense vectors .",introduction,0,14,4,0,26
text-classification,9,"Despite the success of these methods , the major challenge in this task is that extracting features from a single sentence limits the performance .",introduction,0,15,5,0,25
text-classification,9,"To overcome this limitation , recent works attempted to augment different kinds of features to the sentence , such as the neighboring sentences and the topics of the sentences .",introduction,0,16,6,0,30
text-classification,9,"However , these methods used domain - dependent contexts thatare only effective when the domain of the task is appropriate .",introduction,0,17,7,0,21
text-classification,9,"For one thing , neighboring sentences may not be available in some tasks such as question type classification .",introduction,0,18,8,0,19
text-classification,9,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",introduction,0,19,9,0,27
text-classification,9,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts thatare always available no matter what the task domain is .",introduction,0,20,10,0,32
text-classification,9,We observe two opportunities when using translations .,introduction,0,21,11,0,8
text-classification,9,"First , each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class .",introduction,0,22,12,0,23
text-classification,9,contrasts the sentence vectors of the original English sentences and their Arabictranslated sentences in the question type classification task .,introduction,0,23,13,0,20
text-classification,9,A yellow circle signifies a clear separation of a class .,introduction,0,24,14,0,11
text-classification,9,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",introduction,0,25,15,0,38
text-classification,9,"Meanwhile , location type questions ( in orange ) are better classified in English .",introduction,0,26,16,0,15
text-classification,9,"Second , the original sentences may include languagespecific ambiguity , which maybe resolved when presented with its translations .",introduction,0,27,17,0,19
text-classification,9,"Consider the example English sentence "" The movie is terribly amazing "" for the sentiment classification task .",introduction,0,28,18,0,18
text-classification,9,"In this case , terribly can be used in both positive and negative sense , thus introduces ambiguity in the sentence .",introduction,0,29,19,0,22
text-classification,9,"When translated to Korean , it becomes "" ? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ? "" which means "" The movie is greatly magnificent "" , removing the ambiguity .",introduction,0,30,20,0,44
text-classification,9,The above two observations hold only when translations are supported for ( nearly ) arbitrary language pairs with sufficiently high quality .,introduction,0,31,21,0,22
text-classification,9,"Thankfully , translation services ( e.g. Google Translate )",introduction,0,32,22,0,9
text-classification,9,"Moreover , recent research on neural machine translation ( NMT ) improved the efficiency and even enabled zero - shot translation of models for languages with no parallel data .",introduction,0,33,23,0,30
text-classification,9,"This provides an opportunity to leverage on as many languages as possible to any domain , providing a much wider context compared to the limited contexts provided by past studies .",introduction,0,34,24,0,31
text-classification,9,"However , despite the maturity of translation , naively concatenating their vectors to the original sentence vector may introduce more noise than signals .",introduction,0,35,25,0,24
text-classification,9,The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable .,introduction,0,36,26,0,20
text-classification,9,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .",introduction,0,37,27,0,25
text-classification,9,Suppose there are two translated sentences a and b with slight errors .,introduction,0,38,28,0,13
text-classification,9,"We posit that a can be used to fix b when a is used as a context of b , and vice versa",introduction,0,39,29,0,23
text-classification,9,1 .,introduction,0,40,30,0,2
text-classification,9,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",introduction,0,41,31,0,45
text-classification,9,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",introduction,1,42,32,0,18
text-classification,9,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",introduction,1,43,33,0,34
text-classification,9,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",introduction,1,44,34,0,28
text-classification,9,Noises from translation may cause adverse effects to the vector itself ( e.g. when a noisy vector is directly used for the task ) and relatively to other vectors ( e.g. when a noisy vector is used to fix another noisy vector ) .,introduction,0,45,35,0,44
text-classification,9,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,introduction,1,46,36,0,36
text-classification,9,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",introduction,1,47,37,0,23
text-classification,9,Listed below are the three main strengths of the MCFA attachment .,introduction,0,48,38,0,12
text-classification,9,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",introduction,0,49,39,0,20
text-classification,9,( 2 ) MCFA is extensible and improves the accuracy as the number of translated sentences increases .,introduction,0,50,40,0,18
text-classification,9,"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .",introduction,0,51,41,0,20
text-classification,9,"Results show that a convolutional neural network ( CNN ) attached with MCFA significantly improves the classification performance of CNN , achieving state of the 1 Hereon , we mean to "" fix "" as to "" correct , repair , or alter . "" art performance over multiple data sets .",introduction,0,52,42,0,52
text-classification,9,preliminaries,introduction,0,53,43,0,1
text-classification,9,Problem : Translated Sentences as Context,introduction,0,54,44,0,6
text-classification,9,"In this paper , the ultimate task that we solve is the sentence classification task where given a sentence and a list of classes , one is task to classify which class ( e.g. positive or negative sentiment ) among the list of classes does the sentence belong .",introduction,0,55,45,0,49
text-classification,9,"However , the main challenge that we tackle is the task on how to utilize translated sentences as additional context in order to improve the performance of the classifier .",introduction,0,56,46,0,30
text-classification,9,"Specifically , the problem states : given the original sentence s , the goal is to use t 1 , t 2 , ... , tn , or sentences in other languages which are translated from s , as additional context .",introduction,0,57,47,0,42
text-classification,9,Base Model : Convolutional Neural Network .,introduction,0,58,48,0,7
text-classification,9,The base model used is the convolutional neural network ( CNN ) for sentences .,introduction,0,59,49,0,15
text-classification,9,It is a simple variation of the original CNN for texts to be used on sentences .,introduction,0,60,50,0,17
text-classification,9,let xi ?,introduction,0,61,51,0,3
text-classification,9,Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,introduction,0,62,52,0,19
text-classification,9,A convolution operation involves applying a filter matrix W ?,introduction,0,63,53,0,10
text-classification,9,R hd to a window of h words and producing a new feature vector c i using the equation,introduction,0,64,54,0,19
text-classification,9,bias vector and f ( . ) is a non-linear function .,introduction,0,65,55,0,12
text-classification,9,"By doing this on all possible windows of words we produce a feature map c = [ c 1 , c 2 , ... ] .",introduction,0,66,56,0,26
text-classification,9,We then apply a max - over - time pooling operation over the feature map and take the maximum value as the feature vector of the filter .,introduction,0,67,57,0,28
text-classification,9,We do this on all feature vectors and concatenate all the feature vectors to obtain the final feature vector v.,introduction,0,68,58,0,20
text-classification,9,We can then use this vector as input features to train a classifier such as logistic regression .,introduction,0,69,59,0,18
text-classification,9,"We use CNN to create sentence vectors for all sentences s , t 1 , t 2 , ... , tn .",introduction,0,70,60,0,22
text-classification,9,"From hereon , we refer to these vectors as v s , v t1 , v t2 , ... , v tn , respectively .",introduction,0,71,61,0,25
text-classification,9,We refer to them collectively as V .,introduction,0,72,62,0,8
text-classification,9,baseline 1 : naive concatenation .,introduction,0,73,63,0,6
text-classification,9,A simple method in order to use the translated sentences as additional context is to naively concatenate their vectors with the vector of the original sentence .,introduction,0,74,64,0,27
text-classification,9,"That is , we create a wide vectorv = [ v s ; v t1 ; ... ; v tn ] , and use this as the input feature vector of the sentence to the classifier .",introduction,0,75,65,0,37
text-classification,9,This method works fine if the translated sentences are translated properly .,introduction,0,76,66,0,12
text-classification,9,"However , sentences translated using machine translation models usually contain incorrect translation .",introduction,0,77,67,0,13
text-classification,9,"In effect , this method will have adverse effects on the over all performance of the classifier .",introduction,0,78,68,0,18
text-classification,9,This will especially be very evident if the number of additional sentences increases .,introduction,0,79,69,0,14
text-classification,9,baseline 2 : l2 regularization .,introduction,0,80,70,0,6
text-classification,9,"In order to alleviate the problems above , we can use L2 regularization to automatically select useful features by weakening the appropriate weights .",introduction,0,81,71,0,24
text-classification,9,The main problem of this method occurs when almost all of the weights coming from the vectors of the translated sentence are weakened .,introduction,0,82,72,0,24
text-classification,9,This leads to making the additional context vectors useless and to having a similar performance when there are no additional context .,introduction,0,83,73,0,22
text-classification,9,"Ultimately , this method does not make use of the full potential of the additional context .",introduction,0,84,74,0,17
text-classification,9,usability usability ( a ) Self and relative usability modules,introduction,0,85,75,0,10
text-classification,9,model,introduction,0,86,76,0,1
text-classification,9,"To solve the problems of the baselines discussed above , we introduce an attention - based neural multiple context fixing attachment ( MCFA ) 2 , a series of modules attached to the sentence vectors V .",introduction,0,87,77,0,37
text-classification,9,"MCFA attachment is used to fix the sentence vectors , by slightly modifying the per-dimension values of the vector , before concatenating them into the final feature vector .",introduction,0,88,78,0,29
text-classification,9,"The sentence vectors are altered using other sentence vectors as context ( e.g. v t 1 is altered using v s , v t2 , ... , v tn ) .",introduction,0,89,79,0,31
text-classification,9,This results to moving the vectors in the same vector space .,introduction,0,90,80,0,12
text-classification,9,The full architecture is shown in .,introduction,0,91,81,0,7
text-classification,9,self usability module,introduction,0,92,82,0,3
text-classification,9,"To fix a source sentence vector 3 , we use the other sentence vectors as guide to know which dimensions to fix and to what extent do we need to fix them .",introduction,0,93,83,0,33
text-classification,9,"However , other vectors might also contain errors which may reflect to the fixing of the source sentence vector .",introduction,0,94,84,0,20
text-classification,9,"In order to cope with this , we introduce self usability modules .",introduction,0,95,85,0,13
text-classification,9,"A self usability module contains the self usability of the vector ? i ( a ) , which measures how confident sentence a is for the task at hand .",introduction,0,96,86,0,30
text-classification,9,"For example , an ambiguous sentence ( e.g. "" The movie is terribly amazing "" ) may receive a low self usability , while a clear and definite sentence ( e.g. "" The movie is very good "" ) may receive a high self usability .",introduction,0,97,87,0,46
text-classification,9,"Mathematically , we calculate the self usability of the vector vi of sentence i , denoted as ? i ( v i ) , using the equation",introduction,0,98,88,0,27
text-classification,9,is a matrix to be learned .,introduction,0,99,89,0,7
text-classification,9,The produced value is a single real number from 0 to 1 .,introduction,0,100,90,0,13
text-classification,9,We pre-calculate the self usability of all sentence vectors vi ?,introduction,0,101,91,0,11
text-classification,9,v .,introduction,0,102,92,0,2
text-classification,9,"These are used in the next module , the relative usability module .",introduction,0,103,93,0,13
text-classification,9,relative usability,introduction,0,104,94,0,2
text-classification,9,module,introduction,0,105,95,0,1
text-classification,9,"Relative usability ? r ( a , b ) measures how useful a can be when fixing b , relative to other sentences .",introduction,0,106,96,0,24
text-classification,9,"There are two main differences between ? i ( a ) and ? r ( a , b ) .",introduction,0,107,97,0,20
text-classification,9,"First , ? i ( a ) is calculated before a knows about b while ? r ( a , b ) is calculated when a knows about b.",introduction,0,108,98,0,29
text-classification,9,"Second , ? r ( a , b ) can below even though ? i ( a ) is not .",introduction,0,109,99,0,21
text-classification,9,This means that a is notable to help in fixing the wrong information in b .,introduction,0,110,100,0,16
text-classification,9,"Here , we extend the additive attention module and use it as a method to calculate the relative usability of two sentences of different languages .",introduction,0,111,101,0,26
text-classification,9,"To better visualize the original attention mechanism , we present the equations below .",introduction,0,112,102,0,14
text-classification,9,One major challenge in using the attention mechanism in our problem is that the sentence vectors do not belong to the same vector space .,introduction,0,113,103,0,25
text-classification,9,"Moreover , one characteristic of our problem is that the sentence vectors can be both a source and a context vector ( e.g. v scan be both sand ti in Equation 1 ) .",introduction,0,114,104,0,34
text-classification,9,"Because of these , we can not directly use the additive attention module .",introduction,0,115,105,0,14
text-classification,9,We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ?,introduction,0,116,106,0,22
text-classification,9,"R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",introduction,0,117,107,0,37
text-classification,9,"Finally , we incorporate the self usability function ?",introduction,0,118,108,0,9
text-classification,9,i ( v k ) to reflect the self usability of a sentence .,introduction,0,119,109,0,14
text-classification,9,"Ultimately , the relative usability denoted as ? r ( v i , v j ) is calculated using the equations below , where is the multiplication of a vector and a scalar through broadcasting .",introduction,0,120,110,0,36
text-classification,9,vector fixing module,introduction,0,121,111,0,3
text-classification,9,The vector fixing module applies the attention weights to the sentence vectors and creates an integrated context vector .,introduction,0,122,112,0,19
text-classification,9,We then use this vector alongside with the source sentence vector to create a weighted gate vector .,introduction,0,123,113,0,18
text-classification,9,The weighted gate vector is used to determine to what extent should a dimension of the source sentence vector be altered .,introduction,0,124,114,0,22
text-classification,9,The common way to apply the attention weights to the context vectors and create an integrated context vector c i is to directly do weighted sum of all the context vectors .,introduction,0,125,115,0,32
text-classification,9,"However , this is not possible because the context vectors are not on the same space .",introduction,0,126,116,0,17
text-classification,9,"Thus , we use a projection matrix U k ?",introduction,0,127,117,0,10
text-classification,9,R dd to linearly project the sentence vector v k to transform the sentence vectors into a common vector space .,introduction,0,128,118,0,21
text-classification,9,The integrated context vector c i is then calculated as,introduction,0,129,119,0,10
text-classification,9,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ?",introduction,0,130,120,0,27
text-classification,9,R 2dd is a trainable parameter and ?,introduction,0,131,121,0,8
text-classification,9,is the element - wise multiplication procedure .,introduction,0,132,122,0,8
text-classification,9,The weighted gate vector is a vector of real numbers between 0 and 1 to modify the intensity of per-dimension values of the sentence vector .,introduction,0,133,123,0,26
text-classification,9,This causes the vector to move in the same vector space towards the correct direction .,introduction,0,134,124,0,16
text-classification,9,"An alternative approach to do vector correction is using a residual - style correction , where instead of multiplying agate vector , a residual vector is added to the original vector .",introduction,0,135,125,0,32
text-classification,9,"However , this approach makes the correction not interpretable ; it is hard to explain what does adding a value to a specific dimension mean .",introduction,0,136,126,0,26
text-classification,9,One major advantage of MCFA is that the corrections in the vectors are interpretable ; the weights in the gate vector correspond to the importance of the per-dimension features of the vector .,introduction,0,137,127,0,33
text-classification,9,"The altered vector ? v s , ... , v tn are then concatenated and fed directly as an input vector to the logistic regression classifier for training .",introduction,0,138,128,0,29
text-classification,9,experiments,experiment,0,139,1,0,1
text-classification,9,experimental setting,experiment,0,140,1,0,2
text-classification,9,We test our model on four different data sets as listed below and summarized in .,experiment,0,141,2,0,16
text-classification,9,( a ) MR 4 : Movie reviews data where the task is to classify whether the review sentence has positive or negative sentiment .,experiment,0,142,3,0,25
text-classification,9,( b ) SUBJ : Subjectivity data where the task is to classify whether the sentence is subjective or objective .,experiment,0,143,4,0,21
text-classification,9,( c ) CR 5 : Customer reviews where,experiment,0,144,5,0,9
text-classification,9,The task is to classify whether the review sentence is positive or negative .,experiment,0,145,6,0,14
text-classification,9,( d ) TREC 6 : TREC question data set the task is to classify the type of question .,experiment,0,146,7,0,20
text-classification,9,All our data sets are in English .,experiment,0,147,8,0,8
text-classification,9,"For the additional contexts , we use ten other languages , selected based on their diversity and their performance on prior experiments : Arabic , Finnish , French , Italian , Korean , Mongolian , Norwegian , Polish , Russian , and Ukranian .",experiment,0,148,9,0,44
text-classification,9,We translate the data sets using Google Translate .,experiment,0,149,10,0,9
text-classification,9,Tokenization is done using the polyglot library 7 .,experiment,0,150,11,0,9
text-classification,9,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,experiment,0,151,12,0,26
text-classification,9,"For N = 1 , we only show the accuracy of the best classifier for conciseness .",experiment,0,152,13,0,17
text-classification,9,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",experiment,1,153,14,0,31
text-classification,9,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",experiment,1,154,15,0,19
text-classification,9,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,experiment,1,155,16,0,14
text-classification,9,"We also use an l 2 constraint of 3 , following for accurate comparisons .",experiment,0,156,17,0,15
text-classification,9,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,experiment,0,157,18,0,17
text-classification,9,"During training , we use mini-batch size of 50 .",experiment,1,158,19,0,10
text-classification,9,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,experiment,1,159,20,0,16
text-classification,9,We perform early stopping using a random 10 % of the training set as the development set .,experiment,1,160,21,0,18
text-classification,9,"We present several competing models , listed below to compare the performance of our model .",experiment,0,161,22,0,16
text-classification,9,uses topics as additional contexts and changes the CNN architecture .,experiment,0,162,23,0,11
text-classification,9,TopCNN uses two types of topics : word- specific topic and sentence - specific topic ; and ( D ) CNN+ B1 and CNN +,experiment,0,163,24,0,25
text-classification,9,B2 are the two baselines presented in this paper .,experiment,0,164,25,0,10
text-classification,9,We do not show results from RNN models because they were shown to be less effective in sentence classification in our prior experiments .,experiment,0,165,26,0,24
text-classification,9,"For models with additional context , we further use an ensemble classification model using a commonly used method by averaging the class probability scores generated by the multiple variants ( in our model 's case , N = 1 and N = 10 models ) , following .",experiment,0,166,27,0,48
text-classification,9,results and discussion,result,0,167,1,0,3
text-classification,9,We report the classification accuracy of the competing models in .,result,0,168,2,0,11
text-classification,9,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,result,1,169,3,0,27
text-classification,9,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",result,1,170,4,0,31
text-classification,9,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .",result,1,171,5,0,19
text-classification,9,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .",result,1,172,6,0,16
text-classification,9,"We emphasize that we only use the basic CNN as our sentence encoder for our experiments , yet still achieve state of the art performance :",result,0,173,7,0,26
text-classification,9,Classification accuracies of competing models .,result,0,174,8,0,6
text-classification,9,"C refers to the additional context , N refers to the number of translations .",result,0,175,9,0,15
text-classification,9,"In TopCNN , word refers to using word - specific topic while sentence refers to using sentence - specific topic .",result,0,176,10,0,21
text-classification,9,Accuracies colored red are accuracies that perform worse than CNN .,result,0,177,11,0,11
text-classification,9,Previous state of the art results and the results of our best model are bold - faced .,result,0,178,12,0,18
text-classification,9,The winning result is underlined .,result,0,179,13,0,6
text-classification,9,"The number inside the parenthesis indicates the increase from the base model , CNN . on most data sets .",result,0,180,14,0,20
text-classification,9,"Hence , MCFA is successful in effectively using translations as additional context to improve the performance of the classifier .",result,0,181,15,0,20
text-classification,9,"We compare our model ( CNN + MCFA ) and the baselines discussed above ( CNN + B1 , CNN + B2 ) .",result,0,182,16,0,24
text-classification,9,"On all settings , our model outperforms the baselines .",result,0,183,17,0,10
text-classification,9,"When N = 10 , the performance of our model increases over the performance when N = 1 , however the performance of CNN + B1 decreases when compared to the performance when N = 1 .",result,0,184,18,0,37
text-classification,9,We also show the accuracies of the worst classifiers when N = 1 in .,result,0,185,19,0,15
text-classification,9,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .",result,0,186,20,0,34
text-classification,9,"This is resolved by CNN + B2 by applying L2 regularization , however the increase in performance is marginal .",result,0,187,21,0,20
text-classification,9,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .",result,0,188,22,0,30
text-classification,9,"Overall , we conclude that translations are better additional contexts than topics .",result,0,189,23,0,13
text-classification,9,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .",result,0,190,24,0,33
text-classification,9,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",result,0,191,25,0,27
text-classification,9,model interpretation,result,0,192,26,0,2
text-classification,9,We first provide examples shown in on how the self usability module determines the score of sentences .,result,0,193,27,0,18
text-classification,9,"In the first example , it is hard to classify whether the translated sentence is positive or negative , thus it is given a low self usability score .",result,0,194,28,0,29
text-classification,9,"In the second example , although the sentence contains mistranslations , these are minimal and may actually help the classifier by telling it that thirst for violence is not a attention ( negative sentence ) the mothman prophecies , which is mostly a bore , seems to exist only for its climactic setpiece . negative phrase .",result,0,195,29,0,57
text-classification,9,"Thus , it is given a high self usability score .",result,0,196,30,0,11
text-classification,9,shows two data instance examples where we show the attention weights given to the other contexts when fixing a Korean sentence .,result,0,197,31,0,22
text-classification,9,"The larger the attention weight is , the more the context is used to fix the Korean sentence .",result,0,198,32,0,19
text-classification,9,In the Original sentence : skip this turd and pick your nose instead because you 're sure to get more out of the latter experience .,result,0,199,33,0,26
text-classification,9,korean translation :,result,0,200,34,0,3
text-classification,9,human re-translation :,result,0,201,35,0,3
text-classification,9,"In order to get more from the latter experience , you need to skip this puddle and choose your nose .",result,0,202,36,0,21
text-classification,9,self,result,0,203,37,0,1
text-classification,9,Usability : 0.3958 ( a ) Low self usability example Original sentence : michael moore 's latest documentary about america 's thirst for violence is his best film yet . . .,result,0,204,38,0,32
text-classification,9,korean translation :,result,0,205,39,0,3
text-classification,9,human re-translation :,result,0,206,40,0,3
text-classification,9,"Michael Moore 's latest American documentary "" Violent Scene "" is his best film yet . . .",result,0,207,41,0,18
text-classification,9,self,result,0,208,42,0,1
text-classification,9,Usability : 1.0000 ( b ) High self usability example you know that ten bucks you 'd spend on a ticket ?,result,0,209,43,0,22
text-classification,9,just send it to cranky .,result,0,210,44,0,6
text-classification,9,we do n't get paid enough to sit through crap like this .,result,0,211,45,0,13
text-classification,9,nn ( altered ),result,0,212,46,0,4
text-classification,9,"after scenes of nonsense , you 'll be wistful for the testosteronecharged wizardry of jerry bruckheimer productions , especially because half past dead is like the rock on walmart budget . :",result,0,213,47,0,32
text-classification,9,"Two example sentences , from English ( first ) and Korean ( second ) vector spaces , and their nearest neighbors ( NN ) on both the unaltered and altered vector spaces .",result,0,214,48,0,33
text-classification,9,We only show the original English sentences for the Korean example for conciseness .,result,0,215,49,0,14
text-classification,9,"first example , the Korean sentence contains translation errors ; especially , the words bore and climactic setpiece were not translated and were only spelled using the Korean alphabet .",result,0,216,50,0,30
text-classification,9,"In this example , the English attention weight is larger than the Korean attention weight .",result,0,217,51,0,16
text-classification,9,"In the second example , the Korean sentence correctly translates all parts of the English sentence , except for the phrase as it does in trouble .",result,0,218,52,0,27
text-classification,9,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",result,0,219,53,0,24
text-classification,9,"Thus , the Korean attention weight is larger .",result,0,220,54,0,9
text-classification,9,shows the PCA visualization of the unaltered and the altered vectors of four different languages .,result,0,221,55,0,16
text-classification,9,"In the first example , the unaltered sentence vectors are mostly in the middle of the vector space , making it hard to draw a boundary between the two examples .",result,0,222,56,0,31
text-classification,9,"After the fixing , the boundary is much clearer .",result,0,223,57,0,10
text-classification,9,We also show the English sentence vectors in the second example .,result,0,224,58,0,12
text-classification,9,"Even without fixing the unaltered English sentence vectors , it is easy to distinguish both classes .",result,0,225,59,0,17
text-classification,9,"After the fix , the sentence vectors in the middle of the space are moved , making the distinction more obvious and clearer .",result,0,226,60,0,24
text-classification,9,We also provide quantitative evidence by showing that the Mahalanobis distance between the two classes in the altered vectors are significantly farther than that of the unaltered vectors .,result,0,227,61,0,29
text-classification,9,We also show two examples sentences from English and Korean vector spaces and their corresponding nearest neighbors on both the unaltered and altered vector spaces in Table 5 .,result,0,228,62,0,29
text-classification,9,"In the first example , the unaltered vector focuses on the meaning of "" wasted yours "" in the sentence , which puts it near sentences regarding wasted time or money .",result,0,229,63,0,32
text-classification,9,"After fixing , the sentence vector focuses its meaning on the slow yet worth - the - wait pace of the movie , thus moving it closer to the correct vectors .",result,0,230,64,0,32
text-classification,9,"In the second example , all three sentences have highly descriptive tones , however , the nearest neighbor on the altered space is hyperbolically negative , comparing the movie to a description unrelated to the movie itself .",result,0,231,65,0,38
text-classification,9,nn ( unaltered ),result,0,232,66,0,4
text-classification,9,"in the new release of cinema paradiso , the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .",result,0,233,67,0,35
text-classification,9,related work,related work,0,234,1,0,2
text-classification,9,One way to improve the performance of a sentence classifier is to introduce new context .,related work,0,235,2,0,16
text-classification,9,"Common and obvious kinds of context are the neighboring sentences of the sentence , and the document where the sentence belongs .",related work,0,236,3,0,22
text-classification,9,Topics of the words in the sentence induced by a topic model were also used as contexts .,related work,0,237,4,0,18
text-classification,9,"In this paper , we introduce yet another type of additional context , sentence translations , which to the best of our knowledge have not been used previously .",related work,0,238,5,0,29
text-classification,9,Sentence encoders trained from neural machine translation ( NMT ) systems were also used for transfer learning .,related work,0,239,6,0,18
text-classification,9,demonstrated that altered - length sentence vectors from NMT encoders outperform sentence vectors from monolingual encoders on semantic similarity tasks .,related work,0,240,7,0,21
text-classification,9,Recent work used representation of each word in the sentence to create a sentence representation suitable for multiple NLP tasks .,related work,0,241,8,0,21
text-classification,9,"Our work shares the commonality of using NMT for another task , but instead of using NMT to encode our sentences , we use it to translate the sentences into new contexts .",related work,0,242,9,0,33
text-classification,9,Increasing the number of data instances of the training set has also been explored to improve the performance of a classifier .,related work,0,243,10,0,22
text-classification,9,"Recent methods include the usage of thesaurus , paraphrases , among others .",related work,0,244,11,0,13
text-classification,9,These simple variation techniques are preferred because they are found to be very effective despite their simplicity .,related work,0,245,12,0,18
text-classification,9,"Our work similarly augments training data , not by adding data instances ( vertical augmentation ) , but rather by adding more context ( horizontal augmentation ) .",related work,0,246,13,0,28
text-classification,9,"Though the paraphrase of p can be alternatively used as an augmented context , this could not leverage the added semantics coming from another language , as discussed in Section 1 .",related work,0,247,14,0,32
text-classification,9,conclusion,related work,0,248,15,0,1
text-classification,9,This paper investigates the use of translations as better additional contexts for sentence classification .,related work,0,249,16,0,15
text-classification,9,"To answer the problem on mistranslations , we propose multiple context fixing attachment ( MCFA ) to fix the context vectors using other context vectors .",related work,0,250,17,0,26
text-classification,9,We show that our method improves the classification performance and achieves state - of - the - art perfor - mance on multiple data sets .,related work,0,251,18,0,26
text-classification,9,"In our future work , we plan to use and extend our model to other complex NLP tasks .",related work,0,252,19,0,19
text-classification,1,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,title,1,2,1,0,11
text-classification,1,abstract,abstract,0,3,1,0,1
text-classification,1,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",abstract,1,4,2,1,28
text-classification,1,We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ' text region embedding + pooling ' .,abstract,0,5,3,0,32
text-classification,1,"Under this framework , we explore a more sophisticated region embedding method using Long Short - Term Memory ( LSTM ) .",abstract,0,6,4,0,22
text-classification,1,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",abstract,0,7,5,0,26
text-classification,1,We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings .,abstract,0,8,6,0,18
text-classification,1,The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data .,abstract,0,9,7,0,22
text-classification,1,"The results indicate that on this task , embeddings of text regions , which can convey complex concepts , are more useful than embeddings of single words in isolation .",abstract,0,10,8,0,30
text-classification,1,We report performances exceeding the previous best results on four benchmark datasets .,abstract,0,11,9,0,13
text-classification,1,introduction,introduction,0,12,1,0,1
text-classification,1,"Text categorization is the task of assigning labels to documents written in a natural language , and it has numerous real - world applications including sentiment analysis as well as traditional topic assignment tasks .",introduction,0,13,2,0,35
text-classification,1,"The state - of - the art methods for text categorization had long been linear predictors ( e.g. , SVM with a linear kernel ) with either bag - ofword or bag - of - n- gram vectors ( hereafter bow ) as input , e.g. , .",introduction,0,14,3,0,48
text-classification,1,"This , however , A convolutional neural network ( CNN ) ) is a feedforward neural network with convolution layers interleaved with pooling layers , originally developed for image processing .",introduction,0,15,4,0,31
text-classification,1,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .",introduction,0,16,5,0,45
text-classification,1,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .",introduction,0,17,6,0,22
text-classification,1,"In its simplest form , onehot CNN works as follows .",introduction,0,18,7,0,11
text-classification,1,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",introduction,0,19,8,0,90
text-classification,1,The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,introduction,0,20,9,0,19
text-classification,1,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",introduction,1,21,10,0,41
text-classification,1,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) ,",introduction,0,22,11,0,27
text-classification,1,"where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",introduction,0,23,12,0,52
text-classification,1,"It is simple and fast to compute , and considering its simplicity , the method works surprisingly well if the region size is appropriately set .",introduction,0,24,13,0,26
text-classification,1,"However , there are also potential shortcomings .",introduction,0,25,14,0,8
text-classification,1,"The region size must be fixed , which may not be optimal as the size of relevant regions may vary .",introduction,0,26,15,0,21
text-classification,1,"Practically , the region size can not be very large as the number of parameters to be learned ( components of W ) depends on it .",introduction,0,27,16,0,27
text-classification,1,JZ15 proposed variations to alleviate these issues .,introduction,0,28,17,0,8
text-classification,1,"For example , a bow - input variation allows x above to be a bow vector of the region .",introduction,0,29,18,0,20
text-classification,1,"This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",introduction,0,30,19,0,24
text-classification,1,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",introduction,1,31,20,0,48
text-classification,1,LSTM ) is a recurrent neural network .,introduction,0,32,21,0,8
text-classification,1,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ?",introduction,0,33,22,0,39
text-classification,1,"1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .",introduction,0,34,23,0,36
text-classification,1,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,introduction,0,35,24,0,19
text-classification,1,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",introduction,0,36,25,0,21
text-classification,1,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",introduction,1,37,26,0,31
text-classification,1,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",introduction,1,38,27,0,27
text-classification,1,our findings are threefold .,introduction,0,39,28,0,5
text-classification,1,"First , in the supervised setting , our simplification strategy leads to higher accuracy and faster training than previous LSTM .",introduction,0,40,29,0,21
text-classification,1,"Second , accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input .",introduction,0,41,30,0,26
text-classification,1,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .",introduction,0,42,31,0,19
text-classification,1,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .",introduction,0,43,32,0,34
text-classification,1,"Overall , our results show that for text categorization , embeddings of text regions , which can convey higher - level concepts than single words in isolation , are useful , and that useful region embeddings can be learned without going through word embedding learning .",introduction,0,44,33,0,46
text-classification,1,We report performances exceeding the previous best results on four benchmark datasets .,introduction,0,45,34,0,13
text-classification,1,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,introduction,0,46,35,0,11
text-classification,1,preliminary,introduction,0,47,36,0,1
text-classification,1,"On text , LSTM has been used for labeling or generating words .",introduction,0,48,37,0,13
text-classification,1,"It has been also used for representing short sentences mostly for sentiment analysis , and some of them rely on syntactic parse trees ; see e.g. , .",introduction,0,49,38,0,28
text-classification,1,"Unlike these studies , this work as well as JZ15 focuses on classifying general full - length documents without any special linguistic knowledge .",introduction,0,50,39,0,24
text-classification,1,"Similarly , DL15 applied LSTM to categorizing general full - length documents .",introduction,0,51,40,0,13
text-classification,1,"Therefore , our empirical comparisons will focus on DL15 and JZ15 , both of which reported new state of the art results .",introduction,0,52,41,0,23
text-classification,1,"Let us first introduce the general LSTM formulation , and then briefly describe DL15 's model as it illustrates the challenges in using LSTMs for this task .",introduction,0,53,42,0,28
text-classification,1,lstm,introduction,0,54,43,0,1
text-classification,1,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. ,",introduction,0,55,44,0,22
text-classification,1,where denotes element - wise multiplication and ?,introduction,0,56,45,0,8
text-classification,1,"is an element - wise squash function to make the gating values in [ 0 , 1 ] .",introduction,0,57,46,0,19
text-classification,1,we fix ? to sigmoid .,introduction,0,58,47,0,6
text-classification,1,x t ?,introduction,0,59,48,0,3
text-classification,1,"Rd is the input from the lower layer at time step t , where d would be , for example , size of vocabulary if the input was a one - hot vector representing a word , or the dimensionality of word vector if the lower layer was a word embedding layer .",introduction,0,60,49,0,53
text-classification,1,"With q LSTM units , the dimensionality of the weight matrices and bias vectors , which need to be trained , are W ( ) ? R qd , U ( ) ? R qq , and b ( ) ?",introduction,0,61,50,0,41
text-classification,1,R q for all types .,introduction,0,62,51,0,6
text-classification,1,"The centerpiece of LSTM is the memory cells ct , designed to counteract the risk of vanishing / exploding gradients , thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks .",introduction,0,63,52,0,37
text-classification,1,The forget gate ft is for resetting the memory cells .,introduction,0,64,53,0,11
text-classification,1,The input gate it and output gate o t control the input and output of the memory cells .,introduction,0,65,54,0,19
text-classification,1,Word - vector LSTM ( wv - LSTM ) [ DL15 ] DL15 's application of LSTM to text categorization is straightforward .,introduction,0,66,55,0,23
text-classification,1,"As illustrated in , for each document , the output of the LSTM layer is the output of the last time step ( corresponding to the last word of the document ) , which represents the whole document ( document embedding ) .",introduction,0,67,56,0,43
text-classification,1,"Like many other studies of LSTM on text , words are first converted to low - dimensional dense word vectors via a word embedding layer ; therefore , we call it word - vector LSTM or wv - LSTM .",introduction,0,68,57,0,40
text-classification,1,DL15 observed that wv - LSTM underperformed linear predictors and its training was unstable .,introduction,0,69,58,0,15
text-classification,1,This was attributed to the fact that documents are long .,introduction,0,70,59,0,11
text-classification,1,"In addition , we found that training and testing of wv - LSTM is time / resource consuming .",introduction,0,71,60,0,19
text-classification,1,"To put it into perspective , using a GPU , one epoch of wv - LSTM training takes nearly 20 times longer than that of one - hot CNN training even though it achieves poorer accuracy ( the first two rows of ) .",introduction,0,72,61,0,44
text-classification,1,"This is due to the sequential nature of LSTM , i.e. , computation at time t requires the output of time t ? 1 , whereas modern computation depends on parallelization for speedup .",introduction,0,73,62,0,34
text-classification,1,"Documents in a mini-batch can be processed in parallel , but the variability of document lengths reduces the degree of parallelization 1 .",introduction,0,74,63,0,23
text-classification,1,It was shown in DL15 that training becomes stable and accuracy improves drastically when LSTM and the word embedding layer are jointly pre-trained with either the language model learning objective ( predicting the next word ) or autoencoder objective ( memorizing the document ) .,introduction,0,75,64,0,45
text-classification,1,Supervised LSTM for text categorization,introduction,0,76,65,0,5
text-classification,1,"Within the framework of ' region embedding + pooling ' for text categorization , we seek effective and efficient use of LSTM as an alternative region embedding method .",introduction,0,77,66,0,29
text-classification,1,"This section focuses on an end - to - end supervised setting so that there is no additional data ( e.g. , unlabeled data ) or additional algorithm ( e.g. , for learning a word embedding ) .",introduction,0,78,67,0,38
text-classification,1,Our general strategy is to simplify the model as much as possible .,introduction,0,79,68,0,13
text-classification,1,"We start with elimination of the word embedding layer so that one - hot vectors are directly fed to LSTM , which we call one - hot LSTM in short .",introduction,0,80,69,0,31
text-classification,1,Elimination of the word embedding layer,introduction,0,81,70,0,6
text-classification,1,Facts : A word embedding is a linear operation that can be written as Vx t with x t being a one - hot vector and columns of V being word vectors .,introduction,0,82,71,0,33
text-classification,1,"Therefore , by replacing the LSTM weights W ( ) with W ( ) V and removing the word embedding layer , a word - vector LSTM can be turned into a one - hot LSTM without changing the model behavior .",introduction,0,83,72,0,42
text-classification,1,"Thus , word - vector LSTM is not more expressive than one - hot LSTM ; rather , a merit , if any , of training with a word embedding layer would be through imposing restrictions ( e.g. , a low - rank V makes a less expressive model ) to achieve good prior / regularization effects .",introduction,0,84,73,0,58
text-classification,1,"In the end - to - end supervised setting , a word embedding matrix V would need to be initialized randomly and trained as part of the model .",introduction,0,85,74,0,29
text-classification,1,"In the preliminary experiments under our framework , we were unable to improve accuracy over one - hot LSTM by inclusion of such a randomly initialized word embedding layer ; i.e. , random vectors failed to provide good prior effects .",introduction,0,86,75,0,41
text-classification,1,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .",introduction,0,87,76,0,34
text-classification,1,"If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",introduction,0,88,77,0,24
text-classification,1,"We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .",introduction,0,89,78,0,30
text-classification,1,"Altogether , elimination of the word embedding layer was found to be useful ; thus , we base our work on one - hot LSTM .",introduction,0,90,79,0,26
text-classification,1,more simplifications,introduction,0,91,80,0,2
text-classification,1,We introduce four more useful modifications to wv - LSTM that lead to higher accuracy or faster training .,introduction,0,92,81,0,19
text-classification,1,pooling : simplifying sub - problems,introduction,0,93,82,0,6
text-classification,1,Our framework of ' region embedding + pooling ' has a simplification effect as follows .,introduction,0,94,83,0,16
text-classification,1,"In wv - LSTM , the sub-problem that LSTM needs to solve is to represent the entire document by one vector ( document embedding ) .",introduction,0,95,84,0,26
text-classification,1,We make this easy by changing it to detecting regions of text ( of arbitrary sizes ) thatare relevant to the task and representing them by vectors ( region embedding ) .,introduction,0,96,85,0,32
text-classification,1,"As illustrated in , we let the LSTM layer emit vectors ht at each time step , and let pooling aggregate them into a document vector .",introduction,0,97,86,0,27
text-classification,1,"With wv - LSTM , LSTM has to remember relevant information until it gets to the end of the document even if relevant information was observed 10 K words away .",introduction,0,98,87,0,31
text-classification,1,The task of our LSTM is easier as it is allowed to forget old things via the forget gate and can focus on representing the concepts conveyed by smaller segments such as phrases or sentences .,introduction,0,99,88,0,36
text-classification,1,A related architecture appears in the Deep Learning Tutorials 2 though it uses a word embedding .,introduction,0,100,89,0,17
text-classification,1,"Another related work is , which combined pooling with non -LSTM recurrent networks and a word embedding .",introduction,0,101,90,0,18
text-classification,1,Chopping for speeding up training,introduction,0,102,91,0,5
text-classification,1,"In addition to simplifying the sub-problem , pooling has the merit of enabling faster training via chopping .",introduction,0,103,92,0,18
text-classification,1,"Since we set the goal of LSTM to embedding text regions instead of documents , it is no longer crucial to go through the document from the beginning to the end sequentially .",introduction,0,104,93,0,33
text-classification,1,"At the time of training , we can chop each document into segments of a fixed length that is sufficiently long ( e.g. , 50 or 100 ) and process all the segments in a mini batch in parallel as if these segments were individual documents .",introduction,0,105,94,0,47
text-classification,1,( Note that this is done only in the LSTM layer and pooling is done over the entire document . ),introduction,0,106,95,0,21
text-classification,1,We perform testing without chopping .,introduction,0,107,96,0,6
text-classification,1,"That is , we train LSTM with approximations of sequences for speedup and test with real sequences for better accuracy .",introduction,0,108,97,0,21
text-classification,1,"There is a risk of chopping important phrases ( e.g. , "" do n't | like it "" ) , and this can be easily avoided by having segments slightly overlap .",introduction,0,109,98,0,32
text-classification,1,"However , we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping .",introduction,0,110,99,0,24
text-classification,1,Removing the input / output gates,introduction,0,111,100,0,6
text-classification,1,"We found that when LSTM is followed by pooling , the presence of input and output gates typically does not improve accuracy , while removing them nearly halves the time and memory required for training and testing .",introduction,0,112,101,0,38
text-classification,1,"It is intuitive , in particular , that pooling can make the output gate unnecessary ; the role of the output gate is to prevent undesirable information from entering the output ht , and such irrelevant information can be filtered out by max - pooling .",introduction,0,113,102,0,46
text-classification,1,"Without the input and output gates , the LSTM formulation can be simplified to :",introduction,0,114,103,0,15
text-classification,1,( 2 ),introduction,0,115,104,0,3
text-classification,1,This is equivalent to fixing it and o t to all ones .,introduction,0,116,105,0,13
text-classification,1,"It is in spirit similar to Gated Recurrent Units but simpler , having fewer gates .",introduction,0,117,106,0,16
text-classification,1,Bidirectional LSTM for better accuracy,introduction,0,118,107,0,5
text-classification,1,The changes from wv - LSTM above substantially reduce the time and 2 http://deeplearning.net/tutorial/lstm.html,introduction,0,119,108,0,14
text-classification,1,One - hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement .,introduction,0,120,109,0,28
text-classification,1,"As shown in , we concatenate the output of a forward LSTM ( left to right ) and a backward LSTM ( right to left ) , which is referred to as bidirectional LSTM in the literature .",introduction,0,121,110,0,38
text-classification,1,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .",introduction,0,122,111,0,23
text-classification,1,"shows how much accuracy and / or training speed can be improved by elimination of the word embedding layer , pooling , chopping , removing the input / output gates , and adding the backward LSTM .",introduction,0,123,112,0,37
text-classification,1,experiments ( supervised ),experiment,1,124,1,0,4
text-classification,1,"We used four datasets : IMDB , Elec , RCV1 ( second - level topics ) , and 20 - newsgroup ( 20 NG ) 3 , to facilitate direct comparison with JZ15 and DL15 .",experiment,0,125,2,0,36
text-classification,1,The first three were used in JZ15 .,experiment,0,126,3,0,8
text-classification,1,IMDB and 20 NG were used in DL15 .,experiment,0,127,4,0,9
text-classification,1,The datasets are summarized in .,experiment,0,128,5,0,6
text-classification,1,The data was converted to lower - case letters .,experiment,0,129,6,0,10
text-classification,1,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .",experiment,0,130,7,0,46
text-classification,1,"Data. "" avg "" / "" max "" : the average / maximum length of documents ( #words ) of the training / test data .",experiment,0,131,8,0,26
text-classification,1,"IMDB and Elec are for sentiment classification ( positive vs. negative ) of movie reviews and Amazon electronics product reviews , respectively .",experiment,0,132,9,0,23
text-classification,1,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .",experiment,0,133,10,0,25
text-classification,1,zero mean and standard deviation 0.01 .,experiment,0,134,11,0,7
text-classification,1,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,experiment,0,135,12,0,19
text-classification,1,"Hyper parameters such as learning rates were chosen based on the performance on the development data , which was a held - out portion of the training data , and training was redone using all the training data with the chosen parameters .",experiment,0,136,13,0,43
text-classification,1,"We used the same pooling method as used in JZ15 , which parameterizes the number of pooling regions so that pooling is done fork non-overlapping regions of equal size , and the resulting k vectors are concatenated to make one vector per document .",experiment,0,137,14,0,44
text-classification,1,"The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",experiment,0,138,15,0,53
text-classification,1,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",experiment,1,139,16,0,47
text-classification,1,Now we review the non -LSTM baseline methods .,experiment,0,140,17,0,9
text-classification,1,The last row of shows the best one - hot CNN results within the constraints above .,experiment,0,141,18,0,17
text-classification,1,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .",experiment,0,142,19,0,47
text-classification,1,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",experiment,1,143,20,0,20
text-classification,1,"However , on RCV1 , it underperforms both .",experiment,0,144,21,0,9
text-classification,1,We conjecture that this is because strict word order is not very useful on RCV1 .,experiment,0,145,22,0,16
text-classification,1,This point can also be observed in the SVM and CNN performances .,experiment,0,146,23,0,13
text-classification,1,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",experiment,0,147,24,0,28
text-classification,1,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",experiment,0,148,25,0,26
text-classification,1,This is presumably because the former can more easily cover variability of expressions indicative of topics .,experiment,0,149,26,0,17
text-classification,1,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .",experiment,0,150,27,0,22
text-classification,1,More on one - hot CNN vs. one - hot LSTM LSTM can embed regions of variable ( and possibly large ) sizes whereas CNN requires the region size to be fixed .,experiment,0,151,28,0,33
text-classification,1,We attribute to this fact the small improvements of oh - 2 LSTMp over oh - CNN in .,experiment,0,152,29,0,19
text-classification,1,"However , this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes .",experiment,0,153,30,0,19
text-classification,1,We show in the table above that one - hot CNNs with two layers ( of 1000 feature maps each ) with two different region sizes 4 rival oh - 2 LST Mp .,experiment,0,154,31,0,34
text-classification,1,"Although these models are larger than those in , training / testing is still faster than the LSTM models due to simplicity of the region embeddings .",experiment,0,155,32,0,27
text-classification,1,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .",experiment,0,156,33,0,20
text-classification,1,This maybe because the amount of training data is not sufficient enough to learn the relevance of longer word sequences .,experiment,0,157,34,0,21
text-classification,1,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",experiment,0,158,35,0,31
text-classification,1,Comparison with the previous best results on 20 NG,experiment,0,159,36,0,9
text-classification,1,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",experiment,0,160,37,0,32
text-classification,1,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",experiment,0,161,38,0,14
text-classification,1,"The previous best results on the other datasets use unlabeled data , and we will review them with our semi-supervised results .",experiment,0,162,39,0,22
text-classification,1,semi-supervised lstm,experiment,0,163,40,0,2
text-classification,1,"To exploit unlabeled data as an additional resource , we use a non-linear extension of two - view feature learning , whose linear version appeared in our earlier work .",experiment,0,164,41,0,30
text-classification,1,This was used in JZ15 b to learn from unlabeled data a region embedding embodied by a convolution layer .,experiment,0,165,42,0,20
text-classification,1,In this work we use it to learn a region embedding embodied by a one - hot LSTM .,experiment,0,166,43,0,19
text-classification,1,Let us start with a brief review of non-linear two - view feature learning .,experiment,0,167,44,0,15
text-classification,1,Two - view embedding ( tv-embedding ) [ JZ15 b ],experiment,0,168,45,0,11
text-classification,1,A rough sketch is as follows .,experiment,0,169,46,0,7
text-classification,1,Consider two views of the input .,experiment,0,170,47,0,7
text-classification,1,An embedding is called a tv-embedding if the embedded view is as good as the original view for the purpose of predicting the other view .,experiment,0,171,48,0,26
text-classification,1,"If the two views and the labels ( classification targets ) are related to one another only through some hidden states , then the tv-embedded view is as good as the original view for the purpose of classification .",experiment,0,172,49,0,39
text-classification,1,Such an embedding is useful provided that its dimensionality is much lower than the original view .,experiment,0,173,50,0,17
text-classification,1,JZ15 b applied this idea by regarding text regions embedded by the convolution layer as one view and their surrounding context as the other view and training a tv-embedding ( embodied by a convolution layer ) on unlabeled data .,experiment,0,174,51,0,40
text-classification,1,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .",experiment,0,175,52,0,25
text-classification,1,"we consider the following two views : the words we have already seen in the document ( view - 1 ) , and the next few words ( view - 2 ) .",experiment,0,176,53,0,33
text-classification,1,The task of tv-embedding learning is to predict view - 2 based on view - 1 .,experiment,0,177,54,0,17
text-classification,1,"We train one - hot LSTMs in both directions , as in , on unlabeled data .",experiment,0,178,55,0,17
text-classification,1,"For this purpose , we use the input and output gates as well as the forget gate as we found them to be useful .",experiment,0,179,56,0,25
text-classification,1,learning lstm tv-embeddings,experiment,0,180,57,0,3
text-classification,1,The theory of tv-embedding says that the region embeddings obtained in this way are useful for the task of interest if the two views are related to each other through the concepts relevant to the task .,experiment,0,181,58,0,37
text-classification,1,"To reduce undesirable relations between the views such as syntactic relations , JZ15 b performed vocabulary control to remove function words from ( and only from ) the vocabulary of the target view , which we found useful also for LSTM .",experiment,0,182,59,0,42
text-classification,1,We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :,experiment,0,183,60,0,24
text-classification,1,) .,experiment,0,184,61,0,2
text-classification,1,"x j t is the output of a tv-embedding ( an LSTM trained with unlabeled data ) indexed by j at time step t , and S is a set of tv-embeddings which contains the two LSTMs going forward and backward as in .",experiment,0,185,62,0,44
text-classification,1,"Although it is possible to fine - tune the tv-embeddings with labeled data , for simplicity and faster training , we fixed them in our experiments .",experiment,0,186,63,0,27
text-classification,1,Combining LSTM tv-embeddings and CNN tv-embeddings,experiment,0,187,64,0,6
text-classification,1,"It is easy to see that the set S above can be expanded with any tv-embeddings , not only those in the form of LSTM ( LSTM tv-embeddings ) but also with the tv-embeddings in the form of convolution layers ( CNN tv-embeddings ) such as those obtained in JZ15 b .",experiment,0,188,65,0,52
text-classification,1,"Similarly , it is possible to use LSTM tv-embeddings to produce additional input to CNN .",experiment,0,189,66,0,16
text-classification,1,"While both LSTM tv-embeddings and CNN tv-embeddings are region embeddings , their formulations are very different from each other ; therefore , we expect that they complement each other and bring further performance improvements when combined .",experiment,0,190,67,0,37
text-classification,1,We will empirically confirm these conjectures in the experiments below .,experiment,0,191,68,0,11
text-classification,1,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,experiment,0,192,69,0,19
text-classification,1,6.66 6.08 9.24 5 oh -CNN 1200 - dim CNN tv-embed .,experiment,0,193,70,0,12
text-classification,1,"6.81 6.57 7.97 our framework , which uses unlabeled data to produce additional input to LSTM instead of pre-training .",experiment,0,194,71,0,20
text-classification,1,semi-supervised experiments,experiment,1,195,1,0,2
text-classification,1,"We used IMDB , Elec , and RCV1 for our semi-supervised experiments ; 20 NG was excluded due to the absence of standard unlabeled data .",experiment,0,196,2,0,26
text-classification,1,summarizes the unlabeled data .,experiment,0,197,3,0,5
text-classification,1,"To experiment with LSTM tv-embeddings , we trained two LSTMs ( forward and backward ) with 100 units each on unlabeled data .",experiment,0,198,4,0,23
text-classification,1,The training objective was to predict the next k words where k was set to 20 for RCV1 and 5 for others .,experiment,0,199,5,0,23
text-classification,1,"Similar to JZ15 b , we minimized weighted square",experiment,0,200,6,0,9
text-classification,1,") 2 where i goes through the time steps , z represents the next k words by a bow vector , and p is the model output ; ?",experiment,0,201,7,0,29
text-classification,1,"i , j were set to achieve negative sampling effect for speed - up ; vocabulary control was performed for reducing undesirable relations between views , which sets the vocabulary of the target ( i.e. , the k words ) to the 30 K most frequent words excluding function words ( or stop words on RCV1 ) .",experiment,0,202,8,0,58
text-classification,1,Other details followed the supervised experiments .,experiment,0,203,9,0,7
text-classification,1,"Our semi-supervised one - hot bidirectional LSTM with pooling ( oh - 2 LSTM p ) in row # 4 of used the two LSTM tv-embeddings trained on unlabeled data as described above , to produce additional input to one - hot LSTMs in two directions ( 500 units each ) .",experiment,0,204,10,0,52
text-classification,1,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .",experiment,0,205,11,0,28
text-classification,1,We review the semi-supervised performance of wv - LSTMs ) .,experiment,0,206,12,0,11
text-classification,1,"In DL15 the model consisted of a word embedding layer of 512 dimensions , an LSTM layer with 1024 units , and 30 hidden units on top of the LSTM layer ; the word embedding layer and the LSTM were pre-trained with unlabeled data and were fine - tuned with labeled data ; pre-training used either the language model objective or autoencoder objective .",experiment,0,207,13,0,64
text-classification,1,"The error rate on IMDB is from DL15 , and those on Elec and RCV1 are our best effort to perform pre-training with the language model objective .",experiment,0,208,14,0,28
text-classification,1,"We used the same configuration on Elec as DL15 ; however , on RCV1 , which has 55 classes , 30 hidden units turned out to be too few and we changed it to 1000 .",experiment,0,209,15,0,36
text-classification,1,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .",experiment,0,210,16,0,22
text-classification,1,"Previous studies on LSTM for text often convert words into pre-trained word vectors , and word2vec is a popular choice for this purpose .",experiment,0,211,17,0,24
text-classification,1,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .",experiment,1,212,18,0,40
text-classification,1,The word vectors were optionally updated ( finetuned ) during training .,experiment,0,213,19,0,12
text-classification,1,Two types of word vectors were tested .,experiment,0,214,20,0,8
text-classification,1,The Google News word vectors were trained by word2vec on a huge ( 100 billion - word ) news corpus and are provided publicly .,experiment,0,215,21,0,25
text-classification,1,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",experiment,0,216,22,0,22
text-classification,1,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .",experiment,0,217,23,0,22
text-classification,1,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .",experiment,0,218,24,0,27
text-classification,1,"We attribute the superiority of the models with tv-embeddings to the fact that they learn , from unlabeled data , embeddings of text regions , which can convey higher - level concepts than single words in isolation .",experiment,0,219,25,0,38
text-classification,1,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .",experiment,1,220,26,0,48
text-classification,1,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,experiment,0,221,27,0,27
text-classification,1,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",experiment,0,222,28,0,28
text-classification,1,"As discussed earlier , we attribute the superiority of one - hot CNN on RCV1 to its unique way of representing parts of documents via bow input .",experiment,0,223,29,0,28
text-classification,1,Experiments combining CNN tv-embeddings and LSTM tv-embeddings,experiment,0,224,30,0,7
text-classification,1,In Section 3.3 we noted that LSTM tv-embeddings and CNN tv-embeddings can be naturally combined .,experiment,0,225,31,0,16
text-classification,1,We experimented with this idea in the following two settings ..,experiment,0,226,32,0,11
text-classification,1,Comparison with previous best results .,experiment,0,227,33,0,6
text-classification,1,error rates ( % ) .,experiment,0,228,34,0,6
text-classification,1,""" U "" : Was unlabeled data used ?",experiment,0,229,35,0,9
text-classification,1,""" Co - tr. optimized "" : co-training using oh - CNN as a base learner with parameters ( e.g. , when to stop ) optimized on the test data ; it demonstrates the difficulty of exploiting unlabeled data on these tasks .",experiment,0,230,36,0,43
text-classification,1,"In one setting , oh - 2 LSTMp takes additional input from five embeddings : two LSTM tv-embeddings used in and three CNN tv-embeddings from JZ15 b obtained by three distinct combinations of training objectives and input representations , which are publicly provided .",experiment,0,231,37,0,44
text-classification,1,"These CNN tv-embeddings were trained to be applied to text regions of size k at every location taking bow input , where k is 5 on IMDB / Elec and 20 on RCV1 .",experiment,0,232,38,0,34
text-classification,1,"We connect each of the CNN tv-embeddings to an LSTM by aligning the centers of the regions of the former with the LSTM time steps ; e.g. , the CNN tv-embedding result on the first five words is passed to the LSTM at the time step on the third word .",experiment,0,233,39,0,51
text-classification,1,"In the second setting , we trained one - hot CNN with these five types of tv-embeddings by replacing ( 1 ) max ( 0 , Wx + b ) by max ( 0 , Wx + j W ( j ) x j + b ) where x j is the output of the j - th tv-embedding with the same alignment as above .",experiment,0,234,40,0,66
text-classification,1,Rows 3 - 4 of show the results of these two types of models .,experiment,0,235,41,0,15
text-classification,1,"For comparison , we also show the results of the LSTM with LSTM tv-embeddings only ( row# 1 ) and the CNN with CNN tv-embeddings only ( row # 2 ) .",experiment,0,236,42,0,32
text-classification,1,"To see the effects of combination , compare row# 3 with row# 1 , and compare row # 4 with row # 2 .",experiment,0,237,43,0,24
text-classification,1,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .",experiment,0,238,44,0,49
text-classification,1,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .",experiment,0,239,45,0,22
text-classification,1,Comparison with the previous best results,experiment,0,240,46,0,6
text-classification,1,The previous best results in the literature are shown in Table 7 .,experiment,0,241,47,0,13
text-classification,1,"More results of previous semi-supervised models can be found in JZ15b , all of which clearly underperform the semi-supervised one - hot CNN of .",experiment,0,242,48,0,25
text-classification,1,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .",experiment,0,243,49,0,29
text-classification,1,"Many more of the previous results on IMDB can be found in , all of which are over 10 % except for 8.78 by bi-gram NBSVM .",experiment,0,244,50,0,27
text-classification,1,7.42 by paragraph vectors ) and 6.51 by JZ15 b were considered to be large improvements .,experiment,0,245,51,0,17
text-classification,1,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .",experiment,0,246,52,0,32
text-classification,1,conclusion,experiment,0,247,53,0,1
text-classification,1,"Within the general framework of ' region embedding + pooling ' for text categorization , we explored region embeddings via one - hot LSTM .",experiment,0,248,54,0,25
text-classification,1,"The region embedding of onehot LSTM rivaled or outperformed that of the state - of - the art one - hot CNN , proving its effectiveness .",experiment,0,249,55,0,27
text-classification,1,We also found that the models with either one of these two types of region embedding strongly outperformed other methods including previous LSTM .,experiment,0,250,56,0,24
text-classification,1,"The best results were obtained by combining the two types of region embedding trained on unlabeled data , suggesting that their strengths are complementary .",experiment,0,251,57,0,25
text-classification,1,"As a result , we reported substantial improvements over the previous best results on benchmark datasets .",experiment,0,252,58,0,17
text-classification,1,"At a high level , our results indicate the following .",experiment,0,253,59,0,11
text-classification,1,"First , on this task , embeddings of text regions , which can convey higher - level concepts , are more useful than embeddings of single words in isolation .",experiment,0,254,60,0,30
text-classification,1,"Second , useful region embeddings can be learned by working with one - hot vectors directly , either on labeled data or unlabeled data .",experiment,0,255,61,0,25
text-classification,1,"Finally , a promising future direction might be to seek , under this framework , new region embedding methods with complementary benefits .",experiment,0,256,62,0,23
text-classification,5,Universal Language Model Fine - tuning for Text Classification,title,1,2,1,0,9
text-classification,5,abstract,abstract,0,3,1,0,1
text-classification,5,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch .",abstract,0,4,2,0,25
text-classification,5,"We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques thatare key for fine - tuning a language model .",abstract,0,5,3,0,40
text-classification,5,"Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets .",abstract,0,6,4,0,32
text-classification,5,"Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data .",abstract,0,7,5,0,21
text-classification,5,We opensource our pretrained models and code 1 .,abstract,0,8,6,0,9
text-classification,5,introduction,introduction,0,9,1,0,1
text-classification,5,Inductive transfer learning has had a large impact on computer vision ( CV ) .,introduction,0,10,2,0,15
text-classification,5,"Applied CV models ( including object detection , classification , and segmentation ) are rarely trained from scratch , but instead are fine - tuned from models that have been pretrained on ImageNet , MS - COCO , and other datasets .",introduction,0,11,3,0,42
text-classification,5,"Text classification is a category of Natural Language Processing ( NLP ) tasks with real - world applications such as spam , fraud , and bot detection , emergency response , and commercial document classification , such as for legal discovery .",introduction,0,12,4,0,42
text-classification,5,1 http://nlp.fast.ai/ulmfit.,introduction,0,13,5,0,2
text-classification,5,equal contribution .,introduction,0,14,6,0,3
text-classification,5,"Jeremy focused on the algorithm development and implementation , Sebastian focused on the experiments and writing .",introduction,0,15,7,0,17
text-classification,5,"While Deep Learning models have achieved state - of - the - art on many NLP tasks , these models are trained from scratch , requiring large datasets , and days to converge .",introduction,0,16,8,0,34
text-classification,5,Research in NLP focused mostly on transductive transfer .,introduction,0,17,9,0,9
text-classification,5,"For inductive transfer , fine - tuning pretrained word embeddings , a simple transfer technique that only targets a model 's first layer , has had a large impact in practice and is used in most state - of - the - art models .",introduction,0,18,10,0,45
text-classification,5,"Recent approaches that concatenate embeddings derived from other tasks with the input at different layers ) still train the main task model from scratch and treat pretrained embeddings as fixed parameters , limiting their usefulness .",introduction,0,19,11,0,36
text-classification,5,"In light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models .",introduction,0,20,12,0,25
text-classification,5,"However , inductive transfer via finetuning has been unsuccessful for NLP .",introduction,0,21,13,0,12
text-classification,5,"first proposed finetuning a language model ( LM ) but require millions of in - domain documents to achieve good performance , which severely limits its applicability .",introduction,0,22,14,0,28
text-classification,5,We show that not the idea of LM fine - tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption .,introduction,0,23,15,0,28
text-classification,5,LMs overfit to small datasets and suffered catastrophic forgetting when fine - tuned with a classifier .,introduction,0,24,16,0,17
text-classification,5,"Compared to CV , NLP models are typically more shallow and thus require different fine - tuning methods .",introduction,0,25,17,0,19
text-classification,5,"We propose a new method , Universal Language Model Fine - tuning ( ULMFiT ) that addresses these issues and enables robust inductive transfer learning for any NLP task , akin to fine - tuning Image Net models :",introduction,0,26,18,0,39
text-classification,5,The same 3 - layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans - fer learning approaches on six widely studied text classification tasks .,introduction,0,27,19,0,35
text-classification,5,"On IMDb , with 100 labeled examples , ULMFiT matches the performance of training from scratch with 10 and - given 50 k unlabeled examples - with 100 more data .",introduction,0,28,20,0,31
text-classification,5,contributions,introduction,0,29,21,0,1
text-classification,5,Our contributions are the following :,introduction,0,30,22,0,6
text-classification,5,"1 ) We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .",introduction,1,31,23,0,33
text-classification,5,"2 ) We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .",introduction,1,32,24,0,33
text-classification,5,"3 ) We significantly outperform the state - of - the - art on six representative text classification datasets , with an error reduction of 18 - 24 % on the majority of datasets .",introduction,0,33,25,0,35
text-classification,5,4 ) We show that our method enables extremely sample - efficient transfer learning and perform an extensive ablation analysis .,introduction,0,34,26,0,21
text-classification,5,5 ) We make the pretrained models and our code available to enable wider adoption .,introduction,0,35,27,0,16
text-classification,5,related work,related work,0,36,1,0,2
text-classification,5,Transfer learning in CV Features in deep neural networks in CV have been observed to transition from general to task - specific from the first to the last layer .,related work,0,37,2,0,30
text-classification,5,"For this reason , most work in CV focuses on transferring the first layers of the model .",related work,0,38,3,0,18
text-classification,5,Sharif achieve state - of - theart results using features of an Image Net model as input to a simple classifier .,related work,0,39,4,0,22
text-classification,5,"In recent years , this approach has been superseded by fine - tuning either the last or several of the last layers of a pretrained model and leaving the remaining layers frozen .",related work,0,40,5,0,33
text-classification,5,hypercolumns,related work,0,41,6,0,1
text-classification,5,"In NLP , only recently have methods been proposed that go beyond transferring word embeddings .",related work,0,42,7,0,16
text-classification,5,The prevailing approach is to pretrain embeddings that capture additional context via other tasks .,related work,0,43,8,0,15
text-classification,5,"Embeddings at different levels are then used as features , concatenated either with the word embeddings or with the inputs at intermediate layers .",related work,0,44,9,0,24
text-classification,5,"This method is known as hypercolumns in CV 2 and is used by , who use language modeling , paraphrasing , entailment , and Machine Translation ( MT ) respectively for pretraining .",related work,0,45,10,0,33
text-classification,5,"require engineered custom architectures , while we show state - of - the - art performance with the same basic architecture across a range of tasks .",related work,0,46,11,0,27
text-classification,5,"In CV , hypercolumns have been nearly entirely superseded by end - to - end fine - tuning .",related work,0,47,12,0,19
text-classification,5,multi - task learning,related work,0,48,13,0,4
text-classification,5,A related direction is multi-task learning ( MTL ) .,related work,0,49,14,0,10
text-classification,5,This is the approach taken by and who add a language modeling objective to the model that is trained jointly with the main task model .,related work,0,50,15,0,26
text-classification,5,"MTL requires the tasks to be trained from scratch every time , which makes it inefficient and often requires careful weighting of the taskspecific objective functions .",related work,0,51,16,0,27
text-classification,5,fine - tuning,related work,0,52,17,0,3
text-classification,5,"Fine- tuning has been used successfully to transfer between similar tasks , e.g. in QA , for distantly supervised sentiment analysis , or MT domains but has been shown to fail between unrelated ones .",related work,0,53,18,0,35
text-classification,5,"also fine - tune a language model , but overfit with 10 k labeled examples and require millions of in - domain documents for good performance .",related work,0,54,19,0,27
text-classification,5,"In contrast , ULMFiT leverages general - domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state - of the - art results also on small datasets .",related work,0,55,20,0,36
text-classification,5,Universal Language Model Fine- tuning,method,0,56,1,0,5
text-classification,5,"We are interested in the most general inductive transfer learning setting for NLP ( Pan and Yang , 2010 ) :",method,0,57,2,1,21
text-classification,5,"Given a static source task T Sand any target task T T with T S = T T , we would like to improve performance on T T .",method,0,58,3,0,29
text-classification,5,Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP :,method,0,59,4,0,18
text-classification,5,"It captures many facets of language relevant for downstream tasks , such as long - term dependencies , hierarchical relations , and sentiment .",method,0,60,5,0,24
text-classification,5,"In contrast to tasks like and entailment , it provides data in near- unlimited quantities for most domains and languages .",method,0,61,6,0,21
text-classification,5,"Additionally , a pretrained LM can be easily adapted to the idiosyncrasies of a target",method,0,62,7,0,15
text-classification,5,The full LM is fine - tuned on target task data using discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( STLR ) to learn task - specific features .,method,0,63,8,0,36
text-classification,5,c),method,0,64,9,0,1
text-classification,5,"The classifier is fine - tuned on the target task using gradual unfreezing , ' Discr ' , and STLR to preserve low - level representations and adapt high - level ones ( shaded : unfreezing stages ; black : frozen ) .",method,0,65,10,0,43
text-classification,5,"task , which we show significantly improves performance ( see Section 5 ) .",method,0,66,11,0,14
text-classification,5,"Moreover , language modeling already is a key component of existing tasks such as MT and dialogue modeling .",method,0,67,12,0,19
text-classification,5,"Formally , language modeling induces a hypothesis space H that should be useful for many other NLP tasks .",method,0,68,13,0,19
text-classification,5,"We propose Universal Language Model Finetuning ( ULMFiT ) , which pretrains a language model ( LM ) on a large general - domain corpus and fine - tunes it on the target task using novel techniques .",method,0,69,14,0,38
text-classification,5,The method is universal in the sense that it meets these practical criteria :,method,0,70,15,0,14
text-classification,5,"1 ) It works across tasks varying in document size , number , and label type ; 2 ) it uses a single architecture and training process ; 3 ) it requires no custom feature engineering or preprocessing ; and 4 ) it does not require additional in - domain documents or labels .",method,0,71,16,0,54
text-classification,5,"In our experiments , we use the state - of - theart language model AWD - LSTM , a regular LSTM ( with no attention , short - cut connections , or other sophisticated additions ) with various tuned dropout hyperparameters .",method,0,72,17,0,42
text-classification,5,"Analogous to CV , we expect that downstream performance can be improved by using higherperformance language models in the future .",method,0,73,18,0,21
text-classification,5,"ULMFiT consists of the following steps , which we show in : a) General - domain LM pretraining ( 3.1 ) ; b ) target task LM fine - tuning ( 3.2 ) ; and c ) target task classifier fine - tuning ( 3.3 ) .",method,0,74,19,0,47
text-classification,5,We discuss these in the following sections .,method,0,75,20,0,8
text-classification,5,general - domain lm pretraining,method,0,76,21,0,5
text-classification,5,An Image Net - like corpus for language should be large and capture general properties of language .,method,0,77,22,0,18
text-classification,5,"We pretrain the language model on Wikitext - 103 consisting of 28,595 preprocessed Wikipedia articles and 103 million words .",method,0,78,23,0,20
text-classification,5,Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled examples .,method,0,79,24,0,18
text-classification,5,"We leave the exploration of more diverse pretraining corpora to future work , but expect that they would boost performance .",method,0,80,25,0,21
text-classification,5,"While this stage is the most expensive , it only needs to be performed once and improves performance and convergence of downstream models .",method,0,81,26,0,24
text-classification,5,Target task LM fine - tuning,method,0,82,27,0,6
text-classification,5,"No matter how diverse the general - domain data used for pretraining is , the data of the target task will likely come from a different distribution .",method,0,83,28,0,28
text-classification,5,We thus fine - tune the LM on data of the target task .,method,0,84,29,0,14
text-classification,5,"Given a pretrained general - domain LM , this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data , and it allows us to train a robust LM even for small datasets .",method,0,85,30,0,40
text-classification,5,"We propose discriminative fine - tuning and slanted triangular learning rates for fine - tuning the LM , which we introduce in the following .",method,0,86,31,0,25
text-classification,5,discriminative fine - tuning,method,0,87,32,0,4
text-classification,5,"As different layers capture different types of information , they should be fine - tuned to different extents .",method,0,88,33,0,19
text-classification,5,"To this end , we propose a novel fine - tuning method , discriminative fine - tuning 3 .",method,0,89,34,0,19
text-classification,5,"Instead of using the same learning rate for all layers of the model , discriminative fine - tuning allows us to tune each layer with different learning rates .",method,0,90,35,0,29
text-classification,5,"For context , the regular stochastic gradient descent ( SGD ) update of a model 's parameters ?",method,0,91,36,0,18
text-classification,5,at time step t looks like the following :,method,0,92,37,0,9
text-classification,5,where ?,method,0,93,38,0,2
text-classification,5,is the learning rate and ? ?,method,0,94,39,0,7
text-classification,5,J ( ? ) is the gradient with regard to the model 's objective function .,method,0,95,40,0,16
text-classification,5,"For discriminative fine - tuning , we split the parameters ? into {?",method,0,96,41,0,13
text-classification,5,"1 , . . . , ?",method,0,97,42,0,7
text-classification,5,l } where ?,method,0,98,43,0,4
text-classification,5,l contains the parameters of the model at the l - th layer and L is the number of layers of the model .,method,0,99,44,0,24
text-classification,5,"similarly , we obtain {? 1 , . . . , ?",method,0,100,45,0,12
text-classification,5,l } where ?,method,0,101,46,0,4
text-classification,5,l is the learning rate of the l - th layer .,method,0,102,47,0,12
text-classification,5,The SGD update with discriminative finetuning is then the following :,method,0,103,48,0,11
text-classification,5,We empirically found it to work well to first choose the learning rate ?,method,0,104,49,0,14
text-classification,5,L of the last layer by fine - tuning only the last layer and using ?,method,0,105,50,0,16
text-classification,5,l?1 = ? l / 2.6 as the learning rate for lower layers .,method,0,106,51,0,14
text-classification,5,slanted triangular learning rates,method,0,107,52,0,4
text-classification,5,"For adapting its parameters to task - specific features , we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters .",method,0,108,53,0,37
text-classification,5,Using the same learning rate ( LR ) or an annealed learning rate throughout training is not the best way to achieve this behaviour .,method,0,109,54,0,25
text-classification,5,"Instead , we propose slanted triangular learning rates ( STLR ) , which first linearly increases the learning rate and then linearly decays it according to the following update schedule , which can be seen in :",method,0,110,55,0,37
text-classification,5,"where T is the number of training iterations 4 , cut f rac is the fraction of iterations we increase 3 An unrelated method of the same name exists for deep Boltzmann machines",method,0,111,56,0,33
text-classification,5,"In other words , the number of epochs times the number of updates per epoch .",method,0,112,57,0,16
text-classification,5,"the LR , cut is the iteration when we switch from increasing to decreasing the LR , p is the fraction of the number of iterations we have increased or will decrease the LR respectively , ratio specifies how much smaller the lowest LR is from the maximum LR ? max , and ?",method,0,113,58,0,54
text-classification,5,t is the learning rate at iteration t.,method,0,114,59,0,8
text-classification,5,"We generally use cut f rac = 0.1 , ratio = 32 and ? max = 0.01 .",method,0,115,60,0,18
text-classification,5,"STLR modifies triangular learning rates ( Smith , 2017 ) with a short increase and along decay period , which we found key for good performance .",method,0,116,61,1,27
text-classification,5,"In Section 5 , we compare against aggressive cosine annealing , a similar schedule that has recently been used to achieve state - of - the - art performance in CV .",method,0,117,62,0,32
text-classification,5,6 : The slanted triangular learning rate schedule used for ULMFiT as a function of the number of training iterations .,method,0,118,63,0,21
text-classification,5,Target task classifier fine - tuning,method,0,119,64,0,6
text-classification,5,"Finally , for fine - tuning the classifier , we augment the pretrained language model with two additional linear blocks .",method,0,120,65,0,21
text-classification,5,"Following standard practice for CV classifiers , each block uses batch normalization and dropout , with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer .",method,0,121,66,0,39
text-classification,5,Note that the parameters in these task - specific classifier layers are the only ones thatare learned from scratch .,method,0,122,67,0,20
text-classification,5,The first linear layer takes as the input the pooled last hidden layer states .,method,0,123,68,0,15
text-classification,5,concat pooling,method,0,124,69,0,2
text-classification,5,"The signal in text classification tasks is often contained in a few words , which may occur anywhere in the document .",method,0,125,70,0,22
text-classification,5,"As input documents can consist of hundreds of words , information may get lost if we only consider the last hidden state of the model .",method,0,126,71,0,26
text-classification,5,"For this reason , we concatenate the hidden state at the last time step h T of the document with both the max - pooled and the mean - pooled representation of the hidden states over as many time steps as fit in GPU memory H = {h 1 , . . . , h T }:",method,0,127,72,0,57
text-classification,5,where [ ] is concatenation .,method,0,128,73,0,6
text-classification,5,Fine - tuning the target classifier is the most critical part of the transfer learning method .,method,0,129,74,0,17
text-classification,5,"Overly aggressive fine - tuning will cause catastrophic forgetting , eliminating the benefit of the information captured through language modeling ; too cautious fine - tuning will lead to slow convergence ( and resultant overfitting ) .",method,0,130,75,0,37
text-classification,5,"Besides discriminative finetuning and triangular learning rates , we propose gradual unfreezing for fine - tuning the classifier .",method,0,131,76,0,19
text-classification,5,gradual unfreezing,method,0,132,77,0,2
text-classification,5,"Rather than fine - tuning all layers at once , which risks catastrophic forgetting , we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge :",method,0,133,78,0,35
text-classification,5,We first unfreeze the last layer and fine - tune all unfrozen layers for one epoch .,method,0,134,79,0,17
text-classification,5,"We then unfreeze the next lower frozen layer and repeat , until we finetune all layers until convergence at the last iteration .",method,0,135,80,0,23
text-classification,5,"This is similar to ' chain - thaw ' , except that we add a layer at a time to the set of ' thawed ' layers , rather than only training a single layer at a time .",method,0,136,81,0,39
text-classification,5,"While discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing all are beneficial on their own , we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets .",method,0,137,82,0,42
text-classification,5,BPTT for Text Classification ( BPT3C ),method,0,138,1,0,7
text-classification,5,Language models are trained with backpropagation through time ( BPTT ) to enable gradient propagation for large input sequences .,method,0,139,2,0,20
text-classification,5,"In order to make fine - tuning a classifier for large documents feasible , we propose BPTT for Text Classification ( BPT3C ) :",method,0,140,3,0,24
text-classification,5,We divide the document into fixedlength batches of size b.,method,0,141,4,0,10
text-classification,5,"At the beginning of each batch , the model is initialized with the final state of the previous batch ; we keep track of the hidden states for mean and max - pooling ; gradients are back - propagated to the batches whose hidden states contributed to the final prediction .",method,0,142,5,0,51
text-classification,5,"In practice , we use variable length backpropagation sequences .",method,0,143,6,0,10
text-classification,5,"Bidirectional language model Similar to existing work ( Peters et al. , 2017 , 2018 ) , we are not limited to fine - tuning a unidirectional language model .",method,0,144,7,1,30
text-classification,5,"For all our experiments , we pretrain both a forward and a backward LM .",method,0,145,8,0,15
text-classification,5,We fine - tune a classifier for each LM independently using BPT3C and average the classifier predictions .,method,0,146,9,0,18
text-classification,5,experiments,experiment,0,147,1,0,1
text-classification,5,"While our approach is equally applicable to sequence labeling tasks , we focus on text classification tasks in this work due to their important realworld applications .",experiment,0,148,2,0,27
text-classification,5,experimental setup,experiment,0,149,1,0,2
text-classification,5,datasets and tasks,experiment,0,150,2,0,3
text-classification,5,"We evaluate our method on six widely - studied datasets , with varying numbers of documents and varying document length , used by state - of - the - art text classification and transfer learning approaches as instances of three common text classification tasks : sentiment analysis , question classification , and topic classification .",experiment,0,151,3,0,55
text-classification,5,We show the statistics for each dataset and task in .,experiment,0,152,4,0,11
text-classification,5,TBCNN 4.0 Virtual 5.9 LSTM- CNN 3.9 ULMFiT ( ours ) 4.6 ULMFiT ( ours ) 3.6 : Test error rates ( % ) on text classification datasets used by .,experiment,0,153,5,0,31
text-classification,5,topic classification,experiment,0,154,6,0,2
text-classification,5,"For topic classification , we evaluate on the large - scale AG news and DBpedia ontology datasets created by .",experiment,0,155,7,0,20
text-classification,5,pre-processing,experiment,0,156,8,0,1
text-classification,5,We use the same pre-processing as in earlier work .,experiment,0,157,9,0,10
text-classification,5,"In addition , to allow the language model to capture aspects that might be relevant for classification , we add special tokens for upper-case words , elongation , and repetition .",experiment,0,158,10,0,31
text-classification,5,hyperparameters,experiment,0,159,11,0,1
text-classification,5,We are interested in a model that performs robustly across a diverse set of tasks .,experiment,0,160,12,0,16
text-classification,5,"To this end , if not mentioned otherwise , we use the same set of hyperparameters across tasks , which we tune on the IMDb validation set .",experiment,0,161,13,0,28
text-classification,5,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .",experiment,1,162,14,0,32
text-classification,5,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .",experiment,1,163,15,0,39
text-classification,5,The classifier has a hidden layer of size 50 .,experiment,1,164,16,0,10
text-classification,5,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .",experiment,1,165,17,0,25
text-classification,5,"We use a batch size of 64 , a base learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .",experiment,1,166,18,0,40
text-classification,5,We otherwise use the same practices used in .,experiment,0,167,19,0,9
text-classification,5,baselines and comparison models,experiment,0,168,20,0,4
text-classification,5,"For each task , we compare against the current state - of - theart .",experiment,0,169,21,0,15
text-classification,5,"For the IMDb and TREC - 6 datasets , we compare against CoVe , a stateof - the - art transfer learning method for NLP .",experiment,0,170,22,0,26
text-classification,5,"For the AG , Yelp , and DBpedia datasets , we compare against the state - of - the - art text categorization method by .",experiment,0,171,23,0,26
text-classification,5,results,result,0,172,1,0,1
text-classification,5,"For consistency , we report all results as error rates ( lower is better ) .",result,0,173,2,0,16
text-classification,5,We show the test error rates on the IMDb and TREC - 6 datasets used by in .,result,0,174,3,0,18
text-classification,5,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .",result,1,175,4,0,36
text-classification,5,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .",result,1,176,5,0,29
text-classification,5,"This is promising as the existing stateof - the - art requires complex architectures , multiple forms of attention and sophisticated embedding schemes , while our method employs a regular LSTM with dropout .",result,0,177,6,0,34
text-classification,5,"We note that the language model fine - tuning approach of only achieves an error of 7.64 vs. 4.6 for our method on IMDb , demonstrating the benefit of transferring knowledge from a large Image Net - like corpus using our fine - tuning techniques .",result,0,178,7,0,46
text-classification,5,IMDb in particular is reflective of realworld datasets :,result,0,179,8,0,9
text-classification,5,"It s documents are generally a few paragraphs long - similar to emails ( e.g for legal discovery ) and online comments ( e.g for community management ) ; and sentiment analysis is similar to many commercial applications , e.g. product response tracking and support email routing .",result,0,180,9,0,48
text-classification,5,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .",result,1,181,10,0,40
text-classification,5,"Nevertheless , the competitive performance on TREC - 6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences - in the case of TREC - 6to several paragraphs for IMDb .",result,0,182,11,0,42
text-classification,5,"Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by , we consistently outperform their approach on both datasets .",result,0,183,12,0,31
text-classification,5,"We show the test error rates on the larger AG , DBpedia , Yelp - bi , and Yelp - full datasets in .",result,0,184,13,0,24
text-classification,5,Our method again outperforms the state - of the - art significantly .,result,0,185,14,0,13
text-classification,5,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .",result,1,186,15,0,24
text-classification,5,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .",result,1,187,16,0,27
text-classification,5,analysis,result,0,188,17,0,1
text-classification,5,"In order to assess the impact of each contribution , we perform a series of analyses and ablations .",result,0,189,18,0,19
text-classification,5,"We run experiments on three corpora , IMDb , TREC - 6 , and AG thatare representative of different tasks , genres , and sizes .",result,0,190,19,0,26
text-classification,5,"For all experiments , we split off 10 % of the training set and report error rates on this validation set with unidirectional LMs .",result,0,191,20,0,25
text-classification,5,We fine - tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping .,result,0,192,21,0,19
text-classification,5,low - shot learning,result,1,193,22,0,4
text-classification,5,One of the main benefits of transfer learning is being able to train a model for,result,0,194,23,0,16
text-classification,5,pretraining,result,0,195,24,0,1
text-classification,5,IMDb TREC - 6 AG Without pretraining 5.63 10.67 5.52 With pretraining 5.00 5.69 5.38 : Validation error rates for ULMFiT with and without pretraining .,result,0,196,25,0,26
text-classification,5,a task with a small number of labels .,result,0,197,26,0,9
text-classification,5,We evaluate ULMFiT on different numbers of labeled examples in two settings : only labeled examples are used for LM fine - tuning ( 'supervised ' ) ; and all task data is available and can be used to fine - tune the LM ( ' semi-supervised ' ) .,result,0,198,27,0,50
text-classification,5,We compare ULM - FiT to training from scratch - which is necessary for hypercolumn - based approaches .,result,0,199,28,0,19
text-classification,5,"We split off balanced fractions of the training data , keep the validation set fixed , and use the same hyperparameters as before .",result,0,200,29,0,24
text-classification,5,We show the results in .,result,0,201,30,0,6
text-classification,5,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .",result,1,202,31,0,38
text-classification,5,"If we allow ULMFiT to also utilize unlabeled examples ( 50 k for IMDb , 100 k for AG ) , at 100 labeled examples , we match the performance of training from scratch with 50 and 100 more data on AG and IMDb respectively .",result,0,203,32,0,46
text-classification,5,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .",result,1,204,33,0,28
text-classification,5,impact of pretraining,result,0,205,34,0,3
text-classification,5,We compare using no pretraining with pretraining on WikiText - 103 in .,result,0,206,35,0,13
text-classification,5,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .",result,1,207,36,0,20
text-classification,5,"However , even for large datasets , pretraining improves performance .",result,0,208,37,0,11
text-classification,5,impact of lm quality,result,1,209,38,0,4
text-classification,5,"In order to gauge the importance of choosing an appropriate LM , we compare a vanilla LM with the same hyperparameters without any dropout 8 with the AWD - LSTM LM with tuned dropout parameters in .",result,0,210,39,0,37
text-classification,5,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .",result,1,211,40,0,20
text-classification,5,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .",result,1,212,41,0,22
text-classification,5,Impact of LM fine - tuning,result,0,213,42,0,6
text-classification,5,"We compare no finetuning against fine - tuning the full model ( ' Full ' ) , the most commonly used fine - tuning method , with and without discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) in .",result,0,214,43,0,50
text-classification,5,Fine - tuning the LM is most beneficial for larger datasets .,result,1,215,44,0,12
text-classification,5,"' Discr ' and ' Stlr ' improve performance across all three datasets and are necessary on the smaller TREC - 6 , where regular fine - tuning is not beneficial .",result,0,216,45,0,32
text-classification,5,Impact of classifier fine - tuning,result,0,217,46,0,6
text-classification,5,"We compare training from scratch , fine - tuning the full model ( ' Full ' ) , only fine - tuning the last layer ( ' Last ' ) , ' Chain - thaw ' , and gradual unfreezing ( ' Freez ' ) .",result,0,218,47,0,46
text-classification,5,We furthermore assess the importance of discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) .,result,0,219,48,0,26
text-classification,5,"We compare the latter to an alternative , aggressive cosine annealing schedule ( ' Cos ' ) .",result,0,220,49,0,18
text-classification,5,"We use a learning rate ? L = 0.01 for ' Discr ' , learning rates",result,0,221,50,0,16
text-classification,5,8,result,0,222,51,0,1
text-classification,5,"To avoid overfitting , we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier . of 0.001 and 0.0001 for the last and all other layers respectively for ' Chain - thaw ' as in , and a learning rate of 0.001 otherwise .",result,0,223,52,0,52
text-classification,5,We show the results in .,result,0,224,53,0,6
text-classification,5,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .",result,1,225,54,0,67
text-classification,5,' Freez ' provides similar performance as ' Full ' .,result,0,226,55,0,11
text-classification,5,"' Discr ' consistently boosts the performance of ' Full ' and ' Freez ' , except for the large AG .",result,0,227,56,0,22
text-classification,5,"Cosine annealing is competitive with slanted triangular learning rates on large data , but under-performs on smaller datasets .",result,0,228,57,0,19
text-classification,5,"Finally , full ULMFiT classifier fine - tuning ( bottom row ) achieves the best performance on IMDB and TREC - 6 and competitive performance on AG .",result,0,229,58,0,28
text-classification,5,"Importantly , ULMFiT is the only method that shows excellent performance across the board - and is therefore the only universal method .",result,0,230,59,0,23
text-classification,5,classifier fine - tuning behavior,result,0,231,60,0,5
text-classification,5,"While our results demonstrate that how we fine - tune the classifier makes a significant difference , fine - tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful .",result,0,232,61,0,38
text-classification,5,"To better understand the fine - tuning behavior of our model , we compare the validation error of the classifier fine - tuned with ULMFiT and ' Full ' during training in .",result,0,233,62,0,33
text-classification,5,"On all datasets , fine - tuning the full model leads to the lowest error comparatively early in training , e.g. already after the first epoch on IMDb .",result,0,234,63,0,29
text-classification,5,The error then increases as the model starts to overfit and knowledge captured through pretraining is lost .,result,0,235,64,0,18
text-classification,5,"In contrast , ULMFiT is more stable and suffers from no such catastrophic forgetting ; performance remains similar or improves until late epochs , which shows the positive effect of the learning rate schedule .",result,0,236,65,0,35
text-classification,5,impact of bidirectionality,result,0,237,66,0,3
text-classification,5,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .",result,1,238,67,0,30
text-classification,5,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,result,1,239,68,0,21
text-classification,5,discussion and future directions,result,0,240,69,0,4
text-classification,5,"While we have shown that ULMFiT can achieve state - of - the - art performance on widely used text classification tasks , we believe that language model fine - tuning will be particularly useful in the following settings compared to existing transfer learning approaches : a) NLP for non-English languages , where training data for supervised pretraining tasks is scarce ; b ) new NLP tasks where no state - of - the - art architecture exists ; and c) tasks with limited amounts of labeled data ( and some amounts of unlabeled data ) .",result,0,241,70,0,97
text-classification,5,"Given that transfer learning and particularly fine - tuning for NLP is under - explored , many future directions are possible .",result,0,242,71,0,22
text-classification,5,"One possible direction is to improve language model pretraining and fine - tuning and make them more scalable : for Image Net , predicting far fewer classes only incurs a small performance drop , while recent work shows that an alignment between source and target task label sets is important ) - focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training .",result,0,243,72,0,76
text-classification,5,"Language modeling can also be augmented with additional tasks in a multi-task learning fashion or enriched with additional supervision , e.g. syntax - sensitive dependencies to create a model that is more general or better suited for certain downstream tasks , ideally in a weakly - supervised manner to retain its universal properties .",result,0,244,73,0,54
text-classification,5,Another direction is to apply the method to novel tasks and models .,result,0,245,74,0,13
text-classification,5,"While an extension to sequence labeling is straightforward , other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine - tune .",result,0,246,75,0,32
text-classification,5,"Finally , while we have provided a series of analyses and ablations , more studies are required to better understand what knowledge a pretrained language model captures , how this changes during fine - tuning , and what information different tasks require .",result,0,247,76,0,43
text-classification,5,conclusion,result,0,248,77,0,1
text-classification,5,"We have proposed ULMFiT , an effective and extremely sample - efficient transfer learning method that can be applied to any NLP task .",result,0,249,78,0,24
text-classification,5,We have also proposed several novel fine - tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks .,result,0,250,79,0,27
text-classification,5,Our method significantly outperformed existing transfer learning techniques and the stateof - the - art on six representative text classification tasks .,result,0,251,80,0,22
text-classification,5,We hope that our results will catalyze new developments in transfer learning for NLP .,result,0,252,81,0,15
text-classification,7,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,1,2,1,0,9
text-classification,7,abstract,abstract,0,3,1,0,1
text-classification,7,"In this study , we explore capsule networks with dynamic routing for text classification .",abstract,0,4,2,0,15
text-classification,7,"We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information or have not been successfully trained .",abstract,0,5,3,0,32
text-classification,7,A series of experiments are conducted with capsule networks on six text classification benchmarks .,abstract,0,6,4,0,15
text-classification,7,"Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets , which shows the effectiveness of capsule networks for text classification .",abstract,0,7,5,0,28
text-classification,7,We additionally show that capsule networks exhibit significant improvement when transfer single - label to multi-label text classification over the competitors .,abstract,0,8,6,0,22
text-classification,7,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",abstract,0,9,7,0,30
text-classification,7,1 Codes are publicly available at : https://github.com/andyweizhao/capsule_text_classification .,abstract,1,10,8,0,9
text-classification,7,introduction,introduction,0,11,1,0,1
text-classification,7,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,introduction,1,12,2,0,14
text-classification,7,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",introduction,0,13,3,0,33
text-classification,7,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",introduction,0,14,4,0,25
text-classification,7,"But it could be very difficult for a computer to predict which presidential candidate is favored by its author , or whether the author 's view in the article is more liberal or more conservative .",introduction,0,15,5,0,36
text-classification,7,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",introduction,1,16,6,0,41
text-classification,7,"It is therefore not a surprise that distributed representations of words , a.k.a. word embeddings , have received great attention from NLP community addressing the question "" what "" to be modeled at the basic level .",introduction,0,17,7,0,37
text-classification,7,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .",introduction,0,18,8,0,36
text-classification,7,"A common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",introduction,0,19,9,0,38
text-classification,7,"Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",introduction,0,20,10,0,34
text-classification,7,"Those two approaches , albeit quite different from the computational perspective , actually follow a common measure to be diagnosed regarding their answers to the "" what "" question .",introduction,0,21,11,0,30
text-classification,7,"In neural network approaches , spatial patterns aggregated at lower levels contribute to representing higher level concepts .",introduction,0,22,12,0,18
text-classification,7,"Here , they form a recursive process to articulate what to be modeled .",introduction,0,23,13,0,14
text-classification,7,"For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",introduction,0,24,14,0,30
text-classification,7,It then hierarchically builds such pattern extraction pipelines at multiple levels .,introduction,0,25,15,0,12
text-classification,7,"Being a spatially sensitive model , CNN pays a price for the inefficiency of replicating feature detectors on a grid .",introduction,0,26,16,0,21
text-classification,7,"As argued in , one has to choose between replicating detectors whose size grows exponentially with the number of dimensions , or increasing the volume of the labeled training set in a similar exponential way .",introduction,0,27,17,0,36
text-classification,7,"On the other hand , methods that are spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns .",introduction,0,28,18,0,27
text-classification,7,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",introduction,0,29,19,0,16
text-classification,7,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,introduction,0,30,20,0,21
text-classification,7,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,introduction,0,31,21,0,18
text-classification,7,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,introduction,0,32,22,0,19
text-classification,7,A metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,introduction,0,33,23,0,34
text-classification,7,"As an outcome , their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints .",introduction,0,34,24,0,28
text-classification,7,"In our work , we follow a similar spirit to use this technique in modeling texts .",introduction,0,35,25,0,17
text-classification,7,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",introduction,0,36,26,0,39
text-classification,7,We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks .,introduction,0,37,27,0,22
text-classification,7,"More importantly , we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods .",introduction,0,38,28,0,23
text-classification,7,our model,introduction,0,39,29,0,2
text-classification,7,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",introduction,1,40,30,0,17
text-classification,7,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",introduction,1,41,31,0,24
text-classification,7,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",introduction,1,42,32,0,17
text-classification,7,"In the rest of this section , we elaborate the key components in detail .",introduction,0,43,33,0,15
text-classification,7,n - gram convolutional layer,introduction,1,44,34,0,5
text-classification,7,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,introduction,1,45,35,0,22
text-classification,7,suppose x ?,introduction,0,46,36,0,3
text-classification,7,R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,introduction,0,47,37,0,24
text-classification,7,let xi ?,introduction,0,48,38,0,3
text-classification,7,RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,introduction,0,49,39,0,19
text-classification,7,Let W a ? R K 1,introduction,0,50,40,0,7
text-classification,7,"V be the filter for the convolution operation , where K 1 is the N - gram size while sliding over a sentence for the purpose of detecting features at different positions .",introduction,0,51,41,0,33
text-classification,7,A filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ?,introduction,0,52,42,0,33
text-classification,7,"R L?K 1 + 1 , each element ma i ?",introduction,0,53,43,0,11
text-classification,7,R of the feature map is produced by,introduction,0,54,44,0,8
text-classification,7,"where is element - wise multiplication , b 0 is a bias term , and f is a nonlinear activate function ( i.e. , ReLU ) .",introduction,0,55,45,0,27
text-classification,7,We have described the process by which one feature is extracted from one filter .,introduction,0,56,46,0,15
text-classification,7,"Hence , for a = 1 , . . . , B , totally B filters with the same N - gram size , one can generate B feature maps which can be rearranged as",introduction,0,57,47,0,35
text-classification,7,primary capsule layer,introduction,1,58,48,0,3
text-classification,7,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,introduction,1,59,49,0,41
text-classification,7,suppose pi ?,introduction,0,60,50,0,3
text-classification,7,"Rd denotes the instantiated parameters of a capsule , where d is the dimension of the capsule .",introduction,0,61,51,0,18
text-classification,7,let w b ? r,introduction,0,62,52,0,5
text-classification,7,Bd be the filter shared in different sliding windows .,introduction,0,63,53,0,10
text-classification,7,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ?",introduction,0,64,54,0,19
text-classification,7,"R B , then the corresponding N - gram phrases in the form of capsule are produced with",introduction,0,65,55,0,18
text-classification,7,convcaps capsule,introduction,0,66,56,0,2
text-classification,7,"Probability column - list of capsules p ? R ( L?K 1 + 1 ) d , each capsule pi ?",introduction,0,67,57,0,21
text-classification,7,Rd in the column - list is computed as,introduction,0,68,58,0,9
text-classification,7,"where g is nonlinear squash function through the entire vector , b 1 is the capsule bias term .",introduction,0,69,59,0,19
text-classification,7,"For all C filters , the generated capsule feature maps can be rearranged as",introduction,0,70,60,0,14
text-classification,7,where totally ( L ? K 1 + 1 ) C d-dimensional vectors are collected as capsules in P .,introduction,0,71,61,0,20
text-classification,7,child - parent relationships,introduction,0,72,62,0,4
text-classification,7,"As argued in , capsule network tries to address the representational limitation and exponential inefficiencies of convolutions with transformation matrices .",introduction,0,73,63,0,21
text-classification,7,It allows the networks to automatically learn child - parent ( or partwhole ) relationships .,introduction,0,74,64,0,16
text-classification,7,"In text classification tasks , different sentences with the same category are supposed to have the similar topic but with different viewpoints .",introduction,0,75,65,0,23
text-classification,7,"In this paper , we explore two different types of transformation matrices to generate prediction vector ( vote ) j|i ?",introduction,0,76,66,0,21
text-classification,7,Rd from it s child capsule i to the parent capsule j.,introduction,0,77,67,0,12
text-classification,7,The first one shares weights W t 1 ?,introduction,0,78,68,0,9
text-classification,7,"RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",introduction,0,79,69,0,23
text-classification,7,"Formally , each corresponding vote can be computed by :",introduction,0,80,70,0,10
text-classification,7,where u i is a child - capsule in the layer below and b j|i is the capsule bias term .,introduction,0,81,71,0,21
text-classification,7,"In the second design , we replace the shared weight matrix W t 1 j with non-shared weight matrix W t 2 i , j , where the weight matrices W t 2 ?",introduction,0,82,72,0,34
text-classification,7,R HN dd and H is the number of child capsules in the layer below .,introduction,0,83,73,0,16
text-classification,7,dynamic routing,introduction,1,84,74,0,2
text-classification,7,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,introduction,1,85,75,0,34
text-classification,7,"For each potential parent , the capsule network can increase or decrease the connection strength by dynamic routing , which is more effective than the primitive routing strategies such as max - pooling in CNN that essentially detects whether a feature is present in any position of the text , but loses spatial information about the feature .",introduction,0,86,76,0,58
text-classification,7,We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules :,introduction,0,87,77,0,20
text-classification,7,orphan category,introduction,0,88,78,0,2
text-classification,7,"Inspired by , an additional "" orphan "" category is added to the network , which can capture the "" background "" information of the text such as stop words and the words that are unrelated to specific categories , helping the capsule network model the child - parent relationship more efficiently .",introduction,0,89,79,0,53
text-classification,7,"Adding "" orphan "" category in the text is more effective than in image since there is no single consistent "" background "" object in images , while the stop words are consistent in texts such as predicate "" s "" , "" am "" and pronouns "" his "" , "" she "" .",introduction,0,90,80,0,55
text-classification,7,leaky - softmax,introduction,0,91,81,0,3
text-classification,7,We explore Leaky - Softmax in the place of standard softmax while updating connection strength between the children capsules and their parents .,introduction,0,92,82,0,23
text-classification,7,"Despite the orphan category in the last capsule layer , we also need a light - weight method between two consecutive layers to route the noise child capsules to extra dimension without any additional parameters and computation consuming .",introduction,0,93,83,0,39
text-classification,7,coefficients amendment,introduction,0,94,84,0,2
text-classification,7,We also attempt to use the probability of existence of child capsules in the layer below to iteratively amend the connection strength as Eq.6 .,introduction,0,95,85,0,25
text-classification,7,"Algorithm 1 : Dynamic Routing Algorithm 1 procedure ROUTING ( j|i , j|i , r , l ) 2 Initialize the logits of coupling coefficients b j|i = 0 3 for r iterations do 4 for all capsule i in layer land capsule j in layer l + 1 :",introduction,0,96,86,0,50
text-classification,7,for all capsule i in layer land capsule j in,introduction,0,97,87,0,10
text-classification,7,"Given each prediction vector j|i and its probability of existence j|i , where j|i = i , each iterative coupling coefficient of connection strength c j|i is updated by",introduction,0,98,88,0,29
text-classification,7,where b j|i is the logits of coupling coefficients .,introduction,0,99,89,0,10
text-classification,7,Each parent capsule v j in the layer above is a weighted sum over all prediction vectors j|i :,introduction,0,100,90,0,19
text-classification,7,"where a j is the probabilities of parent capsules , g is nonlinear squash function through the entire vector .",introduction,0,101,91,0,20
text-classification,7,"Once all of the parent capsules are produced , each coupling coefficient b j|i is updated by :",introduction,0,102,92,0,18
text-classification,7,"For simplicity of notation , the parent capsules and their probabilities in the layer above are denoted as v , a = Routing ( )",introduction,0,103,93,0,25
text-classification,7,"where denotes all of the child capsules in the layer below , v denotes all of the parent - capsules and their probabilities a.",introduction,0,104,94,0,24
text-classification,7,Our dynamic routing algorithm is summarized in Algorithm,introduction,0,105,95,0,8
text-classification,7,1 .,introduction,0,106,96,0,2
text-classification,7,convolutional capsule layer,introduction,1,107,97,0,3
text-classification,7,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .",introduction,1,108,98,0,22
text-classification,7,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,introduction,1,109,99,0,28
text-classification,7,Suppose W c 1 ? R Ddd and W c 2 ?,introduction,0,110,100,0,12
text-classification,7,r k,introduction,0,111,101,0,2
text-classification,7,"2 CDdd denote shared and non-shared weights , respectively , where K 2 C is the number of child capsules in a local region in the layer below , Dis the number of parent capsules which the child capsules are sent to .",introduction,0,112,102,0,43
text-classification,7,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b?",introduction,0,113,103,0,20
text-classification,7,"where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .",introduction,0,114,104,0,38
text-classification,7,"Then , we use routingby - agreement to produce parent capsules feature maps totally ( L?K 1 ? K 2 + 2 ) D d-dimensional capsules in this layer .",introduction,0,115,105,0,30
text-classification,7,"When using the non-shared weights across the child capsules , we replace the transformation matrix W c 1 j in Eq. ( 10 ) with W c 2 j .",introduction,0,116,106,0,30
text-classification,7,fully connected capsule layer,introduction,1,117,107,0,4
text-classification,7,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,introduction,1,118,108,0,32
text-classification,7,R Edd or W d 2 ?,introduction,0,119,109,0,7
text-classification,7,R HEdd followed by routing - by - agreement to produce final capsule v j ?,introduction,0,120,110,0,16
text-classification,7,Rd and its probability a j ?,introduction,0,121,111,0,7
text-classification,7,r for each category .,introduction,0,122,112,0,5
text-classification,7,"Here , H is the number of child capsules in the layer below , E is the number of categories plus an extra orphan category .",introduction,0,123,113,0,26
text-classification,7,The Architectures of Capsule Network,introduction,1,124,114,0,5
text-classification,7,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,introduction,1,125,115,0,20
text-classification,7,"Capsule - B Capsule - A starts with an embedding layer which transforms each word in the corpus to a 300 - dimensional ( V = 300 ) word vector , followed by a 3 - gram ( K 1 = 3 ) convolutional layer with 32 filters ( B = 32 ) and astride of 1 with ReLU non-linearity .",introduction,0,126,116,0,61
text-classification,7,"All the other layers are capsule layers starting with a B d primary capsule layer with 32 filters ( C = 32 ) , followed by a 3 C d d ( K 2 = 3 ) convolutional capsule layer with 16 filters ( D = 16 ) and a fully connected capsule layer in sequence .",introduction,0,127,117,0,57
text-classification,7,Each capsule has 16 - dimensional ( d = 16 ) instantiated parameters and their length ( norm ) can describe the probability of the existence of capsules .,introduction,0,128,118,0,29
text-classification,7,"The capsule layers are connected by the transformation matrices , and each connection is also multiplied by a routing coefficient that is dynamically computed by routing by agreement mechanism .",introduction,0,129,119,0,30
text-classification,7,"The basic structure of Capsule - B is similar to Capsule - A except that we adopt three parallel networks with filter windows ( N ) of 3 , 4 , 5 in the N - gram convolutional layer ( see ) .",introduction,0,130,120,0,43
text-classification,7,The final output of the fully connected capsule layer is fed into the average pooling to produce the final results .,introduction,0,131,121,0,21
text-classification,7,"In this way , Capsule - B can learn more meaningful and comprehensive text representation .",introduction,0,132,122,0,16
text-classification,7,3 experimental setup,experiment,0,133,1,0,3
text-classification,7,experimental datasets,experiment,0,134,1,0,2
text-classification,7,"In order to evaluate the effectiveness of our model , we conduct a series of experiments on six bench - marks including : movie reviews ( MR ) , Stanford Sentiment Treebankan extension of MR ( SST - 2 ) , Subjectivity dataset ( Subj ) , TREC question dataset ( TREC ) , customer review ( CR ) , and AG 's news corpus .",experiment,0,135,2,0,66
text-classification,7,"These benchmarks cover several text classification tasks such as sentiment classification , question categorization , news categorization .",experiment,0,136,3,0,18
text-classification,7,The detailed statistics are presented in,experiment,0,137,4,0,6
text-classification,7,implementation details,experiment,0,138,5,0,2
text-classification,7,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",experiment,1,139,6,0,16
text-classification,7,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,experiment,1,140,7,0,17
text-classification,7,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,experiment,1,141,8,0,16
text-classification,7,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,experiment,1,142,9,0,25
text-classification,7,baseline methods,method,0,143,1,0,2
text-classification,7,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",method,1,144,2,1,81
text-classification,7,experimental results,experiment,0,145,1,0,2
text-classification,7,quantitative evaluation,experiment,0,146,2,0,2
text-classification,7,"In our experiments , the evaluation metric is classification accuracy .",experiment,0,147,3,0,11
text-classification,7,We summarize the experimental results in .,experiment,0,148,4,0,7
text-classification,7,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",experiment,1,149,5,0,29
text-classification,7,"In particular , our model substantially and consistently outperforms",experiment,0,150,6,0,9
text-classification,7,ablation study,experiment,0,151,7,0,2
text-classification,7,"To analyze the effect of varying different components of our capsule architecture for text classification , we also report the ablation test of the capsule - B model in terms of using different setups of the capsule network .",experiment,0,152,8,0,39
text-classification,7,The experimental results are summarized in .,experiment,0,153,9,0,7
text-classification,7,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",experiment,1,154,10,0,45
text-classification,7,More comprehensive comparison results are demonstrated in . 4 in Supplementary Material .,experiment,0,155,11,0,13
text-classification,7,Single - Label to Multi - Label Text Classification,experiment,0,156,12,0,9
text-classification,7,Capsule network demonstrates promising performance in single - label text classification which as - signs a label from a predefined set to a text ( see ) .,experiment,0,157,13,0,28
text-classification,7,"Multi-label text classification is , however , a more challenging practical problem .",experiment,0,158,14,0,13
text-classification,7,"From singlelabel to multi-label ( with n category labels ) text classification , the label space is expanded from n to 2 n , thus more training is required to cover the whole label space .",experiment,0,159,15,0,36
text-classification,7,"For single - label texts , it is practically easy to collect and annotate the samples .",experiment,0,160,16,0,17
text-classification,7,"However , the burden of collection and annotation for a large scale multi-label text dataset is generally extremely high .",experiment,0,161,17,0,20
text-classification,7,"How deep neural networks ( e.g. , CNN and LSTM ) best cope with multi-label text classification still remains a problem since obtaining large scale of multi-label dataset is a timeconsuming and expensive process .",experiment,0,162,18,0,35
text-classification,7,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .",experiment,0,163,19,0,27
text-classification,7,"With feature property as part of the information extracted by capsules , we may generalize the model better to multi-label text classification without an over extensive amount of labeled data .",experiment,0,164,20,0,31
text-classification,7,The evaluation is carried on the Reuters - 21578 dataset .,experiment,0,165,21,0,11
text-classification,7,"This dataset consists of 10,788 documents from the Reuters financial newswire service , where each document contains either multiple labels or a single label .",experiment,0,166,22,0,25
text-classification,7,We reprocess the corpus to evaluate the capability of capsule networks of transferring from single - label to multi-label text classification .,experiment,0,167,23,0,22
text-classification,7,"For dev and training , we only use the single - label documents in the Reuters dev and training sets .",experiment,0,168,24,0,21
text-classification,7,"For testing , Reuters - Multi - label only uses the multi-label documents in testing dataset , while Reuters - Full includes all documents in test set .",experiment,0,169,25,0,28
text-classification,7,The characteristics of these two datasets are described in .,experiment,0,170,26,0,10
text-classification,7,"Following ( Sorower , 2010 ) , we adopt Micro Averaged Precision ( Precision ) , Micro Averaged Recall ( Recall ) and Micro Averaged F1 scores ( F1 ) as the evaluation metrics for multi-label text classification .",experiment,0,171,27,1,39
text-classification,7,"Any of these scores are firstly computed on individual class labels and then averaged over all classes , called label - based measures .",experiment,0,172,28,0,24
text-classification,7,"In addition , we also measure the Exact Match Ratio ( ER ) which considers partially correct prediction as incorrect and only counts fully correct samples .",experiment,0,173,29,0,27
text-classification,7,The experimental results are summarized in .,experiment,0,174,30,0,7
text-classification,7,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",experiment,0,175,31,0,43
text-classification,7,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .",experiment,0,176,32,0,25
text-classification,7,This is within our expectation since the capsule network is capable of preserving the instantiated parameters of the categories trained by singlelabel documents .,experiment,0,177,33,0,24
text-classification,7,The capsule network has much stronger transferring capability than the conventional deep neural networks .,experiment,0,178,34,0,15
text-classification,7,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",experiment,0,179,35,0,27
text-classification,7,connection strength visualization,experiment,0,180,36,0,3
text-classification,7,"To visualize the connection strength between capsule layers clearly , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , where the primary capsules denote N-gram phrases in the form of capsules .",experiment,0,181,37,0,44
text-classification,7,"The connection strength shows the importance of each primary capsule for text categories , acting like a parallel attention mechanism .",experiment,0,182,38,0,21
text-classification,7,This should allow the capsule networks to recognize multiple categories in the text even though the model is trained on singlelabel documents .,experiment,0,183,39,0,23
text-classification,7,"Due to space reasons , we choose a multilabel document from Reuters - Multi - label test set whose category labels ( i.e. , Interest Rates and Money / Foreign Exchange ) are correctly predicted ( fully correct ) by our model with high confidence ( p > 0.8 ) to report in .",experiment,0,184,40,0,54
text-classification,7,"The categoryspecific phrases such as "" interest rates "" and "" foreign exchange "" are highlighted with red color .",experiment,0,185,41,0,20
text-classification,7,We use the tag cloud to visualize the 3 - gram phrases for Interest Rates and Money / Foreign Exchange categories .,experiment,0,186,42,0,22
text-classification,7,"The stronger the connection strength , the bigger the font size .",experiment,0,187,43,0,12
text-classification,7,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .",experiment,0,188,44,0,24
text-classification,7,"The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules , as shown in To experimentally verify the convergence of the routing algorithm , we also plot learning curve to show the training loss overtime with different iterations of routing .",experiment,0,189,45,0,50
text-classification,7,"From , we observe that the Capsule - B with 3 or 5 iterations of routing optimizes the loss faster and converges to a lower loss at the end than the capsule network with 1 iteration .",experiment,0,190,46,0,37
text-classification,7,u.k .,experiment,0,191,47,0,2
text-classification,7,MONEY RATES FIRM ON LAWSON STERLING TARGETS,experiment,0,192,48,0,7
text-classification,7,interest rates,experiment,0,193,49,0,2
text-classification,7,Money / Foreign Exchange Interest rates on the London money market were slightly firmer on news U.K .,experiment,0,194,50,0,18
text-classification,7,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",experiment,0,195,51,0,21
text-classification,7,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",experiment,0,196,52,0,29
text-classification,7,Sterling opened 0.3 points lower in trade weighted terms at 71.3 .,experiment,0,197,53,0,12
text-classification,7,Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates .,experiment,0,198,54,0,24
text-classification,7,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,experiment,0,199,55,0,24
text-classification,7,"Base lending rates even less likely in the near term , dealers said .",experiment,0,200,56,0,14
text-classification,7,"The feeling remains in the market , however , that fundamental factors have not really changed and that arise in U.K .",experiment,0,201,57,0,22
text-classification,7,Interest rates is not very likely .,experiment,0,202,58,0,7
text-classification,7,"The market is expected to continue at around these levels , reflecting the current 10 pct base rate level , for sometime .",experiment,0,203,59,0,23
text-classification,7,The key three months interbank rate was 1 / 16 point firmer at 10 9 - 7 /8 pct .,experiment,0,204,60,0,20
text-classification,7,orphan,experiment,0,205,61,0,1
text-classification,7,Mergers / Acquisitions Money / Foreign Exchange Trade Interest Rates,experiment,0,206,62,0,10
text-classification,7,related work,related work,0,207,1,0,2
text-classification,7,"Early methods for text classification adopted the typical features such as bag - of - words , n-grams , and their TF - IDF features as input of machine learning algorithms such as support vector machine ( SVM ) , naive Bayes ( NB ) for classification .",related work,0,208,2,0,48
text-classification,7,"However , these models usually heavily relied on laborious feature engineering or massive extra linguistic resources .",related work,0,209,3,0,17
text-classification,7,Recent advances in deep neural networks and representation learning have substantially improved the performance of text classification tasks .,related work,0,210,4,0,19
text-classification,7,"The dominant approaches are recurrent neural net -works , in particular LSTMs and CNNs. reported on a series of experiments with CNNs trained on top of pre-trained word vectors for sentence - level classification tasks .",related work,0,211,5,0,36
text-classification,7,The CNN models improved upon the state of the art on 4 out of 7 tasks .,related work,0,212,6,0,17
text-classification,7,offered an empirical exploration on the use of character - level convolutional networks ( Convnets ) for text classification and the experiments showed that Convnets outperformed the traditional models .,related work,0,213,7,0,30
text-classification,7,"proposed a simple and efficient text classification method fastText , which could be trained on a billion words within ten minutes .",related work,0,214,8,0,22
text-classification,7,proposed a very deep convolutional networks ( with 29 convolutional layers ) for text classification .,related work,0,215,9,0,16
text-classification,7,generalized the LSTM to the tree - structured network topologies ( Tree - LSTM ) that achieved best results on two text classification tasks .,related work,0,216,10,0,25
text-classification,7,"Recently , a novel type of neural network is proposed using the concept of capsules to improve the representational limitations of firstly introduced the concept of "" capsules "" to address the representational limitations of CNNs and RNNs .",related work,0,217,11,0,39
text-classification,7,Capsules with transformation matrices allowed networks to automatically learn part - whole relationships .,related work,0,218,12,0,14
text-classification,7,"Consequently , proposed capsule networks that replaced the scalar - output feature detectors of CNNs with vector - output capsules and max - pooling with routing - by - agreement .",related work,0,219,13,0,31
text-classification,7,The capsule network has shown its potential by achieving a state - of - the - art result on MNIST data .,related work,0,220,14,0,22
text-classification,7,"Unlike max - pooling in CNN , however , Capsule network do not throwaway information about the precise position of the entity within the region .",related work,0,221,15,0,26
text-classification,7,"For lowlevel capsules , location information is placecoded by which capsule is active .",related work,0,222,16,0,14
text-classification,7,further tested out the application of capsule networks on CIFAR data with higher dimensionality .,related work,0,223,17,0,15
text-classification,7,"proposed a new iterative routing procedure between capsule layers based on the EM algorithm , which achieves significantly better accuracy on the small NORB data set .",related work,0,224,18,0,27
text-classification,7,generalized existing routing methods within the framework of weighted kernel density estimation .,related work,0,225,19,0,13
text-classification,7,"To date , no work investigates the performance of capsule networks in NLP tasks .",related work,0,226,20,0,15
text-classification,7,This study herein takes the lead in this topic .,related work,0,227,21,0,10
text-classification,7,conclusion,related work,0,228,22,0,1
text-classification,7,"In this paper , we investigated capsule networks with dynamic routing for text classification .",related work,0,229,23,0,15
text-classification,7,Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the disturbance of noisy capsules .,related work,0,230,24,0,21
text-classification,7,Extensive experiments on six text classification benchmarks show the effectiveness of capsule networks in text classification .,related work,0,231,25,0,17
text-classification,7,"More importantly , capsule networks also show significant improvement when transferring single - label to multi-label text classifications over strong baseline methods .",related work,0,232,26,0,23
text-classification,7,supplementary material,related work,0,233,27,0,2
text-classification,7,"To better demonstrate the orphan and other categories with top unigrams , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , similar to the settings in section 5.1 .",related work,0,234,28,0,41
text-classification,7,"Here , the primary capsules denote uni-grams in the form of capsules .",related work,0,235,29,0,13
text-classification,7,"We picked top - 20 uni-gram ( words ) from four categories ( i.e. , Orphan category , Trade category , Money Exchange category and Interest Rates category ) sorted by their connection strengths .",related work,0,236,30,0,35
text-classification,7,"Money / Foreign Exchange Following is the text of a statement by the Group of Seven - the U.S. , Japan , West Germany , France , Britain , Italy and Canada - issued after a Washington meeting yesterday .",related work,0,237,31,0,40
text-classification,7,1 . The finance ministers and central bank governors of seven major industrial countries met today .,related work,0,238,32,0,17
text-classification,7,They continued the process of multilateral surveillance of their economies pursuant to the arrangements for strengthened economic policy coordination agreed at the 1986 Tokyo summit of their heads of state or government .,related work,0,239,33,0,33
text-classification,7,"2 . The ministers and governors reaffirmed the commitment to the cooperative approach agreed at the recent Paris meeting , and noted the progress achieved in implementing the undertakings embodied in the Louvre Agreement .",related work,0,240,34,0,35
text-classification,7,"In this connection they welcomed the proposals just announced by the governing Liberal Democratic Party in Japan for extraordinary and urgent measures to stimulate Japan 's economy through early implementation of a large supplementary budget exceeding those of previous years , as well as unprecedented front - end loading of public works expenditures .",related work,0,241,35,0,54
text-classification,7,They concluded that present and prospective progress in implementing the policy undertakings at the Louvre and in this statement provided a basis for continuing close cooperation to foster the stability of exchange rates .,related work,0,242,36,0,34
text-classification,7,index,related work,0,243,37,0,1
text-classification,4,Learning Context - Sensitive Convolutional Filters for Text Processing,title,1,2,1,0,9
text-classification,4,abstract,abstract,0,3,1,0,1
text-classification,4,Convolutional neural networks ( CNNs ) have recently emerged as a popular building block for natural language processing ( NLP ) .,abstract,0,4,2,0,22
text-classification,4,"Despite their success , most existing CNN models employed in NLP share the same learned ( and static ) set of filters for all input sentences .",abstract,0,5,3,0,27
text-classification,4,"In this paper , we consider an approach of using a small meta network to learn contextsensitive convolutional filters for text processing .",abstract,1,6,4,0,23
text-classification,4,The role of meta network is to abstract the contextual information of a sentence or document into a set of input -aware filters .,abstract,0,7,5,0,24
text-classification,4,"We further generalize this framework to model sentence pairs , where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations .",abstract,0,8,6,0,24
text-classification,4,"In our benchmarks on four different tasks , including ontology classification , sentiment analysis , answer sentence selection , and paraphrase identification , our proposed model , a modified CNN with context - sensitive filters , consistently outperforms the standard CNN and attention - based CNN baselines .",abstract,0,9,7,0,48
text-classification,4,"By visualizing the learned context - sensitive filters , we further validate and rationalize the effectiveness of proposed framework .",abstract,0,10,8,0,20
text-classification,4,introduction,introduction,0,11,1,0,1
text-classification,4,"In the last few years , convolutional neural networks ( CNNs ) have demonstrated remarkable progress in various natural language processing applications , including sentence / document classification , text sequence matching , generic text representations , language modeling , machine translation and abstractive sentence summarization .",introduction,0,12,2,0,47
text-classification,4,CNNs are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly .,introduction,0,13,3,0,18
text-classification,4,"As an encoder network for text , CNNs typically convolve a set of filters , of window size n , with an inputsentence embedding matrix obtained via word2vec or Glove .",introduction,0,14,4,0,31
text-classification,4,"Different filter sizes n maybe used within the same model , exploiting meaningful semantic features from different n-gram fragments .",introduction,0,15,5,0,20
text-classification,4,"The learned weights of CNN filters , in most cases , are assumed to be fixed regardless of the input text .",introduction,0,16,6,0,22
text-classification,4,"As a result , the rich contextual information inherent in natural language sequences may not be fully captured .",introduction,0,17,7,0,19
text-classification,4,"As demonstrated in , the context of a word tends to greatly influence its contribution to the final supervised tasks .",introduction,0,18,8,0,21
text-classification,4,"This observation is consistent with the following intuition : when reading different types of documents , e.g. , academic papers or newspaper articles , people tend to adopt distinct strategies for better and more effective understanding , leveraging the fact that the same words or phrases may have different meaning or imply different things , depending on context .",introduction,0,19,9,0,59
text-classification,4,Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations .,introduction,0,20,10,0,17
text-classification,4,"One common strategy is the attention mechanism , which is typically employed on top of a CNN ( or Long Short - Term Memory ( LSTM ) ) layer to guide the extraction of semantic features .",introduction,0,21,11,0,37
text-classification,4,"For the embedding of a single sentence , proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations .",introduction,0,22,12,0,28
text-classification,4,"However , their model needs considerably more parameters to achieve performance gains over traditional CNNs .",introduction,0,23,13,0,16
text-classification,4,"To match sentence pairs , introduced an attentionbased CNN model , which re-weights the convolution inputs or outputs , to extract interdepen - dent sentence representations . ; explore a compare and aggregate framework to directly capture the wordby - word matching between two paired sentences .",introduction,0,24,14,0,47
text-classification,4,"However , these approaches suffer from the problem of high matching complexity , since a similarity matrix between pairwise words needs to be computed , and thus it is computationally inefficient or even prohibitive when applied to long sentences .",introduction,0,25,15,0,40
text-classification,4,"In this paper , we propose a generic approach to learn context - sensitive convolutional filters for natural language understanding .",introduction,1,26,16,0,21
text-classification,4,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .",introduction,1,27,17,0,33
text-classification,4,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .",introduction,1,28,18,0,43
text-classification,4,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .",introduction,1,29,19,0,22
text-classification,4,"Moreover , since the generated filters in our framework can adapt to different conditional information available ( labels or paired sentences ) , they can be naturally generalized to model sentence pairs .",introduction,0,30,20,0,33
text-classification,4,"In this regard , we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context - sensitive representations .",introduction,1,31,21,0,25
text-classification,4,"We investigate the effectiveness of our Adaptive Context - sensitive CNN ( ACNN ) framework on several text processing tasks : ontology classification , sentiment analysis , answer sentence selection and paraphrase identification .",introduction,0,32,22,0,34
text-classification,4,We show that the proposed methods consistently outperforms the standard CNN and attention - based CNN baselines .,introduction,0,33,23,0,18
text-classification,4,"Our work provides a new perspective on how to incorporate contextual information into text representations , which can be combined with more sophisticated structures to achieve even better performance in the future .",introduction,0,34,24,0,33
text-classification,4,related work,related work,0,35,1,0,2
text-classification,4,"Learning deep text representations has attracted much attention recently , since they can potentially benefit a wide range of NLP applications .",related work,0,36,2,0,22
text-classification,4,CNNs have been extensively investigated as the encoder networks of natural language .,related work,0,37,3,0,13
text-classification,4,Our work is inline with previous efforts on improving the adaptivity and flexibility of convolutional neural networks .,related work,0,38,4,0,18
text-classification,4,proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation .,related work,0,39,5,0,18
text-classification,4,"De introduced an architecture to generate the future frames conditioned on given image ( s ) , by adapting the CNN filter weights to the motion within previous video frames .",related work,0,40,6,0,31
text-classification,4,"Although CNNs have been widely adopted in a large number of NLP applications , improving the adaptivity of vanilla CNN modules has been considerably less studied .",related work,0,41,7,0,27
text-classification,4,"To the best of our knowledge , the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences .",related work,0,42,8,0,29
text-classification,4,"Our use of a meta network to generate parameters for another network is directly inspired by the recent success of hypernetworks for textgeneration tasks , and dynamic parameter - prediction for video - frame generation .",related work,0,43,9,0,36
text-classification,4,"In contrast to these works that focus on generation problems , our model is based on context - sensitive CNN filters and is aimed at abstracting more informative and predictive sentence features .",related work,0,44,10,0,33
text-classification,4,"Most similar to our work , designed a meta network to generate compositional functions over tree - structured neural networks for encapsulating sentence features .",related work,0,45,11,0,25
text-classification,4,"However , their model is only suitable for encoding individual sentences , while our framework can be readily generalized to capture the interactions between sentence pairs .",related work,0,46,12,0,27
text-classification,4,"Moreover , our framework is based on CNN models , which is advantageous due to fewer parameters and highly parallelizable computations relative to sequential - based models .",related work,0,47,13,0,28
text-classification,4,model,related work,0,48,14,0,1
text-classification,4,Basic CNN for text representations,related work,0,49,15,0,5
text-classification,4,"The CNN architectures in are typically utilized for extracting sentence representations , by a composition of a convolutional layer and a max - pooling operation over all resulting feature maps .",related work,0,50,16,0,31
text-classification,4,"Let the words of a sentence of length T ( padded where necessary ) be x 1 , x 2 , ... , x T .",related work,0,51,17,0,26
text-classification,4,The sentence can be represented as a matrix X ?,related work,0,52,18,0,10
text-classification,4,"R d T , where each column represents a d-dimensional embedding of the corresponding word .",related work,0,53,19,0,16
text-classification,4,"In the convolutional layer , a set of filters with weights W ?",related work,0,54,20,0,13
text-classification,4,"R Khd is convolved with every window of h words within the sentence , i.e. , {x 1:h , x 2:h+1 , . . . , x T ?h+1:T } , where K is the number of output feature maps ( and filters ) .",related work,0,55,21,0,45
text-classification,4,"In this manner , feature maps p for these h-gram text fragments are generated as :",related work,0,56,22,0,16
text-classification,4,"where i = 1 , 2 , ... , T ? h + 1 and denotes the convolution operator at the ith shift location .",related work,0,57,23,0,25
text-classification,4,parameter b ?,related work,0,58,24,0,3
text-classification,4,"R K is the bias term and f ( ) is a non-linear function , implemented as a rectified linear unit ( ReLU ) in our experiments .",related work,0,59,25,0,28
text-classification,4,"The output feature maps of the convolutional layer , i.e. , p ? R K(T ?h+1 ) are then passed to the pooling layer , which takes the maximum value in every row of p , forming a K-dimensional vector , z.",related work,0,60,26,0,42
text-classification,4,This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments .,related work,0,61,27,0,23
text-classification,4,"Moreover , the max - over - time nature of the pooling operation guarantees that the size of the obtained representation is independent of the sentence length .",related work,0,62,28,0,28
text-classification,4,"Note that in basic CNN sentence encoders , filter weights are the same for different inputs , which maybe suboptimal for feature extraction , especially in the case where conditional information is available .",related work,0,63,29,0,34
text-classification,4,learning context - sensitive filters,related work,0,64,30,0,5
text-classification,4,"The proposed architecture to learn contextsensitive filters is composed of two principal modules : ( i ) a filter generation module , which produces a set of filters conditioned on the input sentence ; and ( ii ) an adaptive convolution module , which applies the generated filters to an input sentence ( this sentence maybe either the same as or different from the first input , as discussed further in Section 3.3 ) .",related work,0,65,31,0,75
text-classification,4,"The two modules are jointly differentiable , and the over all architecture can be trained in an end - to - end manner .",related work,0,66,32,0,24
text-classification,4,"Since the generated filters are sample - specific , our ACNN feature extractor for text tends to have stronger predictive power than a basic CNN encoder .",related work,0,67,33,0,27
text-classification,4,The general ACNN framework is shown schematically in .,related work,0,68,34,0,9
text-classification,4,convolution module,related work,0,69,35,0,2
text-classification,4,context - aware filters,related work,0,70,36,0,4
text-classification,4,filter generation module,related work,0,71,37,0,3
text-classification,4,"I 'll go back and try other dishes Filter generation module Instead of utilizing fixed filter weights W for different inputs ( as ( 1 ) ) , our model generates a set of filters conditioned on the input sentence X. Given an input X , the filter - generation module can be implemented , in principle , as any deep ( differentiable ) architecture .",related work,0,72,38,0,66
text-classification,4,"However , in order to handle input sentences of variable length common in natural language , we design a generic filter generation module to produce filters with a predefined size .",related work,0,73,39,0,31
text-classification,4,"First , the input X is encapsulated into a fixedlength vector ( code ) z with the dimension of l , via a basic CNN model , where one convolutional layer is employed along with the pooling operation ( as described in Section 3.1 ) .",related work,0,74,40,0,46
text-classification,4,"On top of this hidden representation z , a deconvolutional layer , which performs transposed operations of convolutions , is further applied to produce a unique set of filters for X ( as illustrated in ) :",related work,0,75,41,0,37
text-classification,4,(,related work,0,76,42,0,1
text-classification,4,where ?,related work,0,77,43,0,2
text-classification,4,e and ?,related work,0,78,44,0,3
text-classification,4,"dare the learned parameters in each layer of the filter - generating module , respectively .",related work,0,79,45,0,16
text-classification,4,"Specifically , we convolve z with a filter of size ( f s , l , k x , k y ) , where f sis the number of generated filters and the kernel size is ( k x , k y ) .",related work,0,80,46,0,44
text-classification,4,"The output will be a tensor of shape ( f s , k x , k y ) .",related work,0,81,47,0,19
text-classification,4,"Since the dimension of hidden representation z is independent of input - sentence length , this framework guarantees that the generated filters are of the same shape and size for every sentence .",related work,0,82,48,0,33
text-classification,4,"Intuitively , the encoding part of filter generation module abstracts the information from sentence X into z .",related work,0,83,49,0,18
text-classification,4,"Based on this representation , the deconvolutional up - sampling layer determines a set of fixedsize , fine - grained filters f for the specific input .",related work,0,84,50,0,27
text-classification,4,adaptive convolution module,related work,0,85,51,0,3
text-classification,4,The adaptive convolution module takes as inputs the generated filters f and an input sentence .,related work,0,86,52,0,16
text-classification,4,This sentence and the input to the filter - generation module maybe identical ( as in ) or different ( as in .,related work,0,87,53,0,23
text-classification,4,"With the sample - specific filters , the input sentence is adaptively encoded , again , via a basic CNN architecture as in Section 3.1 , i.e. , one convolutional and one pooling layer .",related work,0,88,54,0,35
text-classification,4,"Notably , there are no additional parameters in the adaptive convolution module ( no bias term is employed ) .",related work,0,89,55,0,20
text-classification,4,"Our ACNN framework can be seen as a generalization of the basic CNN , which can be represented as an ACNN by setting the outputs of the filter - generation module to a constant , regardless of the contextual information from input sentence ( s ) .",related work,0,90,56,0,47
text-classification,4,"Because of the learning - to - learn nature of the proposed ACNN framework , it tends to have greater representational power than the basic CNN .",related work,0,91,57,0,27
text-classification,4,Extension to text sequence matching,related work,0,92,58,0,5
text-classification,4,"Considering the ability of our ACNN framework to generate context - sensitive filters , it can be naturally generalized to the task of text sequence matching .",related work,0,93,59,0,27
text-classification,4,"In this section , we will describe the proposed Adaptive Question Answering ( AdaQA ) model in the context of answer sentence selection task .",related work,0,94,60,0,25
text-classification,4,Note that the corresponding model can be readily adapted to other sentence matching problems as well ( see Section 5.2 ) .,related work,0,95,61,0,22
text-classification,4,"Given a factual question q ( associated with a list of candidate answers {a 1 , a 2 , . . . , am } and their corresponding labels y = {y 1 , y 2 , . . . , y m } ) , the goal of the model is to identify the correct answers from the set of candidates .",related work,0,96,62,0,63
text-classification,4,"For i = 1 , 2 , . . . , m , if a i correctly answers q , then y i = 1 , and otherwise y i =",related work,0,97,63,0,31
text-classification,4,"0 . Therefore , the task can be cast as a classification problem where , given an unlabeled question - answer pair ( q i , a i ) , we seek to predict the judgement y i .",related work,0,98,64,0,39
text-classification,4,"Conventionally , a question q and an answer a are independently encoded by two basic CNNs to fixed - length vector representations , denoted h q and ha , respectively .",related work,0,99,65,0,31
text-classification,4,They are then directly employed to predict the judgement y .,related work,0,100,66,0,11
text-classification,4,"This strategy could be suboptimal , since no communication ( information sharing ) occurs between the questionanswer pair until the top prediction layer .",related work,0,101,67,0,24
text-classification,4,"Intuitively , while the model is inferring the representation for a question , if the meaning of the answer is The AdaQA model can be divided into three modules : filter generation , adaptive convolution , and matching modules , as depicted schematically in .",related work,0,102,68,0,45
text-classification,4,"Assume there is a question - answer pair to be matched , represented by word - embedding matrices , i.e. Q ? R Tqd and A ?",related work,0,103,69,0,27
text-classification,4,"R Tad , where d is the embedding dimension and T q and Ta are respective sentence lengths .",related work,0,104,70,0,19
text-classification,4,"First , they are passed to two filter - generation modules , to produce two sets of filters that encapsulate features of the corresponding input sentences .",related work,0,105,71,0,27
text-classification,4,"Similar to the setup in Section 3.2 , we also employ a two - step process to produce the filters .",related work,0,106,72,0,21
text-classification,4,"For a question Q , the generating process is :",related work,0,107,73,0,10
text-classification,4,"where CNN and DCNN denote the basic CNN unit and deconvolution layer , respectively , as discussed in Section 2.1 .",related work,0,108,74,0,21
text-classification,4,parameters ?,related work,0,109,75,0,2
text-classification,4,q e and ?,related work,0,110,76,0,4
text-classification,4,q dare to be learned .,related work,0,111,77,0,6
text-classification,4,"The same process can be utilized to produce encodings z a and filters fa for the answer input , A , with parameters ?",related work,0,112,78,0,24
text-classification,4,a e and ?,related work,0,113,79,0,4
text-classification,4,"ad , respectively .",related work,0,114,80,0,4
text-classification,4,"The two sets of filter weights are then passed to adaptive convolution modules , along with Q and A , to obtain the extracted question and answer embeddings .",related work,0,115,81,0,29
text-classification,4,"That is , the question embedding is convolved with the filters produced by the answer and vise versa (? q and ?",related work,0,116,82,0,22
text-classification,4,a are the bias terms to be learned ) .,related work,0,117,83,0,10
text-classification,4,The key idea is to abstract informa-tion from the answer ( or question ) that is pertinent to the corresponding question ( or answer ) .,related work,0,118,84,0,26
text-classification,4,"Compared to a Siamese CNN architecture , our model selectively encapsulates the most important features for judgement prediction , removing less vital information .",related work,0,119,85,0,24
text-classification,4,We then employ the question and answer representations h q ?,related work,0,120,86,0,11
text-classification,4,"rn h , ha ?",related work,0,121,87,0,5
text-classification,4,Rn h as inputs to the matching module ( where n h is the dimension of question / answer embeddings ) .,related work,0,122,88,0,22
text-classification,4,"Following , the matching function is defined as :",related work,0,123,89,0,9
text-classification,4,where ?,related work,0,124,90,0,2
text-classification,4,"and denote an element - wise subtraction and element - wise product , respectively .",related work,0,125,91,0,15
text-classification,4,[h a ; h b ] indicates that ha and h bare stacked as column vectors .,related work,0,126,92,0,17
text-classification,4,The resulting matching vector t ?,related work,0,127,93,0,6
text-classification,4,R 4n h is then sent through an MLP layer ( with sigmoid activation function and parameters ?,related work,0,128,94,0,18
text-classification,4,"to be learned ) to model the desired conditional distribution p ( y i = 1|h q , ha ) .",related work,0,129,95,0,21
text-classification,4,"Notably , we share the weights of filter generating networks for both the question and answer , so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters .",related work,0,130,96,0,38
text-classification,4,All three modules in AdaQA model are jointly trained end - to - end .,related work,0,131,97,0,15
text-classification,4,"Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks , such as paraphrase identification ( see Section 5.2 ) .",related work,0,132,98,0,26
text-classification,4,connections to attention mechanism,related work,0,133,99,0,4
text-classification,4,"The adaptive context - sensitive filter generation mechanism proposed here bears close resemblance to attention mechanism ) widely adopted in the NLP community , in the sense that both methods intend to incorporate rich contextual information into text representations .",related work,0,134,100,0,40
text-classification,4,"However , attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers , and assigns different weights to each unit according to a context vector .",related work,0,135,101,0,32
text-classification,4,"By contrast , in our context - sensitive filter generation mechanism , the contextual information is inherently encoded into the convolutional filters , which directly interact with the input sentence during the convolution encoding operation .",related work,0,136,102,0,36
text-classification,4,"Notably , according to our experiments , the proposed filter generation module can be readily combined with ( standard ) attention mechanisms to further enhance the modeling expressiveness of CNN encoder .",related work,0,137,103,0,32
text-classification,4,experimental setup,experiment,0,138,1,0,2
text-classification,4,datasets,experiment,0,139,2,0,1
text-classification,4,We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks .,experiment,0,140,3,0,19
text-classification,4,"Specifically , we consider two large - scale document classification datasets :",experiment,0,141,4,0,12
text-classification,4,"Yelp Reviews Polarity , and DBPedia ontology datasets .",experiment,0,142,5,0,9
text-classification,4,"For Yelp reviews , we seek to predict a binary label ( positive or negative ) regarding one review about a restaurant .",experiment,0,143,6,0,23
text-classification,4,"DBpedia is extracted from Wikipedia by crowd - sourcing and is categorized into 14 non-overlapping ontology classes , including Company , Athlete , Natural Place , etc .",experiment,0,144,7,0,28
text-classification,4,"We sample 15 % of the training data as the validation set , to select hyperparameters for our models and perform early stopping .",experiment,0,145,8,0,24
text-classification,4,"For sentence matching , we evaluate the AdaQA model on two datasets for open - domain question answering : Wiki QA and SelQA .",experiment,0,146,9,0,24
text-classification,4,"Given a question , the task is to rank the corresponding candidate answers , which , in the case of WikiQA , are sentences extracted from the summary section of a related Wikipedia article .",experiment,0,147,10,0,35
text-classification,4,"To facilitate comparison with existing results , we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset .",experiment,0,148,11,0,27
text-classification,4,"We also consider the task of paraphrase identification with the Quora Question Pairs dataset , with the same data splits as in .",experiment,0,149,12,0,23
text-classification,4,A summary of all datasets is presented in .,experiment,0,150,13,0,9
text-classification,4,training details,experiment,0,151,14,0,2
text-classification,4,"For the document classification experiments , we randomly initialize the word embeddings uniformly within [ ? 0.001 , 0.001 ] and update them during training .",experiment,1,152,15,0,26
text-classification,4,"For the generated filters , we set the window size as h = 5 , with K = 100 feature maps ( the dimension of z is set as 100 ) .",experiment,1,153,16,0,32
text-classification,4,"For direct comparison , we employ the same filter shape / size settings as in our basic CNN implementation .",experiment,0,154,17,0,20
text-classification,4,"A one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .",experiment,1,155,18,0,32
text-classification,4,"The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .",experiment,1,156,19,0,21
text-classification,4,"We observed that a larger dropout rate ( e.g. , 0.5 ) will hurt performance on document classifications and make training significantly slower .",experiment,0,157,20,0,24
text-classification,4,"For the sentence matching tasks , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",experiment,1,158,21,0,30
text-classification,4,"As for the filters , we set the window size as h = 5 , with K = 300 feature maps .",experiment,1,159,22,0,22
text-classification,4,"As described in Section 3.3 , the vector t , output from the matching module , is fed to the prediction layer , implemented as a one - layer MLP followed by the sigmoid function .",experiment,0,160,23,0,36
text-classification,4,"We use Adam to train the models , with a learning rate of 3 10 ?4 .",experiment,1,161,24,0,17
text-classification,4,"Dropout , with a rate of 0.5 , is employed on the word embedding layer .",experiment,1,162,25,0,16
text-classification,4,The hyperparameters are selected by choosing the best model on the validation set .,experiment,0,163,26,0,14
text-classification,4,All models are implemented with TensorFlow and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12 GB memory .,experiment,1,164,27,0,22
text-classification,4,baselines,experiment,0,165,28,0,1
text-classification,4,"For document classification , we consider several baseline models : ( i ) ngrams , a bag - of - means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams ( up to 5 - grams ) from the training set and use their corresponding counts as features ; ( ii ) small / large word CNN : 6 layer word - based convolutional networks , with 256/1024 features at each layer , denoted as small / large , respectively ; ( iii ) deep CNN : deep convolutional neural networks with 9/17 /29 layers .",experiment,1,166,29,0,100
text-classification,4,"To evaluate the effectiveness of proposed AdaQA model , we compare it with several CNN - based sequence matching baselines , including Vanilla CNN , attentive pooling networks , and ABCNN ( where an attention mechanism is employed over the two sentence representations ) .",experiment,0,167,30,0,45
text-classification,4,evaluation metrics,experiment,0,168,31,0,2
text-classification,4,"For document categorization and paraphrase identification tasks , we em - , are reported by , and are reported by .",experiment,0,169,32,0,21
text-classification,4,ploy the percentage of correct predictions on the test set to evaluate and compare different models .,experiment,0,170,33,0,17
text-classification,4,"For the answer sentence selection task , mean average precision ( MAP ) and mean reciprocal rank ( MRR ) are utilized as the corresponding evaluation metrics .",experiment,0,171,34,0,28
text-classification,4,experimental results,experiment,0,172,1,0,2
text-classification,4,document classification,experiment,1,173,2,0,2
text-classification,4,"To explicitly explore whether our ACNN model can leverage the input-aware filter weights for better sentence representation , we perform a comparison between the basic CNN and ACNN models with only a single filter , which are denoted as S - CNN , S - ACNN , respectively ( this setting may not yield best over all performance , since only a single filter is used , but it allows us to isolate the impact of adaptivity ) .",experiment,0,174,3,0,79
text-classification,4,"As illustrated in , S - ACNN significantly outperforms S - CNN on both datasets , demonstrating the advantage of the filtergeneration module in our ACNN framework .",experiment,1,175,4,0,28
text-classification,4,"As a result , with only one convolutional filter and thus very limited modeling capacity , our S - ACNN model tends to be much more expressive than the basic CNN model , due to the flexibility of applying different filters to different sentences .",experiment,0,176,5,0,45
text-classification,4,We further experiment on both ACNN and CNN models with multiple filters .,experiment,0,177,6,0,13
text-classification,4,The corresponding document categorization accuracies are presented in .,experiment,0,178,7,0,9
text-classification,4,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .",experiment,1,179,8,0,24
text-classification,4,"Moreover , our method exhibits higher accuracy than n-grams , which is a very strong baseline as shown in .",experiment,0,180,9,0,20
text-classification,4,We attribute the superior performance of the ACNN framework to its stronger ( adaptive ) feature - extraction ability .,experiment,0,181,10,0,20
text-classification,4,"Moreover , our M - ACNN also achieves slightly better performance than self - attentive sentence embeddings proposed in , which requires significant more parameters than our method .",experiment,1,182,11,0,29
text-classification,4,Effect of number of filters,experiment,0,183,12,0,5
text-classification,4,"To further demonstrate that the performance gains in document categorization experiments originates from the improved adaptivity of our ACNN framework , we implement the basic CNN model with different numbers of filter sizes , ranging from 1 to 1000 .",experiment,0,184,13,0,40
text-classification,4,"As illustrated in ( a ) , when the filter size is larger than 100 , the test accuracy of the standard CNN model does not show any noticeable improvement with more filters .",experiment,0,185,14,0,34
text-classification,4,"More importantly , even with a filter size of 1000 , the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100 .",experiment,0,186,15,0,33
text-classification,4,"Given these observations , we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature extraction of our ACNN framework .",experiment,0,187,16,0,27
text-classification,4,answer sentence selection,experiment,1,188,17,0,3
text-classification,4,"To elucidate the role of different parts ( modules ) in our AdaQA model , we implement several model variants for comparison : ( i ) a "" vanilla "" CNN model that independently encodes two sentence representations for matching ; ( ii ) a self - adaptive , and marked with are from .",experiment,0,189,18,0,55
text-classification,4,"ACNN - based model where the question / answer sentence generates adaptive filters only to convolve with the input itself ; ( iii ) a one - way ACNN model where only the answer sentence representation is extracted with adaptive filters , which are generated conditioned on the question ; ( iv ) a two - way AdaQA model as described in Section 2.4 , where both sentences are adaptively encoded , with filters generated conditioned on the other sequence ; ( v ) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks ( see Section 3.4 ) , we experiment with another model variant that combines the proposed context - sensitive filter generation mechanism with the multi-perspective attention layer introduced in .",experiment,0,190,19,0,133
text-classification,4,"show experimental results of our models on WikiQA and Se lQA datasets , along with other state - of - the - art methods .",experiment,0,191,20,0,25
text-classification,4,"Note that the self - adaptive ACNN model variant , which generates filters only for the input itself ( without any interactions before the top matching module ) , slightly outperforms the vanilla CNN Siamese model .",experiment,0,192,21,0,37
text-classification,4,"Combined with the results in document categorization experiments , we believe that our ACNN framework , in its simplest form , can be utilized as a powerful feature extractor for transforming natural language sentences into fixed - length vectors .",experiment,0,193,22,0,40
text-classification,4,"More importantly , our two - way AdaQA model exhibits superior results compared with the one - way variant as well as other CNN - based baseline models on the WikiQA dataset .",experiment,0,194,23,0,33
text-classification,4,This observation indicates that the bidirectional filter gener - Model Accuracy Siamese - CNN 0.7960,experiment,0,195,24,0,15
text-classification,4,Multi-Perspective-CNN 0 . 8138 AdaQA ( two-way ) 0.8516 AdaQA ( two-way ) + att. 0.8794 ation mechanism is strongly associated with the performance gains .,experiment,0,196,25,0,26
text-classification,4,"While combined with the multi-perspective attention layers , adopted after the ACNN encoding layer , our two - way AdaQA model achieves even better performance .",experiment,0,197,26,0,26
text-classification,4,"This suggests that the proposed strategy is complementary , in terms of the incorporation of rich contextual information , to the standard attention mechanism .",experiment,0,198,27,0,25
text-classification,4,"The same trend is also observed on the SelQA dataset ( as shown in ) , which is a much larger dataset than Wiki QA .",experiment,0,199,28,0,26
text-classification,4,"Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .",experiment,1,200,29,0,23
text-classification,4,"We attribute the improvement to two potential advantages of our AdaQA model : ( i ) for the two previous baseline methods , the interaction between question and answer takes place either before or after convolution .",experiment,0,201,30,0,37
text-classification,4,"However , in our AdaQA model , the communication between two sentences is inherent in the convolution operation , and thus can provide the abstracted features with more flexibility ; ( ii ) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer , which could enable the model to recover from initial local maxima corresponding to incorrect predictions .",experiment,0,202,31,0,68
text-classification,4,paragraph identification,experiment,0,203,32,0,2
text-classification,4,"Considering that the proposed AdaQA model can be readily generalized to other text sequence matching problems , we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset .",experiment,0,204,33,0,35
text-classification,4,"To ensure a fair comparison , we employ the same data splits as in .",experiment,0,205,34,0,15
text-classification,4,"As illustrated in , our twoway AdaQA model again exhibits superior performances compared with basic CNN models ( as reported in ) .",experiment,0,206,35,0,23
text-classification,4,discussion,experiment,0,207,36,0,1
text-classification,4,reasoning ability,experiment,0,208,37,0,2
text-classification,4,"To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model , we further categorize the questions in the WikiQA test set into 5 types containing : ' What ' , ' Where ' , ' How ' , ' When ' or ' Who ' .",experiment,0,209,38,0,52
text-classification,4,We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types .,experiment,0,210,39,0,19
text-classification,4,"Similar to the findings in , we observe that the ' How ' question is the hardest to answer , with the lowest MAP scores .",experiment,0,211,40,0,26
text-classification,4,"However , our AdaQA model improves most over the basic CNN on the ' How ' type question , see ( b ) .",experiment,0,212,41,0,24
text-classification,4,"Further comparing our results with NASM in , our AdaQA model ( with a MAP score of 0.579 ) outperforms their reported ' How ' question MAP scores ( 0.524 ) by a large margin , indicating that the adaptive convolutional filter - generation mechanism improves the model 's ability to read and reason over natural language sentences .",experiment,0,213,42,0,59
text-classification,4,filter visualization,experiment,0,214,43,0,2
text-classification,4,"To better understand what information has been encoded into our contextsensitive filters , we visualize one of the filters for sentences within the test set ( on the DBpedia dataset ) with t- SNE .",experiment,0,215,44,0,35
text-classification,4,The corresponding results are shown in ( c ) .,experiment,0,216,45,0,10
text-classification,4,"It can be observed that the filters for documents with the same label ( ontology ) are grouped into clusters , indicating that for different types of document , ACNN has leveraged distinct convolutional filters for better feature extraction .",experiment,0,217,46,0,40
text-classification,4,"We presented a context - sensitive convolutional filter - generation mechanism , introducing a meta network to adaptively produce a set of input -aware filters .",experiment,0,218,47,0,26
text-classification,4,"In this manner , the filter weights vary from sample to sample , providing the CNN encoder network with more modeling flexibility and capacity .",experiment,0,219,48,0,25
text-classification,4,"This framework is further generalized to model question - answer sentence pairs , leveraging a twoway feature abstraction process .",experiment,0,220,49,0,20
text-classification,4,"We evaluate our models on several document - categorization and sentence matching benchmarks , and they consistently outperform the standard CNN and attentionbased CNN baselines , demonstrating the effectiveness of our framework .",experiment,0,221,50,0,33
text-classification,6,Universal Sentence Encoder,title,1,2,1,0,3
text-classification,6,abstract,abstract,0,3,1,0,1
text-classification,6,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,abstract,1,4,2,0,19
text-classification,6,The models are efficient and result in accurate performance on diverse transfer tasks .,abstract,0,5,3,0,14
text-classification,6,Two variants of the encoding models allow for trade - offs between accuracy and compute resources .,abstract,0,6,4,0,17
text-classification,6,"For both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance .",abstract,0,7,5,0,29
text-classification,6,Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning .,abstract,0,8,6,0,26
text-classification,6,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,abstract,1,9,7,0,15
text-classification,6,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .",abstract,1,10,8,0,24
text-classification,6,We obtain encouraging results on Word Embedding Association Tests ( WEAT ) targeted at detecting model bias .,abstract,0,11,9,0,18
text-classification,6,Our pre-trained sentence encoding models are made freely available for download and on TF Hub .,abstract,0,12,10,0,16
text-classification,6,introduction,introduction,0,13,1,0,1
text-classification,6,Limited amounts of training data are available for many NLP tasks .,introduction,0,14,2,0,12
text-classification,6,This presents a challenge for data hungry deep learning methods .,introduction,0,15,3,0,11
text-classification,6,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .",introduction,0,16,4,0,26
text-classification,6,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,introduction,0,17,5,0,28
text-classification,6,"However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .",introduction,0,18,6,0,16
text-classification,6,"In this paper , we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks .",introduction,0,19,7,0,26
text-classification,6,We include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size .,introduction,0,20,8,0,24
text-classification,6,We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data .,introduction,0,21,9,0,23
text-classification,6,The sentence encoding models are made publicly available on TF Hub .,introduction,0,22,10,0,12
text-classification,6,Engineering characteristics of models used for transfer learning are an important consideration .,introduction,0,23,11,0,13
text-classification,6,We discuss modeling trade - offs regarding memory requirements as well as compute time on CPU and GPU .,introduction,0,24,12,0,19
text-classification,6,Resource consumption comparisons are made for sentences of varying lengths .,introduction,0,25,13,0,11
text-classification,6,"import tensorflow_hub as hub embed = hub.Module ( "" https://tfhub.dev/google/ "" "" universal- sentence - encoder / 1 "" ) embedding = embed ( [",introduction,0,26,14,0,25
text-classification,6,""" The quick brown fox jumps over the lazy dog . "" ] )",introduction,0,27,15,0,14
text-classification,6,Listing 1 : Python example code for using the universal sentence encoder .,introduction,0,28,16,0,13
text-classification,6,model toolkit,introduction,0,29,17,0,2
text-classification,6,We make available two new models for encoding sentences into embedding vectors .,introduction,0,30,18,0,13
text-classification,6,"One makes use of the transformer architecture , while the other is formulated as a deep averaging network ( DAN ) .",introduction,0,31,19,0,22
text-classification,6,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,introduction,1,32,20,0,17
text-classification,6,The models take as input English strings and produce as output a fixed dimensional embedding representation of the string .,introduction,0,33,21,0,20
text-classification,6,Listing 1 provides a minimal code snippet to convert a sentence into a tensor containing its sentence embedding .,introduction,0,34,22,0,19
text-classification,6,The embedding tensor can be used directly or incorporated into larger model graphs for specific tasks .,introduction,0,35,23,0,17
text-classification,6,"As illustrated in , the sentence embeddings can be trivially used to compute sentence level semantic similarity scores that achieve excellent performance on the semantic textual similarity ( STS ) Benchmark .",introduction,0,36,24,0,32
text-classification,6,"When included within larger models , the sentence encoding models can be fine tuned for specific tasks using gradient based updates .",introduction,0,37,25,0,22
text-classification,6,encoders,introduction,0,38,26,0,1
text-classification,6,We introduce the model architecture for our two encoding models in this section .,introduction,0,39,27,0,14
text-classification,6,Our two encoders have different design goals .,introduction,0,40,28,0,8
text-classification,6,One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption .,introduction,0,41,29,0,20
text-classification,6,The other targets efficient inference with slightly reduced accuracy .,introduction,0,42,30,0,10
text-classification,6,transformer,introduction,0,43,31,0,1
text-classification,6,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,introduction,1,44,32,0,20
text-classification,6,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,introduction,1,45,33,0,31
text-classification,6,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,introduction,1,46,34,0,29
text-classification,6,The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding .,introduction,0,47,35,0,21
text-classification,6,The encoding model is designed to be as general purpose as possible .,introduction,1,48,36,0,13
text-classification,6,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,introduction,1,49,37,0,20
text-classification,6,The supported tasks include : a Skip - Thought like task for the unsupervised learning from arbitrary running text ; a conversational input - response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data .,introduction,0,50,38,0,43
text-classification,6,The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .,introduction,0,51,39,0,22
text-classification,6,"As will be shown in the experimental results below , the transformer based encoder achieves the best over all transfer task performance .",introduction,0,52,40,0,23
text-classification,6,"However , this comes at the cost of compute time and memory usage scaling dramatically with sentence length .",introduction,0,53,41,0,19
text-classification,6,deep averaging network ( dan ),introduction,1,54,42,0,6
text-classification,6,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,introduction,1,55,43,0,42
text-classification,6,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",introduction,1,56,44,0,25
text-classification,6,The DAN encoder is trained similarly to the Transformer based encoder .,introduction,0,57,45,0,12
text-classification,6,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,introduction,1,58,46,0,22
text-classification,6,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,introduction,1,59,47,0,21
text-classification,6,"Similar to , our results demonstrate that DANs achieve strong baseline performance on text classification tasks .",introduction,0,60,48,0,17
text-classification,6,encoder training data,introduction,0,61,49,0,3
text-classification,6,Unsupervised training data for the sentence encoding models are drawn from a variety of web sources .,introduction,0,62,50,0,17
text-classification,6,"The sources are Wikipedia , web news , web question - answer pages and discussion forums .",introduction,0,63,51,0,17
text-classification,6,We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference ( SNLI ) corpus .,introduction,0,64,52,0,20
text-classification,6,"Similar to the findings of , we observe that training to SNLI improves transfer performance .",introduction,0,65,53,0,16
text-classification,6,transfer tasks,introduction,0,66,54,0,2
text-classification,6,This section presents an overview of the data used for the transfer learning experiments and the Word Embedding Association Test ( WEAT ) data used to characterize model bias .,introduction,0,67,55,0,30
text-classification,6,"summarizes the number of samples provided by the test portion of each evaluation set and , when available , the size of the dev and training data .",introduction,0,68,56,0,28
text-classification,6,MR : Movie review snippet sentiment on a five star scale .,introduction,1,69,57,0,12
text-classification,6,CR : Sentiment of sentences mined from customer reviews .,introduction,1,70,58,0,10
text-classification,6,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,introduction,1,71,59,0,12
text-classification,6,MPQA : Phrase level opinion polarity from news data .,introduction,1,72,60,0,10
text-classification,6,TREC : Fine grained question classification sourced from TREC .,introduction,1,73,61,0,10
text-classification,6,SST : Binary phrase level sentiment classification .,introduction,1,74,62,0,8
text-classification,6,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,introduction,1,75,63,0,20
text-classification,6,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,introduction,1,76,64,0,23
text-classification,6,"For sentence classification transfer tasks , the output of the transformer and DAN sentence encoders are provided to a task specific DNN .",introduction,0,77,65,0,23
text-classification,6,"For the pairwise semantic similarity task , we directly assess the similarity of the sentence embeddings produced by our two encoders .",introduction,0,78,66,0,22
text-classification,6,"As shown Eq. 1 , we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance .",introduction,0,79,67,0,30
text-classification,6,"5 sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /?",introduction,0,80,68,0,21
text-classification,6,( 1 ),introduction,0,81,69,0,3
text-classification,6,baselines,introduction,0,82,70,0,1
text-classification,6,"For each transfer task , we include baselines that only make use of word level transfer and baselines that make use of no transfer learning at all .",introduction,0,83,71,0,28
text-classification,6,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .",introduction,1,84,72,0,25
text-classification,6,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,introduction,1,85,73,0,25
text-classification,6,The baselines that use pretrained word embeddings allow us to contrast word versus sentence level transfer .,introduction,0,86,74,0,17
text-classification,6,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,introduction,1,87,75,0,17
text-classification,6,combined transfer models,introduction,0,88,76,0,3
text-classification,6,We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation :,introduction,0,89,77,0,21
text-classification,6,Model performance on transfer tasks .,introduction,0,90,78,0,6
text-classification,6,use,introduction,0,91,79,0,1
text-classification,6,T is the universal sentence encoder ( USE ) using Transformer .,introduction,0,92,80,0,12
text-classification,6,use,introduction,0,93,81,0,1
text-classification,6,Dis the universal encoder DAN model .,introduction,0,94,82,0,7
text-classification,6,"Models tagged with w2 v w.e. make use of pre-training word2vec skip - gram embeddings for the transfer task model , while models tagged with lrn w.e. use randomly initialized word embeddings that are learned only on the transfer task data .",introduction,0,95,83,0,42
text-classification,6,Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments .,introduction,0,96,84,0,23
text-classification,6,Pairwise similarity scores are computed directly using the sentence embeddings from the universal sentence encoder as in Eq. ( 1 ) .,introduction,0,97,85,0,22
text-classification,6,to the transfer task classification layers .,introduction,0,98,86,0,7
text-classification,6,"For completeness , we also explore concatenating the representations from sentence level transfer models with the baseline models that do not make use of word level transfer learning .",introduction,0,99,87,0,29
text-classification,6,experiments,experiment,0,100,1,0,1
text-classification,6,Transfer task model hyperparamaters are tuned using a combination of Vizier and light manual tuning .,experiment,0,101,2,0,16
text-classification,6,"When available , model hyperparameters are tuned using task dev sets .",experiment,0,102,3,0,12
text-classification,6,"Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .",experiment,0,103,4,0,28
text-classification,6,Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs .,experiment,0,104,5,0,24
text-classification,6,Transfer learning is critically important when training data for a target task is limited .,experiment,0,105,6,0,15
text-classification,6,We explore the impact on task performance of varying the amount of training data available for the task both with and without the use of transfer learning .,experiment,0,106,7,0,28
text-classification,6,"Contrasting the transformer and DAN based encoders , we demonstrate trade - offs in model complexity and the amount of data required to reach a desired level of accuracy on a task .",experiment,0,107,8,0,33
text-classification,6,"To assess bias in our encoding models , we evaluate the strength of various associations learned by our model on WEAT word lists .",experiment,0,108,9,0,24
text-classification,6,We compare our result to those of who discovered that word embeddings could be used to reproduce human performance on implicit association tasks for both benign and potentially undesirable associations .,experiment,0,109,10,0,31
text-classification,6,results,result,0,110,1,0,1
text-classification,6,Transfer task performance is summarized in Table 2 .,result,0,111,2,0,9
text-classification,6,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,result,1,112,3,0,25
text-classification,6,"Hoewver , transfer learning using the simpler and fast DAN encoder can for some tasks perform as well or better than the more sophisticated transformer encoder .",result,0,113,4,0,27
text-classification,6,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,result,1,114,5,0,22
text-classification,6,The best performance on most tasks is obtained by models that make use of both sentence and word level transfer .,result,0,115,6,0,21
text-classification,6,illustrates transfer task performance for varying amounts of training data .,result,0,116,7,0,11
text-classification,6,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .",result,1,117,8,0,21
text-classification,6,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .",result,1,118,9,0,24
text-classification,6,contrasts 's findings on bias within GloVe embeddings with the DAN variant of the universal encoder .,result,0,119,10,0,17
text-classification,6,"Similar to GloVe , our model reproduces human associations between flowers vs. insects and pleasantness vs. unpleasantness .",result,0,120,11,0,18
text-classification,6,"However , our model demonstrates weaker associations than GloVe for probes targeted at revealing at ageism , racism and sexism .",result,0,121,12,0,21
text-classification,6,The differences in word association patterns can be attributed to differences in the training data composition and the mixture of tasks used to train the sentence embeddings .,result,0,122,13,0,28
text-classification,6,discussion,result,0,123,14,0,1
text-classification,6,Transfer learning leads to performance improvements on many tasks .,result,0,124,15,0,10
text-classification,6,Using transfer learning is more critical when less training data is available .,result,0,125,16,0,13
text-classification,6,"When task performance is close , the correct modeling choice should take into account engineering trade - offs regarding the memory and compute 6 Researchers and developers are strongly encouraged to independently verify whether biases in their over all model or model components impacts their use case .",result,0,126,17,0,48
text-classification,6,For resources on ML fairness visit https://developers.google.com/machinelearning/fairness-overview/.,result,0,127,18,0,7
text-classification,6,resource requirements introduced by the different models that could be used .,result,0,128,19,0,12
text-classification,6,resource usage,result,0,129,20,0,2
text-classification,6,This section describes memory and compute resource usage for the transformer and DAN sentence encoding models for different sentence lengths .,result,0,130,21,0,21
text-classification,6,Figure 2 plots model resource usage against sentence length .,result,0,131,22,0,10
text-classification,6,compute usage,result,0,132,23,0,2
text-classification,6,"The transformer model time complexity is O ( n 2 ) in sentence length , while the DAN model is O ( n ) .",result,0,133,24,0,25
text-classification,6,"As seen in ( a - b ) , for short sentences , the transformer encoding model is only moderately slower than the much simpler DAN model .",result,0,134,25,0,28
text-classification,6,"However , compute time for transformer increases noticeably as sentence length increases .",result,0,135,26,0,13
text-classification,6,"In contrast , the compute time for the DAN model stays nearly constant as sentence length is increased .",result,0,136,27,0,19
text-classification,6,"Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .",result,0,137,28,0,27
text-classification,6,memory usage,result,0,138,29,0,2
text-classification,6,"The transformer model space complexity also scales quadratically , O ( n 2 ) , in sentence length , while the DAN model space complexity is constant in the length of the sentence . Similar to compute usage , memory usage for the transformer model increases quickly with sentence length , while the memory usage for the DAN model remains constant .",result,0,139,30,0,62
text-classification,6,"We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .",result,0,140,31,0,26
text-classification,6,"Since the transformer model only needs to store unigram embeddings , for short sequences it requires nearly half as much memory as the DAN model .",result,0,141,32,0,26
text-classification,6,conclusion,result,0,142,33,0,1
text-classification,6,Both the transformer and DAN based universal encoding models provide sentence level embeddings that demonstrate strong transfer performance on a number of NLP tasks .,result,0,143,34,0,25
text-classification,6,The sentence level embeddings surpass the performance of transfer learning using word level embeddings alone .,result,0,144,35,0,16
text-classification,6,Models that make use of sentence and word level transfer achieve the best over all performance .,result,0,145,36,0,17
text-classification,6,We observe that transfer learning is most helpful when limited training data is available for the transfer task .,result,0,146,37,0,19
text-classification,6,The encoding models make different trade - offs regarding accuracy and model complexity that should be considered when choosing the best model for a particular application .,result,0,147,38,0,27
text-classification,6,The pre-trained encoding models will be made publicly available for research and use in applications that can benefit from a better understanding of natural language .,result,0,148,39,0,26
text-classification,2,Bag of Tricks for Efficient Text Classification,title,1,2,1,0,7
text-classification,2,abstract,abstract,0,3,1,0,1
text-classification,2,This paper explores a simple and efficient baseline for text classification .,abstract,1,4,2,0,12
text-classification,2,"Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation .",abstract,0,5,3,0,33
text-classification,2,"We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU , and classify half a million sentences among 312K classes in less than a minute .",abstract,0,6,4,0,36
text-classification,2,introduction,introduction,0,7,1,0,1
text-classification,2,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",introduction,0,8,2,0,27
text-classification,2,"Recently , models based on neural networks have become increasingly popular .",introduction,0,9,3,0,12
text-classification,2,"While these models achieve very good performance in practice , they tend to be relatively slow both at train and test time , limiting their use on very large datasets .",introduction,0,10,4,0,31
text-classification,2,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",introduction,0,11,5,0,15
text-classification,2,"Despite their simplicity , they often obtain stateof - the - art performances if the right features are used .",introduction,0,12,6,0,20
text-classification,2,They also have the potential to scale to very large corpus .,introduction,0,13,7,0,12
text-classification,2,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",introduction,0,14,8,0,28
text-classification,2,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",introduction,0,15,9,0,50
text-classification,2,"We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",introduction,0,16,10,0,21
text-classification,2,model architecture,introduction,0,17,11,0,2
text-classification,2,"A simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",introduction,0,18,12,0,34
text-classification,2,"However , linear classifiers do not share parameters among features and classes .",introduction,0,19,13,0,13
text-classification,2,This possibly limits their generalization in the context of large output space where some classes have very few examples .,introduction,0,20,14,0,20
text-classification,2,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,introduction,0,21,15,0,22
text-classification,2,shows a simple linear model with rank constraint .,introduction,1,22,16,0,9
text-classification,2,The first weight matrix A is a look - up table over the words .,introduction,1,23,17,0,15
text-classification,2,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",introduction,1,24,18,0,21
text-classification,2,The text representa - tion is an hidden variable which can be potentially be reused .,introduction,0,25,19,0,16
text-classification,2,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",introduction,0,26,20,0,20
text-classification,2,We use the softmax function f to compute the probability distribution over the predefined classes .,introduction,1,27,21,0,16
text-classification,2,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",introduction,0,28,22,0,18
text-classification,2,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",introduction,0,29,23,0,26
text-classification,2,This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .,introduction,0,30,24,0,19
text-classification,2,hierarchical softmax,introduction,0,31,25,0,2
text-classification,2,"When the number of classes is large , computing the linear classifier is computationally expensive .",introduction,0,32,26,0,16
text-classification,2,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",introduction,0,33,27,0,27
text-classification,2,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",introduction,1,34,28,0,21
text-classification,2,"During training , the computational complexity drops to O ( h log 2 ( k ) ) .",introduction,0,35,29,0,18
text-classification,2,The hierarchical softmax is also advantageous at test time when searching for the most likely class .,introduction,0,36,30,0,17
text-classification,2,Each node is associated with a probability that is the probability of the path from the root to that node .,introduction,0,37,31,0,21
text-classification,2,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",introduction,0,38,32,0,25
text-classification,2,This means that the probability of anode is always lower than the one of its parent .,introduction,0,39,33,0,17
text-classification,2,Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability .,introduction,0,40,34,0,28
text-classification,2,"In practice , we observe a reduction of the complexity to O ( h log 2 ( k ) ) at test time .",introduction,0,41,35,0,24
text-classification,2,"This approach is further extended to compute the T - top targets at the cost of O ( log ( T ) ) , using a binary heap .",introduction,0,42,36,0,29
text-classification,2,n - gram features,introduction,0,43,37,0,4
text-classification,2,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,introduction,0,44,38,0,21
text-classification,2,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",introduction,1,45,39,0,22
text-classification,2,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,introduction,1,46,40,0,18
text-classification,2,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",introduction,1,47,41,0,36
text-classification,2,experiments,experiment,0,48,1,0,1
text-classification,2,We evaluate fastText on two different tasks .,experiment,0,49,2,0,8
text-classification,2,"First , we compare it to existing text classifers on the problem of sentiment analysis .",experiment,0,50,3,0,16
text-classification,2,"Then , we evaluate its capacity to scale to large output space on a tag prediction dataset .",experiment,0,51,4,0,18
text-classification,2,"Note that our model could be implemented with the Vowpal Wabbit library , 2 but we observe in practice , that our tailored implementation is at least 2 - 5 faster .",experiment,0,52,5,0,32
text-classification,2,sentiment analysis,experiment,1,53,6,0,2
text-classification,2,datasets and baselines .,experiment,0,54,7,0,4
text-classification,2,We employ the same 8 datasets and evaluation protocol of .,experiment,0,55,8,0,11
text-classification,2,We report the n-grams and TFIDF baselines from We also compare to following their evaluation protocol .,experiment,0,56,9,0,17
text-classification,2,We report their main baselines as well as their two approaches based on recurrent networks ( Conv - GRNN and LSTM - GRNN ) .,experiment,0,57,10,0,25
text-classification,2,results .,result,0,58,1,0,2
text-classification,2,We present the results in .,result,0,59,2,0,6
text-classification,2,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",result,1,60,3,0,31
text-classification,2,"On this task , adding bigram information improves the performance by 1 - 4 % .",result,1,61,4,0,16
text-classification,2,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",result,1,62,5,0,22
text-classification,2,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",result,1,63,6,0,28
text-classification,2,"Finally , shows that our method is competitive with the methods presented in .",result,0,64,7,0,14
text-classification,2,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,result,0,65,8,0,22
text-classification,2,"Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference in accuracy .",result,0,66,9,0,21
text-classification,2,We show a few correct and incorrect tag predictions .,result,0,67,10,0,10
text-classification,2,"up compared to neural network based methods increases with the size of the dataset , going up to at least a 15,000 speed - up .",result,0,68,11,0,26
text-classification,2,tag prediction,result,1,69,12,0,2
text-classification,2,dataset and baselines .,result,0,70,13,0,4
text-classification,2,"To test scalability of our approach , further evaluation is carried on the YFCC100M dataset which consists of almost 100M images with captions , titles and tags .",result,0,71,14,0,28
text-classification,2,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,result,0,72,15,0,21
text-classification,2,"We remove the words and tags occurring less than 100 times and split the data into a train , validation and test set .",result,0,73,16,0,24
text-classification,2,"The train set contains 91,188,648 examples ( 1.5B tokens ) .",result,0,74,17,0,11
text-classification,2,"The validation has 930,497 examples and the test set 543,424 .",result,0,75,18,0,11
text-classification,2,"The vocabulary size is 297,141 and there are 312,116 unique tags .",result,0,76,19,0,12
text-classification,2,We will release a script that recreates this dataset so that our numbers could be reproduced .,result,0,77,20,0,17
text-classification,2,We report precision at 1 .,result,0,78,21,0,6
text-classification,2,We consider a frequency - based baseline which predicts the most frequent tag .,result,0,79,22,0,14
text-classification,2,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",result,0,80,23,1,31
text-classification,2,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",result,0,81,24,0,24
text-classification,2,Results and training time . and 200 .,result,0,82,25,0,8
text-classification,2,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",result,0,83,26,0,23
text-classification,2,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",result,1,84,27,0,45
text-classification,2,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .",result,1,85,28,0,19
text-classification,2,The speedup of the test phase is even more significant ( a 600 speedup ) .,result,0,86,29,0,16
text-classification,2,shows some qualitative examples .,result,0,87,30,0,5
text-classification,2,discussion and conclusion,result,0,88,31,0,3
text-classification,2,"In this work , we propose a simple baseline method for text classification .",result,0,89,32,0,14
text-classification,2,"Unlike unsupervisedly trained word vectors from word2vec , our word features can be averaged together to form good sentence representations .",result,0,90,33,0,21
text-classification,2,"In several tasks , fastText obtains performance on par with recently proposed methods inspired by deep learning , while being much faster .",result,0,91,34,0,23
text-classification,2,"Although deep neural networks have in theory much higher representational power than shallow models , it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them .",result,0,92,35,0,36
text-classification,2,We will publish our code so that the research community can easily build on top of our work .,result,0,93,36,0,19
text-classification,3,On the Role of Text Preprocessing in Neural Network Architectures : An Evaluation Study on Text Categorization and Sentiment Analysis,title,1,2,1,0,20
text-classification,3,abstract,abstract,0,3,1,0,1
text-classification,3,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .",abstract,1,4,2,0,28
text-classification,3,"Despite its importance , text preprocessing has not received much attention in the deep learning literature .",abstract,0,5,3,0,17
text-classification,3,"In this paper we investigate the impact of simple text preprocessing decisions ( particularly tokenizing , lemmatizing , lowercasing and multiword grouping ) on the performance of a standard neural text classifier .",abstract,0,6,4,0,33
text-classification,3,We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis .,abstract,0,7,5,0,15
text-classification,3,"While our experiments show that a simple tokenization of input text is generally adequate , they also highlight significant degrees of variability across preprocessing techniques .",abstract,0,8,6,0,26
text-classification,3,"This reveals the importance of paying attention to this usually - overlooked step in the pipeline , particularly when comparing different models .",abstract,0,9,7,0,23
text-classification,3,"Finally , our evaluation provides insights into the best preprocessing practices for training word embeddings .",abstract,0,10,8,0,16
text-classification,3,introduction,introduction,0,11,1,0,1
text-classification,3,"Words are often considered as the basic constituents of texts for many languages , including English .",introduction,0,12,2,0,17
text-classification,3,The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words .,introduction,0,13,3,0,18
text-classification,3,"However , in practise , other preprocessing techniques can be ( and are ) further used together with tokenization .",introduction,0,14,4,0,20
text-classification,3,"These include lemmatization , lowercasing and 1 Note that although word - based models are mainstream in NLP in general and text classification in particular , recent work has also considered other linguistic units , such as characters or word senses .",introduction,0,15,5,0,42
text-classification,3,"These techniques require a different kind of preprocessing and , while they have been shown effective in various settings , in this work we only focus on the mainstream word - based models .",introduction,0,16,6,0,34
text-classification,3,"multiword grouping , among others .",introduction,0,17,7,0,6
text-classification,3,"Although these preprocessing decisions have been studied in the context of conventional text classification techniques , little attention has been paid to them in the more recent neural - based models .",introduction,0,18,8,0,32
text-classification,3,"The most similar study to ours is , which analyzed different encoding levels for English and Asian languages such as Chinese , Japanese and Korean .",introduction,0,19,9,0,26
text-classification,3,"As opposed to our work , their analysis was focused on UTF - 8 bytes , characters , words , romanized characters and romanized words as encoding levels , rather than the preprocessing techniques analyzed in this paper .",introduction,0,20,10,0,39
text-classification,3,"Additionally , word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems .",introduction,0,21,11,0,21
text-classification,3,"However , while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus , the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied .",introduction,0,22,12,0,37
text-classification,3,"In this paper we focus on the role of preprocessing the input text , particularly in how it is split into individual ( meaning - bearing ) tokens and how it affects the performance of standard neural text classification models based on .",introduction,1,23,13,0,43
text-classification,3,"CNNs have proven to be effective in a wide range of NLP applications , in - cluding text classification tasks such as topic categorization and polarity detection , which are the tasks considered in this work .",introduction,0,24,14,0,37
text-classification,3,The goal of our evaluation study is to find answers to the following two questions :,introduction,0,25,15,0,16
text-classification,3,1 .,introduction,0,26,16,0,2
text-classification,3,Are neural network architectures ( in particular CNNs ) affected by seemingly small preprocessing decisions in the input text ?,introduction,0,27,17,0,20
text-classification,3,2 .,introduction,0,28,18,0,2
text-classification,3,Does the preprocessing of the embeddings ' underlying training corpus have an impact on the final performance of a state - of - the - art neural network text classifier ?,introduction,0,29,19,0,31
text-classification,3,"According to our experiments in topic categorization and polarity detection , these decisions are important in certain cases .",introduction,0,30,20,0,19
text-classification,3,"Moreover , we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting .",introduction,0,31,21,0,30
text-classification,3,The accompanying materials of this submission can be downloaded at the following repository : github.com/pedrada88/preproc-textclassification .,introduction,1,32,22,0,16
text-classification,3,text preprocessing,introduction,0,33,23,0,2
text-classification,3,"Given an input text , words are gathered as input units of classification models through tokenization .",introduction,0,34,24,0,17
text-classification,3,We refer to the corpus which is only tokenized as vanilla .,introduction,0,35,25,0,12
text-classification,3,"For example , given the sentence "" Apple is asking its manufacturers to move Mac - Book Air production to the United States . "" ( running example ) , the vanilla tokenized text would be as follows ( white spaces delimiting different word units ) :",introduction,0,36,26,0,47
text-classification,3,Apple is asking its manufacturers to move MacBook Air production to the United States .,introduction,0,37,27,0,15
text-classification,3,"We additionally consider three simple preprocessing techniques to be applied to an input text : lowercasing ( Section 2.1 ) , lemmatizing ( Section 2.2 ) and multiword grouping ( Section 2.3 ) .",introduction,0,38,28,0,34
text-classification,3,lowercasing,introduction,0,39,29,0,1
text-classification,3,This is the simplest preprocessing technique which consists of lowercasing each single token of the input text :,introduction,0,40,30,0,18
text-classification,3,apple is asking its manufacturers to move macbook air production to the united states .,introduction,0,41,31,0,15
text-classification,3,"Due to its simplicity , lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages .",introduction,0,42,32,0,22
text-classification,3,"Despite its desirable property of reducing sparsity and vocabulary size , lowercasing may negatively impact system 's performance by increasing ambiguity .",introduction,0,43,33,0,22
text-classification,3,"For instance , the Apple company in our example and the apple fruit would be considered as identical entities .",introduction,0,44,34,0,20
text-classification,3,lemmatizing,introduction,0,45,35,0,1
text-classification,3,The process of lemmatizing consists of replacing a given token with its corresponding lemma :,introduction,0,46,36,0,15
text-classification,3,Apple be ask its manufacturer to move Mac - Book Air production to the United States .,introduction,0,47,37,0,17
text-classification,3,Lemmatization has been traditionally a standard preprocessing technique for linear text classification systems .,introduction,0,48,38,0,14
text-classification,3,"However , it is rarely used as a preprocessing stage in neural - based systems .",introduction,0,49,39,0,16
text-classification,3,"The main idea behind lemmatization is to reduce sparsity , as different inflected forms of the same lemma may occur infrequently ( or not at all ) during training .",introduction,0,50,40,0,30
text-classification,3,"However , this may come at the cost of neglecting important syntactic nuances .",introduction,0,51,41,0,14
text-classification,3,multiword grouping,introduction,0,52,42,0,2
text-classification,3,This last preprocessing technique consists of grouping consecutive tokens together into a single token if found in a given inventory :,introduction,0,53,43,0,21
text-classification,3,Apple is asking its manufacturers to move MacBook Air production to the United States .,introduction,0,54,44,0,15
text-classification,3,"The motivation behind this step lies in the idiosyncratic nature of multiword expressions , e.g. United States in the example .",introduction,0,55,45,0,21
text-classification,3,The meaning of these multiword expressions are often hardly traceable from their individual tokens .,introduction,0,56,46,0,15
text-classification,3,"As a result , treating multiwords as single units may lead to better training of a given model .",introduction,0,57,47,0,19
text-classification,3,"Because of this , word embedding toolkits such as Word2vec propose statistical approaches for extracting these multiwords , or directly include multiwords along with single words in their pretrained embedding spaces .",introduction,0,58,48,0,32
text-classification,3,"We considered two tasks for our experiments : topic categorization , i.e. assigning a topic to a given document from a pre-defined set of topics , and polarity detection , i.e. detecting if the sentiment of a given piece of text is positive or negative .",introduction,0,59,49,0,46
text-classification,3,Two different settings were studied : ( 1 ) word embedding 's training corpus and the evaluation dataset were preprocessed in a similar manner ( Section 3.2 ) ; and ( 2 ) the two were preprocessed differently ( Section 3.3 ) .,introduction,0,60,50,0,43
text-classification,3,In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation .,introduction,0,61,51,0,21
text-classification,3,experimental setup,experiment,0,62,1,0,2
text-classification,3,We tried with two classification models .,experiment,1,63,2,0,7
text-classification,3,"The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .",experiment,1,64,3,0,20
text-classification,3,"In the second model , we add a recurrent layer ( specifically an LSTM ) before passing the pooled features directly to the fully connected softmax layer .",experiment,1,65,4,0,28
text-classification,3,The inclusion of this LSTM layer has been shown to be able to effectively replace multiple layers of convolution and be beneficial particularly for large inputs .,experiment,0,66,5,0,27
text-classification,3,"These models were used for both topic categorization and polarity detection tasks , with slight hyperparameter variations given their different natures ( mainly in their text size ) which were fixed across all datasets .",experiment,0,67,6,0,35
text-classification,3,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,experiment,1,68,7,0,24
text-classification,3,4 .,experiment,0,69,8,0,2
text-classification,3,evaluation datasets .,experiment,0,70,9,0,3
text-classification,3,"For the topic categorization task we used the BBC news dataset 5 , 20 News , Reuters 6 and The code for this CNN implementation is the same as in , which is available at https://github.com/pilehvar/sensecnn",experiment,0,71,10,0,36
text-classification,3,4 Context window of 5 words and hierarchical softmax .,experiment,0,72,11,0,10
text-classification,3,5 http://mlg.ucd.ie/datasets/bbc.html,experiment,0,73,12,0,2
text-classification,3,"6 Due to the large number of labels in the original Reuters ( i.e. 91 ) and to be consistent with the other datasets , we reduce the dataset to its 8 most frequent labels , a reduction already performed in previous works .",experiment,0,74,13,0,44
text-classification,3,preprocessing .,experiment,0,75,14,0,2
text-classification,3,Four different techniques ( see Section 2 ) were used to preprocess the datasets as well as the corpus which was used to train word embeddings ( i.e. UMBC ) .,experiment,0,76,15,0,31
text-classification,3,For tokenization and lemmatization we relied on Stanford CoreNLP .,experiment,0,77,16,0,10
text-classification,3,"As for multiwords , we used the phrases from the pre-trained Google News Word2vec vectors , which were obtained using a simple statistical approach .",experiment,0,78,17,0,25
text-classification,3,12 shows the accuracy 13 of the classification models using our four preprocessing techniques .,experiment,0,79,18,0,15
text-classification,3,We observe a certain variability of results depending on the preprocessing techniques used ( aver -7 ftp://medir.ohsu.edu/pub/ohsumed,experiment,0,80,19,0,17
text-classification,3,8 Both PL04 and PL05 were downloaded from http://www.cs.cornell.edu/people/pabo/movie-review-data/,experiment,0,81,20,0,9
text-classification,3,9 http://www.rottentomatoes.com,experiment,0,82,21,0,2
text-classification,3,"10 We mapped the numerical value of phrases to either negative ( from 0 to 0.4 ) or positive ( from 0.6 to 1 ) , removing the neutral phrases according to the scale ( from 0.4 to 0.6 ) .",experiment,0,83,22,0,41
text-classification,3,"For the datasets with train - test partitions , the sizes of the test sets are the following : 7,532 for 20 News ; 12,733 for Ohsumed ; 25,000 for IMDb ; and 1,000 for RTC .",experiment,0,84,23,0,37
text-classification,3,For future work it would be interesting to explore more complex methods to learn embeddings for multiword expressions .,experiment,0,85,24,0,19
text-classification,3,Computed by averaging accuracy of two different runs .,experiment,0,86,25,0,9
text-classification,3,"The statistical significance was calculated according to an unpaired t- test at the 5 % significance level . age variability 14 of 2.4 % for the CNN + LSTM model , including a statistical significance gap in seven of the nine datasets ) , which proves the influence of preprocessing on the final results .",experiment,0,87,26,0,55
text-classification,3,It is perhaps not surprising that the lowest variance of results is seen in the datasets with the larger training data ( i.e. RTC and Stanford ) .,experiment,0,88,27,0,28
text-classification,3,"This suggests that the preprocessing decisions are not so important when the training data is large enough , but they are indeed relevant in benchmarks where the training data is limited .",experiment,0,89,28,0,32
text-classification,3,"As far as the individual preprocessing techniques are concerned , the vanilla setting ( tokenization only ) proves to be consistent across datasets and tasks , as it performs in the same ballpark as the best result in 8 of the 9 datasets for both models ( with no noticeable differences between topic categorization and polarity detection ) .",experiment,0,90,29,0,59
text-classification,3,"The only topic categorization dataset in which tokenization does not seem enough is Ohsumed , which , unlike the more general nature of other categorization datasets ( news ) , belongs to a specialized domain ( medical ) for which fine - grained distinctions are required to classify cardiovascular diseases .",experiment,0,91,30,0,51
text-classification,3,"In particular for this dataset , word embeddings trained on a general - domain corpus like UMBC may not accurately capture the specialized meaning of medical terms and hence , sparsity becomes an issue .",experiment,0,92,31,0,35
text-classification,3,"In fact , lowercasing and lemmatizing , which are mainly aimed at reducing sparsity , outperform the vanilla setting by over six points in the CNN + LSTM setting and clearly outperform the other preprocessing techniques on the single CNN model as well .",experiment,0,93,32,0,44
text-classification,3,experiment 1 : preprocessing effect,experiment,1,94,1,0,5
text-classification,3,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .",experiment,1,95,2,0,21
text-classification,3,"Even though lemmatization has proved useful in conventional linear models as an effective way to deal with sparsity , neural network architectures seem to be more capable of overcoming sparsity thanks to the generalization power of word embeddings .",experiment,0,96,3,0,39
text-classification,3,experiment 2 : cross-preprocessing,experiment,1,97,1,0,4
text-classification,3,This experiment aims at studying the impact of using different word embeddings ( with differently preprocessed training corpora ) on tokenized datasets ( vanilla setting ) .,experiment,0,98,2,0,27
text-classification,3,shows the results for this experiment .,experiment,0,99,3,0,7
text-classification,3,"In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best over all performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .",experiment,1,100,4,0,65
text-classification,3,In this case the same set of words is learnt but single tokens inside multiword expressions are not trained .,experiment,0,101,5,0,20
text-classification,3,"Instead , these single tokens are considered in isolation only , without the added noise when considered inside the multiword expression as well .",experiment,0,102,6,0,24
text-classification,3,"For instance , the word Apple has a clearly different meaning in isolation from the one inside :",experiment,0,103,7,0,18
text-classification,3,Cross - preprocessing evaluation : accuracy on the topic categorization and polarity detection tasks using different sets of word embeddings to initialize the embedding layer of the two classifiers .,experiment,0,104,8,0,30
text-classification,3,All datasets were preprocessed similarly according to the vanilla setting .,experiment,0,105,9,0,11
text-classification,3,indicates results thatare statistically significant with respect to the top result .,experiment,0,106,10,0,12
text-classification,3,"the multiword expression Big Apple , hence it can be seen as beneficial not to train the word",experiment,0,107,11,0,18
text-classification,3,Apple when part of this multiword expression .,experiment,0,108,12,0,8
text-classification,3,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",experiment,1,109,13,0,32
text-classification,3,"This could provide hints on the excellent results provided by pre-trained Word2vec embeddings trained on the Google News corpus , which learns multiwords similarly to our setting .",experiment,0,110,14,0,28
text-classification,3,"Apart from this somewhat surprising finding , the use of the embeddings trained on a simple tokenized corpus ( i.e. vanilla ) proved again competitive , as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help .",experiment,1,111,15,0,41
text-classification,3,"In fact , the relatively weaker performance of lemmatization and lowercasing in this crossprocessing experiment is somehow expected as the coverage of word embeddings in vanilla - tokenized datasets is limited , e.g. , many entities which are capitalized in the datasets are not covered in the case of lowercasing , and inflected forms are missing in the case of lemmatizing .",experiment,0,112,16,0,62
text-classification,3,conclusions,experiment,0,113,17,0,1
text-classification,3,In this paper we analyzed the impact of simple text preprocessing decisions on the performance of a standard word - based neural text classifier .,experiment,0,114,18,0,25
text-classification,3,Our evaluations highlight the importance of being careful in the choice of how to preprocess our data and to be consistent when comparing different systems .,experiment,0,115,19,0,26
text-classification,3,"In general , a simple tokenization works equally or better than more complex pre-processing techniques such as lemmatization or multiword grouping , except for domain - specific datasets ( such as the medical dataset in our experiments ) in which sole tokenization performs poorly .",experiment,0,116,20,0,45
text-classification,3,"Additionally , word embeddings trained on multiword - grouped corpora perform surprisingly well when applied to simple tokenized datasets .",experiment,0,117,21,0,20
text-classification,3,"This property has often been overlooked and , to the best of our knowledge , we test the hypothesis for the first time .",experiment,0,118,22,0,24
text-classification,3,"In fact , this finding could partially explain the long - lasting success of pre-trained Word2vec embeddings , which specifically learn multiword embeddings as part of their pipeline .",experiment,0,119,23,0,29
text-classification,3,"Moreover , our analysis shows that there is a high variance in the results depending on the preprocessing choice ( 2.4 % on average for the best performing model ) , especially when the training data is not large enough to generalize .",experiment,0,120,24,0,43
text-classification,3,"Further analysis and experimentation would be required to fully understand the significance of these results ; but , this work can be viewed as a starting point for studying the impact of text preprocessing in deep learning models .",experiment,0,121,25,0,39
text-classification,3,We hope that our findings will encourage future researchers to carefully select and report these preprocessing decisions when evaluating or comparing different models .,experiment,0,122,26,0,24
text-classification,3,"Finally , as future work , we plan to extend our analysis to other tasks ( e.g. question answering ) , languages ( particularly morphologically rich languages for which these results may vary ) and preprocessing techniques ( e.g. stopword removal or part - of - speech tagging ) .",experiment,0,123,27,0,50
text-classification,0,Character - level Convolutional Networks for Text Classification,title,1,2,1,0,8
text-classification,0,*,title,0,3,1,0,1
text-classification,0,abstract,abstract,0,4,1,0,1
text-classification,0,This article offers an empirical exploration on the use of character - level convolutional networks ( ConvNets ) for text classification .,abstract,0,5,2,0,22
text-classification,0,We constructed several largescale datasets to show that character - level convolutional networks could achieve state - of - the - art or competitive results .,abstract,0,6,3,0,26
text-classification,0,"Comparisons are offered against traditional models such as bag of words , n-grams and their TFIDF variants , and deep learning models such as word - based ConvNets and recurrent neural networks .",abstract,0,7,4,0,33
text-classification,0,There are also related works that use character - level features for language processing .,abstract,0,8,5,0,15
text-classification,0,"These include using character - level n-grams with linear classifiers [ 15 ] , and incorporating character - level features to ConvNets [ 28 ] [ 29 ] .",abstract,0,9,6,0,29
text-classification,0,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",abstract,0,10,7,0,36
text-classification,0,Improvements for part - of - speech tagging and information retrieval were observed .,abstract,0,11,8,0,14
text-classification,0,introduction,introduction,0,12,1,0,1
text-classification,0,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",introduction,1,13,2,0,25
text-classification,0,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,introduction,0,14,3,0,21
text-classification,0,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",introduction,0,15,4,0,33
text-classification,0,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",introduction,0,16,5,0,34
text-classification,0,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",introduction,0,17,6,0,25
text-classification,0,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",introduction,1,18,7,0,27
text-classification,0,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,introduction,1,19,8,0,20
text-classification,0,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",introduction,0,20,9,0,22
text-classification,0,An extensive set of comparisons is offered with traditional models and other deep learning models .,introduction,0,21,10,0,16
text-classification,0,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,introduction,0,22,11,0,17
text-classification,0,"It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .",introduction,0,23,12,0,31
text-classification,0,These approaches have been proven to be competitive to traditional models .,introduction,0,24,13,0,12
text-classification,0,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,introduction,0,25,14,0,20
text-classification,0,"This simplification of engineering could be crucial for a single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",introduction,0,26,15,0,34
text-classification,0,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,introduction,0,27,16,0,21
text-classification,0,character - level convolutional networks,introduction,0,28,17,0,5
text-classification,0,"In this section , we introduce the design of character - level ConvNets for text classification .",introduction,0,29,18,0,17
text-classification,0,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",introduction,0,30,19,0,18
text-classification,0,key modules,introduction,0,31,20,0,2
text-classification,0,"The main component is the temporal convolutional module , which simply computes a 1 - D convolution .",introduction,0,32,21,0,18
text-classification,0,Suppose we have a discrete input function g ( x ) ?,introduction,0,33,22,0,12
text-classification,0,"[ 1 , l ] ?",introduction,0,34,23,0,6
text-classification,0,Rand a discrete kernel function,introduction,0,35,24,0,5
text-classification,0,where c = k ? d + 1 is an offset constant .,introduction,0,36,25,0,13
text-classification,0,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",introduction,0,37,26,0,72
text-classification,0,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",introduction,0,38,27,0,30
text-classification,0,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,introduction,0,39,28,0,30
text-classification,0,One key module that helped us to train deeper models is temporal max - pooling .,introduction,0,40,29,0,16
text-classification,0,It is the 1 - D version of the max - pooling module used in computer vision .,introduction,0,41,30,0,18
text-classification,0,Given a discrete input function g ( x ) ?,introduction,0,42,31,0,10
text-classification,0,"[ 1 , l ] ?",introduction,0,43,32,0,6
text-classification,0,"R , the max - pooling function h ( y ) ?",introduction,0,44,33,0,12
text-classification,0,"[ 1 , ( l ? k) / d + 1 ] ?",introduction,0,45,34,0,13
text-classification,0,R of g ( x ) is defined as,introduction,0,46,35,0,9
text-classification,0,where c = k ? d + 1 is an offset constant .,introduction,0,47,36,0,13
text-classification,0,"This very pooling module enabled us to train ConvNets deeper than 6 layers , where all others fail .",introduction,0,48,37,0,19
text-classification,0,The analysis by might shed some light on this .,introduction,0,49,38,0,10
text-classification,0,"The non-linearity used in our model is the rectifier or thresholding function h ( x ) = max {0 , x} , which makes our convolutional layers similar to rectified linear units ( ReLUs ) .",introduction,0,50,39,0,36
text-classification,0,"The algorithm used is stochastic gradient descent ( SGD ) with a minibatch of size 128 , using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times .",introduction,0,51,40,0,35
text-classification,0,Each epoch takes a fixed number of random training samples uniformly sampled across classes .,introduction,0,52,41,0,15
text-classification,0,This number will later be detailed for each dataset sparately .,introduction,0,53,42,0,11
text-classification,0,The implementation is done using Torch 7 .,introduction,0,54,43,0,8
text-classification,0,character quantization,introduction,0,55,44,0,2
text-classification,0,Our models accept a sequence of encoded characters as input .,introduction,0,56,45,0,11
text-classification,0,"The encoding is done by prescribing an alphabet of size m for the input language , and then quantize each character using 1 - of - m encoding ( or "" one - hot "" encoding ) .",introduction,0,57,46,0,38
text-classification,0,"Then , the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l 0 .",introduction,0,58,47,0,22
text-classification,0,"Any character exceeding length l 0 is ignored , and any characters thatare not in the alphabet including blank characters are quantized as all - zero vectors .",introduction,0,59,48,0,28
text-classification,0,"The character quantization order is backward so that the latest reading on characters is always placed near the begin of the output , making it easy for fully connected layers to associate weights with the latest reading .",introduction,0,60,49,0,38
text-classification,0,Later we also compare with models that use a different alphabet in which we distinguish between upper-case and lower - case letters .,introduction,0,61,50,0,23
text-classification,0,model design,introduction,0,62,51,0,2
text-classification,0,We designed 2 ConvNets - one large and one small .,introduction,0,63,52,0,11
text-classification,0,They are both 9 layers deep with 6 convolutional layers and 3 fully - connected layers .,introduction,0,64,53,0,17
text-classification,0,gives an illustration .,introduction,0,65,54,0,4
text-classification,0,some text,introduction,0,66,55,0,2
text-classification,0,convolutions,introduction,0,67,56,0,1
text-classification,0,Max - pooling Length Feature Quantization ...,introduction,0,68,57,0,7
text-classification,0,conv. and pool. layers,introduction,0,69,58,0,4
text-classification,0,fully - connected :,introduction,0,70,59,0,4
text-classification,0,illustration of our model,introduction,0,71,60,0,4
text-classification,0,"The input have number of features equal to 70 due to our character quantization method , and the input feature length is 1014 .",introduction,0,72,61,0,24
text-classification,0,It seems that 1014 characters could already capture most of the texts of interest .,introduction,0,73,62,0,15
text-classification,0,We also insert 2 dropout modules in between the 3 fully - connected layers to regularize .,introduction,0,74,63,0,17
text-classification,0,They have dropout probability of 0.5 .,introduction,0,75,64,0,7
text-classification,0,"lists the configurations for convolutional layers , and table 2 lists the configurations for fully - connected ( linear ) layers .",introduction,0,76,65,0,22
text-classification,0,We initialize the weights using a Gaussian distribution .,introduction,0,77,66,0,9
text-classification,0,"The mean and standard deviation used for initializing the large model is ( 0 , 0.02 ) and small model ( 0 , 0.05 ) . :",introduction,0,78,67,0,27
text-classification,0,Fully - connected layers used in our experiments .,introduction,0,79,68,0,9
text-classification,0,The number of output units for the last layer is determined by the problem .,introduction,0,80,69,0,15
text-classification,0,"For example , for a 10 - class classification problem it will be 10 .",introduction,0,81,70,0,15
text-classification,0,depends on the problem,introduction,0,82,71,0,4
text-classification,0,"For different problems the input lengths maybe different ( for example in our case l 0 = 1014 ) , and so are the frame lengths .",introduction,0,83,72,0,27
text-classification,0,"From our model design , it is easy to know that given input length l 0 , the output frame length after the last convolutional layer ( but before any of the fully - connected layers ) isl 6 = ( l 0 ? 96 ) / 27 .",introduction,0,84,73,0,49
text-classification,0,This number multiplied with the frame size at layer 6 will give the input dimension the first fully - connected layer accepts .,introduction,0,85,74,0,23
text-classification,0,Layer Output Units Large Output Units,introduction,0,86,75,0,6
text-classification,0,data augmentation using thesaurus,introduction,0,87,76,0,4
text-classification,0,Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models .,introduction,0,88,77,0,20
text-classification,0,These techniques usually work well when we could find appropriate invariance properties that the model should possess .,introduction,0,89,78,0,18
text-classification,0,"In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .",introduction,0,90,79,0,38
text-classification,0,"Therefore , the best way to do data augmentation would have been using human rephrases of sentences , but this is unrealistic and expensive due the large volume of samples in our datasets .",introduction,0,91,80,0,34
text-classification,0,"As a result , the most natural choice in data augmentation for us is to replace words or phrases with their synonyms .",introduction,0,92,81,0,23
text-classification,0,"We experimented data augmentation by using an English thesaurus , which is obtained from the mytheas component used in LibreOffice 1 project .",introduction,0,93,82,0,23
text-classification,0,"That thesaurus in turn was obtained from Word - Net , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning .",introduction,0,94,83,0,32
text-classification,0,"To decide on how many words to replace , we extract all replaceable words from the given text and randomly chooser of them to be replaced .",introduction,0,95,84,0,27
text-classification,0,The probability of number r is determined by a geometric distribution with parameter pin which P [ r ] ? pr .,introduction,0,96,85,0,22
text-classification,0,The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [ s ] ? q s .,introduction,0,97,86,0,28
text-classification,0,"This way , the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning .",introduction,0,98,87,0,22
text-classification,0,We will report the results using this new data augmentation technique with p = 0.5 and q = 0.5 .,introduction,0,99,88,0,20
text-classification,0,comparison models,introduction,0,100,89,0,2
text-classification,0,"To offer fair comparisons to competitive models , we conducted a series of experiments with both traditional and deep learning methods .",introduction,0,101,90,0,22
text-classification,0,"We tried our best to choose models that can provide comparable and competitive results , and the results are reported faithfully without any model selection .",introduction,0,102,91,0,26
text-classification,0,traditional methods,method,0,103,1,0,2
text-classification,0,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,method,0,104,2,0,20
text-classification,0,The classifier used is a multinomial logistic regression in all these models .,method,0,105,3,0,13
text-classification,0,Bag - of - words and its TFIDF .,method,0,106,4,0,9
text-classification,0,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",method,0,107,5,0,24
text-classification,0,"For the normal bag - of - words , we use the counts of each word as the features .",method,0,108,6,0,20
text-classification,0,"For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",method,0,109,7,0,21
text-classification,0,The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .,method,0,110,8,0,27
text-classification,0,The features are normalized by dividing the largest feature value .,method,0,111,9,0,11
text-classification,0,Bag - of - ngrams and its TFIDF .,method,0,112,10,0,9
text-classification,0,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",method,0,113,11,0,31
text-classification,0,The feature values are computed the same way as in the bag - of - words model .,method,0,114,12,0,18
text-classification,0,Bag - of - means on word embedding .,method,0,115,13,0,9
text-classification,0,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",method,0,116,14,0,33
text-classification,0,We take into consideration all the words that appeared more than 5 times in the training subset .,method,0,117,15,0,18
text-classification,0,The dimension of the embedding is 300 .,method,0,118,16,0,8
text-classification,0,The bag - of - means features are computed the same way as in the bag - of - words model .,method,0,119,17,0,22
text-classification,0,The number of means is 5000 .,method,0,120,18,0,7
text-classification,0,deep learning methods,method,0,121,1,0,3
text-classification,0,Recently deep learning methods have started to be applied to text classification .,method,0,122,2,0,13
text-classification,0,"We choose two simple and representative models for comparison , in which one is word - based ConvNet and the other a simple long - short term memory ( LSTM ) recurrent neural network model .",method,0,123,3,0,36
text-classification,0,word - based convnets .,method,0,124,4,0,5
text-classification,0,"Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",method,0,125,5,0,36
text-classification,0,We offer comparisons with both using the pretrained word2vec embedding and using lookup tables .,method,0,126,6,0,15
text-classification,0,"The embedding size is 300 in both cases , in the same way as our bagof - means model .",method,0,127,7,0,20
text-classification,0,"To ensure fair comparison , the models for each case are of the same size as our character - level ConvNets , in terms of both the number of layers and each layer 's output size .",method,0,128,8,0,37
text-classification,0,Experiments using a thesaurus for data augmentation are also conducted .,method,0,129,9,0,11
text-classification,0,LSTM LSTM LSTM ... : long - short term memory,method,0,130,10,0,10
text-classification,0,long - short term memory .,method,0,131,11,0,6
text-classification,0,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",method,0,132,12,0,22
text-classification,0,"The LSTM model used in our case is word - based , using pretrained word2vec embedding of size 300 as in previous models .",method,0,133,13,0,24
text-classification,0,"The model is formed by taking mean of the outputs of all LSTM cells to form a feature vector , and then using multinomial logistic regression on this feature vector .",method,0,134,14,0,31
text-classification,0,The output dimension is 512 .,method,0,135,15,0,6
text-classification,0,"The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",method,0,136,16,0,17
text-classification,0,We also used gradient clipping in which the gradient norm is limited to 5 . gives an illustration .,method,0,137,17,0,19
text-classification,0,mean,method,0,138,18,0,1
text-classification,0,choice of alphabet,method,0,139,19,0,3
text-classification,0,"For the alphabet of English , one apparent choice is whether to distinguish between upper-case and lower - case letters .",method,0,140,20,0,21
text-classification,0,We report experiments on this choice and observed that it usually ( but not always ) gives worse results when such distinction is made .,method,0,141,21,0,25
text-classification,0,"One possible explanation might be that semantics do not change with different letter cases , therefore there is a benefit of regularization .",method,0,142,22,0,23
text-classification,0,Large - scale Datasets and Results,method,0,143,23,0,6
text-classification,0,"Previous research on ConvNets in different are as has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .",method,0,144,24,0,36
text-classification,0,"However , most open datasets for text classification are quite small , and large - scale datasets are splitted with a significantly smaller training set than testing .",method,0,145,25,0,28
text-classification,0,"Therefore , instead of confusing our community more by using them , we built several large - scale datasets for our experiments , ranging from hundreds of thousands to several millions of samples .",method,0,146,26,0,34
text-classification,0,is a summary .,method,0,147,27,0,4
text-classification,0,sogou news corpus .,method,0,148,28,0,4
text-classification,0,"This dataset is a combination of the Sogo u CA and Sogo uCS news corpora , containing in total 2,909,551 news articles in various topic channels .",method,0,149,29,0,27
text-classification,0,"We then labeled each piece of news using its URL , by manually classifying the their domain names .",method,0,150,30,0,19
text-classification,0,This gives us a large corpus of news articles labeled with their categories .,method,0,151,31,0,14
text-classification,0,There are a large number categories but most of them contain only few articles .,method,0,152,32,0,15
text-classification,0,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",method,0,153,33,0,25
text-classification,0,"The number of training samples selected for each class is 90,000 and testing 12,000 .",method,0,154,34,0,15
text-classification,0,"Although this is a dataset in Chinese , we used pypinyin package combined with jieba Chinese segmentation system to produce Pinyin - a phonetic romanization of Chinese .",method,0,155,35,0,28
text-classification,0,The models for English can then be applied to this dataset without change .,method,0,156,36,0,14
text-classification,0,The fields used are title and content . :,method,0,157,37,0,9
text-classification,0,Testing errors of all the models .,method,0,158,38,0,7
text-classification,0,numbers are in percentage .,method,0,159,39,0,5
text-classification,0,""" Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",method,0,160,40,0,18
text-classification,0,""" w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",method,0,161,41,0,23
text-classification,0,DBpedia is a crowd - sourced community effort to extract structured information from Wikipedia .,method,0,162,42,0,15
text-classification,0,The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014 .,method,0,163,43,0,15
text-classification,0,"From each of these 14 ontology classes , we randomly choose 40,000 training samples and 5,000 testing samples .",method,0,164,44,0,19
text-classification,0,The fields we used for this dataset contain title and abstract of each Wikipedia article .,method,0,165,45,0,16
text-classification,0,yelp reviews .,method,0,166,46,0,3
text-classification,0,The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 .,method,0,167,47,0,14
text-classification,0,"This dataset contains 1,569,264 samples that have review texts .",method,0,168,48,0,10
text-classification,0,"Two classification tasks are constructed from this dataset - one predicting full number of stars the user has given , and the other predicting a polarity label by considering stars 1 and 2 negative , and 3 and 4 positive .",method,0,169,49,0,41
text-classification,0,"The full dataset has 130,000 training samples and 10,000 testing samples in each star , and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity .",method,0,170,50,0,31
text-classification,0,yahoo!,method,0,171,51,0,1
text-classification,0,answers dataset .,method,0,172,52,0,3
text-classification,0,we obtained yahoo!,method,0,173,53,0,3
text-classification,0,Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo !,method,0,174,54,0,12
text-classification,0,webscope program .,method,0,175,55,0,3
text-classification,0,"The corpus contains 4,483,032 questions and their answers .",method,0,176,56,0,9
text-classification,0,We constructed a topic classification dataset from this corpus using 10 largest main categories .,method,0,177,57,0,15
text-classification,0,"Each class contains 140,000 training samples and 5,000 testing samples .",method,0,178,58,0,11
text-classification,0,"The fields we used include question title , question content and best answer .",method,0,179,59,0,14
text-classification,0,amazon reviews .,method,0,180,60,0,3
text-classification,0,"We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products .",method,0,181,61,0,30
text-classification,0,"Similarly to the Yelp review dataset , we also constructed 2 datasets - one full score prediction and another polarity prediction .",method,0,182,62,0,22
text-classification,0,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",method,0,183,63,0,32
text-classification,0,The fields used are review title and review content .,method,0,184,64,0,10
text-classification,0,lists all the testing errors we obtained from these datasets for all the applicable models .,method,0,185,65,0,16
text-classification,0,"Note that since we do not have a Chinese thesaurus , the Sogou News dataset does not have any results using thesaurus augmentation .",method,0,186,66,0,24
text-classification,0,We labeled the best result in blue and worse result in red .,method,0,187,67,0,13
text-classification,0,Figure 3 : Relative errors with comparison models,method,0,188,68,0,8
text-classification,0,"To understand the results in table 4 further , we offer some empirical analysis in this section .",method,0,189,69,0,18
text-classification,0,"To facilitate our analysis , we present the relative errors in with respect to comparison models .",method,0,190,70,0,17
text-classification,0,"Each of these plots is computed by taking the difference between errors on comparison model and our character - level ConvNet model , then divided by the comparison model error .",method,0,191,71,0,31
text-classification,0,All ConvNets in the figure are the large models with thesaurus augmentation respectively .,method,0,192,72,0,14
text-classification,0,character - level,method,0,193,73,0,3
text-classification,0,ConvNet is an effective method .,method,0,194,74,0,6
text-classification,0,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,method,1,195,75,0,24
text-classification,0,This is a strong indication that language could also bethought of as a signal no different from any other kind .,method,0,196,76,0,21
text-classification,0,shows 12 random first - layer patches learnt by one of our character - level ConvNets for DBPedia dataset .,method,0,197,77,0,20
text-classification,0,Dataset size forms a dichotomy between traditional and ConvNets models .,method,0,198,78,0,11
text-classification,0,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,method,1,199,79,0,20
text-classification,0,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",method,0,200,80,0,44
text-classification,0,Conv Nets may work well for user - generated data .,method,0,201,81,0,11
text-classification,0,User- generated data vary in the degree of how well the texts are curated .,method,0,202,82,0,15
text-classification,0,"For example , in our million scale datasets , Amazon reviews tend to be raw user-inputs , whereas users might be extra careful in their writings on Yahoo !",method,0,203,83,0,29
text-classification,0,answers .,method,0,204,84,0,2
text-classification,0,"Plots comparing word - based deep models ( figures 3 c , 3 d and 3 e ) show that character - level ConvNets work better for less curated user - generated texts .",method,0,205,85,0,34
text-classification,0,This property suggests that ConvNets may have better applicability to real - world scenarios .,method,0,206,86,0,15
text-classification,0,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",method,1,207,87,0,37
text-classification,0,Choice of alphabet makes a difference .,method,0,208,88,0,7
text-classification,0,shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,method,0,209,89,0,17
text-classification,0,"For million - scale datasets , it seems that not making such distinction usually works better .",method,0,210,90,0,17
text-classification,0,"One possible explanation is that there is a regularization effect , but this is to be validated .",method,0,211,91,0,18
text-classification,0,Semantics of tasks may not matter .,method,0,212,92,0,7
text-classification,0,Our datasets consist of two kinds of tasks : sentiment analysis ( Yelp and Amazon reviews ) and topic classification ( all others ) .,method,0,213,93,0,25
text-classification,0,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,method,0,214,94,0,19
text-classification,0,Bag - of - means is a misuse of word2vec .,method,0,215,95,0,11
text-classification,0,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,method,0,216,96,0,31
text-classification,0,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",method,1,217,97,0,26
text-classification,0,"However , our experiments does not speak for any other language processing tasks or use of word2vec in any other way .",method,0,218,98,0,22
text-classification,0,There is no free lunch .,method,0,219,99,0,6
text-classification,0,Our experiments once again verifies that there is not a single machine learning model that can work for all kinds of datasets .,method,0,220,100,0,23
text-classification,0,The factors discussed in this section could all play a role in deciding which method is the best for some specific application .,method,0,221,101,0,23
text-classification,0,conclusion and outlook,method,0,222,102,0,3
text-classification,0,This article offers an empirical study on character - level convolutional networks for text classification .,method,0,223,103,0,16
text-classification,0,We compared with a large number of traditional and deep learning models using several largescale datasets .,method,0,224,104,0,17
text-classification,0,"On one hand , analysis shows that character - level ConvNet is an effective method .",method,0,225,105,0,16
text-classification,0,"On the other hand , how well our model performs in comparisons depends on many factors , such as dataset size , whether the texts are curated and choice of alphabet .",method,0,226,106,0,32
text-classification,0,"In the future , we hope to apply character - level ConvNets for a broader range of language processing tasks especially when structured outputs are needed .",method,0,227,107,0,27
question-answering,8,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,title,1,2,1,0,9
question-answering,8,abstract,abstract,0,3,1,0,1
question-answering,8,Machine comprehension of text is an important problem in natural language processing .,abstract,1,4,2,0,13
question-answering,8,"A recently released dataset , the Stanford Question Answering Dataset ( SQuAD ) , offers a large number of real questions and their answers created by humans through crowdsourcing .",abstract,0,5,3,0,30
question-answering,8,"SQuAD provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths .",abstract,0,6,4,0,38
question-answering,8,We propose an end - to - end neural architecture for the task .,abstract,0,7,5,0,14
question-answering,8,"The architecture is based on match - LSTM , a model we proposed previously for textual entailment , and Pointer Net , a sequence - to - sequence model proposed by Vinyals et al. ( 2015 ) to constrain the output tokens to be from the input sequences .",abstract,0,8,6,1,49
question-answering,8,We propose two ways of using Pointer Net for our task .,abstract,0,9,7,0,12
question-answering,8,Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. ( 2016 ) using logistic regression and manually crafted features .,abstract,0,10,8,1,30
question-answering,8,introduction,introduction,0,11,1,0,1
question-answering,8,Machine comprehension of text is one of the ultimate goals of natural language processing .,introduction,0,12,2,0,15
question-answering,8,"While the ability of a machine to understand text can be assessed in many different ways , in recent years , several benchmark datasets have been created to focus on answering questions as away to evaluate machine comprehension .",introduction,0,13,3,0,39
question-answering,8,"In this setup , typically the machine is first presented with apiece of text such as a news article or a story .",introduction,0,14,4,0,23
question-answering,8,The machine is then expected to answer one or multiple questions related to the text .,introduction,0,15,5,0,16
question-answering,8,"In most of the benchmark datasets , a question can be treated as a multiple choice question , whose correct answer is to be chosen from a set of provided candidate answers .",introduction,0,16,6,0,33
question-answering,8,"Presumably , questions with more given candidate answers are more challenging .",introduction,0,17,7,0,12
question-answering,8,The Stanford Question Answering Dataset ( SQuAD ) introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text .,introduction,0,18,8,0,30
question-answering,8,"Moreover , unlike some other datasets whose questions and answers were created automatically in Cloze style , the questions and answers in SQu AD were created by humans through crowdsourcing , which makes the dataset more realistic .",introduction,0,19,9,0,38
question-answering,8,"Given these advantages of the SQuAD dataset , in this paper , we focus on this new dataset to study machine comprehension of text .",introduction,0,20,10,0,25
question-answering,8,A sample piece of text and three of its associated questions are shown in .,introduction,0,21,11,0,15
question-answering,8,"Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering , including syntactic parsing , named entity recognition , question classification , semantic parsing , etc .",introduction,0,22,12,0,40
question-answering,8,"Recently , with the advances of applying neural network models in NLP , there has been much interest in building end - to - end neural architectures for various NLP tasks , including several pieces of work on machine comprehension .",introduction,0,23,13,0,41
question-answering,8,"However , given the properties of previous machine comprehension datasets , existing end - to - end neural architectures for the task either rely on the candidate answers or assume that the In 1870 , Tesla moved to Karlovac , to attend school at the Higher Real Gymnasium , where he was profoundly influenced by a math teacher Martin Sekuli ?.",introduction,0,24,14,0,61
question-answering,8,"The classes were held in German , as it was a school within the Austro-Hungarian Military Frontier .",introduction,0,25,15,0,18
question-answering,8,"Tesla was able to perform integral calculus in his head , which prompted his teachers to believe that he was cheating .",introduction,0,26,16,0,22
question-answering,8,"He finished a four - year term in three years , graduating in 1873 .",introduction,0,27,17,0,15
question-answering,8,1 .,introduction,0,28,18,0,2
question-answering,8,In what language were the classes given ?,introduction,0,29,19,0,8
question-answering,8,german,introduction,0,30,20,0,1
question-answering,8,2 . Who was Tesla 's main influence in Karlovac ?,introduction,0,31,21,0,11
question-answering,8,martin sekuli ?,introduction,0,32,22,0,3
question-answering,8,3 . Why did Tesla go to Karlovac ?,introduction,0,33,23,0,9
question-answering,8,attend school at the Higher Real Gymnasium :,introduction,0,34,24,0,8
question-answering,8,"A paragraph from Wikipedia and three associated questions together with their answers , taken from the SQuAD dataset .",introduction,0,35,25,0,19
question-answering,8,The tokens in bold in the paragraph are our predicted answers while the texts next to the questions are the ground truth answers .,introduction,0,36,26,0,24
question-answering,8,"answer is a single token , which make these methods unsuitable for the SQuAD dataset .",introduction,0,37,27,0,16
question-answering,8,"In this paper , we propose a new end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .",introduction,1,38,28,0,28
question-answering,8,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",introduction,1,39,29,0,34
question-answering,8,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",introduction,1,40,30,0,50
question-answering,8,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,introduction,1,41,31,0,23
question-answering,8,We also further extend the boundary model with a search mechanism .,introduction,1,42,32,0,12
question-answering,8,Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by .,introduction,0,43,33,0,18
question-answering,8,"Moreover , using an ensemble of several of our models , we can achieve very competitive performance on SQuAD .",introduction,0,44,34,0,20
question-answering,8,Our contributions can be summarized as follows :,introduction,0,45,35,0,8
question-answering,8,"( 1 ) We propose two new end - to - end neural network models for machine comprehension , which combine match - LSTM and Ptr- Net to handle the special properties of the SQuAD dataset .",introduction,0,46,36,0,37
question-answering,8,"( 2 ) We have achieved the performance of an exact match score of 67.9 % and an F1 score of 77.0 % on the unseen test dataset , which is much better than the featureengineered solution .",introduction,0,47,37,0,38
question-answering,8,"Our performance is also close to the state of the art on SQuAD , which is 71.6 % in terms of exact match and 80.4 % in terms of F1 from Salesforce Research .",introduction,0,48,38,0,34
question-answering,8,( 3 ) Our further analyses of the models reveal some useful insights for further improving the method .,introduction,0,49,39,0,19
question-answering,8,"Beisdes , we also made our code available online 1 .",introduction,0,50,40,0,11
question-answering,8,method,method,0,51,1,0,1
question-answering,8,"In this section , we first briefly review match - LSTM and Pointer Net .",method,0,52,2,0,15
question-answering,8,These two pieces of existing work lay the foundation of our method .,method,0,53,3,0,13
question-answering,8,We then present our end - to - end neural architecture for machine comprehension .,method,0,54,4,0,15
question-answering,8,match - lstm,method,0,55,5,0,3
question-answering,8,"I na recent work on learning natural language inference , we proposed a match - LSTM model for predicting textual entailment .",method,0,56,6,0,22
question-answering,8,"In textual entailment , two sentences are given where one is a premise and the other is a hypothesis .",method,0,57,7,0,20
question-answering,8,"To predict whether the premise entails the hypothesis , the match - LSTM model goes through the tokens of the hypothesis sequentially .",method,0,58,8,0,23
question-answering,8,"At each position of the hypothesis , attention mechanism is used to obtain a weighted vector representation of the premise .",method,0,59,9,0,21
question-answering,8,"This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM , which we call the match - LSTM .",method,0,60,10,0,33
question-answering,8,The match - LSTM essentially sequentially aggregates the matching of the attention - weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction . :,method,0,61,11,0,34
question-answering,8,An overview of our two models .,method,0,62,12,0,7
question-answering,8,"Both models consist of an LSTM preprocessing layer , a match - LSTM layer and an Answer Pointer layer .",method,0,63,13,0,20
question-answering,8,"For each match - LSTM in a particular direction , h q i , which is defined as H q ?",method,0,64,14,0,21
question-answering,8,"i , is computed using the ?",method,0,65,15,0,7
question-answering,8,"in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",method,0,66,16,0,19
question-answering,8,proposed a Pointer Network ( Ptr - Net ) model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence .,method,0,67,17,0,34
question-answering,8,"Instead of picking an output token from a fixed vocabulary , Ptr - Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol .",method,0,68,18,0,33
question-answering,8,The pointer mechanism has inspired some recent work on language processing .,method,0,69,19,0,12
question-answering,8,Here we adopt Ptr- Net in order to construct answers using tokens from the input text .,method,0,70,20,0,17
question-answering,8,pointer net,method,0,71,21,0,2
question-answering,8,our method,method,0,72,1,0,2
question-answering,8,"Formally , the problem we are trying to solve can be formulated as follows .",method,0,73,2,0,15
question-answering,8,"We are given apiece of text , which we refer to as a passage , and a question related to the passage .",method,0,74,3,0,23
question-answering,8,The passage is represented by matrix P ?,method,0,75,4,0,8
question-answering,8,"R dP , where P is the length ( number of tokens ) of the passage and d is the dimensionality of word embeddings .",method,0,76,5,0,25
question-answering,8,"Similarly , the question is represented by matrix Q ?",method,0,77,6,0,10
question-answering,8,R d Q where Q is the length of the question .,method,0,78,7,0,12
question-answering,8,Our goal is to identify a subsequence from the passage as the answer to the question .,method,0,79,8,0,17
question-answering,8,"As pointed out earlier , since the output tokens are from the input , we would like to adopt the Pointer Net for this problem .",method,0,80,9,0,26
question-answering,8,"A straightforward way of applying Ptr - Net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage , because Ptr - Net does not make the consecutivity assumption .",method,0,81,10,0,48
question-answering,8,"Specifically , we represent the answer as a sequence of integers a = ( a 1 , a 2 , . . . ) , where each a i is an integer between 1 and P , indicating a certain position in the passage .",method,0,82,11,0,45
question-answering,8,"Alternatively , if we want to ensure consecutivity , that is , if we want to ensure that we indeed select a subsequence from the passage as an answer , we can use the Ptr-Net to predict only the start and the end of an answer .",method,0,83,12,0,47
question-answering,8,"In this case , the Ptr - Net only needs to select two tokens from the input passage , and all the tokens between these two tokens in the passage are treated as the answer .",method,0,84,13,0,36
question-answering,8,"Specifically , we can represent the answer to be predicted as two integers a = ( a s , a e ) , where a s an a e are integers between 1 and P .",method,0,85,14,0,36
question-answering,8,We refer to the first setting above as a sequence model and the second setting above as a boundary model .,method,0,86,15,0,21
question-answering,8,"For either model , we assume that a set of training examples in the form of triplets {( P n , Q n , an ) } N n=1 are given .",method,0,87,16,0,32
question-answering,8,An overview of the two neural network models are shown in .,method,0,88,17,0,12
question-answering,8,Both models consist of three layers :,method,0,89,18,0,7
question-answering,8,( 1 ) An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs .,method,0,90,19,0,17
question-answering,8,( 2 ) A match - LSTM layer that tries to match the passage against the question .,method,0,91,20,0,18
question-answering,8,( 3 ) An Answer Pointer ( Ans - Ptr ) layer that uses Ptr-Net to select a set of tokens from the passage as the answer .,method,0,92,21,0,28
question-answering,8,The difference between the two models only lies in the third layer .,method,0,93,22,0,13
question-answering,8,lstm,method,0,94,23,0,1
question-answering,8,preprocessing layer,method,0,95,24,0,2
question-answering,8,The purpose for the LSTM preprocessing layer is to incorporate contextual information into the representation of each token in the passage and the question .,method,0,96,25,0,25
question-answering,8,"We use a standard one - directional LSTM 2 to process the passage and the question separately , as shown below :",method,0,97,26,0,22
question-answering,8,The resulting matrices H p ?,method,0,98,27,0,6
question-answering,8,R lP and H q ?,method,0,99,28,0,6
question-answering,8,"R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",method,0,100,29,0,22
question-answering,8,"In other words , the i th column vector hp i ( or h q i ) in H p ( or H q ) represents the i th token in the passage ( or the question ) together with some contextual information from the left .",method,0,101,30,0,47
question-answering,8,match- lstm,method,0,102,31,0,2
question-answering,8,layer,method,0,103,32,0,1
question-answering,8,We apply the match - LSTM model proposed for textual entailment to our machine comprehension problem by treating the question as a premise and the passage as a hypothesis .,method,0,104,33,0,30
question-answering,8,The match - LSTM sequentially goes through the passage .,method,0,105,34,0,10
question-answering,8,"At position i of the passage , it first uses the standard word - by - word attention mechanism to obtain attention weight vector ? ? ?",method,0,106,35,0,27
question-answering,8,i ?,method,0,107,36,0,2
question-answering,8,r q as follows :,method,0,108,37,0,5
question-answering,8,"where W q , W p , W r ?",method,0,109,38,0,10
question-answering,8,"R ll , b p , w ?",method,0,110,39,0,8
question-answering,8,r land b ?,method,0,111,40,0,4
question-answering,8,"R are parameters to be learned , ? ? hr i?1 ?",method,0,112,41,0,12
question-answering,8,"R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",method,0,113,42,0,53
question-answering,8,"Essentially , the resulting attention weight ? ? ?",method,0,114,43,0,9
question-answering,8,"i , j above indicates the degree of matching between the i th token in the passage with the j th token in the question .",method,0,115,44,0,26
question-answering,8,"Next , we use the attention weight vector ? ? ?",method,0,116,45,0,11
question-answering,8,i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ?,method,0,117,46,0,25
question-answering,8,z i :,method,0,118,47,0,3
question-answering,8,this vector ? ?,method,0,119,48,0,4
question-answering,8,z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,method,0,120,49,0,21
question-answering,8,where ? ?,method,0,121,50,0,3
question-answering,8,hr i ?,method,0,122,51,0,3
question-answering,8,r l .,method,0,123,52,0,3
question-answering,8,We further build a similar match - LSTM in the reverse direction .,method,0,124,53,0,13
question-answering,8,The purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage .,method,0,125,54,0,21
question-answering,8,"To build this reverse match - LSTM , we first define",method,0,126,55,0,11
question-answering,8,"Note that the parameters here ( W q , W p , W r , b p , wand b ) are the same as used in Eqn .",method,0,127,56,0,29
question-answering,8,( 2 ) .,method,0,128,57,0,4
question-answering,8,we then define ? ?,method,0,129,58,0,5
question-answering,8,z i in a similar way and finally define ? ?,method,0,130,59,0,11
question-answering,8,hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,method,0,131,60,0,21
question-answering,8,we define h r ?,method,0,132,61,0,5
question-answering,8,R 2 lP as the concatenation of the two :,method,0,133,62,0,10
question-answering,8,answer pointer layer,method,0,134,63,0,3
question-answering,8,"The top layer , the Answer Pointer ( Ans - Ptr ) layer , is motivated by the Pointer Net introduced by .",method,0,135,64,0,23
question-answering,8,This layer uses the sequence H r as input .,method,0,136,65,0,10
question-answering,8,Recall that we have two different models :,method,0,137,66,0,8
question-answering,8,The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage .,method,0,138,67,0,21
question-answering,8,"The boundary model produces only the start token and the end token of the answer , and then all the tokens between these two in the original passage are considered to be the answer .",method,0,139,68,0,35
question-answering,8,We now explain the two models separately .,method,0,140,69,0,8
question-answering,8,the sequence model :,method,0,141,70,0,4
question-answering,8,"Recall that in the sequence model , the answer is represented by a sequence of integers a = ( a 1 , a 2 , . . . ) indicating the positions of the selected tokens in the original passage .",method,0,142,71,0,41
question-answering,8,The Ans - Ptr layer models the generation of these integers in a sequential manner .,method,0,143,72,0,16
question-answering,8,"Because the length of an answer is not fixed , in order to stop generating answer tokens at certain point , we allow each a k to take up an integer value between 1 and P + 1 , where P + 1 is a special value indicating the end of the answer .",method,0,144,73,0,54
question-answering,8,"Once a k is set to be P + 1 , the generation of the answer stops .",method,0,145,74,0,18
question-answering,8,"In order to generate the k th answer token indicated by a k , first , the attention mechanism is used again to obtain an attention weight vector ? k ? R ( P + 1 ) , where ? k , j ( 1 ? j ? P + 1 ) is the probability of selecting the j th token from the passage as the k th token in the answer , and ? k , ( P + 1 ) is the probability of stopping the answer generation at position k. ?",method,0,146,75,0,94
question-answering,8,k is modeled as follows :,method,0,147,76,0,6
question-answering,8,where h r ?,method,0,148,77,0,4
question-answering,8,"R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ?",method,0,149,78,0,40
question-answering,8,"R ll , b a , v ?",method,0,150,79,0,8
question-answering,8,r land c ?,method,0,151,80,0,4
question-answering,8,"R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ?",method,0,152,81,0,27
question-answering,8,R l is the hidden vector at position k ?,method,0,153,82,0,10
question-answering,8,1 of an answer LSTM as defined below :,method,0,154,83,0,9
question-answering,8,"We can then model the probability of generating the answer sequence as p ( a| H r ) = k p ( a k | a 1 , a 2 , . . . , a k?1 , H r ) ,",method,0,155,84,0,42
question-answering,8,"and p ( a k = j|a 1 , a 2 , . . . , a k?1 , H r ) = ? k , j .",method,0,156,85,0,28
question-answering,8,"To train the model , we minimize the following loss function based on the training examples :",method,0,157,86,0,17
question-answering,8,"log p ( a n | P n , Q n ) .",method,0,158,87,0,13
question-answering,8,the boundary model :,method,0,159,88,0,4
question-answering,8,"The boundary model works in a way very similar to the sequence model above , except that instead of predicting a sequence of indices a 1 , a 2 , . . . , we only need to predict two indices a sand a e .",method,0,160,89,0,46
question-answering,8,"So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to H r , and the probability of generating an answer is simply modeled as p ( a| H r ) = p ( a s | H r ) p ( a e | a s , H r ) . Here "" Search "" refers to globally searching the spans with no more than 15 tokens , "" b "" refers to using bi-directional pre-processing LSTM , and "" en "" refers to ensemble method .",method,0,161,90,0,102
question-answering,8,We further extend the boundary model by incorporating a search mechanism .,method,0,162,91,0,12
question-answering,8,"Specifically , during prediction , we try to limit the length of the span and globally search the span with the highest probability computed by p ( a s ) p ( a e ) .",method,0,163,92,0,36
question-answering,8,"Besides , as the boundary has a sequence of fixed number of values , bi-directional Ans - Ptr can be simply combined to fine - tune the correct span .",method,0,164,93,0,30
question-answering,8,experiments,experiment,0,165,1,0,1
question-answering,8,"In this section , we present our experiment results and perform some analyses to better understand how our models works .",experiment,0,166,2,0,21
question-answering,8,data,experiment,0,167,3,0,1
question-answering,8,We use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .,experiment,0,168,4,0,17
question-answering,8,Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics .,experiment,0,169,5,0,16
question-answering,8,"Each passage is a single paragraph from a Wikipedia article , and each passage has around 5 questions associated with it .",experiment,0,170,6,0,22
question-answering,8,"In total , there are 23,215 passages and 107,785 questions .",experiment,0,171,7,0,11
question-answering,8,"The data has been split into a training set ( with 87,599 question - answer pairs ) , a development set ( with 10,570 questionanswer pairs ) and a hidden test set .",experiment,0,172,8,0,33
question-answering,8,experiment settings,experiment,0,173,1,0,2
question-answering,8,"We first tokenize all the passages , questions and answers .",experiment,0,174,2,0,11
question-answering,8,The resulting vocabulary contains 117K unique words .,experiment,0,175,3,0,8
question-answering,8,We use word embeddings from GloVe to initialize the model .,experiment,1,176,4,0,11
question-answering,8,Words not found in Glo Ve are initialized as zero vectors .,experiment,0,177,5,0,12
question-answering,8,The word embeddings are not updated during the training of the model .,experiment,0,178,6,0,13
question-answering,8,The dimensionality l of the hidden layers is set to be 150 or 300 .,experiment,1,179,7,0,15
question-answering,8,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,experiment,1,180,8,0,20
question-answering,8,Each update is computed through a minibatch of 30 instances .,experiment,1,181,9,0,11
question-answering,8,We do not use L2-regularization .,experiment,0,182,10,0,6
question-answering,8,"The performance is measured by two metrics : percentage of exact match with the ground truth answers , and word - level F 1 score when comparing the tokens in the predicted answers with the tokens in the ground truth answers .",experiment,0,183,11,0,42
question-answering,8,Note that in the development set and the test set each question has around three ground truth answers .,experiment,0,184,12,0,19
question-answering,8,F1 scores with the best matching answers are used to compute the average F1 score .,experiment,0,185,13,0,16
question-answering,8,results,result,0,186,1,0,1
question-answering,8,The results of our models as well as the results of the baselines given by and are shown in .,result,0,187,2,0,20
question-answering,8,We can see that both of our two models have clearly outper - :,result,0,188,3,0,14
question-answering,8,Visualization of the attention weights ?,result,0,189,4,0,6
question-answering,8,for three questions associated with the same passage .,result,0,190,5,0,9
question-answering,8,"outperformed the logistic regression model by , which relies on carefully designed features .",result,1,191,6,0,14
question-answering,8,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .",result,1,192,7,0,27
question-answering,8,"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .",result,0,193,8,0,23
question-answering,8,The improvement of our models over the logistic regression model shows that our end - to - end neural network models without much feature engineering are very effective on this task and this dataset .,result,0,194,9,0,35
question-answering,8,"Considering the effectiveness of boundary model , we further explore this model .",result,0,195,10,0,13
question-answering,8,"Observing that most of the answers are the spans with relatively small sizes , we simply limit the largest predicted span to have no more than 15 tokens and conducted experiment with span searching",result,0,196,11,0,34
question-answering,8,"This resulted in 1.5 % improvement in F1 on the development data and that outperformed the DCR model , which also introduced some language features such as POS and NE into their model .",result,0,197,12,0,34
question-answering,8,"Besides , we tried to increase the memory dimension l in the model or add bi-directional pre-processing LSTM or add bi-directional Ans - Ptr .",result,0,198,13,0,25
question-answering,8,The improvement on the development data using the first two methods is quite small .,result,0,199,14,0,15
question-answering,8,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .",result,1,200,15,0,22
question-answering,8,"Finally , we explore the ensemble method by simply computing the product of the boundary probabilities collected from 5 boundary models and then searching the most likely span with no more than 15 tokens .",result,0,201,16,0,35
question-answering,8,This ensemble method achieved the best performance as shown in the table .,result,0,202,17,0,13
question-answering,8,further analyses,result,0,203,18,0,2
question-answering,8,"To better understand the strengths and weaknesses of our models , we perform some further analyses of the results below .",result,0,204,19,0,21
question-answering,8,"First , we suspect that longer answers are harder to predict .",result,0,205,20,0,12
question-answering,8,"To verify this hypothesis , we analysed the performance in terms of both exact match and F 1 score with respect to the answer length on the development set .",result,0,206,21,0,30
question-answering,8,"For example , for questions whose answers contain more than 9 tokens , the F 1 score of the boundary model drops to around 55 % and the exact match score drops to only around 30 % , compared to the F 1 score and exact match score of close to 72 % and 67 % , respectively , for questions with single - token answers .",result,0,207,22,0,67
question-answering,8,And that supports our hypothesis .,result,0,208,23,0,6
question-answering,8,"Next , we analyze the performance of our models on different groups of questions .",result,0,209,24,0,15
question-answering,8,"We use a crude way to split the questions into different groups based on a set of question words we have defined , including "" what , "" "" how , "" "" who , "" "" when , "" "" which , "" "" where , "" and "" why . """,result,0,210,25,0,53
question-answering,8,These different question words roughly refer to questions with different types of answers .,result,0,211,26,0,14
question-answering,8,"For example , "" when "" questions look for temporal expressions as answers , whereas "" where "" questions look for locations as answers .",result,0,212,27,0,25
question-answering,8,"According to the performance on the development data set , our models work the best for "" when "" questions .",result,0,213,28,0,21
question-answering,8,This maybe because in this dataset temporal expressions are relatively easier to recognize .,result,0,214,29,0,14
question-answering,8,"Other groups of questions whose answers are noun phrases , such as "" what "" questions , "" which "" questions and "" where "" questions , also get relatively better results .",result,0,215,30,0,33
question-answering,8,"On the other hand , "" why "" questions are the hardest to answer .",result,0,216,31,0,15
question-answering,8,"This is not surprising because the answers to "" why "" questions can be very diverse , and they are not restricted to any certain type of phrases .",result,0,217,32,0,29
question-answering,8,"Finally , we would like to check whether the attention mechanism used in the match - LSTM layer is effective in helping the model locate the answer .",result,0,218,33,0,28
question-answering,8,We show the attention weights ?,result,0,219,34,0,6
question-answering,8,in .,result,0,220,35,0,2
question-answering,8,In the figure the darker the color is the higher the weight is .,result,0,221,36,0,14
question-answering,8,We can see that some words have been well aligned based on the attention weights .,result,0,222,37,0,16
question-answering,8,"For example , the word "" German "" in the passage is aligned well to the word "" language "" in the first question , and the model successfully predicts "" German "" as the answer to the question .",result,0,223,38,0,40
question-answering,8,"For the question word "" who "" in the second question , the word "" teacher "" actually receives relatively higher attention weight , and the model has predicted the phrase "" Martin Sekulic "" after that as the answer , which is correct .",result,0,224,39,0,45
question-answering,8,"For the last question that starts with "" why "" , the attention weights are more evenly distributed and it is not clear which words have been aligned to "" why "" .",result,0,225,40,0,33
question-answering,8,related work,related work,0,226,1,0,2
question-answering,8,"Machine comprehension of text has gained much attention in recent years , and increasingly researchers are building data - drive , end - to - end neural network models for the task .",related work,0,227,2,0,33
question-answering,8,We will first review the recently released datasets and then some end - to - end models on this task .,related work,0,228,3,0,21
question-answering,8,datasets,related work,0,229,4,0,1
question-answering,8,"A number of datasets for studying machine comprehension were created in Cloze style by removing a single token from a sentence in the original corpus , and the task is to predict the missing word .",related work,0,230,5,0,36
question-answering,8,was also created by human annotators .,related work,0,231,6,0,7
question-answering,8,"Different from the previous two , however , the SQuAD dataset does not provide candidate answers , and thus all possible subsequences from the given passage have to be considered as candidate answers .",related work,0,232,7,0,34
question-answering,8,"Besides the datasets above , there are also a few other datasets created for machine comprehension , such as WikiReading dataset and bAbI dataset , but they are quite different from the datasets above in nature .",related work,0,233,8,0,37
question-answering,8,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,related work,0,234,9,0,9
question-answering,8,There have been a number of studies proposing end - to - end neural network models for machine comprehension .,related work,0,235,10,0,20
question-answering,8,A common approach is to use recurrent neural networks ( RNNs ) to process the given text and the question in order to predictor generate the answers .,related work,0,236,11,0,28
question-answering,8,Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage .,related work,0,237,12,0,21
question-answering,8,"Given that answers often come from the given passage , Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers .",related work,0,238,13,0,31
question-answering,8,"Compared with existing work , we use match - LSTM to match a question and a given passage , and we use Pointer Network in a different way such that we can generate answers that contain multiple tokens from the given passage .",related work,0,239,14,0,43
question-answering,8,conclusions,related work,0,240,15,0,1
question-answering,8,"In this paper , We developed two models for the machine comprehension problem defined in the Stanford Question Answering ( SQuAD ) dataset , both making use of match - LSTM and Pointer Network .",related work,0,241,16,0,35
question-answering,8,"Experiments on the SQuAD dataset showed that our second model , the boundary model , could achieve an exact match score of 67.6 % and an F 1 score of 77 % on the test dataset , which is better than our sequence model and 's feature - engineered model .",related work,0,242,17,0,51
question-answering,8,"In the future , we plan to look further into the different types of questions and focus on those questions which currently have low performance , such as the "" why ' questions .",related work,0,243,18,0,34
question-answering,8,We also plan to test how our models could be applied to other machine comprehension datasets .,related work,0,244,19,0,17
question-answering,8,shows the numbers of answers with different lengths .,related work,0,245,20,0,9
question-answering,8,Bottom : Plot ( 3 ) shows the performance our the two models on different types of questions .,related work,0,246,21,0,19
question-answering,8,Plot ( 4 ) shows the numbers of different types of questions .,related work,0,247,22,0,13
question-answering,8,a appendix,related work,0,248,23,0,2
question-answering,8,"We show the performance breakdown by answer lengths and question types for our sequence model , boundary model and the ensemble model in .",related work,0,249,24,0,24
question-answering,9,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,title,1,2,1,0,8
question-answering,9,abstract,abstract,0,3,1,0,1
question-answering,9,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",abstract,1,4,2,0,23
question-answering,9,Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline .,abstract,0,5,3,0,28
question-answering,9,"However , Rajpurkar et al . ( 2016 ) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text .",abstract,0,6,4,1,27
question-answering,9,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",abstract,0,7,5,0,35
question-answering,9,We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers .,abstract,0,8,6,0,28
question-answering,9,Our approach improves upon the best published results of Wang & Jiang ( 2016 ) by 5 % and decreases the error of Rajpurkar et al. 's baseline by > 50 %.,abstract,0,9,7,1,32
question-answering,9,"Recently , Rajpurkar et al. ( 2016 ) released the less restricted SQUAD dataset 1 that does not place any constraints on the set of allowed answers , other than that they should be drawn from the evidence document .",abstract,0,10,8,1,40
question-answering,9,Rajpurkar et al. proposed a baseline system that chooses answers from the constituents identified by an existing syntactic parser .,abstract,0,11,9,1,20
question-answering,9,"This allows them to prune the O ( N 2 ) answer candidates in each document of length N , but it also effectively renders 20.7 % of all questions unanswerable .",abstract,0,12,10,0,32
question-answering,9,"Subsequent work by Wang & Jiang ( 2016 ) significantly improve upon this baseline by using an endto - end neural network architecture to identify answer spans by labeling either individual words , or the start and end of the answer span .",abstract,0,13,11,1,43
question-answering,9,"Both of these methods do not make independence assumptions about substructures , but they are susceptible to search errors due to greedy training and decoding .",abstract,0,14,12,0,26
question-answering,9,1,abstract,0,15,13,0,1
question-answering,9,introduction,introduction,0,16,1,0,1
question-answering,9,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,introduction,1,17,2,0,21
question-answering,9,"The reading comprehension task is of practical interest - we want computers to be able to read the world 's text and then answer our questions - and , since we believe it requires deep language understanding , it has also become a flagship task in NLP research .",introduction,0,18,3,0,49
question-answering,9,A number of reading comprehension datasets have been developed that focus on answer selection from a small set of alternatives defined by annotators or existing NLP pipelines that can not be trained end - to - end .,introduction,0,19,4,0,38
question-answering,9,"Subsequently , the models proposed for this task have tended to make use of the limited set of candidates , basing their predictions on mention - level attention weights , or centering classifiers , or network memories on candidate locations .",introduction,0,20,5,0,41
question-answering,9,"In contrast , here we argue that it is beneficial to simplify the decoding procedure by enumerating all possible answer spans .",introduction,0,21,6,0,22
question-answering,9,"By explicitly representing each answer span , our model can be globally normalized during training and decoded exactly during evaluation .",introduction,0,22,7,0,21
question-answering,9,"A naive approach to building the O ( N 2 ) spans of up to length N would require a network that is cubic in size with respect to the passage length , and such a network would be untrainable .",introduction,0,23,8,0,41
question-answering,9,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .",introduction,1,24,9,0,27
question-answering,9,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .",introduction,1,25,10,0,29
question-answering,9,"In our experiments , we show an increase in performance over of 5 % in terms of exact match to a reference answer , and 3.6 % in terms of predicted answer F1 with respect to the reference .",introduction,0,26,11,0,39
question-answering,9,"On both of these metrics , we close the gap between Rajpurkar et al. 's baseline and the human - performance upper-bound by > 50 %.",introduction,0,27,12,1,26
question-answering,9,extractive question answering,introduction,0,28,13,0,3
question-answering,9,task definition,introduction,0,29,14,0,2
question-answering,9,"Extractive question answering systems take as input a question q = {q 0 , . . . , q n } and a passage of text p = {p 0 , . . . , pm } from which they predict a single answer span a = a start , a end , represented as a pair of indices into p.",introduction,0,30,15,0,61
question-answering,9,"Machine learned extractive question answering systems , such as the one presented here , learn a predictor function f ( q , p ) ?",introduction,0,31,16,0,25
question-answering,9,"a from a training dataset of q , p , a triples .",introduction,0,32,17,0,13
question-answering,9,related work,related work,0,33,1,0,2
question-answering,9,"For the SQUAD dataset , the original paper from implemented a linear model with sparse features based on n-grams and part - of - speech tags present in the question and the candidate answer .",related work,0,34,2,0,35
question-answering,9,"Other than lexical features , they also used syntactic information in the form of dependency paths to extract more general features .",related work,0,35,3,0,22
question-answering,9,"They set a strong baseline for following work and also presented an in depth analysis , showing that lexical and syntactic features contribute most strongly to their model 's performance .",related work,0,36,4,0,31
question-answering,9,"Subsequent work by use an end - to - end neural network method that uses a Match - LSTM to model the question and the passage , and uses pointer networks to extract the answer span from the passage .",related work,0,37,5,0,40
question-answering,9,This model resorts to greedy decoding and falls short in terms of performance compared to our model ( see Section 5 for more detail ) .,related work,0,38,6,0,26
question-answering,9,"While we only compare to published baselines , there are other unpublished competitive systems on the SQUAD leaderboard , as listed in footnote",related work,0,39,7,0,23
question-answering,9,4 .,related work,0,40,8,0,2
question-answering,9,"A task that is closely related to extractive question answering is the Cloze task , in which the goal is to predict a concealed span from a declarative sentence given a passage of supporting text .",related work,0,41,9,0,36
question-answering,9,presented a Cloze dataset in which the task is to predict the correct entity in an incomplete sentence given an abstractive summary of a news article .,related work,0,42,10,0,27
question-answering,9,hermann et al .,related work,0,43,11,1,4
question-answering,9,also present various neural architectures to solve the problem .,related work,0,44,12,0,10
question-answering,9,"Although this dataset is large and varied in domain , recent analysis by shows that simple models can achieve close to the human upper bound .",related work,0,45,13,0,26
question-answering,9,"As noted by the authors of the SQUAD paper , the annotated answers in the SQUAD dataset are often spans that include non-entities and can be longer phrases , unlike the Cloze datasets , thus making the task more challenging .",related work,0,46,14,0,41
question-answering,9,"Another , more traditional line of work has focused on extractive question answering on sentences , where the task is to extract a sentence from a document , given a question .",related work,0,47,15,0,32
question-answering,9,"Relevant datasets include datasets from the annual TREC evaluations and WikiQA , where the latter dataset specifically focused on Wikipedia passages .",related work,0,48,16,0,22
question-answering,9,"There has been a line of interesting recent publications using neural architectures , focused on this variety of extractive question answering .",related work,0,49,17,0,22
question-answering,9,"These methods model the question and a candidate answer sentence , but do not focus on possible candidate answer spans that may contain the answer to the given question .",related work,0,50,18,0,30
question-answering,9,"In this work , we focus on the more challenging problem of extracting the precise answer span .",related work,0,51,19,0,18
question-answering,9,model,related work,0,52,20,0,1
question-answering,9,"We propose a model architecture called RASOR 2 illustrated in , that explicitly computes embedding representations for candidate answer spans .",related work,0,53,21,0,21
question-answering,9,"In most structured prediction problems ( e.g. sequence labeling or parsing ) , the number of possible output structures is exponential in the input length , and computing representations for every candidate is prohibitively expensive .",related work,0,54,22,0,36
question-answering,9,"However , we exploit the simplicity of our task , where we can trivially and tractably enumerate all candidates .",related work,0,55,23,0,20
question-answering,9,"This facilitates an expressive model that computes joint representations of every answer span , that can be globally normalized during learning .",related work,0,56,24,0,22
question-answering,9,"In order to compute these span representations , we must aggregate information from the passage and the question for every answer candidate .",related work,0,57,25,0,23
question-answering,9,"For the example in , RASOR computes an embedding for the candidate answer spans : fixed to , fixed to the , to the , etc .",related work,0,58,26,0,27
question-answering,9,A naive approach for these aggregations would require a network that is cubic in size with respect to the passage length .,related work,0,59,27,0,22
question-answering,9,"Instead , our model reduces this to a quadratic size by reusing recurrent computations for shared substructures ( i.e. common passage words ) from different spans .",related work,0,60,28,0,27
question-answering,9,"Since the choice of answer span depends on the original question , we must incorporate this information into the computation of the span representation .",related work,0,61,29,0,25
question-answering,9,We model this by augmenting the passage word embeddings with additional embedding representations of the question .,related work,0,62,30,0,17
question-answering,9,"In this section , we motivate and describe the architecture for RASOR in a top - down manner .",related work,0,63,31,0,19
question-answering,9,scoring answer spans,related work,0,64,32,0,3
question-answering,9,"The goal of our extractive question answering system is to predict the single best answer span among all candidates from the passage p , denoted as A ( p ) .",related work,0,65,33,0,31
question-answering,9,"Therefore , we define a probability distribution over all possible answer spans given the question q and passage p , and the predictor function finds the answer span with the maximum likelihood :",related work,0,66,34,0,33
question-answering,9,One might be tempted to introduce independence assumptions that would enable cheaper decoding .,related work,0,67,35,0,14
question-answering,9,"For example , this distribution can be modeled as ( 1 ) a product of conditionally independent distributions ( binary ) for every word or ( 2 ) a product of conditionally independent distributions ( over words ) for the start and end indices of the answer span .",related work,0,68,36,0,49
question-answering,9,"However , we show in Section 5.2 that such independence assumptions hurt the accuracy of the model , and instead we only assume a fixed - length representation ha of each candidate span that is scored and normalized with a softmax layer ( Span score and Softmax in ) :",related work,0,69,37,0,50
question-answering,9,where FFNN ( ) denotes a fully connected feed - forward neural network that provides a non-linear mapping of its input embedding .,related work,0,70,38,0,23
question-answering,9,rasor : recurrent span representation,related work,0,71,39,0,5
question-answering,9,"The previously defined probability distribution depends on the answer span representations , ha .",related work,0,72,40,0,14
question-answering,9,"When computing ha , we assume access to representations of individual passage words that have been augmented with a representation of the question .",related work,0,73,41,0,24
question-answering,9,"We denote these question - focused passage word embeddings as {p * 1 , . . . , p * m } and describe their creation in Section 3.3 .",related work,0,74,42,0,30
question-answering,9,"In order to reuse computation for shared substructures , we use a bidirectional LSTM to encode the left and right context of every p * i ( Passage - level BiLSTM in ) .",related work,0,75,43,0,34
question-answering,9,This allows us to simply concatenate the bidirectional LSTM ( BiLSTM ) outputs at the endpoints of a span to jointly encode its inside and outside information ( Span embedding in :,related work,0,76,44,0,32
question-answering,9,where BILSTM ( ) denotes a BiLSTM over it s input embedding sequence and p * i is the concatenation of forward and backward outputs at time - step i .,related work,0,77,45,0,31
question-answering,9,"While the visualization in shows a single layer BiLSTM for simplicity , we use a multi - layer BiLSTM in our experiments .",related work,0,78,46,0,23
question-answering,9,"The concatenated output of each layer is used as input for the subsequent layer , allowing the upper layers to depend on the entire passage .",related work,0,79,47,0,26
question-answering,9,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,related work,0,80,48,0,6
question-answering,9,"Computing the question - focused passage word embeddings {p * 1 , . . . , p * m } requires integrating question information into the passage .",related work,0,81,49,0,28
question-answering,9,The architecture for this integration is flexible and likely depends on the nature of the dataset .,related work,0,82,50,0,17
question-answering,9,"For the SQUAD dataset , we find that both passage - aligned and passageindependent question representations are effective at incorporating this contextual information , and experiments will show that their benefits are complementary .",related work,0,83,51,0,34
question-answering,9,"To incorporate these question representations , we simply concatenate them with the passage word embeddings ( Question - focused passage word embedding in ) .",related work,0,84,52,0,25
question-answering,9,We use fixed pretrained embeddings to represent question and passage words .,related work,0,85,53,0,12
question-answering,9,"Therefore , in the following discussion , notation for the words are interchangeable with their embedding representations .",related work,0,86,54,0,18
question-answering,9,Question - independent passage word embedding,related work,0,87,55,0,6
question-answering,9,"The first component simply looks up the pretrained word embedding for the passage word , pi .",related work,0,88,56,0,17
question-answering,9,passage - aligned question representation,related work,0,89,57,0,5
question-answering,9,"In this dataset , the question - passage pairs often contain large lexical overlap or similarity near the correct answer span .",related work,0,90,58,0,22
question-answering,9,"To encourage the model to exploit these similarities , we include a fixed - length representation of the question based on soft - alignments with the passage word .",related work,0,91,59,0,29
question-answering,9,"The alignments are computed via neural attention , and we use the variant proposed by , where attention scores are dot products between non-linear mappings of word embeddings .",related work,0,92,60,0,29
question-answering,9,q align i = n j=1 a ij q j,related work,0,93,61,0,10
question-answering,9,passage - independent question representation,related work,0,94,62,0,5
question-answering,9,We also include a representation of the question that does not depend on the passage and is shared for all passage words .,related work,0,95,63,0,23
question-answering,9,"Similar to the previous question representation , an attention score is computed via a dot -product , except the question word is compared to a universal learned embedding rather any particular passage word .",related work,0,96,64,0,34
question-answering,9,"Additionally , we incorporate contextual information with a BiLSTM before aggregating the outputs using this attention mechanism .",related work,0,97,65,0,18
question-answering,9,The goal is to generate a coarse - grained summary of the question that depends on word order .,related work,0,98,66,0,19
question-answering,9,"Formally , the passage - independent question representation q indep is computed as follows :",related work,0,99,67,0,15
question-answering,9,This representation is a bidirectional generalization of the question representation recently proposed by for a different question - answering task .,related work,0,100,68,0,21
question-answering,9,"Given the above three components , the complete question - focused passage word embedding for pi is their concatenation :",related work,0,101,69,0,20
question-answering,9,learning,related work,0,102,70,0,1
question-answering,9,"Given the above model specification , learning is straightforward .",related work,0,103,71,0,10
question-answering,9,We simply maximize the loglikelihood of the correct answer candidates and backpropagate the errors end - to - end .,related work,0,104,72,0,20
question-answering,9,experimental setup,experiment,0,105,1,0,2
question-answering,9,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,experiment,1,106,2,0,25
question-answering,9,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,experiment,0,107,3,0,27
question-answering,9,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .",experiment,1,108,4,0,35
question-answering,9,Hidden layers in the feed forward neural networks use rectified linear units .,experiment,1,109,5,0,13
question-answering,9,Answer candidates are limited to spans with at most 30 words .,experiment,0,110,6,0,12
question-answering,9,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .",experiment,1,111,7,0,67
question-answering,9,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,experiment,1,112,8,0,42
question-answering,9,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,experiment,1,113,9,0,36
question-answering,9,results,result,0,114,1,0,1
question-answering,9,"We train on the 80 k ( question , passage , answer span ) triples in the SQUAD training set and report results on the 10k examples in the SQUAD development and test sets .",result,0,115,2,0,35
question-answering,9,"All results are calculated using the official SQUAD evaluation script , which reports exact answer match and F1 overlap of the unigrams between the predicted answer and the closest labeled answer from the 3 reference answers given in the SQUAD development set .",result,0,116,3,0,43
question-answering,9,comparisons to other work,result,0,117,4,0,4
question-answering,9,Our model with recurrent span representations ( RASOR ) is compared to all previously published systems,result,0,118,5,0,16
question-answering,9,4 . published a logistic regression baseline as well as human performance on the SQUAD task .,result,0,119,6,0,17
question-answering,9,"The logistic regression baseline uses the output of an existing syntactic parser both as a constraint on the set of allowed answer spans , and as a method of creating sparse features for an answer -centric scoring model .",result,0,120,7,0,39
question-answering,9,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",result,1,121,8,0,43
question-answering,9,More closely related to RASOR is the boundary model with Match - LSTMs and Pointer Networks by .,result,0,122,9,0,18
question-answering,9,"Their model similarly uses recurrent networks to learn embeddings of each passage word in the context of the question , and it can also capture interactions between endpoints , since the end index probability distribution is conditioned on the start index .",result,0,123,10,0,42
question-answering,9,"However , both training and evaluation are greedy , making their system susceptible to search errors when decoding .",result,0,124,11,0,19
question-answering,9,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .",result,1,125,12,0,33
question-answering,9,model variations,result,0,126,13,0,2
question-answering,9,We investigate two main questions in the following ablations and comparisons .,result,0,127,14,0,12
question-answering,9,( 1 ) How important are the two methods of representing the question described in Section 3.3 ?,result,0,128,15,0,18
question-answering,9,( 2 ) What is the impact of learning a loss function that accurately reflects the span prediction task ?,result,0,129,16,0,20
question-answering,9,Question representations shows the performance of RASOR when either of the two question representations described in Section 3.3 is removed .,result,0,130,17,0,21
question-answering,9,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .",result,1,131,18,0,24
question-answering,9,"If the question is only integrated through the inclusion of a passage - independent representation , performance drops drastically .",result,0,132,19,0,20
question-answering,9,"The passage - independent question representation over the BiLSTM is less important , but it still accounts for over 3 % exact match and F 1 .",result,0,133,20,0,27
question-answering,9,The input of both of these components is analyzed qualitatively in Section 6 .,result,0,134,21,0,14
question-answering,9,question representation em f1,result,0,135,22,0,4
question-answering,9,Only passage - independent 48.7 56.6 Only passage - aligned 63.1 71.3 RASOR 66.4 74.9,result,0,136,23,0,15
question-answering,9,( a ) Ablation of question representations .,result,0,137,24,0,8
question-answering,9,learning objective em f1,result,0,138,25,0,4
question-answering,9,Membership prediction 57.9 69.7 BIO sequence prediction 63.9 73.0 Endpoints prediction 65.3 75.1 Span prediction w/ log loss 65.2 73.6 ( b ) Comparisons for different learning objectives given the same passage - level BiLSTM .,result,0,139,26,0,36
question-answering,9,learning objectives,result,0,140,27,0,2
question-answering,9,"Given a fixed architecture that is capable of encoding the input questionpassage pairs , there are many ways of setting up a learning objective to encourage the model to predict the correct span .",result,0,141,28,0,34
question-answering,9,"In , we provide comparisons of some alternatives ( learned end - toend ) given only the passage - level BiLSTM from RASOR .",result,0,142,29,0,24
question-answering,9,"In order to provide clean comparisons , we restrict the alternatives to objectives thatare trained and evaluated with exact decoding .",result,0,143,30,0,21
question-answering,9,The simplest alternative is to consider this task as binary classification for every word ( Membership prediction in ) .,result,0,144,31,0,20
question-answering,9,"In this baseline , we optimize the logistic loss for binary labels indicating whether passage words belong to the correct answer span .",result,0,145,32,0,23
question-answering,9,"At prediction time , a valid span can be recovered in linear time by finding the maximum contiguous sum of scores .",result,0,146,33,0,22
question-answering,9,proposed a sequence - labeling scheme that is similar to the above baseline ( BIO sequence prediction in ) .,result,0,147,34,0,20
question-answering,9,We follow their proposed model and learn a conditional random field ( CRF ) layer after the passage - level BiLSTM to model transitions between the different labels .,result,0,148,35,0,29
question-answering,9,"At prediction time , a valid span can be recovered in linear time using Viterbi decoding , with hard transition constraints to enforce a single contiguous output .",result,0,149,36,0,28
question-answering,9,We also consider a model that independently predicts the two endpoints of the answer span ( Endpoints prediction in ) .,result,0,150,37,0,21
question-answering,9,This model uses the softmax loss over passage words during learning .,result,0,151,38,0,12
question-answering,9,"When decoding , we only need to enforce the constraint that the start index is no greater than the end index .",result,0,152,39,0,22
question-answering,9,"Without the interactions between the endpoints , this can be computed in linear time .",result,0,153,40,0,15
question-answering,9,Note that this model has the same expressivity as RASOR if the span - level FFNN were removed .,result,0,154,41,0,19
question-answering,9,"Lastly , we compare with a model using the same architecture as RASOR but is trained with a binary logistic loss rather than a softmax loss over spans ( Span prediction w/ logistic loss in ) .",result,0,155,42,0,37
question-answering,9,The trend in shows that the model is better at leveraging the supervision as the learning objective more accurately reflects the fundamental task at hand : determining the best answer span .,result,0,156,43,0,32
question-answering,9,"First , we observe general improvements when using labels that closely align with the task .",result,1,157,44,0,16
question-answering,9,"For example , the labels for membership prediction simply happens to provide single contiguous spans in the supervision .",result,0,158,45,0,19
question-answering,9,The model must consider far more possible answers than it needs to ( the power set of all words ) .,result,0,159,46,0,21
question-answering,9,The same problem holds for BIO sequence predictionthe model must do additional work to learn the semantics of the BIO tags .,result,0,160,47,0,22
question-answering,9,"On the other hand , in RASOR , the semantics of an answer span is naturally encoded by the set of labels .",result,0,161,48,0,23
question-answering,9,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .",result,1,162,49,0,17
question-answering,9,"RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",result,1,163,50,0,26
question-answering,9,"While this does not provide improvements for predicting the correct region of the answer ( captured by the F1 metric , which drops by 0.2 ) , it is more likely to predict a clean answer span that matches human judgment exactly ( captured by the exact - match metric ) .",result,0,164,51,0,52
question-answering,9,shows how the performances of RASOR and the endpoint predictor introduced in Section 5.2 degrade as the lengths of their predictions increase .,result,0,165,52,0,23
question-answering,9,It is clear that explicitly modeling interactions between end markers is increasingly important as the span grows in length .,result,0,166,53,0,20
question-answering,9,"The passageindependent question representation pays most attention to the words that could attach to the answer in the passage ( "" brought "" , "" against "" ) or describe the answer category ( "" people "" ) .",result,0,167,54,0,39
question-answering,9,"Meanwhile , the passage - aligned question representation pays attention to similar words .",result,0,168,55,0,14
question-answering,9,"The top predictions for both examples are all valid syntactic constituents , and they all have the correct semantic category .",result,0,169,56,0,21
question-answering,9,"However , RASOR assigns almost as much probability mass to it 's incorrect third prediction "" British "" as it does to the top scoring correct prediction "" Egyptian "" .",result,0,170,57,0,31
question-answering,9,"This showcases a common failure case for RASOR , where it can find an answer of the correct type close to a phrase that overlaps with the question - but it can not accurately represent the semantic dependency on that phrase .",result,0,171,58,0,42
question-answering,9,analysis,result,0,172,59,0,1
question-answering,9,conclusion,result,0,173,60,0,1
question-answering,9,We have shown a novel approach for perform extractive question answering on the SQUAD dataset by explicitly representing and scoring answer span candidates .,result,0,174,61,0,24
question-answering,9,The core of our model relies on a recurrent network that enables shared computation for the shared substructure across span candidates .,result,0,175,62,0,22
question-answering,9,"We explore different methods of encoding the passage and question , showing the benefits of including both passage - independent and passage - aligned question representations .",result,0,176,63,0,27
question-answering,9,"While we show that this encoding method is beneficial for the task , this is orthogonal to the core contribution of efficiently computing span representation .",result,0,177,64,0,26
question-answering,9,"In future work , we plan to explore alternate architectures that provide input to the recurrent span representations .",result,0,178,65,0,19
question-answering,1,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,1,2,1,0,9
question-answering,1,abstract,abstract,0,3,1,0,1
question-answering,1,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,1,4,2,0,15
question-answering,1,A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,abstract,0,5,3,0,20
question-answering,1,"As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .",abstract,0,6,4,0,28
question-answering,1,"The proposed models not only nicely represent the hierarchical structures of sentences with their layerby - layer composition and pooling , but also capture the rich matching patterns at different levels .",abstract,0,7,5,0,32
question-answering,1,"Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .",abstract,0,8,6,0,29
question-answering,1,The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .,abstract,0,9,7,0,29
question-answering,1,introduction,introduction,0,10,1,0,1
question-answering,1,Matching two potentially heterogenous language objects is central to many natural language applications .,introduction,1,11,2,0,14
question-answering,1,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",introduction,0,12,3,0,45
question-answering,1,"Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",introduction,0,13,4,0,40
question-answering,1,"Natural language sentences have complicated structures , both sequential and hierarchical , thatare essential for understanding them .",introduction,0,14,5,0,18
question-answering,1,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,introduction,1,15,6,0,26
question-answering,1,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",introduction,1,16,7,0,28
question-answering,1,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",introduction,1,17,8,0,44
question-answering,1,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",introduction,0,18,9,0,28
question-answering,1,This is part of our continuing effort 1 in understanding natural language objects and the matching between them .,introduction,0,19,10,0,19
question-answering,1,Our main contributions can be summarized as follows .,introduction,0,20,11,0,9
question-answering,1,"First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .",introduction,0,21,12,0,73
question-answering,1,roadmap,introduction,0,22,13,0,1
question-answering,1,"We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .",introduction,0,23,14,0,28
question-answering,1,"Based on that , in Section 3 , we propose two architectures for sentence matching , with a detailed discussion of their relation .",introduction,0,24,15,0,24
question-answering,1,"In Section 4 , we briefly discuss the learning of the proposed architectures .",introduction,0,25,16,0,14
question-answering,1,"Then in Section 5 , we report our empirical study , followed by a brief discussion of related work in Section 6 .",introduction,0,26,17,0,23
question-answering,1,convolutional sentence model,introduction,0,27,18,0,3
question-answering,1,We start with proposing a new convolutional architecture for modeling sentences .,introduction,0,28,19,0,12
question-answering,1,"As illustrated in , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .",introduction,0,29,20,0,52
question-answering,1,"As in most convolutional models , we use convolution units with a local "" receptive field "" and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .",introduction,0,30,21,0,40
question-answering,1,convolution,introduction,0,31,22,0,1
question-answering,1,"As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .",introduction,0,32,23,0,35
question-answering,1,"Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is",introduction,0,33,24,0,26
question-answering,1,and it s matrix form is z,introduction,0,34,25,0,7
question-answering,1,gives the output of feature map of type - f for location i in Layer - ;,introduction,0,35,26,0,17
question-answering,1,"w ( , f ) is the parameters for f on Layer - , with matrix form",introduction,0,36,27,0,17
question-answering,1,"? ( ) is the activation function ( e.g. , Sigmoid or Relu )",introduction,0,37,28,0,14
question-answering,1,?,introduction,0,38,29,0,1
question-answering,1,( ?1 ) i denotes the segment of Layer - ?,introduction,0,39,30,0,11
question-answering,1,"1 for the convolution at location i , whil",introduction,0,40,31,0,9
question-answering,1,concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .,introduction,0,41,32,0,17
question-answering,1,max - pooling,introduction,0,42,33,0,3
question-answering,1,"We take a max - pooling in every two - unit window for every f , after each convolution",introduction,0,43,34,0,19
question-answering,1,The effects of pooling are two - fold :,introduction,0,44,35,0,9
question-answering,1,"1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .",introduction,0,45,36,0,42
question-answering,1,length variability,introduction,0,46,37,0,2
question-answering,1,The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .,introduction,0,47,38,0,21
question-answering,1,"More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .",introduction,0,48,39,0,22
question-answering,1,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .",introduction,0,49,40,0,37
question-answering,1,"For any given sentence input x , the output of type - f filter for location i in the th layer is given by",introduction,0,50,41,0,24
question-answering,1,"( 2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .",introduction,0,51,42,0,56
question-answering,1,"Actually it creates a natural hierarchy of all - zero padding ( as illustrated in ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .",introduction,0,52,43,0,46
question-answering,1,"The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .",introduction,0,53,44,0,27
question-answering,1,"gives an example on what could happen on the first two layers with input sentence "" The cat sat on the mat "" .",introduction,0,54,45,0,24
question-answering,1,"Just for illustration purpose , we present a dramatic choice of parameters ( by turning off some elements in W ) to make the convolution units focus on different segments within a 3 - word window .",introduction,0,55,46,0,37
question-answering,1,"For example , some feature maps ( group 2 ) give compositions for "" the cat "" and "" cat sat "" , each being a vector .",introduction,0,56,47,0,28
question-answering,1,"Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .",introduction,0,57,48,0,26
question-answering,1,"The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between "" on the "" and "" the mat "" for feature maps group 2 from the rightmost two sliding windows .",introduction,0,58,49,0,40
question-answering,1,Some Analysis on the Convolutional Architecture,introduction,0,59,50,0,6
question-answering,1,relation to recursive models,introduction,0,60,51,0,4
question-answering,1,"Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .",introduction,0,61,52,0,26
question-answering,1,"First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .",introduction,0,62,53,0,35
question-answering,1,"Instead , it takes multiple choices of composition via a large feature map ( encoded in w ( , f ) for different f ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .",introduction,0,63,54,0,50
question-answering,1,"With any window width k ? 3 , the type of composition would be much richer than that of RAE .",introduction,0,64,55,0,21
question-answering,1,"Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .",introduction,0,65,56,0,31
question-answering,1,"However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .",introduction,0,66,57,0,24
question-answering,1,"For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a "" global "" synthesis on the learned sentence representation .",introduction,0,67,58,0,29
question-answering,1,"Relation to "" Shallow "" Convolutional Models",introduction,0,68,59,0,7
question-answering,1,"The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .",introduction,0,69,60,0,40
question-answering,1,"This type of models , with local convolutions and a global pooling , essentially do a "" soft "" local template matching and is able to detect local features useful for a certain task .",introduction,0,70,61,0,35
question-answering,1,"Since the sentencelevel sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .",introduction,0,71,62,0,23
question-answering,1,It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .,introduction,0,72,63,0,35
question-answering,1,convolutional matching models,introduction,0,73,64,0,3
question-answering,1,"Based on the discussion in Section 2 , we propose two related convolutional architectures , namely ARC - I and ARC - II ) , for matching two sentences .",introduction,0,74,65,0,30
question-answering,1,architecture - i ( arc - i ),introduction,0,75,66,0,8
question-answering,1,"Architecture - I ( ARC - I ) , as illustrated in , takes a conventional approach :",introduction,0,76,67,0,18
question-answering,1,"It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .",introduction,0,77,68,0,28
question-answering,1,"It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .",introduction,0,78,69,0,22
question-answering,1,"Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .",introduction,0,79,70,0,76
question-answering,1,"In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .",introduction,0,80,71,0,25
question-answering,1,"This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .",introduction,0,81,72,0,29
question-answering,1,architecture - ii ( arc - ii ),introduction,0,82,73,0,8
question-answering,1,"In view of the drawback of Architecture - I , we propose Architecture - II ( ARC - II ) that is built directly on the interaction space between two sentences .",introduction,0,83,74,0,32
question-answering,1,"It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .",introduction,0,84,75,0,34
question-answering,1,"Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through "" one-dimensional "" ( 1D ) convolutions .",introduction,0,85,76,0,32
question-answering,1,"For segment ion S X and segment j on S Y , we have the feature map",introduction,0,86,77,0,17
question-answering,1,where ?,introduction,0,87,78,0,2
question-answering,1,"i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z",introduction,0,88,79,0,23
question-answering,1,.,introduction,0,89,80,0,1
question-answering,1,Clearly the 1D convolution preserves the location information about both segments .,introduction,0,90,81,0,12
question-answering,1,"After that in Layer - 2 , it performs a 2D max - pooling in non-overlapping 2 2 windows ( illustrated in )",introduction,0,91,82,0,23
question-answering,1,"( 4 ) In Layer - 3 , we perform a 2D convolution on k 3 k 3 windows of output from Layer - 2 :",introduction,0,92,83,0,26
question-answering,1,( 5 ),introduction,0,93,84,0,3
question-answering,1,"This could goon for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .",introduction,0,94,85,0,25
question-answering,1,the 2d - convolution,introduction,0,95,86,0,4
question-answering,1,"After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation z ( ) i , j which encodes the information from both sentences .",introduction,0,96,87,0,42
question-answering,1,The general two - dimensional convolution is formulated as z ( ),introduction,0,97,88,0,12
question-answering,1,where ?,introduction,0,98,89,0,2
question-answering,1,concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,introduction,0,99,90,0,14
question-answering,1,"This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .",introduction,0,100,91,0,28
question-answering,1,"This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :",introduction,0,101,92,0,19
question-answering,1,1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .,introduction,0,102,93,0,20
question-answering,1,"contains information about the words in S X before those in z ( ) i + 1 , j , although they maybe generated with slightly different segments in S Y , due to the 2D pooling ( illustrated in .",introduction,0,103,94,0,41
question-answering,1,"The orders is however retained in a "" conditional "" sense .",introduction,0,104,95,0,12
question-answering,1,"Our experiments show that when ARC - II is trained on the ( S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting , which however does not happen with ARC - I.",introduction,0,105,96,0,60
question-answering,1,model generality,introduction,0,106,97,0,2
question-answering,1,It is not hard to show that ARC - II actually subsumes ARC - I as a special case .,introduction,0,107,98,0,20
question-answering,1,"Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .",introduction,0,108,99,0,51
question-answering,1,"More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .",introduction,0,109,100,0,52
question-answering,1,"As a result , the output for each filter f , denoted z",introduction,0,110,101,0,13
question-answering,1,"( 1 , f ) 1:n , 1:n ( n is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in ARC - I .",introduction,0,111,102,0,43
question-answering,1,"Clearly the 2D pooling that follows will reduce to 1 D pooling , with this separateness preserved .",introduction,0,112,103,0,18
question-answering,1,"If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .",introduction,0,113,104,0,55
question-answering,1,"As suggested by the order - preserving property and the generality of ARC - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .",introduction,0,114,105,0,53
question-answering,1,"As a result , ARC - II can naturally blend two seemingly diverging processes :",introduction,0,115,106,0,15
question-answering,1,"1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .",introduction,0,116,107,0,33
question-answering,1,This intuition is verified by the superior performance of ARC - II in experiments ( Section 5 ) on different matching tasks .,introduction,0,117,108,0,23
question-answering,1,training,introduction,0,118,109,0,1
question-answering,1,We employ a discriminative training strategy with a large margin objective .,introduction,0,119,110,0,12
question-answering,1,"Suppose that we are given the following triples ( x , y + , y ? )",introduction,0,120,111,0,17
question-answering,1,"from the oracle , with x matched with y + better than with y ? .",introduction,0,121,112,0,16
question-answering,1,We have the following ranking - based loss as objective :,introduction,0,122,113,0,11
question-answering,1,"where s ( x , y) is predicted matching score for ( x , y ) , and ?",introduction,0,123,114,0,19
question-answering,1,includes the parameters for convolution layers and those for the MLP .,introduction,0,124,115,0,12
question-answering,1,The optimization is relatively straightforward for both architectures with the standard back - propagation .,introduction,0,125,116,0,15
question-answering,1,The gating function ( see Section 2 ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .,introduction,0,126,117,0,32
question-answering,1,"In other words , We use stochastic gradient descent for the optimization of models .",introduction,1,127,118,0,15
question-answering,1,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,introduction,1,128,119,0,26
question-answering,1,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .",introduction,1,129,120,0,30
question-answering,1,"For small datasets ( less than 10 k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .",introduction,0,130,121,0,29
question-answering,1,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .",introduction,1,131,122,0,53
question-answering,1,"Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .",introduction,0,132,123,0,32
question-answering,1,We vary the maximum length of words for different tasks to cope with its longest sentence .,introduction,0,133,124,0,17
question-answering,1,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .",introduction,1,134,125,0,30
question-answering,1,"ARC - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while ARC - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .",introduction,0,135,126,0,53
question-answering,1,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .",introduction,1,136,127,0,33
question-answering,1,experiments,experiment,0,137,1,0,1
question-answering,1,"We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .",experiment,0,138,2,0,26
question-answering,1,"Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .",experiment,0,139,3,0,45
question-answering,1,"Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .",experiment,0,140,4,0,28
question-answering,1,competitor methods,method,0,141,1,0,2
question-answering,1,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,method,1,142,2,0,21
question-answering,1,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :",method,1,143,3,0,54
question-answering,1,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",method,1,144,4,0,34
question-answering,1,We use the SENNA - type sentence model for sentence representation ;,method,1,145,5,0,12
question-answering,1,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",method,1,146,6,0,28
question-answering,1,"All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .",method,0,147,7,0,42
question-answering,1,experiment i : sentence,experiment,0,148,1,0,4
question-answering,1,completion,experiment,0,149,2,0,1
question-answering,1,This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .,experiment,0,150,3,0,23
question-answering,1,"Basically , we take a sentence from Reuterswith two "" balanced "" clauses ( with 8 ? 28 words ) divided by one comma , and use the first clause as S X and the second as S Y .",experiment,0,151,4,0,40
question-answering,1,The task is then to recover the original second clause for any given first clause .,experiment,0,152,5,0,16
question-answering,1,The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .,experiment,0,153,6,0,21
question-answering,1,"We deliberately make the task harder by using negative second clauses similar to the original ones 4 , both in training and testing .",experiment,0,154,7,0,24
question-answering,1,One representative example is given as follows :,experiment,0,155,8,0,8
question-answering,1,"All models are trained on 3 million triples ( from 600K positive pairs ) , and tested on 50K positive pairs , each accompanied by four negatives , with results shown in .",experiment,0,156,9,0,33
question-answering,1,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .",experiment,0,157,10,0,27
question-answering,1,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .",experiment,0,158,11,0,21
question-answering,1,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",experiment,0,159,12,0,34
question-answering,1,"It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..",experiment,0,160,13,0,61
question-answering,1,"This task is slightly easier than Experiment I , with more training instances and purely random negatives .",experiment,0,161,14,0,18
question-answering,1,"It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime ? rest ) .",experiment,0,162,15,0,28
question-answering,1,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .",experiment,0,163,16,0,26
question-answering,1,Experiment II : Matching A Response to A Tweet,experiment,0,164,17,0,9
question-answering,1,experiment iii : paraphrase identification,experiment,0,165,1,0,5
question-answering,1,"Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .",experiment,0,166,2,0,23
question-answering,1,is included to test our methods on matching homogenous objects .,experiment,0,167,3,0,11
question-answering,1,"Here we use the benchmark MSRP dataset , which contains 4,076 instances for training and 1,725 for test .",experiment,0,168,4,0,19
question-answering,1,We use all the training instances and report the test performance from early stopping .,experiment,0,169,5,0,15
question-answering,1,"As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .",experiment,0,170,6,0,24
question-answering,1,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .",experiment,0,171,7,0,64
question-answering,1,discussions,experiment,0,172,8,0,1
question-answering,1,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,experiment,1,173,9,0,22
question-answering,1,"It s superiority over ARC - I , however , is less salient when the sentences have deep grammatical structures and the matching relies lesson the local matching patterns , as in Experiment - I .",experiment,0,174,10,0,36
question-answering,1,"This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .",experiment,0,175,11,0,34
question-answering,1,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .",experiment,1,176,12,0,41
question-answering,1,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .",experiment,1,177,13,0,50
question-answering,1,It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .,experiment,0,178,14,0,34
question-answering,1,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,experiment,1,179,15,0,20
question-answering,1,"We hypothesize that the Word2 Vec embedding is trained in such away that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .",experiment,0,180,16,0,37
question-answering,1,This is in contrast with other bag - of - words models like DEEPMATCH .,experiment,0,181,17,0,15
question-answering,1,related work,related work,0,182,1,0,2
question-answering,1,"Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .",related work,0,183,2,0,21
question-answering,1,"When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .",related work,0,184,3,0,29
question-answering,1,"Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .",related work,0,185,4,0,35
question-answering,1,Our models are related to the long thread of work on sentence representation .,related work,0,186,5,0,14
question-answering,1,"Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .",related work,0,187,6,0,39
question-answering,1,There is very little work on convolutional modeling of language .,related work,0,188,7,0,11
question-answering,1,"In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .",related work,0,189,8,0,19
question-answering,1,"This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .",related work,0,190,9,0,32
question-answering,1,conclusion,related work,0,191,10,0,1
question-answering,1,"We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .",related work,0,192,11,0,28
question-answering,1,Empirical study shows our models can outperform competitors on a variety of matching tasks .,related work,0,193,12,0,15
question-answering,5,Iterative Alternating Neural Attention for Machine Reading,title,1,2,1,0,7
question-answering,5,abstract,abstract,0,3,1,0,1
question-answering,5,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",abstract,1,4,2,0,26
question-answering,5,"Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document .",abstract,0,5,3,0,38
question-answering,5,Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .,abstract,0,6,4,0,32
question-answering,5,introduction,introduction,0,7,1,0,1
question-answering,5,"Recently , the idea of training machine comprehension models that can read , understand , and answer questions about a text has come closer to reality principally through two factors .",introduction,0,8,2,0,31
question-answering,5,"The first is the advent of deep learning techniques , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data .",introduction,0,9,3,0,30
question-answering,5,"The second factor is the formulation of standard machine comprehension benchmarks based on Cloze - style queries , which permit fast integration loops between model conception and experimental evaluation .",introduction,0,10,4,0,30
question-answering,5,Cloze - style queries are created by deleting a particular word in a natural - language statement .,introduction,0,11,5,0,18
question-answering,5,The task is to guess which word was deleted .,introduction,0,12,6,0,10
question-answering,5,"In a pragmatic approach , recent work formed such questions by extracting a sentence from a larger document .",introduction,0,13,7,0,19
question-answering,5,"In contrast to considering a stand - alone statement , the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word .",introduction,0,14,8,0,33
question-answering,5,Such contextual dependencies may also be injected by removing a word from a short human - crafted summary of a larger body of text .,introduction,0,15,9,0,25
question-answering,5,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,introduction,0,16,10,0,20
question-answering,5,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",introduction,0,17,11,0,24
question-answering,5,The missing word is assumed to appear in the document .,introduction,0,18,12,0,11
question-answering,5,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .",introduction,1,19,13,0,29
question-answering,5,The model first reads the document and the query using a recurrent neural network .,introduction,1,20,14,0,15
question-answering,5,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .",introduction,1,21,15,0,28
question-answering,5,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .",introduction,1,22,16,0,30
question-answering,5,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,introduction,1,23,17,0,21
question-answering,5,"This permits our model to reason about different parts of the query in a sequential way , based on the information that has been gathered previously from the document .",introduction,0,24,18,0,30
question-answering,5,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .",introduction,1,25,19,0,21
question-answering,5,This paper makes the following contributions .,introduction,0,26,20,0,7
question-answering,5,"We present a novel iterative , alternating attention mechanism that , unlike existing models , does not compress the query to a single representation , but instead alternates its attention between the query and the document to obtain a fine - grained query representation within a fixed computation time .",introduction,0,27,21,0,50
question-answering,5,Our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes .,introduction,0,28,22,0,15
question-answering,5,It obtains state - of - theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks .,introduction,0,29,23,0,28
question-answering,5,task description,introduction,0,30,24,0,2
question-answering,5,One of the advantages of using Cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention .,introduction,0,31,25,0,32
question-answering,5,The CBT and corpora are two such datasets .,introduction,0,32,26,0,9
question-answering,5,The CBT 1 corpus was generated from well - known children 's books available through Project Gutenberg .,introduction,0,33,27,0,18
question-answering,5,Documents consist of 20 - sentence excerpts from these books .,introduction,0,34,28,0,11
question-answering,5,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,introduction,0,35,29,0,22
question-answering,5,The dataset is divided into four subsets depending on the type of the word replaced .,introduction,0,36,30,0,16
question-answering,5,"The subsets are named entity , common noun , verb , and preposition .",introduction,0,37,31,0,14
question-answering,5,"We will focus our evaluation solely on the first two subsets , i.e. CBT - NE ( named entity ) and CBT - CN ( common nouns ) , since the latter two are relatively simple as demonstrated by .",introduction,0,38,32,0,40
question-answering,5,The CNN 2 corpus was generated from news articles available through the CNN website .,introduction,0,39,33,0,15
question-answering,5,"The documents are given by the full articles themselves , which are accompanied by short , bullet - point summary statements .",introduction,0,40,34,0,22
question-answering,5,"Instead of extracting a query from the articles themselves , the authors replace a named entity within each article summary with an anonymous placeholder token .",introduction,0,41,35,0,26
question-answering,5,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ?",introduction,0,42,36,0,51
question-answering,5,a is,introduction,0,43,37,0,2
question-answering,5,alternating iterative attention,introduction,0,44,38,0,3
question-answering,5,Our model is represented in .,introduction,0,45,39,0,6
question-answering,5,It s workflow has three steps .,introduction,0,46,40,0,7
question-answering,5,"First is the encoding phase , in which we compute a set of vector representations , acting as a memory of the content of the input document and query .",introduction,0,47,41,0,30
question-answering,5,"Next , the inference phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful .",introduction,0,48,42,0,33
question-answering,5,"To accomplish this , we use an iterative process that , at each iteration , alternates attentive memory accesses to the query and the document .",introduction,0,49,43,0,26
question-answering,5,"Finally , the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer .",introduction,0,50,44,0,28
question-answering,5,We describe each of the phases in the following sections .,introduction,0,51,45,0,11
question-answering,5,bidirectional encoding,introduction,0,52,46,0,2
question-answering,5,"The input to the encoding phase is a sequence of words X = ( x 1 , . . . , x | X | ) , such as a document or a query , drawn from a vocabulary V .",introduction,0,53,47,0,41
question-answering,5,Each word is represented by a continuous word embedding x ?,introduction,0,54,48,0,11
question-answering,5,Rd stored in a word embedding matrix X ? R | V |d .,introduction,0,55,49,0,14
question-answering,5,The sequence X is processed using a recurrent neural network encoder with gated recurrent units ( GRU ) .,introduction,0,56,50,0,19
question-answering,5,"For each position i in the input sequence , the GRU takes as input the word embedding xi and updates a hidden Figure 1 : Our model first encodes the query and the document by means of bidirectional GRU networks .",introduction,0,57,51,0,41
question-answering,5,"Then , it deploys an iterative inference mechanism that alternates between attending query encodings ( 1 ) and document encodings ( 2 ) given the query attended state .",introduction,0,58,52,0,29
question-answering,5,The results of the alternating attention is gated and fed back into the inference GRU .,introduction,0,59,53,0,16
question-answering,5,"Even if the encodings are computed only once , the query representation is dynamic and changes throughout the inference process .",introduction,0,60,54,0,21
question-answering,5,"After a fixed number of steps T , the weights of the document attention are used to estimate the probability of the answer P ( a|Q , D ) .",introduction,0,61,55,0,30
question-answering,5,by :,introduction,0,62,56,0,2
question-answering,5,"where hi , r i and u i ?",introduction,0,63,57,0,9
question-answering,5,"Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ?",introduction,0,64,58,0,21
question-answering,5,"R hd , H {r , u , h} ?",introduction,0,65,59,0,10
question-answering,5,"R hh are the parameters of the GRU , ?",introduction,0,66,60,0,10
question-answering,5,is the sigmoid function and is the elementwise multiplication .,introduction,0,67,61,0,10
question-answering,5,The hidden state hi acts as a representation of the word x i in the context of the preceding sequence inputs x < i .,introduction,0,68,62,0,25
question-answering,5,"In order to incorporate information from the future tokens x > i , we choose to process the sequence in reverse with an additional GRU .",introduction,0,69,63,0,26
question-answering,5,"Therefore , the encoding phase maps each token xi to a contextual representation given by the concatenation of the forward and backward GRU hidden",introduction,0,70,64,0,24
question-answering,5,we denote byq i ?,introduction,0,71,65,0,5
question-answering,5,r 2h andd i ?,introduction,0,72,66,0,5
question-answering,5,R 2h the contextual encodings for word i in the query Q and the document D respectively .,introduction,0,73,67,0,18
question-answering,5,iterative alternating attention,introduction,0,74,68,0,3
question-answering,5,This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer .,introduction,0,75,69,0,27
question-answering,5,The inference is modelled by an additional recurrent GRU network .,introduction,0,76,70,0,11
question-answering,5,The recurrent network iteratively performs an alternating search step to gather information that maybe useful to predict the answer .,introduction,0,77,71,0,20
question-answering,5,"In particular , at each time step : ( 1 ) it performs an attentive read on the query encodings , resulting in a query glimpse , qt , and ( 2 ) given the current query glimpse , it extracts a conditional document glimpse , d t , representing the parts of the document thatare relevant to the current query glimpse .",introduction,0,78,72,0,63
question-answering,5,"In turn , both attentive reads are conditioned on the previous hidden state of the inference GRU s t ?1 , summarizing the information that has been gathered from the query and the document up to time t.",introduction,0,79,73,0,38
question-answering,5,The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process .,introduction,0,80,74,0,26
question-answering,5,query attentive read,introduction,0,81,75,0,3
question-answering,5,"Given the query encodings {q i } , we formulate a query glimpse qt at timestep t by :",introduction,0,82,76,0,19
question-answering,5,"where q i , tare the query attention weights and A q ?",introduction,0,83,77,0,13
question-answering,5,"R 2hs , where sis the dimensionality of the inference GRU state , and a q ?",introduction,0,84,78,0,17
question-answering,5,r 2 h .,introduction,0,85,79,0,4
question-answering,5,"The attention we use here is similar to the formulation used in , but with two differences .",introduction,0,86,80,0,18
question-answering,5,"First , we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step .",introduction,0,87,81,0,29
question-answering,5,This simple bilinear attention has been successfully used in .,introduction,0,88,82,0,10
question-answering,5,"Second , we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t?1 .",introduction,0,89,83,0,33
question-answering,5,This is similar to what is achieved by the original attention mechanism proposed in without the burden of the additional tanh layer .,introduction,0,90,84,0,23
question-answering,5,document attentive read,introduction,0,91,85,0,3
question-answering,5,The alternating attention continues by probing the document given the current query glimpse qt .,introduction,0,92,86,0,15
question-answering,5,"In particular , the document attention weights are computed based on both the previous search state and the currently selected query glimpse qt :",introduction,0,93,87,0,24
question-answering,5,"where d i , tare the attention weights for each word in the document and A d ?",introduction,0,94,88,0,18
question-answering,5,R 2 h ( s + 2h ) and ad ?,introduction,0,95,89,0,11
question-answering,5,r 2 h .,introduction,0,96,90,0,4
question-answering,5,Note that the document attention is also conditioned on s t?1 .,introduction,0,97,91,0,12
question-answering,5,"This allows the model to perform transitive reasoning on the document side , i.e. to use previously obtained document information to bias future attended locations , which is particularly important for natural language inference tasks .",introduction,0,98,92,0,36
question-answering,5,gating search results,result,0,99,1,0,3
question-answering,5,"In order to update its recurrent state , the inference GRU may evolve on the basis of the information gathered from the current inference step , i.e.",result,0,100,2,0,27
question-answering,5,"However , the current query glimpse maybe too general or the document may not contain the information specified in the query glimpse , i.e. the query or the document attention weights maybe nearly uniform .",result,0,101,3,0,35
question-answering,5,We include a gating mechanism that is designed to reset the current query and document glimpses in the case that the current search is not fruitful .,result,0,102,4,0,27
question-answering,5,"Formally , we implement a gating mech -",result,0,103,5,0,8
question-answering,5,", where is the element - wise multiplication and g :",result,0,104,6,0,11
question-answering,5,r s+6h ?,result,0,105,7,0,3
question-answering,5,r 2 h .,result,0,106,8,0,4
question-answering,5,The gate g takes the form of a 2 - layer feed - forward network with sigmoid output unit activation .,result,0,107,9,0,21
question-answering,5,"The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses , making it easier to determine the degree of matching between them .",result,0,108,10,0,29
question-answering,5,"Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. ,",result,0,109,11,0,42
question-answering,5,.,result,0,110,12,0,1
question-answering,5,"Intuitively , the model reviews the query glimpse with respect to the contents of the document glimpse and vice versa .",result,0,111,13,0,21
question-answering,5,answer prediction,result,0,112,14,0,2
question-answering,5,"After a fixed number of time - steps T , the document attention weights obtained in the last search step d i , T are used to predict the probability of the answer given the document and the query P ( a|Q , D ) .",result,0,113,15,0,46
question-answering,5,"Formally , we follow and apply the "" pointer - sum "" loss :",result,0,114,16,0,14
question-answering,5,"where I ( a , D ) is a set of positions where a occurs in the document .",result,0,115,17,0,19
question-answering,5,"The model is trained to maximize log P ( a|Q , D ) over the training corpus .",result,0,116,18,0,18
question-answering,5,training details,result,0,117,19,0,2
question-answering,5,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",result,1,118,20,1,30
question-answering,5,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",result,1,119,21,0,46
question-answering,5,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .",result,1,120,22,0,20
question-answering,5,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",result,1,121,23,0,18
question-answering,5,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",result,1,122,24,0,26
question-answering,5,the inputs to both the query and the document attention mechanisms .,result,0,123,25,0,12
question-answering,5,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .",result,0,124,26,0,30
question-answering,5,"Our model is implemented in Theano , using the Keras library .",result,1,125,27,0,12
question-answering,5,"Computational Complexity Similar to previous state - of - the - art models which use a bidirectional encoder , the major bottleneck of our method is computing the document and query encodings .",result,0,126,28,0,33
question-answering,5,"The alternating attention mechanism runs only for a fixed number of steps ( T = 8 in our tests ) , which is orders of magnitude smaller than a typical document or query in our datasets ( see ) .",result,0,127,29,0,40
question-answering,5,The repeated attentions each require a softmax over ? 1000 locations which is typically fast on recent GPU architectures .,result,0,128,30,0,20
question-answering,5,"Thus , our computation cost is comparable to , but we outperform the latter models on the datasets tested .",result,0,129,31,0,20
question-answering,5,results,result,0,130,1,0,1
question-answering,5,"We report the results of our model on the CBT - CN , CBT - NE and CNN datasets , previously described in Section 2 . reports our results on the CBT - CN and CBT - NE dataset .",result,0,131,2,0,40
question-answering,5,"The Humans , LSTMs and Memory Networks ( Mem NNs ) results are taken from and the Attention - Sum Reader ( AS Reader ) is a state - of - the - art result recently obtained by .",result,0,132,3,0,39
question-answering,5,cbt,result,1,133,4,0,1
question-answering,5,main result,result,0,134,1,0,2
question-answering,5,Our model ( line 7 ) sets a new stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,result,1,135,2,0,40
question-answering,5,This performance gap is only partially reflected on the CBT - NE dataset .,result,0,136,3,0,14
question-answering,5,"We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set , which sits on par with the best baseline .",result,0,137,4,0,30
question-answering,5,"In CBT - NE , the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun .",result,0,138,5,0,28
question-answering,5,We found that approximatively 27.5 % of validation examples and 29.6 % of test examples contain an answer that has never been predicted in the training set .,result,0,139,6,0,28
question-answering,5,"These numbers are considerably lower for the CBT - CN , for which only 2.5 % and 4.6 % of validation and test examples respectively contain an answer that has not been previously seen .",result,0,140,7,0,35
question-answering,5,Ensembles Fusing multiple models generally achieves better generalization .,result,0,141,8,0,9
question-answering,5,"In order to investigate whether this could help achieving better held - out performance on CBT - NE , we adopt a simple strategy and average the predictions of 5 models trained with different random seeds ( line 9 , 3 from and 4 from .",result,0,142,9,0,46
question-answering,5,improvements over the single model and sits at 74.1 on validation and 71.0 on test .,result,0,143,10,0,16
question-answering,5,fixed query attention,result,0,144,11,0,3
question-answering,5,"In order to measure the impact of the query attention step in our model , we constrain the query attention weights q i , t to be uniform , i.e. q i , t = 1 / | Q | , for all t = 1 , . . . , T ( line 6 ) .",result,0,145,12,0,57
question-answering,5,This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work .,result,0,146,13,0,25
question-answering,5,"By comparing line 6 and line 7 , we see that the query attention mechanism allows improvements up to 2.3 points in validation and 4.9 points in test with respect to fixing the query representation throughout the search process .",result,0,147,14,0,40
question-answering,5,A similar scenario was observed on the CNN dataset ..,result,0,148,15,0,10
question-answering,5,cnn,result,1,149,16,0,1
question-answering,5,main result,result,0,150,1,0,2
question-answering,5,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,result,1,151,2,0,46
question-answering,5,We also report the very recent results of the Stanford AR system that came to our attention during the writeup of this article ) ( line 9 ) .,result,0,152,3,0,29
question-answering,5,Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test .,result,0,153,4,0,19
question-answering,5,We note that the latter comparison maybe influenced by different training and initialization strategies .,result,0,154,5,0,15
question-answering,5,"First , Stanford AS uses Glo Ve embeddings , pre-trained from a large external corpus .",result,0,155,6,0,16
question-answering,5,"Second , the system normalizes the output probabilities only over the candidate answers in the document .",result,0,156,7,0,17
question-answering,5,ensembles,result,0,157,8,0,1
question-answering,5,We also report the results using ensembled models .,result,0,158,9,0,9
question-answering,5,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",result,1,159,10,0,34
question-answering,5,Category analysis classified a sample of 100 CNN stories based on the type of inference required to guess the answer .,result,0,160,11,0,21
question-answering,5,"Categories that only require local context matching around the placeholder and the answer in the text are Exact Match , Paraphrasing , and Partial Clue , while those which require higher reasoning skills are Multiple Sentences and Ambiguous .",result,0,161,12,0,39
question-answering,5,"For example , in Exact Match examples , the question placeholder and the answer in the document share several neighboring exact words .",result,0,162,13,0,23
question-answering,5,Category - specific results are reported in : Per-category performance of the Stanford AR and our system .,result,0,163,14,0,18
question-answering,5,"The first three categories require local context matching , the next two global context matching and coreference errors are unanswerable questions .",result,0,164,15,0,22
question-answering,5,"tackled by the neural models , which perform similarly .",result,0,165,16,0,10
question-answering,5,It seems that the iterative alternating attention inference is better able to solve more difficult examples such as Ambiguous / Hard .,result,0,166,17,0,22
question-answering,5,"One hypothesis is that , in contrast to Stanford AR , which uses only one fixedquery attention step , our iterative attention may better explore the documents and queries .",result,0,167,18,0,30
question-answering,5,"Finally , Coreference Errors ( ? 25 % of the corpus ) includes examples with critical coreference resolution errors which may make the questions "" unanswerable "" .",result,0,168,19,0,28
question-answering,5,This is a barrier to achieving accuracies considerably above 75 % .,result,0,169,20,0,12
question-answering,5,"If this estimate is accurate , our ensemble model ( 76.1 % ) maybe approaching near-optimal performance on this dataset .",result,0,170,21,0,21
question-answering,5,discussion,result,0,171,22,0,1
question-answering,5,We inspect the query and document attention weights for an example article from the CNN dataset .,result,0,172,23,0,17
question-answering,5,"The title of the article is "" Dante turns in his grave as Italian language declines "" , and it discusses the decline of Italian language in schools .",result,0,173,24,0,29
question-answering,5,"The plot is shown in . 2 , where locations attended to in the query and document are in the left and right column respectively .",result,0,174,25,0,26
question-answering,5,Each row corresponds to an inference timestep 1 ? t ?,result,0,175,26,0,11
question-answering,5,"8 . At the first step , the query attention focuses on the placeholder token , as its local context is generally important to discriminate the answer .",result,0,176,27,0,28
question-answering,5,"The model first focuses on @entity148 , which corresponds to "" Greek "" in this The approach to teaching @entity6 in @placeholder schools needs a makeover , she says : Visualization of the alternated attention mechanism for an article in CNN , treating about the decline of the Italian language in schools .",result,0,177,28,0,53
question-answering,5,The title of the plot is the query .,result,0,178,29,0,9
question-answering,5,Each row correspond to a timestep .,result,0,179,30,0,7
question-answering,5,"The target is @entity3 which corresponds to the word "" Italian "" .",result,0,180,31,0,13
question-answering,5,across document locations ) .,result,0,181,32,0,5
question-answering,5,"At t = 2 , the query attention moves towards "" schools "" and the model hesitates between "" Italian "" and "" European Union "" ( @entity28 , see step 3 ) , both of which may satisfy the query .",result,0,182,33,0,42
question-answering,5,"At step 3 , the most likely candidates are "" European Union "" and "" Rome "" ( @entity159 ) .",result,0,183,34,0,21
question-answering,5,"As the timesteps unfold , the model learns that "" needs "" maybe important to infer the correct entity , i.e. "" Italian "" .",result,0,184,35,0,25
question-answering,5,"The query sits on the same attended location , while the document attention evolves to become more confident about the answer .",result,0,185,36,0,22
question-answering,5,"We find that , across CBT and CNN examples , the query attention wanders near or focuses on the placeholder location , attempting to discriminate its identity using only local context .",result,0,186,37,0,32
question-answering,5,"For these particular datasets , the majority of questions can be answered after attending only to the words directly neighbouring the placeholder .",result,0,187,38,0,23
question-answering,5,"This aligns with the findings of concerning CNN , which state that the required reasoning and inference levels for this dataset are quite simple .",result,0,188,39,0,25
question-answering,5,"It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words , and thereby necessitates deeper query exploration .",result,0,189,40,0,29
question-answering,5,"Finally , across this work we fixed the number of inference steps T .",result,0,190,41,0,14
question-answering,5,We found that using 8 timesteps works well consistently across the tested datasets .,result,0,191,42,0,14
question-answering,5,"However , we hypothesize that more ( fewer ) timesteps would benefit harder ( easier ) examples .",result,0,192,43,0,18
question-answering,5,A straight - forward extension of the model would be to dynamically select the number of inference steps conditioned on each example .,result,0,193,44,0,23
question-answering,5,related works,related work,0,194,1,0,2
question-answering,5,Neural attention models have been applied recently to a smrgsbord of machine learning and natural language processing problems .,related work,0,195,2,0,19
question-answering,5,"These include , but are not limited to , handwriting recognition , digit classification , machine translation , question answering and caption generation .",related work,0,196,3,0,24
question-answering,5,"In general , attention models keep a memory of states that can be accessed at will by learned attention policies .",related work,0,197,4,0,21
question-answering,5,"In our case , the memory is represented by the set of document and query contextual encodings .",related work,0,198,5,0,18
question-answering,5,"Our model is closely related to , which were also applied to question answering .",related work,0,199,6,0,15
question-answering,5,"The pointer - style attention mechanism that we use to perform the final answer prediction has been proposed by , which in turn was based on the earlier Pointer Networks of .",related work,0,200,7,0,32
question-answering,5,"However , differently from our work , perform only one attention step and embed the query into a single vector representation , corresponding to the concatenation of the last state of the forward and backward GRU networks .",related work,0,201,8,0,38
question-answering,5,"To our knowledge , embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models .",related work,0,202,9,0,25
question-answering,5,"In our model , the repeated , tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer , and then to focus on the parts of the document thatare most salient to the currently - attended query components .",related work,0,203,10,0,55
question-answering,5,A similar attempt in attending different components of the query maybe found in .,related work,0,204,11,0,14
question-answering,5,"In that model , the document is processed once for each query word .",related work,0,205,12,0,14
question-answering,5,"This can be computationally intractable for large documents , since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times .",related work,0,206,13,0,25
question-answering,5,"In contrast , our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps .",related work,0,207,14,0,30
question-answering,5,The inference network is responsible for making sense of the current attention step with respect to what has been gathered before .,related work,0,208,15,0,22
question-answering,5,"In addition to achieving state - of the - art performance , this technique may also prove to be more scalable than alternative query attention models .",related work,0,209,16,0,27
question-answering,5,"Finally , our iterative inference process shares similarities to the iterative hops in Memory Networks .",related work,0,210,17,0,16
question-answering,5,"In that model , the query representation is updated iteratively from hop to hop , although its different components are not attended to separately .",related work,0,211,18,0,25
question-answering,5,"Moreover , we substitute the simple linear update with a GRU network .",related work,0,212,19,0,13
question-answering,5,The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep .,related work,0,213,20,0,30
question-answering,5,conclusion,related work,0,214,21,0,1
question-answering,5,We presented an iterative neural attention model and applied it to machine comprehension tasks .,related work,0,215,22,0,15
question-answering,5,"Our architecture deploys a novel alternating attention mechanism , and tightly integrates successful ideas from past works in machine reading comprehension to obtain state - of - the - art results on three datasets .",related work,0,216,23,0,35
question-answering,5,The iterative alternating attention mechanism continually refines its view of the query and document while aggregating the information required to answer a query .,related work,0,217,24,0,24
question-answering,5,Multiple future research directions maybe envisioned .,related work,0,218,25,0,7
question-answering,5,We plan to dynamically select the optimal number of inference steps required for each example .,related work,0,219,26,0,16
question-answering,5,"Moreover , we suspect that shifting towards stochastic attention should permit us to learn more interesting search policies .",related work,0,220,27,0,19
question-answering,5,"Finally , we believe that our model is fully general and maybe applied in a straightforward way to other tasks such as information retrieval .",related work,0,221,28,0,25
question-answering,7,Neural Semantic Encoders,title,0,2,1,0,3
question-answering,7,abstract,abstract,0,3,1,0,1
question-answering,7,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,abstract,1,4,2,0,16
question-answering,7,"NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations .",abstract,0,5,3,0,34
question-answering,7,NSE can also access 1 multiple and shared memories .,abstract,0,6,4,0,10
question-answering,7,"In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks .",abstract,0,7,5,0,54
question-answering,7,"For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",abstract,0,8,6,0,28
question-answering,7,Recurrent neural networks ( RNNs ) have been successful for modeling sequences,abstract,0,9,7,0,12
question-answering,7,[ 1 ] .,abstract,0,10,8,0,4
question-answering,7,"Particularly , RNNs equipped with internal short memories , such as long short - term memories ( LSTM )",abstract,0,11,9,0,19
question-answering,7,"[ 2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .",abstract,0,12,10,0,17
question-answering,7,LSTM is powerful because it learns to control it s short term memories .,abstract,0,13,11,0,14
question-answering,7,"However , the short term memories in LSTM are a part of the training parameters .",abstract,0,14,12,0,16
question-answering,7,This imposes some practical difficulties in training and modeling long sequences with LSTM .,abstract,0,15,13,0,14
question-answering,7,Recently several studies have explored ways of extending the neural networks with an external memory [ 5 ] [ 6 ] [ 7 ] .,abstract,0,16,14,0,25
question-answering,7,"Unlike LSTM , the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted .",abstract,0,17,15,0,25
question-answering,7,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,abstract,1,18,16,0,25
question-answering,7,NSE offers several desirable properties .,abstract,0,19,17,0,6
question-answering,7,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,abstract,1,20,18,0,30
question-answering,7,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",abstract,1,21,19,0,21
question-answering,7,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,abstract,1,22,20,0,19
question-answering,7,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,abstract,1,23,21,0,30
question-answering,7,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",abstract,1,24,22,0,22
question-answering,7,We evaluate NSE on five different real tasks .,abstract,0,25,23,0,9
question-answering,7,"For four of them , our models set new state - of - theart results .",abstract,0,26,24,0,16
question-answering,7,Our results suggest that a NN model with the shared memory between encoder and decoder is a promising approach for sequence transduction problems such as machine translation and abstractive summarization .,abstract,0,27,25,0,31
question-answering,7,"In particular , we observe that the attention - based neural machine translation can be further improved by shared - memory models .",abstract,0,28,26,0,23
question-answering,7,We also analyze memory access pattern and compositionality in NSE and show that our model captures semantic and syntactic structures of input sentence .,abstract,0,29,27,0,24
question-answering,7,1,abstract,0,30,28,0,1
question-answering,7,"By access we mean changing the memory states by the read , compose and write operations .",abstract,0,31,29,0,17
question-answering,7,related work,related work,0,32,1,0,2
question-answering,7,One of the pioneering work that attempts to extend deep neural networks with an external memory is Neural Turing Machines ( NTM ) .,related work,0,33,2,0,24
question-answering,7,NTM implements a centralized controller and a fixed - sized random access memory .,related work,0,34,3,0,14
question-answering,7,The NTM memory is addressable by both content ( i.e. soft attention ) and location based access mechanisms .,related work,0,35,4,0,19
question-answering,7,The authors evaluated NTM on algorithmic tasks such as copying and sorting sequences .,related work,0,36,5,0,14
question-answering,7,Comparison with Neural Turing Machines : NSE addresses certain drawbacks of NTM .,related work,0,37,6,0,13
question-answering,7,"NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach .",related work,0,38,7,0,21
question-answering,7,"The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation .",related work,0,39,8,0,24
question-answering,7,"In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e. read - write ) in order to process the memory entries and input information .",related work,0,40,9,0,33
question-answering,7,The main advantage of NSE over NTM is in its memory update .,related work,0,41,10,0,13
question-answering,7,"Despite its sophisticated addressing mechanism , the NTM controller does not have mechanism to avoid information collision in the memory .",related work,0,42,11,0,21
question-answering,7,Particularly the NTM controller emits two separate set of access weights ( i.e. read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to .,related work,0,43,12,0,38
question-answering,7,Moreover the fixed - size memory in NTM has no memory allocation or de-allocation protocol .,related work,0,44,13,0,16
question-answering,7,"Therefore unless the controller is intelligent enough to track the previous read / write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales .",related work,0,45,14,0,41
question-answering,7,We think that this is a potential reason that makes NTM hard to train and makes the training not stable .,related work,0,46,15,0,21
question-answering,7,We also note that the effectiveness of the location based addressing introduced in NTM is unclear .,related work,0,47,16,0,17
question-answering,7,"In NSE , we introduce a novel and systematic memory update approach based on the soft attention mechanism .",related work,0,48,17,0,19
question-answering,7,NSE writes new information to the most recently read memory locations .,related work,0,49,18,0,12
question-answering,7,This is accomplished by sharing the same memory key vector between the read and write modules .,related work,0,50,19,0,17
question-answering,7,The NSE memory update is scalable and potentially more robust to train .,related work,0,51,20,0,13
question-answering,7,"NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed .",related work,0,52,21,0,23
question-answering,7,The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation or de-allocation protocols .,related work,0,53,22,0,30
question-answering,7,Each memory location of the NSE memory stores a token representation in input sequence during encoding .,related work,0,54,23,0,17
question-answering,7,"This provides NSE with an anytime - access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention - based encoders .",related work,0,55,24,0,36
question-answering,7,"Lastly , NTM addresses small algorithmic problems while NSE focuses on a set of large - scale language understanding tasks .",related work,0,56,25,0,21
question-answering,7,The RNNSearch model proposed in can be seen as a variation of memory augmented networks due to its ability to read the historic output states of RNNs with soft attention .,related work,0,57,26,0,31
question-answering,7,The work of combines the soft attention with Memory Networks ( Mem NNs ) .,related work,0,58,27,0,15
question-answering,7,"Similar to RNNSearch , MemNNs are designed with non-writable memories .",related work,0,59,28,0,11
question-answering,7,It constructs layered memory representations and showed promising results on both artificial and real question answering tasks .,related work,0,60,29,0,18
question-answering,7,We note that RNNSearch and MemNNs avoid the memory update and management overhead by simply using a non-writable memory storage .,related work,0,61,30,0,21
question-answering,7,Another variation of MemNNs is Dynamic Memory Network that is equipped with an episodic memory and seems to be flexible in different settings .,related work,0,62,31,0,24
question-answering,7,"Although NSE differs from other memory - augumented NN models in many aspects , they all use soft attention mechanism with a type of similarity measures to retrieve relevant information from the external memory .",related work,0,63,32,0,35
question-answering,7,"For example , NTM implements cosine similarity and MemNNs use vector dot product .",related work,0,64,33,0,14
question-answering,7,NSE uses the vector dot product for the similarity measure in NSE because it is faster to compute .,related work,0,65,34,0,19
question-answering,7,"Other related work includes Neural Program - Interpreters , which learns to run sub-programs and to compose them for high - level programs .",related work,0,66,35,0,24
question-answering,7,It uses execution traces to provide the full supervision .,related work,0,67,36,0,10
question-answering,7,Researchers have also explored ways to add unbounded memory to LSTM using a particular data structure .,related work,0,68,37,0,17
question-answering,7,"Although this type of architecture provides a flexible capacity to store information , the memory access is constrained by the data structure used for the memory bank , such as stack and queue .",related work,0,69,38,0,34
question-answering,7,Overall it is expensive to train and to scale the previously proposed memory - based models .,related work,0,70,39,0,17
question-answering,7,Most models required a set of clever engineering tricks to work successfully .,related work,0,71,40,0,13
question-answering,7,Most of the aforementioned memory augmented neural networks have been tested on synthetic tasks whereas in this paper we evaluated NSE on a wide range of real and large - scale natural language applications .,related work,0,72,41,0,35
question-answering,7,proposed approach,related work,0,73,42,0,2
question-answering,7,our training set consists,related work,0,74,43,0,4
question-answering,7,". . , w i Ti of tokens while the output Y i can be either a single target or a sequence .",related work,0,75,44,0,23
question-answering,7,We transform each input token wt to its word embedding x t .,related work,0,76,45,0,13
question-answering,7,"Our Neural Semantic Encoders ( NSE ) model has four main components : read , compose and write modules and an encoding memory M ?",related work,0,77,46,0,25
question-answering,7,"R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",related work,0,78,47,0,25
question-answering,7,Each memory slot vector mt ?,related work,0,79,48,0,6
question-answering,7,R k corresponds to the vector representation of information about word wt in memory .,related work,0,80,49,0,15
question-answering,7,"In particular , the memory is initialized by the embedding vectors {x t } l t=1 and is evolved overtime , through read , compose and write operations .",related work,0,81,50,0,29
question-answering,7,"read , compose and write",related work,0,82,51,0,5
question-answering,7,NSE performs three main operations in every time step .,related work,0,83,52,0,10
question-answering,7,"After initializing the memory slots with the corresponding input representations , NSE processes an embedding vector x t and retrieves a memory slot m r,t that is expected to be associatively coherent ( i.e. semantically associated ) with the current input word wt .",related work,0,84,53,0,44
question-answering,7,The slot location r ( ranging from 1 to l ) is defined by a key vector z t which the read module emits by attending over the memory slots .,related work,0,85,54,0,31
question-answering,7,The compose module implements a composition operation that combines the memory slot with the current input .,related work,0,86,55,0,17
question-answering,7,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,related work,0,87,56,0,27
question-answering,7,"Instead of composing the raw embedding vector x t , we use the hidden state o t produced by the read module at time t",related work,0,88,57,0,25
question-answering,7,"where 1 is a matrix of ones , ?",related work,0,89,58,0,9
question-answering,7,denotes the outer product which duplicates it s left vector l or k times to form a matrix .,related work,0,90,59,0,19
question-answering,7,The read function f LST,related work,0,91,60,0,5
question-answering,7,Mr sequentially maps the word embeddings to the internal space of the memory M t?1 .,related work,0,92,61,0,16
question-answering,7,Then Equation 2 looks for the slots related to the input by computing association degree between each memory slot and the hidden state o t .,related work,0,93,62,0,26
question-answering,7,We calculate the association degree by the dot product and transform this scores to the fuzzy key vector z t by normalizing with sof tmax function .,related work,0,94,63,0,27
question-answering,7,"Since our key vector is fuzzy , the slot to be composed is retrieved by taking weighted sum of the all slots as in Equation .",related work,0,95,64,0,26
question-answering,7,This process can also be seen as the soft attention mechanism .,related work,0,96,65,0,12
question-answering,7,"In Equation 4 and 5 , we compose and process the retrieved slot with the current hidden state and map the resulting vector to the encoder output space .",related work,0,97,66,0,29
question-answering,7,"Finally , we write the new representation to the memory location pointed by the key vector in where the key vector z t emitted by the read module is reused to inform the write module of the most recently read slots .",related work,0,98,67,0,42
question-answering,7,First the slot information that was retrieved is erased and then the new representation is located .,related work,0,99,68,0,17
question-answering,7,NSE performs this iterative process until all words in the input sequence are read .,related work,0,100,69,0,15
question-answering,7,The encoding memories { M } T t=1 and output states {h} T t=1 are further used for the tasks .,related work,0,101,70,0,21
question-answering,7,"Although NSE reads a single word at a time , it has an anytime - access to the entire sequence stored in the encoding memory .",related work,0,102,71,0,26
question-answering,7,"With the encoding memory , NSE maintains a mental image of the input sequence .",related work,0,103,72,0,15
question-answering,7,The memory is initialized with the raw embedding vector at time t = 0 .,related work,0,104,73,0,15
question-answering,7,We term such a freshly initialized memory a baby memory .,related work,0,105,74,0,11
question-answering,7,"As NSE reads more input content in time , the baby memory evolves and refines the encoded mental image .",related work,0,106,75,0,20
question-answering,7,functions are neural networks and are the training parameters in our NSE .,related work,0,107,76,0,13
question-answering,7,"As the name suggests , we use LSTM and multi -layer perceptron ( MLP ) in this paper .",related work,0,108,77,0,19
question-answering,7,"Since NSE is fully differentiable , it can be trained with any gradient descent optimizer .",related work,0,109,78,0,16
question-answering,7,Shared and Multiple Memory Accesses,related work,0,110,79,0,5
question-answering,7,"For sequence to sequence transduction tasks like question answering , natural language inference and machine translation , it is beneficial to access other relevant memories in addition to its own one .",related work,0,111,80,0,32
question-answering,7,The shared or the multiple memory access allows a set of NSEs to exchange knowledge representations and to communicate with each other to accomplish a particular task throughout the encoding memory .,related work,0,112,81,0,32
question-answering,7,"NSE can be extended easily , so that it is able to read from and write to multiple memories simultaneously or multiple NSEs are able to access a shared memory .",related work,0,113,82,0,31
question-answering,7,( b ) depicts a high - level architectural diagram of a multiple memory access - NSE ( MMA - NSE ) .,related work,0,114,83,0,23
question-answering,7,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,related work,0,115,84,0,18
question-answering,7,Given a shared memory Mn ?,related work,0,116,85,0,6
question-answering,7,"R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",related work,0,117,86,0,28
question-answering,7,and this is almost the same as standard NSE .,related work,0,118,87,0,10
question-answering,7,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,related work,0,119,88,0,30
question-answering,7,"In MMA - NSE , the different memory slots are retrieved from the shared memories depending on their encoded semantic representations .",related work,0,120,89,0,22
question-answering,7,They are then composed together with the current input and written back to their corresponding slots .,related work,0,121,90,0,17
question-answering,7,Note that MMA - NSE is capable of accessing a variable number of relevant shared memories once a composition function that takes in dynamic inputs is chosen .,related work,0,122,91,0,28
question-answering,7,experiments,experiment,0,123,1,0,1
question-answering,7,"We describe in this section experiments on five different tasks , in order to show that NSE can be effective and flexible in different settings .",experiment,0,124,2,0,26
question-answering,7,"We report results on natural language inference , question answering ( QA ) , sentence classification , document sentiment analysis and machine translation .",experiment,0,125,3,0,24
question-answering,7,All five tasks challenge a model in terms of language understanding and semantic reasoning .,experiment,0,126,4,0,15
question-answering,7,The models are trained using Adam with hyperparameters selected on development set .,experiment,1,127,5,0,13
question-answering,7,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,experiment,1,128,6,0,27
question-answering,7,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,experiment,1,129,7,0,22
question-answering,7,The word embeddings are fixed during training .,experiment,0,130,8,0,8
question-answering,7,The embeddings for out - of - vocabulary words were set to zero vector .,experiment,0,131,9,0,15
question-answering,7,We crop or pad the input sequence to a fixed length .,experiment,1,132,10,0,12
question-answering,7,A padding vector was inserted when padding .,experiment,0,133,11,0,8
question-answering,7,The models were regularized by using dropouts and an l 2 weight decay .,experiment,1,134,12,0,14
question-answering,7,natural language inference,experiment,1,135,13,0,3
question-answering,7,The natural language inference is one of the main tasks in language understanding .,experiment,0,136,14,0,14
question-answering,7,This task tests the ability of a model to reason about the semantic relationship between two sentences .,experiment,0,137,15,0,18
question-answering,7,"In order to perform well on the task , NSE should be able to capture sentence semantics and be able to reason the relation between a sentence pair , i.e. , whether a premise - hypothesis pair is entailing , contradictory or neutral .",experiment,0,138,16,0,44
question-answering,7,"We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train / dev / test sets and target label indicating their relation .",experiment,0,139,17,0,34
question-answering,7,"Following the setting in the NSE output for each sentence was the input to a MLP , where the input layer computes the concatenation [ h pl ; h h l ] , absolute difference hp l ?",experiment,0,140,18,0,38
question-answering,7,h h land elementwise product hp l h h l of the two sentence representations .,experiment,0,141,19,0,16
question-answering,7,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .",experiment,1,142,20,0,21
question-answering,7,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",experiment,1,143,21,0,35
question-answering,7,The write / read neural nets and the last linear layer were regularized by using 30 % dropouts .,experiment,0,144,22,0,19
question-answering,7,We evaluated three different variations of NSE show in .,experiment,0,145,23,0,10
question-answering,7,The NSE model encodes each sentence simultaneously by using a separate memory for each sentence .,experiment,0,146,24,0,16
question-answering,7,The second model - MMA - NSE first encodes the premise and then the hypothesis sentence by sharing the premise encoded memory in addition to the hypothesis memory .,experiment,0,147,25,0,29
question-answering,7,"For the third model , we use inter-sentence attention which selectively reconstructs the premise representation .",experiment,0,148,26,0,16
question-answering,7,shows the results of our models along with the results of published methods for the task .,experiment,0,149,27,0,17
question-answering,7,The classifier with handcrafted features extracts a set of lexical features .,experiment,0,150,28,0,12
question-answering,7,The next group of models are based on sentence encoding .,experiment,0,151,29,0,11
question-answering,7,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output .",experiment,0,152,30,0,30
question-answering,7,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,experiment,0,153,31,0,19
question-answering,7,"However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",experiment,0,154,32,0,18
question-answering,7,NSE outperformed the previous sentence encoders on this task .,experiment,0,155,33,0,10
question-answering,7,"The MMA - SNE further slightly improved the result , indicating that reading the premise memory is helpful while encoding the hypothesis .",experiment,0,156,34,0,23
question-answering,7,The last set of methods designs inter-sentence relation with parameterized soft attention .,experiment,0,157,35,0,13
question-answering,7,Our MMA - NSE attention model is similar to the LSTM attention model .,experiment,1,158,36,0,14
question-answering,7,"Particularly , it attends over the premise encoder outputs {h p } T t= 1 in respect to the final hypothesis representation h h land constructs an attentively blended vector of the premise .",experiment,0,159,37,0,34
question-answering,7,This model obtained 85.4 % accuracy score .,experiment,1,160,38,0,8
question-answering,7,The best performing model for this task performs tree matching with attention mechanism and LSTM .,experiment,0,161,39,0,16
question-answering,7,answer sentence selection,experiment,1,162,40,0,3
question-answering,7,Answer sentence selection is an integral part of the open - domain question answering .,experiment,0,163,41,0,15
question-answering,7,"For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences .",experiment,0,164,42,0,26
question-answering,7,We experiment on WikiQA dataset constructed from Wikipedia .,experiment,0,165,43,0,9
question-answering,7,"The dataset contains 20,360/2,733/6,165 QA pairs for train / dev / test sets .",experiment,0,166,44,0,14
question-answering,7,"The MLP setup used in the language inference task is kept same , except that we now replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .",experiment,0,167,45,0,34
question-answering,7,where h q land ha l are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,experiment,0,168,46,0,28
question-answering,7,We trained the MMA - NSE attention model to minimize the sigmoid cross entropy loss .,experiment,0,169,47,0,16
question-answering,7,MMA - NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories .,experiment,0,170,48,0,21
question-answering,7,"In our preliminary experiment , we found that the multiple memory access and the attention over answer encoder outputs {h a } T t= 1 are crucial to this problem .",experiment,0,171,49,0,31
question-answering,7,"Following previous work , we adopt MAP and MRR as the evaluation metrics for this task .",experiment,0,172,50,0,17
question-answering,7,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .",experiment,1,173,51,0,26
question-answering,7,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,experiment,1,174,52,0,14
question-answering,7,The word embeddings are pre-trained 300 - D Glove 840B vectors .,experiment,1,175,53,0,12
question-answering,7,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",experiment,1,176,54,0,22
question-answering,7,presents the results of our model and the previous models for the task .,experiment,0,177,55,0,14
question-answering,7,The classifier with handcrafted features is a SVM model trained with a set of features .,experiment,0,178,56,0,16
question-answering,7,The Bigram - CNN model is a simple convolutional neural net .,experiment,0,179,57,0,12
question-answering,7,"While the LSTM and LSTM attention models outperform the previous best result by nearly 5 - 6 % by implementing deep LSTM with three hidden layers , NASM improves it further and sets a strong baseline by combining variational auto - encoder with the soft attention .",experiment,0,180,58,0,47
question-answering,7,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,experiment,1,181,59,0,24
question-answering,7,We used trec_eval script to calculate the evaluation metrics 7 Inclusion of simple word count feature improves the performance by around 0.15 - 0.3 across the board Model MAP MRR Classifier with features 0.5993 0.6068 Paragraph Vector 0.5110 0.5160,experiment,0,182,60,0,39
question-answering,7,Bigram- CNN 0.6190 0.6281 3 - layer LSTM 0.6552 0.6747 3 - layer LSTM attention 0.6639 0.6828 NASM 0.6705 0.6914 MMA - NSE attention 0.6811 0.6993 88.1 47.4 DRNN 86.6 49.8 2 - layer LSTM 86.3 46.0 Bi-LSTM 87.5 49.1 CT- LSTM 88.0 51.0 DMN 88.6 52.1 NSE 89.7 52.8 : Test accuracy for sentence classification .,experiment,0,183,61,0,57
question-answering,7,bin :,experiment,0,184,62,0,2
question-answering,7,"Binary , FG : fine - grained 5 classes .",experiment,0,185,63,0,10
question-answering,7,sentence classification,experiment,1,186,64,0,2
question-answering,7,We evaluated NSE on the Stanford Sentiment Treebank ( SST ) .,experiment,0,187,65,0,12
question-answering,7,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,experiment,0,188,66,0,27
question-answering,7,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,experiment,0,189,67,0,25
question-answering,7,The sentence representations were passed to a two - layer MLP for classification .,experiment,0,190,68,0,14
question-answering,7,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,experiment,1,191,69,0,22
question-answering,7,The second layer is a sof tmax layer .,experiment,1,192,70,0,9
question-answering,7,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,experiment,1,193,71,0,29
question-answering,7,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",experiment,1,194,72,0,35
question-answering,7,The write / read neural nets and the last linear layer were regularized by 50 % dropouts .,experiment,0,195,73,0,18
question-answering,7,compares the result of our model with the state - of - the - art methods on the two subtasks .,experiment,0,196,74,0,21
question-answering,7,Most best performing methods exploited the parse tree provided in the treebank on this task with the exception of the DMN .,experiment,0,197,75,0,22
question-answering,7,The Dynamic Memory Network ( DMN ) model is a memory - augmented network .,experiment,0,198,76,0,15
question-answering,7,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,experiment,1,199,77,0,20
question-answering,7,document sentiment analysis,experiment,1,200,78,0,3
question-answering,7,"We evaluated our models for document - level sentiment analysis on two publically available largescale datasets : the IMDB consisting of 335,018 movie reviews and 10 different classes and Yelp 13 consisting of 348,415 restaurant reviews and 5 different classes .",experiment,0,201,79,0,41
question-answering,7,Each document in the datasets is associated with human ratings and we used these ratings as gold labels for sentiment classification .,experiment,0,202,80,0,22
question-answering,7,"Particularly , we used the pre-split datasets of .",experiment,0,203,81,0,9
question-answering,7,We stack a NSE or LSTM on the top of another NSE for document modeling .,experiment,1,204,82,0,16
question-answering,7,The first NSE encodes the sentences and the second NSE or LSTM takes sentence encoded outputs and constructs document representations .,experiment,0,205,83,0,21
question-answering,7,The document representation is given to a output sof tmax layer .,experiment,0,206,84,0,12
question-answering,7,The whole network is trained jointly by backpropagating the cross entropy loss .,experiment,1,207,85,0,13
question-answering,7,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,experiment,1,208,86,0,29
question-answering,7,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",experiment,1,209,87,0,35
question-answering,7,The write / read neural nets and the document - level NSE / LSTM were regularized by 15 % dropouts and the softmax layer by 20 % dropouts .,experiment,0,210,88,0,29
question-answering,7,"In order to speedup the training , we created document buckets by considering the number of sentences per document , i.e. , documents with the same number of sentences were put together in the same bucket .",experiment,0,211,89,0,37
question-answering,7,The buckets were shuffled and updated per epoch .,experiment,0,212,90,0,9
question-answering,7,"We did not use curriculum scheduling , although it is observed to help sequence training .",experiment,0,213,91,0,16
question-answering,7,shows our results .,result,0,214,1,0,4
question-answering,7,We report two performance metrics : accuracy and MSE .,result,0,215,2,0,10
question-answering,7,"The best results on the task were previously obtained by Conv - GRNN and LSTM - GRNN , which are also stacked models .",result,0,216,3,0,24
question-answering,7,These models first learn the sentence representations with a CNN or LSTM and then combine them for document representation using a gated recurrent neural network ( GRNN : BLEU scores for English - German translation task .,result,0,217,4,0,37
question-answering,7,Yelp 13 dataset has five classes to distinguish .,result,0,218,5,0,9
question-answering,7,The stacked NSEs ( NSE - NSE ) performed slightly better than the NSE - LSTM on the IMDB dataset .,result,0,219,6,0,21
question-answering,7,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,result,0,220,7,0,27
question-answering,7,machine translation,result,1,221,8,0,2
question-answering,7,"Lastly , we conducted an experiment on neural machine translation ( NMT ) .",result,0,222,9,0,14
question-answering,7,The NMT problem is mostly defined within the encoder - decoder framework .,result,0,223,10,0,13
question-answering,7,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation .,result,0,224,11,0,33
question-answering,7,"For an efficient encoding , the attention - based NTM was introduced .",result,0,225,12,0,13
question-answering,7,"For NTM , we implemented three different models .",result,0,226,13,0,9
question-answering,7,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,result,0,227,14,0,19
question-answering,7,"This model ( LSTM - LSTM ) has two LSTM for the encoder / decoder and has the soft attention neural net , which attends over the source sentence and constructs a focused encoding vector for each target word .",result,0,228,15,0,40
question-answering,7,The second model is an NSE - LSTM encoder - decoder which encodes the source sentence with NSE and generates the targets with the LSTM network by using the NSE output states and the attention network .,result,0,229,16,0,37
question-answering,7,"The last model is an NSE - NSE setup , where the encoding part is the same as the NSE - LSTM while the decoder NSE now uses the output state and has an access to the encoder memory , i.e. , the encoder and the decoder NSEs access a shared memory .",result,0,230,17,0,53
question-answering,7,The memory is encoded by the first NSEs and then read / written by the decoder NSEs .,result,0,231,18,0,18
question-answering,7,We used the English - German translation corpus from the IWSLT 2014 evaluation campaign .,result,0,232,19,0,15
question-answering,7,The corpus consists of sentence - aligned translation of TED talks .,result,0,233,20,0,12
question-answering,7,The data was pre-processed and lowercased with the Moses toolkit .,result,0,234,21,0,11
question-answering,7,"We merged the dev2010 and dev2012 sets for development and the tst2010 , tst2011 and tst 2012 sets for test data :",result,0,235,22,0,22
question-answering,7,Word association or composition graphs produced by NSE memory access .,result,0,236,23,0,11
question-answering,7,The directed arcs connect the words thatare composed via compose module .,result,0,237,24,0,12
question-answering,7,The source nodes are input words and the destination nodes ( pointed by the arrows ) correspond to the accessed memory slots .,result,0,238,25,0,23
question-answering,7,< S > denotes the beginning of sequence .,result,0,239,26,0,9
question-answering,7,the number of parameters of the models is roughly the equal .,result,0,240,27,0,12
question-answering,7,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,result,1,241,28,0,27
question-answering,7,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",result,1,242,29,0,47
question-answering,7,We report BLEU score for each models .,result,0,243,30,0,8
question-answering,7,11 5 qualitative analysis,result,0,244,31,0,4
question-answering,7,memory access and compositionality,result,0,245,32,0,4
question-answering,7,NSE is capabable of performing multiscale composition by retrieving associative slots for a particular input at a time step .,result,0,246,33,0,20
question-answering,7,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,result,0,247,34,0,26
question-answering,7,shows the word association graphs for the two sentence picked from SNLI test set .,result,0,248,35,0,15
question-answering,7,The association graph was constructed by inspecting the key vector z .,result,0,249,36,0,12
question-answering,7,"For an input word , we connect it to the most active slot pointed by z 12 .",result,0,250,37,0,18
question-answering,7,"Note the graph components clustered around the semantically rich words : "" sits "" , "" wall "" and "" autumn "" ( a ) and "" Three "" , "" puppies "" , "" tub "" and "" vet "" ( b ) .",result,0,251,38,0,45
question-answering,7,The memory slots corresponding to words thatare semantically rich in the current context are the most frequently accessed .,result,0,252,39,0,19
question-answering,7,"The graph is able to capture certain syntactic structures including phrases ( e.g. , "" hand built rock wall "" ) and modifier relations ( between "" sits "" and "" quietly "" and between "" tub "" and "" sprayed with water "" ) .",result,0,253,40,0,46
question-answering,7,Another interesting property is that the model tends to perform sensible compositions while processing the input sentence .,result,0,254,41,0,18
question-answering,7,"For example , NSE retrieved the memory slot corresponding to "" wall "" or "" Three "" when reading the input "" rock "" or "" are "" .",result,0,255,42,0,29
question-answering,7,in appendix,result,0,256,43,0,2
question-answering,7,"A , we show a step - by - step visualization of NSE memory states for the first sentence .",result,0,257,44,0,20
question-answering,7,Note how the encoding memory is evolved overtime .,result,0,258,45,0,9
question-answering,7,"In time step four ( t = 4 ) , the memory slot for "" quietly "" encodes information about "" quiet ( ly ) little child "" .",result,0,259,46,0,29
question-answering,7,"When t = 6 , the model forms another composition involving "" quietly "" , "" quietly sits "" .",result,0,260,47,0,20
question-answering,7,"In the last time step , we are able to find the most or the least frequently accessed slots in the memory .",result,0,261,48,0,23
question-answering,7,The least accessed slots correspond to function words while the frequently accessed slots are content words and tend to carry out rich semantics and intrinsic compositions found in the input sentence .,result,0,262,49,0,32
question-answering,7,Overall the model is less constrained and is able to compose multiword expressions .,result,0,263,50,0,14
question-answering,7,conclusion,result,0,264,51,0,1
question-answering,7,Our proposed memory augmented neural networks have achieved the state - of - the - art results when evaluated on five representative NLP tasks .,result,0,265,52,0,25
question-answering,7,"NSE is capable of building an efficient architecture of the single , shared and multiple memory accesses for a specific NLP task .",result,0,266,53,0,23
question-answering,7,"For example , for the NLI task NSE accesses premise encoded memory when processing hypothesis .",result,0,267,54,0,16
question-answering,7,"For the QA task , NSE accesses answer encoded memory when reading question for QA .",result,0,268,55,0,16
question-answering,7,"In machine translation , NSE shares a single encoded memory between encoder and decoder .",result,0,269,56,0,15
question-answering,7,Such flexibility in the architectural choice of the NSE memory access allows for the robust models for a better performance .,result,0,270,57,0,21
question-answering,7,The initial state of the NSE memory stores information about each word in the input sequence .,result,0,271,58,0,17
question-answering,7,We in this paper used word embeddings to represent the words in the memory .,result,0,272,59,0,15
question-answering,7,Different variations of word representations such as character - based models are left to be evaluated for memory initialization in the future .,result,0,273,60,0,23
question-answering,7,We plan to extend NSE so that it learns to select and access a relevant subset from a memory set .,result,0,274,61,0,21
question-answering,7,"One could also explore unsupervised variations of NSE , for example , to train them to produce encoding memory and representation vector of entire sentences or documents using either new or existing models such as the skip - gram model .",result,0,275,62,0,41
question-answering,4,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,title,1,2,1,0,11
question-answering,4,abstract,abstract,0,3,1,0,1
question-answering,4,Understanding unstructured text is a major goal within natural language processing .,abstract,1,4,2,0,12
question-answering,4,Comprehension tests pose questions based on short text passages to evaluate such understanding .,abstract,0,5,3,0,14
question-answering,4,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",abstract,1,6,4,0,14
question-answering,4,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",abstract,0,7,5,0,19
question-answering,4,"We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .",abstract,0,8,6,0,19
question-answering,4,"The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .",abstract,0,9,7,0,34
question-answering,4,Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text .,abstract,0,10,8,0,26
question-answering,4,"When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets a new state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",abstract,0,11,9,0,51
question-answering,4,introduction,introduction,0,12,1,0,1
question-answering,4,"Humans learn in a variety of ways - by communication with each other , and by study , the reading of text .",introduction,0,13,2,0,23
question-answering,4,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .",introduction,1,14,3,0,22
question-answering,4,It has garnered significant attention from the machine learning research community in recent years .,introduction,0,15,4,0,15
question-answering,4,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,introduction,1,16,5,0,31
question-answering,4,"Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .",introduction,0,17,6,0,25
question-answering,4,"Given a text passage and a question about its content , a system is tested on its ability to determine the correct answer .",introduction,0,18,7,0,24
question-answering,4,"In this work , we focus on MCTest , a complex but data - limited comprehension benchmark , whose multiple - choice questions require not only extraction but also inference and limited reasoning .",introduction,0,19,8,0,34
question-answering,4,"Inference and reasoning are important human skills that apply broadly , beyond language .",introduction,0,20,9,0,14
question-answering,4,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,introduction,0,21,10,0,21
question-answering,4,"There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",introduction,0,22,11,0,29
question-answering,4,"Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .",introduction,0,23,12,0,30
question-answering,4,"Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .",introduction,0,24,13,0,28
question-answering,4,"Likewise , deep models learn to extract their own features , but this is a data - intensive process .",introduction,0,25,14,0,20
question-answering,4,Our model learns to comprehend at a high level even when data is sparse .,introduction,0,26,15,0,15
question-answering,4,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,introduction,1,27,16,0,22
question-answering,4,We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .,introduction,0,28,17,0,22
question-answering,4,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",introduction,1,29,18,0,37
question-answering,4,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",introduction,1,30,19,0,23
question-answering,4,"As in the semantic perspective , we consider matches over complete sentences .",introduction,0,31,20,0,13
question-answering,4,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",introduction,1,32,21,0,29
question-answering,4,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",introduction,1,33,22,0,49
question-answering,4,Words are represented throughout by embedding vectors .,introduction,0,34,23,0,8
question-answering,4,These distinct perspectives naturally form a hierarchy that we depict in .,introduction,0,35,24,0,12
question-answering,4,"Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .",introduction,0,36,25,0,17
question-answering,4,The perspectives of our model can be considered a type of feature .,introduction,0,37,26,0,13
question-answering,4,"However , they are implemented by parametric differentiable functions .",introduction,0,38,27,0,10
question-answering,4,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .",introduction,0,39,28,0,22
question-answering,4,"Our model , significantly , can be trained end - to - end with backpropagation .",introduction,0,40,29,0,16
question-answering,4,"To facilitate learning with limited data , we also develop a unique training scheme .",introduction,0,41,30,0,15
question-answering,4,We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .,introduction,0,42,31,0,25
question-answering,4,"Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .",introduction,0,43,32,0,19
question-answering,4,We call this technique training wheels .,introduction,0,44,33,0,7
question-answering,4,Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .,introduction,0,45,34,0,22
question-answering,4,"Models designed specifically for MCTest include those of , and more recently , , and .",introduction,0,46,35,0,16
question-answering,4,"In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .",introduction,0,47,36,0,25
question-answering,4,"Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .",introduction,0,48,37,0,22
question-answering,4,the problem,introduction,0,49,38,0,2
question-answering,4,"In this section we borrow from , who laid out the MC problem nicely .",introduction,0,50,39,0,15
question-answering,4,Machine comprehension requires machines to answer questions based on unstructured text .,introduction,0,51,40,0,12
question-answering,4,This can be viewed as selecting the best answer from a set of candidates .,introduction,0,52,41,0,15
question-answering,4,"In the multiple - choice case , candidate answers are predefined , but candidate answers may also be undefined yet restricted ( e.g. , to yes , no , or any noun phrase in the text ) .",introduction,0,53,42,0,38
question-answering,4,"For each question q , let T be the unstructured text and A = {a i } the set of candidate answers to q .",introduction,0,54,43,0,25
question-answering,4,The machine comprehension task reduces to selecting the answer that has the highest evidence given T .,introduction,0,55,44,0,17
question-answering,4,"As in , we combine an answer and a question into a hypothesis , hi = f ( q , a i ) .",introduction,0,56,45,0,24
question-answering,4,"To facilitate comparisons of the text with the hypotheses , we also breakdown the passage into sentences t j , T = {t j }.",introduction,0,57,46,0,25
question-answering,4,"In our setting , q , a i , and t j each represent a sequence of embedding vectors , one for each word and punctuation mark in the respective item .",introduction,0,58,47,0,32
question-answering,4,related work,related work,0,59,1,0,2
question-answering,4,Machine comprehension is currently a hot topic within the machine learning community .,related work,0,60,2,0,13
question-answering,4,"In this section we will focus on the best - performing models applied specifically to MCTest , since it is somewhat unique among MC datasets ( see Section 5 ) .",related work,0,61,3,0,31
question-answering,4,"Generally , models can be divided into two categories : those that use fixed , engineered features , and neural models .",related work,0,62,4,0,22
question-answering,4,The bulk of the work on MCTest falls into the former category .,related work,0,63,5,0,13
question-answering,4,"Manually engineered features often require significant effort on the part of a designer , and / or various auxiliary tools to extract them , and they can not be modified by training .",related work,0,64,6,0,33
question-answering,4,"On the other hand , neural models can be trained end - to - end and typically harness only a single feature : vectorrepresentations of words .",related work,0,65,7,0,27
question-answering,4,Word embeddings are fed into a complex and possibly deep neural network which processes and compares text to question and answer .,related work,0,66,8,0,22
question-answering,4,"Among deep models , mechanisms of attention and working memory are common , as in and .",related work,0,67,9,0,17
question-answering,4,"3.1 Feature - engineering models treated MCTest as a structured prediction problem , searching for a latent answerentailing structure connecting question , answer , and text .",related work,0,68,10,0,27
question-answering,4,This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text .,related work,0,69,11,0,18
question-answering,4,The process of ( latently ) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation .,related work,0,70,12,0,27
question-answering,4,The model uses event and entity coreference links across sentences along with a host of other features .,related work,0,71,13,0,18
question-answering,4,These include specifically trained word vectors for synonymy ; antonymy and class - inclusion relations from external data base sources ; dependencies and semantic role labels .,related work,0,72,14,0,27
question-answering,4,"The model is trained using a latent structural SVM extended to a multitask setting , so that questions are first classified using a pretrained top - level classifier .",related work,0,73,15,0,29
question-answering,4,This enables the system to use different processing strategies for different question categories .,related work,0,74,16,0,14
question-answering,4,The model also combines question and answer into a well - formed statement using the rules of Cucerzan and Agichtein ( 2005 ) .,related work,0,75,17,1,24
question-answering,4,"Our model is simpler than that of in terms of the features it takes in , the training procedure ( stochastic gradient descent vs. alternating minimization ) , question classification ( we use none ) , and question - answer combination ( simple concatenation or mean vs. a set of rules ) .",related work,0,76,18,0,53
question-answering,4,"augmented the baseline feature set from with features for syntax , frame semantics , coreference chains , and word embeddings .",related work,0,77,19,0,21
question-answering,4,They combined features using a linear latent - variable classifier trained to minimize a max - margin loss function .,related work,0,78,20,0,20
question-answering,4,"As in , questions and answers are combined using a set of manually written rules .",related work,0,79,21,0,16
question-answering,4,"The method of achieved the previous state of the art , but has significant complexity in terms of the feature set .",related work,0,80,22,0,22
question-answering,4,"Space does not permit a full description of all models in this category , but see also and .",related work,0,81,23,0,19
question-answering,4,"Despite its relative lack of features , the Parallel - Hierarchical model improves upon the featureengineered state of the art for MCTest by a small amount ( about 1 % absolute ) as detailed in Section 5 .",related work,0,82,24,0,38
question-answering,4,neural models,related work,0,83,25,0,2
question-answering,4,"Neural models have , to date , performed relatively poorly on MCTest .",related work,0,84,26,0,13
question-answering,4,This is because the dataset is sparse and complex .,related work,0,85,27,0,10
question-answering,4,investigated deep - learning approaches concurrently with the present work .,related work,0,86,28,0,11
question-answering,4,"They measured the performance of the Attentive Reader and the Neural Reasoner , both deep , end - to - end recurrent models with attention mechanisms , and also developed an attention - based convolutional network , the HABCNN .",related work,0,87,29,0,40
question-answering,4,"Their network operates on a hierarchy similar to our own , providing further evidence of the promise of hierarchical perspectives .",related work,0,88,30,0,21
question-answering,4,"Specifically , the HABCNN processes text at the sentence level and the snippet level , where the latter combines adjacent sentences ( as we do through an n-gram input ) .",related work,0,89,31,0,31
question-answering,4,Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network .,related work,0,90,32,0,18
question-answering,4,"This encoding modulates attention over sentence and snippet encodings , followed by maxpooling to determine the best matches between question , answer , and text .",related work,0,91,33,0,26
question-answering,4,"As in the present work , matching scores are given by cosine similarity .",related work,0,92,34,0,14
question-answering,4,The HABCNN also makes use of a question classifier .,related work,0,93,35,0,10
question-answering,4,"Despite the shared concepts between the HABCNN and our approach , the Parallel - Hierarchical model performs significantly better on MCTest ( more than 15 % absolute ) as detailed in Section 5 .",related work,0,94,36,0,34
question-answering,4,Other neural models tested in fare even worse .,related work,0,95,37,0,9
question-answering,4,the parallel - hierarchical model,related work,0,96,38,0,5
question-answering,4,Let us now define our machine comprehension model in full .,related work,0,97,39,0,11
question-answering,4,"We first describe each of the perspectives separately , then describe how they are combined .",related work,0,98,40,0,16
question-answering,4,"Below , we use subscripts to index elements of sequences , like word vectors , and superscripts to indicate whether elements come from the text , question , or answer .",related work,0,99,41,0,31
question-answering,4,"In particular , we use the subscripts k , m , n , p to index sequences from the text , question , answer , and hypothesis , respectively , and superscripts t , q , a , h. We depict the model schematically in .",related work,0,100,42,0,46
question-answering,4,semantic perspective,related work,0,101,43,0,2
question-answering,4,The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space .,related work,0,102,44,0,17
question-answering,4,Each sen - tence of the text is a sequence of d-dimensional word vectors :,related work,0,103,45,0,15
question-answering,4,"The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two - layer network that implements weighted sum followed by an affine tranformation and a nonlinearity ; i.e. ,",related work,0,104,46,0,36
question-answering,4,"The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .",related work,0,105,47,0,26
question-answering,4,the scalar ?,related work,0,106,48,0,3
question-answering,4,k is a trainable weight associated to each word in the vocabulary .,related work,0,107,49,0,13
question-answering,4,These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus .,related work,0,108,50,0,19
question-answering,4,"They can , for example , learn to perform the function of stopword lists in a soft , trainable way , to nullify the contribution of unimportant filler words .",related work,0,109,51,0,30
question-answering,4,"The semantic representation of a hypothesis is formed analogously , except that we combine the question word vectors q m and answer word vectors an as a single sequence {h p } = {q m , an }.",related work,0,110,52,0,38
question-answering,4,"For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ?",related work,0,111,53,0,17
question-answering,4,R Dd and bias vector b h A ?,related work,0,112,54,0,9
question-answering,4,rd .,related work,0,113,55,0,2
question-answering,4,These transformations map a text sentence and a hypothesis into a common space where they can be compared .,related work,0,114,56,0,19
question-answering,4,"We compute the semantic match be-tween text sentence and hypothesis using the cosine similarity , M sem = cos ( s t , sh ) .",related work,0,115,57,0,26
question-answering,4,( 2 ),related work,0,116,58,0,3
question-answering,4,word - by - word perspective,related work,0,117,59,0,6
question-answering,4,"The first step in building the word - by - word perspective is to transform word vectors from a text sentence , question , and answer through respective neural functions .",related work,0,118,60,0,31
question-answering,4,"for the text ,",related work,0,119,61,0,4
question-answering,4,?,related work,0,120,62,0,1
question-answering,4,RD and f is again the leaky ReLU .,related work,0,121,63,0,9
question-answering,4,We transform the question and the answer toq m and n analogously using distinct matrices and bias vectors .,related work,0,122,64,0,19
question-answering,4,"In contrast with the semantic perspective , we keep the question and answer candidates separate in the wordby - word perspective .",related work,0,123,65,0,22
question-answering,4,"This is because matches to answer words are inherently more important than matches to question words , and we want our model to learn to use this property .",related work,0,124,66,0,29
question-answering,4,sentential,related work,0,125,67,0,1
question-answering,4,"Inspired by the work of in paraphrase detection , we compute matches between hypotheses and text sentences at the word level .",related work,0,126,68,0,22
question-answering,4,This computation uses the cosine similarity as before :,related work,0,127,69,0,9
question-answering,4,"ca kn = cos (t k , n ) .",related work,0,128,70,0,10
question-answering,4,The word - by - word match between a text sentence and question is determined by taking the maximum over k ( finding the text word that best matches each question word ) and then taking a weighted mean over m ( finding the average match over the full question ) :,related work,0,129,71,0,52
question-answering,4,"here , ?",related work,0,130,72,0,3
question-answering,4,m is the word weight for the question word and Z normalizes these weights to sum to one over the question .,related work,0,131,73,0,22
question-answering,4,"We define the match between a sentence and answer candidate , M a , analogously .",related work,0,132,74,0,16
question-answering,4,"Finally , we combine the matches to question and answer according to",related work,0,133,75,0,12
question-answering,4,here the ?,related work,0,134,76,0,3
question-answering,4,are trainable parameters that control the relative importance of the terms .,related work,0,135,77,0,12
question-answering,4,sequential sliding window,related work,0,136,78,0,3
question-answering,4,The sequential sliding window is related to the original MCTest baseline by .,related work,0,137,79,0,13
question-answering,4,"Our sliding window decays from its focus word according to a Gaussian distribution , which we extend by assigning a trainable weight to each location in the window .",related work,0,138,80,0,29
question-answering,4,This modification enables the window to use information about the distance between word matches ; the original baseline used distance information through a predefined function .,related work,0,139,81,0,26
question-answering,4,"The sliding window scans over the words of the text as one continuous sequence , without sentence breaks .",related work,0,140,82,0,19
question-answering,4,"Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .",related work,0,141,83,0,24
question-answering,4,"This weight is based on a word 's position in the window , which , given a window , depends on its global position k.",related work,0,142,84,0,25
question-answering,4,The cosine similarity is adapted as,related work,0,143,85,0,6
question-answering,4,for the question and analogously for the answer .,related work,0,144,86,0,9
question-answering,4,We initialize the location weights with a Gaussian and fine - tune them during training .,related work,0,145,87,0,16
question-answering,4,"The final matching score , denoted as M sws , is computed as in and with sq km replacing c q km .",related work,0,146,88,0,23
question-answering,4,dependency sliding window,related work,0,147,89,0,3
question-answering,4,"The dependency sliding window operates identically to the linear sliding window , but on a different view of the text passage .",related work,0,148,90,0,22
question-answering,4,The output of this component is M swd and is formed analogously to M sws .,related work,0,149,91,0,16
question-answering,4,The dependency perspective uses the Stanford Dependency Parser as an auxiliary tool .,related work,0,150,92,0,13
question-answering,4,"Thus , the dependency graph can be considered a fixed feature .",related work,0,151,93,0,12
question-answering,4,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .",related work,0,152,94,0,19
question-answering,4,"However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .",related work,0,153,95,0,21
question-answering,4,"Specifically , we run the Stanford Dependency Parser on each text sentence to build a dependency graph .",related work,0,154,96,0,18
question-answering,4,"This graph has n w vertices , one for each word in the sentence .",related work,0,155,97,0,15
question-answering,4,From the dependency graph we form the Laplacian matrix L ?,related work,0,156,98,0,11
question-answering,4,R nwnw and determine its eigenvectors .,related work,0,157,99,0,7
question-answering,4,The second eigenvector u 2 of the Laplacian is known as the Fiedler vector .,related work,0,158,100,0,15
question-answering,4,It is the solution to the minimization,related work,0,159,101,0,7
question-answering,4,"where vi are the vertices of the graph , and ?",related work,0,160,102,0,11
question-answering,4,ij is the weight of the edge from vertex i to vertex j.,related work,0,161,103,0,13
question-answering,4,"The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close , modulated by the connection weights .",related work,0,162,104,0,23
question-answering,4,1,related work,0,163,105,0,1
question-answering,4,This enables us to reorder the words of a sentence based on their proximity in the dependency graph .,related work,0,164,106,0,19
question-answering,4,The reordering of the words is given by the ordered index set I = arg sort ( u 2 ) .,related work,0,165,107,0,21
question-answering,4,"To give an example of how this works , consider the following sentence from MCTest and its dependency - based reordering :",related work,0,166,108,0,22
question-answering,4,"Jenny , Mrs. Mustard 's helper , called the police .",related work,0,167,109,0,11
question-answering,4,"the police , called Jenny helper , Mrs. 's Mustard .",related work,0,168,110,0,11
question-answering,4,Sliding - window - based matching on the original sentence will answer the question,related work,0,169,111,0,14
question-answering,4,Who called the police ? with Mrs. Mustard .,related work,0,170,112,0,9
question-answering,4,"The dependency reordering enables the window to determine the correct answer , Jenny .",related work,0,171,113,0,14
question-answering,4,combining distributed,related work,0,172,114,0,2
question-answering,4,evidence,related work,0,173,115,0,1
question-answering,4,It is important in comprehension to synthesize information found throughout a document .,related work,0,174,116,0,13
question-answering,4,"MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone , but would instead require some form of inference or limited reasoning .",related work,0,175,117,0,29
question-answering,4,It therefore includes questions where the evidence for an answer spans several sentences .,related work,0,176,118,0,14
question-answering,4,"To perform synthesis , our model also takes in ngrams of sentences , i.e. , sentence pairs and triples strung together .",related work,0,177,119,0,22
question-answering,4,"The model treats these exactly as it does single sentences , applying all functions detailed above .",related work,0,178,120,0,17
question-answering,4,A later pooling operation combines scores across all n-grams ( including the singlesentence input ) .,related work,0,179,121,0,16
question-answering,4,This is described in the next subsection .,related work,0,180,122,0,8
question-answering,4,"With n-grams , the model can combine information distributed across contiguous sentences .",related work,0,181,123,0,13
question-answering,4,"In some cases , however , the required evidence is spread across distant sentences .",related work,0,182,124,0,15
question-answering,4,"To give our model some capacity to deal with this scenario , we take the top N sentences as scored by all the preceding functions , and then repeat the scoring computations viewing these top N as a single sentence .",related work,0,183,125,0,41
question-answering,4,The reasoning behind these approaches can be explained well in a probabilistic setting .,related work,0,184,126,0,14
question-answering,4,"If we consider our similarity scores to model the likelihood of a text sentence given a hypothesis , p (t j |h i ) , then the n-gram and top N approaches model a joint probability p (t j 1 , t j 2 , . . . , t j k |h i ) .",related work,0,185,127,0,56
question-answering,4,We can not model the joint probability as a product of individual terms ( score values ) because distributed pieces of evidence are likely not independent .,related work,0,186,128,0,27
question-answering,4,combining perspectives,related work,0,187,129,0,2
question-answering,4,"We use a multilayer perceptron to combine M sem , M word , M swd , and M sws as a final matching score M i for each answer candidate .",related work,0,188,130,0,31
question-answering,4,"This network also pools and combines the separate n-gram scores , and uses a linear activation function .",related work,0,189,131,0,18
question-answering,4,Our over all training objective is to minimize the ranking loss,related work,0,190,132,0,11
question-answering,4,"where is a constant margin , i * indexes the correct answer , and we take the maximum over i so that we are ranking the correct answer over the best - ranked incorrect answer ( of which there are three ) .",related work,0,191,133,0,43
question-answering,4,This approach worked better than comparing the correct answer to the incorrect answers individually as in .,related work,0,192,134,0,17
question-answering,4,"Our implementation of the Parallel - Hierarchical model , using the Keras framework , is available on Github .",related work,0,193,135,0,19
question-answering,4,2,related work,0,194,136,0,1
question-answering,4,training wheels,related work,0,195,137,0,2
question-answering,4,"Before training , we initialized the neural - network components of our model to perform sensible heuristic functions .",related work,0,196,138,0,19
question-answering,4,Training did not converge on the small MCTest without this vital approach .,related work,0,197,139,0,13
question-answering,4,"Empirically , we found that we could achieve above 50 % accuracy on MCTest using a simple sum of word vectors followed by a dot product between the question sum and the hypothesis sum .",related work,0,198,140,0,35
question-answering,4,"Therefore , we initialized the network for the semantic perspective to perform this sum , by initializing A x as the identity matrix and bx A as the zero vector , x ? {t , h} .",related work,0,199,141,0,37
question-answering,4,Recall that the activation function is a ReLU so that positive outputs are unchanged .,related work,0,200,142,0,15
question-answering,4,"We also found basic word - matching scores to be helpful , so we initialized the word - by - word networks likewise .",related work,0,201,143,0,24
question-answering,4,"The network for perspectivecombination was initialized to perform a sum of individual scores , using a zero bias - vector and a weight matrix of ones , since we found that each perspective contributed positively to the over all result .",related work,0,202,144,0,41
question-answering,4,This training wheels approach is related to other techniques from the literature .,related work,0,203,145,0,13
question-answering,4,"For instance , proposed the identity - matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation .",related work,0,204,146,0,26
question-answering,4,"In residual networks , shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model .",related work,0,205,147,0,27
question-answering,4,experiments,experiment,0,206,1,0,1
question-answering,4,the dataset,experiment,0,207,2,0,2
question-answering,4,"MCTest is a collection of 660 elementary - level children 's stories and associated questions , written by human subjects .",experiment,0,208,3,0,21
question-answering,4,"The stories are fictional , ensuring that the answer must be found in the text itself , and carefully limited to what a young child can understand .",experiment,0,209,4,0,28
question-answering,4,The more challenging variant consists of 500 stories with four multiple - choice questions each .,experiment,0,210,5,0,16
question-answering,4,"Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .",experiment,0,211,6,0,27
question-answering,4,MCTest is challenging because it is both complicated and small .,experiment,0,212,7,0,11
question-answering,4,"As per , "" it is very difficult to train statistical models only on MCTest . """,experiment,0,213,8,0,17
question-answering,4,"It s size limits the number of parameters that can be trained , and prevents learning any complex language modeling simultaneously with the capacity to answer questions .",experiment,0,214,9,0,28
question-answering,4,training and model details,experiment,0,215,10,0,4
question-answering,4,In this section we describe important details of the training procedure and model setup .,experiment,0,216,11,0,15
question-answering,4,"For a complete list of hyperparameter settings , our stopword list , and other minutiae , we refer interested readers to our Github repository .",experiment,0,217,12,0,25
question-answering,4,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .",experiment,1,218,13,0,24
question-answering,4,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .",experiment,0,219,14,0,26
question-answering,4,The vectors are 300 - dimensional ( d = 300 ) .,experiment,0,220,15,0,12
question-answering,4,"We do not use a stopword list for the text passage , instead relying on the trainable word weights to ascribe global importance ratings to words .",experiment,0,221,16,0,27
question-answering,4,These weights are initialized with the inverse document frequency ( IDF ) statistic computed over the MCTest corpus .,experiment,0,222,17,0,19
question-answering,4,3,experiment,0,223,18,0,1
question-answering,4,"However , we douse a short stopword list for questions .",experiment,0,224,19,0,11
question-answering,4,"This list nullifies query words such as { Who , what , when , where , how} , along with conjugations of the verbs to do and to be .",experiment,0,225,20,0,30
question-answering,4,"Following earlier methods , we use a heuristic to improve performance on negation questions .",experiment,0,226,21,0,15
question-answering,4,"When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .",experiment,0,227,22,0,24
question-answering,4,The most important technique for training the model was the training wheels approach .,experiment,0,228,23,0,14
question-answering,4,"Without this , training was not effective at all .",experiment,0,229,24,0,10
question-answering,4,The identity initialization requires that the network weight matrices are square ( d = D ) .,experiment,0,230,25,0,17
question-answering,4,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",experiment,1,231,26,0,26
question-answering,4,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .",experiment,0,232,27,0,19
question-answering,4,Our best performing model held networks at the wordby - word level fixed .,experiment,0,233,28,0,14
question-answering,4,"For combining distributed evidence , we used up to trigrams over sentences and our bestperforming model reiterated over the top two sentences ( N = 2 ) .",experiment,0,234,29,0,28
question-answering,4,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .",experiment,1,235,30,1,23
question-answering,4,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,experiment,1,236,31,0,20
question-answering,4,"MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .",experiment,0,237,32,0,52
question-answering,4,presents the performance of featureengineered and neural methods on the MCTest test set .,experiment,0,238,33,0,14
question-answering,4,"Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .",experiment,0,239,34,0,30
question-answering,4,"Clearly , MCTest - 160 is easier .",experiment,0,240,35,0,8
question-answering,4,results,result,0,241,1,0,1
question-answering,4,The first three rows represent featureengineered methods .,result,0,242,2,0,8
question-answering,4,+,result,0,243,3,0,1
question-answering,4,RTE is the best - performing variant of the original baseline published along with MCTest .,result,0,244,4,0,16
question-answering,4,"It uses a lexical sliding window and distance - based measure , augmented with rules for recognizing textual entailment .",result,0,245,5,0,20
question-answering,4,We described the methods of and in Section 3 .,result,0,246,6,0,10
question-answering,4,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ? 1 % ) .",result,1,247,7,0,44
question-answering,4,The method of achieves the best over all result on MCTest - 160 .,result,0,248,8,0,14
question-answering,4,We suspect this is because our neural method suffered from the relative lack of training data .,result,0,249,9,0,17
question-answering,4,The last four rows in are neural methods that we discussed in Section 3 .,result,0,250,10,0,15
question-answering,4,Performance measures are taken from .,result,0,251,11,0,6
question-answering,4,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,result,0,252,12,0,21
question-answering,4,"The Neural Reasoner and the Attentive Reader are large , deep models with hundreds of thousands of parameters , so it is unsurprising that they performed poorly on MCTest .",result,0,253,13,0,30
question-answering,4,"The specificallydesigned HABCNN fared better , its convolutional architecture cutting down on the parameter count .",result,0,254,14,0,16
question-answering,4,"Because there are similarities between our model and the HABCNN , we hypothesize that much of the performance difference is attributable to our training wheels methodology .",result,0,255,15,0,27
question-answering,4,analysis and discussion,result,0,256,16,0,3
question-answering,4,We measure the contribution of each component of the model by ablating it .,result,0,257,17,0,14
question-answering,4,results are given in .,result,0,258,1,0,5
question-answering,4,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .",result,1,259,2,0,16
question-answering,4,"Without this , the model has almost no Method MCTest - 160 accuracy ( % )",result,0,260,3,0,16
question-answering,4,MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .,result,0,261,4,0,14
question-answering,4,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",result,1,262,5,0,26
question-answering,4,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",result,1,263,6,0,18
question-answering,4,Simple word - by - word matching is obviously useful on MCTest .,result,1,264,7,0,13
question-answering,4,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .",result,1,265,8,0,19
question-answering,4,"On the other hand , the dependency - based sliding window makes only a minor contribution .",result,1,266,9,0,17
question-answering,4,we found this surprising .,result,0,267,10,0,5
question-answering,4,It maybe that linearization of the dependency graph removes too much of its information .,result,0,268,11,0,15
question-answering,4,"Finally , the exogenous word weights make a significant contribution of almost 5 % .",result,1,269,12,0,15
question-answering,4,"Analysis reveals that most of our system 's test failures occur on questions about quantity ( e.g. , How many ...? ) and temporal order ( e.g. , Who was invited last ? ) .",result,0,270,13,0,35
question-answering,4,"Quantity questions makeup 9.5 % of our errors on the validation set , while order questions makeup 10.3 % .",result,0,271,14,0,20
question-answering,4,"This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .",result,0,272,15,0,19
question-answering,4,"Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .",result,0,273,16,0,29
question-answering,4,The Parallel - Hierarchical model is simple .,result,0,274,17,0,8
question-answering,4,It does no complex language or sequence modeling .,result,0,275,18,0,9
question-answering,4,It s simplicity is a response to the limited data of MCTest .,result,0,276,19,0,13
question-answering,4,"Nevertheless , the model achieves stateof - the - art results on the multi questions , which ( putatively ) require some limited reasoning .",result,0,277,20,0,25
question-answering,4,Our model is able to handle them reasonably well just by stringing important sentences together .,result,0,278,21,0,16
question-answering,4,"Thus , the model imitates reasoning with a heuristic .",result,0,279,22,0,10
question-answering,4,"This suggests that , to learn true reasoning abilities , MCTest is too simple a dataset - and it is almost certainly too small for this goal .",result,0,280,23,0,28
question-answering,4,"However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .",result,0,281,24,0,19
question-answering,4,"If so , the Parallel - Hierarchical model is a good start on the former .",result,0,282,25,0,16
question-answering,4,"Indeed , if we train the method exclusively on single questions then its results become even more impressive : we can achieve a test accuracy of 79.1 % on MCTest - 500 .",result,0,283,26,0,33
question-answering,4,conclusion,result,0,284,27,0,1
question-answering,4,"We have presented the novel Parallel - Hierarchical model for machine comprehension , and evaluated it on the small but complex MCTest .",result,0,285,28,0,23
question-answering,4,"Our model achieves state - of - the - art results , outperforming several feature - engineered and neural approaches .",result,0,286,29,0,21
question-answering,4,"Working with our model has emphasized to us the following ( not necessarily novel ) concepts , which we record hereto promote further empirical validation .",result,0,287,30,0,26
question-answering,4,Good comprehension of language is supported by hierarchical levels of understanding ( Cf. ) .,result,0,288,31,0,15
question-answering,4,Exogenous attention ( the trainable word weights ) maybe broadly helpful for NLP .,result,0,289,32,0,14
question-answering,4,"The training wheels approach , that is , initializing neural networks to perform sensible heuristics , appears helpful for small datasets .",result,0,290,33,0,22
question-answering,4,"Reasoning over language is challenging , but easily simulated in some cases .",result,0,291,34,0,13
question-answering,6,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,title,1,2,1,0,15
question-answering,6,abstract,abstract,0,3,1,0,1
question-answering,6,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",abstract,1,4,2,0,19
question-answering,6,"We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts .",abstract,0,5,3,0,44
question-answering,6,"QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .",abstract,0,6,4,0,36
question-answering,6,"Our experiments show that QRN produces the state - of - the - art results in bAbI QA and dialog tasks , and in are al goal - oriented dialog dataset .",abstract,0,7,5,0,32
question-answering,6,"In addition , QRN formulation allows parallelization on RNN 's time axis , saving an order of magnitude in time complexity for training and inference .",abstract,0,8,6,0,26
question-answering,6,introduction,introduction,0,9,1,0,1
question-answering,6,"In this paper , we address the problem of question answering ( QA ) when reasoning over multiple facts is required .",introduction,0,10,2,0,22
question-answering,6,"For example , consider we know that Frogs eat insects and Flies are insects .",introduction,0,11,3,0,15
question-answering,6,Then answering Do frogs eat flies ?,introduction,0,12,4,0,7
question-answering,6,requires reasoning over both of the above facts .,introduction,0,13,5,0,9
question-answering,6,"Question answering , more specifically context - based QA , has been extensively studied in machine comprehension tasks .",introduction,0,14,6,0,19
question-answering,6,"However , most of the datasets are primarily focused on lexical and syntactic understanding , and hardly concentrate on inference over multiple facts .",introduction,0,15,7,0,24
question-answering,6,"Recently , several datasets aimed for testing multi-hop reasoning have emerged ; among them are story - based QA and the dialog task .",introduction,0,16,8,0,24
question-answering,6,"Recurrent Neural Network ( RNN ) and its variants , such as Long Short - Term Memory ( LSTM ) and Gated Recurrent Unit ( GRU ) , are popular choices for modeling natural language .",introduction,0,17,9,0,36
question-answering,6,"However , when used for multi-hop reasoning in question answering , purely RNN - based models have shown to perform poorly .",introduction,0,18,10,0,22
question-answering,6,This is largely due to the fact that RNN 's internal memory is inherently unstable over along term .,introduction,0,19,11,0,19
question-answering,6,"For this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory .",introduction,0,20,12,0,22
question-answering,6,The attention mechanism allows these models to focus on a single sentence in each layer .,introduction,0,21,13,0,16
question-answering,6,They can sequentially read multiple relevant sentences from the memory with multiple layers to perform multi-hop reasoning .,introduction,0,22,14,0,18
question-answering,6,"However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them .",introduction,0,23,15,0,30
question-answering,6,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",introduction,1,24,16,0,52
question-answering,6,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",introduction,1,25,17,0,35
question-answering,6,"For instance in , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story .",introduction,0,26,18,0,27
question-answering,6,"After observing the first sentence , Sandra got the apple there , QRN transforms the original question to a reduced query",introduction,0,27,19,0,21
question-answering,6,"Where is Sandra ? , which is presumably . ? and ?",introduction,0,28,20,0,12
question-answering,6,"are update gate and reduce functions , respectively .?",introduction,0,29,21,0,9
question-answering,6,"is assigned to be h 2 5 , the local query at the last time step in the last layer .",introduction,0,30,22,0,21
question-answering,6,"Also , red-colored text is the inferred meanings of the vectors ( see. easier to answer than the original question given the context provided by the first sentence .",introduction,0,31,23,0,29
question-answering,6,"2 Unlike RNN - based models , QRN 's candidate state ( h t in ) does not depend on the previous hidden state ( h t?1 ) .",introduction,0,32,24,0,29
question-answering,6,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .",introduction,0,33,25,0,37
question-answering,6,"In short , the main contribution of QRN is threefold .",introduction,0,34,26,0,11
question-answering,6,"First , QRN is a simple variant of RNN that reduces the query given the context sentences in a differentiable manner .",introduction,0,35,27,0,22
question-answering,6,"Second , QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",introduction,0,36,28,0,28
question-answering,6,"Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) .",introduction,0,37,29,0,37
question-answering,6,"Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",introduction,0,38,30,0,41
question-answering,6,"In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency .",introduction,0,39,31,0,30
question-answering,6,We experimentally demonstrate these contributions by achieving the state - of - the - art results on story - based QA and interactive dialog datasets .,introduction,0,40,32,0,26
question-answering,6,model,introduction,0,41,33,0,1
question-answering,6,"In story - based QA ( or dialog dataset ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user 's last utterance in the dialog ) .",introduction,0,42,34,0,46
question-answering,6,The output is the predicted answer to the question in natural language ( the system 's next utterance in the dialog ) .,introduction,0,43,35,0,23
question-answering,6,The only supervision provided during training is the answer to the question .,introduction,0,44,36,0,13
question-answering,6,"In this paper we particularly focus on end - to - end solutions , i.e. , the only supervision comes from questions and answers , and we restrain from using manually defined rules or external language resources , such as lexicon or dependency parser .",introduction,0,45,37,0,45
question-answering,6,"let x 1 , . . . , x",introduction,0,46,38,0,9
question-answering,6,"T denote the sequence of sentences , where T is the number of sentences in the story , and let q denote the question .",introduction,0,47,39,0,25
question-answering,6,let ?,introduction,0,48,40,0,2
question-answering,6,"denote the predicted answer , and y denote the true answer .",introduction,0,49,41,0,12
question-answering,6,"Our proposed system for end - to - end QA task is divided into three modules ( ) : input module , QRN layers , and output module .",introduction,0,50,42,0,29
question-answering,6,input module .,introduction,0,51,43,0,3
question-answering,6,"Input module maps each sentence x t and the question q to d-dimensional vector space , x t ?",introduction,0,52,44,0,19
question-answering,6,rd and qt ?,introduction,0,53,45,0,4
question-answering,6,rd .,introduction,0,54,46,0,2
question-answering,6,We adopt a previous solution for the input module ( details in Section 5 ) .,introduction,0,55,47,0,16
question-answering,6,qrn layers .,introduction,0,56,48,0,3
question-answering,6,"QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , ? ?",introduction,0,57,49,0,25
question-answering,6,"Rd . A QRN layer refers to the recurrent application of a QRN unit , which can be considered as a variant of RNN with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space .",introduction,0,58,50,0,48
question-answering,6,"The details of the QRN module is explained throughout this section ( 2.1 , 2.2 ) .",introduction,0,59,51,0,17
question-answering,6,output module .,introduction,0,60,52,0,3
question-answering,6,output module maps ?,introduction,0,61,53,0,4
question-answering,6,obtained from QRN to a natural language answer ?.,introduction,0,62,54,0,9
question-answering,6,"Similar to the input module , we adopt a standard solution for the output module ( details in Section 5 ) .",introduction,0,63,55,0,22
question-answering,6,"We first formally define the base model of a QRN unit , and then we explain how we connect the input and output modules to it ( Section 2.1 ) .",introduction,0,64,56,0,31
question-answering,6,We also present a few extensions to the network that can improve QRN 's performance ( Section 2.2 ) .,introduction,0,65,57,0,20
question-answering,6,"Finally , we show that QRN can be parallelized overtime , giving computational advantage over most RNN - based models by one order of magnitude ( Section 3 ) .",introduction,0,66,58,0,30
question-answering,6,qrn unit,introduction,0,67,59,0,2
question-answering,6,"As an RNN - based model , QRN is a single recurrent unit that updates its hidden state ( reduced query ) through time and layers .",introduction,0,68,60,0,27
question-answering,6,"depicts the schematic structure of a QRN unit , and demonstrates how layers are stacked .",introduction,0,69,61,0,16
question-answering,6,A QRN unit accepts two inputs ( local query vector qt ?,introduction,0,70,62,0,12
question-answering,6,"Rd and sentence vector x t ? Rd ) , and two outputs ( reduced query vector ht ?",introduction,0,71,63,0,19
question-answering,6,"Rd , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) .",introduction,0,72,64,0,25
question-answering,6,The local query vector is not necessarily identical to the original query ( question ) vector q .,introduction,0,73,65,0,18
question-answering,6,"In order to compute the outputs , we use update gate function ? :",introduction,0,74,66,0,14
question-answering,6,"Intuitively , the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state .",introduction,0,75,67,0,25
question-answering,6,The reduce function transforms the local query input to a candidate state which is a new reduced ( easier ) query given the sentence .,introduction,0,76,68,0,25
question-answering,6,The outputs are calculated with the following equations :,introduction,0,77,69,0,9
question-answering,6,"where z t is the scalar update gate , h t is the candidate reduced query , and ht is the final reduced query at time step t , ? ( ) is sigmoid activation , tanh ( ) is hyperboolic tangent activation ( applied element - wise ) ,",introduction,0,78,70,0,50
question-answering,6,"is element - wise vector multiplication , and [ ; ] is vector concatenation along the row .",introduction,0,79,71,0,18
question-answering,6,"As a base case , h 0 = 0 .",introduction,0,80,72,0,10
question-answering,6,Here we have explicitly defined ?,introduction,0,81,73,0,6
question-answering,6,"and ? , but they can be any reasonable differentiable functions .",introduction,0,82,74,0,12
question-answering,6,The update gate is similar to the global attention mechanism in that it measures the similarity between the sentence ( a memory slot ) and the query .,introduction,0,83,75,0,28
question-answering,6,"However , a significant difference is that the update gate is computed using sigmoid ( ? ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using softmax function over the entire memory ( hence globally defined ) .",introduction,0,84,76,0,52
question-answering,6,The update gate can be rather considered as local sigmoid attention .,introduction,0,85,77,0,12
question-answering,6,stacking layers,introduction,0,86,78,0,2
question-answering,6,"We just showed the single - layer case of QRN , but QRN with multiple layers is able to perform reasoning over multiple facts more effectively , as shown in the example of .",introduction,0,87,79,0,34
question-answering,6,"In order to stack several layers of QRN , the outputs of the current layer are used as the inputs to the next layer .",introduction,0,88,80,0,25
question-answering,6,"That is , using superscript k to denote the current layer 's index ( assuming 1 - based indexing ) , we let q k + 1 t = h kt .",introduction,0,89,81,0,32
question-answering,6,"Note that x t is passed to the next layer without any modification , so we do not put a layer index on it .",introduction,0,90,82,0,25
question-answering,6,bi-direction .,introduction,0,91,83,0,2
question-answering,6,"So far we have assumed that QRN only needs to look at past sentences , whereas oftentimes , query answers can depend on future sentences .",introduction,0,92,84,0,26
question-answering,6,"For instance , consider a sentence "" John dropped the football . "" at time t.",introduction,0,93,85,0,16
question-answering,6,"Then , even if there is no mention about the "" football "" in the past ( at time i < t ) , it can be implied that "" John "" has the "" football "" at the current time t.",introduction,0,94,86,0,42
question-answering,6,"In order to incorporate the future dependency , we obtain ? ?",introduction,0,95,87,0,12
question-answering,6,ht and ? ?,introduction,0,96,88,0,4
question-answering,6,"ht in both forward and backward directions , respectively , using Equation",introduction,0,97,89,0,12
question-answering,6,3 .,introduction,0,98,90,0,2
question-answering,6,We then add them together to get qt for the next layer .,introduction,0,99,91,0,13
question-answering,6,"That is , are shared between the two directions .",introduction,0,100,92,0,10
question-answering,6,Connecting input and output modules .,introduction,0,101,93,0,6
question-answering,6,depicts how QRN is connected with the input and output modules .,introduction,0,102,94,0,12
question-answering,6,"In the first layer of QRN , q 1 t = q for all t , where q is obtained from the input module by processing the natural language question input q. x t is also obtained from x t by the same input module .",introduction,0,103,95,0,46
question-answering,6,The output at the last time step in the last layer is passed to the output module .,introduction,0,104,96,0,18
question-answering,6,"That is , y = h K t where K represent the number of layers in the network .",introduction,0,105,97,0,19
question-answering,6,Then the output module gives the predicted answer ?,introduction,0,106,98,0,9
question-answering,6,in natural language .,introduction,0,107,99,0,4
question-answering,6,extensions,introduction,0,108,100,0,1
question-answering,6,"Here we introduce a few extensions of QRN , and later in our experiments , we test QRN 's performance with and without each of these extensions .",introduction,0,109,101,0,28
question-answering,6,reset gate .,introduction,0,110,102,0,3
question-answering,6,"Inspired by GRU , we found that it is useful to allow the QRN unit to reset ( nullify ) the candidate reduced query ( i.e. , h t ) when necessary .",introduction,0,111,103,0,33
question-answering,6,For this we use a reset gate function ? :,introduction,0,112,104,0,10
question-answering,6,", which can be defined similarly to the update gate function :",introduction,0,113,105,0,12
question-answering,6,where w ( r ) ?,introduction,0,114,106,0,6
question-answering,6,"R 1d is a weight matrix , and b ( r ) ?",introduction,0,115,107,0,13
question-answering,6,R is a bias term .,introduction,0,116,108,0,6
question-answering,6,Equation 3 is rewritten as,introduction,0,117,109,0,5
question-answering,6,Note that we do not use the reset gate in the last layer .,introduction,0,118,110,0,14
question-answering,6,vector gates .,introduction,0,119,111,0,3
question-answering,6,"As in LSTM and GRU , update and reset gates can be vectors instead of scalar values for fine - controlled gating .",introduction,0,120,112,0,23
question-answering,6,"For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d .",introduction,0,121,113,0,23
question-answering,6,"Then we obtain z t , rt ?",introduction,0,122,114,0,8
question-answering,6,"Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",introduction,0,123,115,0,32
question-answering,6,parallelization,introduction,0,124,116,0,1
question-answering,6,An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time .,introduction,0,125,117,0,23
question-answering,6,"This is in contrast with most RNN - based models that can not be parallelized , where computing the candidate hidden state at time t explicitly requires the previous hidden state .",introduction,0,126,118,0,32
question-answering,6,"In QRN , the final reduced queries ( h t ) can be decomposed into computing over candidate reduced queries ( h t ) , without looking at the previous reduced query .",introduction,0,127,119,0,33
question-answering,6,Here we primarily show that the query update in Equation 3 can be parallelized by rewriting the equation with matrix operations .,introduction,0,128,120,0,22
question-answering,6,The extension to Equation 5 is straightforward .,introduction,0,129,121,0,8
question-answering,6,The proof for QRN with vector gates is shown in Appendix B .,introduction,0,130,122,0,13
question-answering,6,The recursive definition of Equation 3 can be explicitly written as,introduction,0,131,123,0,11
question-answering,6,Let bi = log ( 1 ? z i ) for brevity .,introduction,0,132,124,0,13
question-answering,6,Then we can rewrite Equation 7 as the following equation :,introduction,0,133,125,0,11
question-answering,6,figure,introduction,0,134,126,0,1
question-answering,6,"2 : The schematics of QRN and the two state - of - the - art models , End - to - End Memory Networks and Improved Dynamic Memory Networks ( DMN + ) , simplified to emphasize the differences among the models .",introduction,0,135,127,0,44
question-answering,6,"AGRU is a variant of GRU where the update gate is replaced with soft attention , proposed by .",introduction,0,136,128,0,19
question-answering,6,related work,related work,0,137,1,0,2
question-answering,6,"QRN is inspired by RNN - based models with gating mechanism , such as LSTM and GRU .",related work,0,138,2,0,18
question-answering,6,"While GRU and LSTM use the previous hidden state and the current input to obtain the candidate hidden state , QRN only uses the current two inputs to obtain the candidate reduced query ( equivalent to candidate hidden state ) .",related work,0,139,3,0,41
question-answering,6,"We conjecture that this not only gives computational advantage via parallelization , but also makes training easier , i.e. , avoiding vanishing gradient ( which is critical for long - term dependency ) , overfitting ( by simplifying the model ) , and converging to local minima .",related work,0,140,4,0,48
question-answering,6,"The idea of structurally simplifying ( constraining ) RNNs for learning longer - term patterns has been explored in recent previous work , such as Structurally Constrained Recurrent Network and Strongly - Typed Recurrent Neural Network ( STRNN ) .",related work,0,141,5,0,40
question-answering,6,"QRN is similar to STRNN in that both architectures use gating mechanism , and the gates and the candidate hidden states do not depend on the previous hidden states , which simplifies the recurrent relation .",related work,0,142,6,0,36
question-answering,6,"However , QRN can be distinguished from STRNN in three ways .",related work,0,143,7,0,12
question-answering,6,"First , QRN 's update gate simulates attention mechanism , measuring the relevance between the input sentence and query .",related work,0,144,8,0,20
question-answering,6,"On the other hand , the gates in STRNN can be considered as the simplification of LSTM / GRU by removing their dependency on previous hidden state .",related work,0,145,9,0,28
question-answering,6,"Second , QRN is an RNN that is natively compatible with context - based QA tasks , where the QRN unit accepts two inputs , i.e. each context sentence and query .",related work,0,146,10,0,32
question-answering,6,This is distinct from STRNN which has only one input .,related work,0,147,11,0,11
question-answering,6,"Third , we show that QRN is timewise - parallelizable on GPUs .",related work,0,148,12,0,13
question-answering,6,Our parallelization algorithm is also applicable to STRNN .,related work,0,149,13,0,9
question-answering,6,End - to - end Memory Network ( N2N ) uses external memory with multi - layer attention mechanism to focus on sentences thatare relevant to the question .,related work,0,150,14,0,29
question-answering,6,There are two key differences between N2N and our QRN .,related work,0,151,15,0,11
question-answering,6,"First , N2N summarizes the entire memory in each layer to control the attention in the next layer ( circle nodes in ) .",related work,0,152,16,0,24
question-answering,6,"Instead , QRN does not have any controller node ) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit .",related work,0,153,17,0,30
question-answering,6,"Second , N2N adds time - dependent trainable weights to the sentence representations to model the time dependency of the sentences ( as discussed in Section 1 ) .",related work,0,154,18,0,29
question-answering,6,QRN does not need such additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency .,related work,0,155,19,0,21
question-answering,6,Neural Reasoner and Gated End - toend Memory Network ) are variants of MemN2N that share its fundamental characteristics .,related work,0,156,20,0,20
question-answering,6,Improved Dynamic Memory Network ( DMN + ) uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences .,related work,0,157,21,0,26
question-answering,6,"It consists of two distinct GRUs , one for the time axis ( rectangle nodes in ) and one for the layer axis ( circle nodes in ) .",related work,0,158,22,0,29
question-answering,6,Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights .,related work,0,159,23,0,20
question-answering,6,"DMN + uses the time - axis GRU to summarizes the entire memory in each layer , and then the layer - axis GRU controls the attention weights in each layer .",related work,0,160,24,0,32
question-answering,6,"In contrast , QRN is simply a single recurrent unit without any controller node .",related work,0,161,25,0,15
question-answering,6,experiments,experiment,0,162,1,0,1
question-answering,6,5.1 data bab,experiment,0,163,2,0,3
question-answering,6,"I story - based QA dataset bAb I story - based QA dataset is composed of 20 different tasks ( Appendix A ) , each of which has 1,000 ( 1 k ) synthetically - generated story - question pair .",experiment,0,164,3,0,41
question-answering,6,A story can be as short as two sentences and as long as 200 + sentences .,experiment,0,165,4,0,17
question-answering,6,A system is evaluated on the accuracy of getting the correct answers to the questions .,experiment,0,166,5,0,16
question-answering,6,"The answers are single words or lists ( e.g. "" football , apple "" ) .",experiment,0,167,6,0,16
question-answering,6,Answering questions in each task requires selecting a set of relevant sentences and applying different kinds of logical reasoning over them .,experiment,0,168,7,0,22
question-answering,6,"The dataset also includes 10 k training data ( for each task ) , which allows training more complex models .",experiment,0,169,8,1,21
question-answering,6,Note that DMN + only reports on the 10k dataset .,experiment,0,170,9,0,11
question-answering,6,bab,experiment,0,171,10,0,1
question-answering,6,"I dialog dataset bAb I dialog dataset consists of 5 different tasks , each of which has 1 k synthetically - generated goal - oriented dialogs between a user and the system in the domain of restaurant reservation .",experiment,0,172,11,0,39
question-answering,6,Each dialog is as long as 96 utterances and comes with external knowledge base ( KB ) providing information of each restaurant .,experiment,0,173,12,0,23
question-answering,6,"The authors also provide Out - Of - Vocabulary ( OOV ) version of the dataset , where many of the words and KB keywords in test data are not seen during training .",experiment,0,174,13,0,34
question-answering,6,"A system is evaluated on the accuracy of its response to each utterance of the user , choosing from up to 2500 possible candidate responses .",experiment,0,175,14,0,26
question-answering,6,A system is required not only to understand the user 's request but also refer to previous conversations in order to obtain the context information of the current conversation .,experiment,0,176,15,0,30
question-answering,6,"DSTC2 ( Task 6 ) dialog dataset transformed the Second Dialog State Tracking Challenge ( DSTC2 ) dataset into the same format as the bAbI dialog dataset , for the measurement of performance on are al dataset .",experiment,0,177,16,0,38
question-answering,6,"Each dialog can be as long as 800 + utterances , and a system needs to choose from 2407 possible candidate responses for each utterance of the user .",experiment,0,178,17,0,29
question-answering,6,"Note that the evaluation metric of the original DSTC2 is different from that of the transformed DSTC2 , so previous work on the original DSTC2 should not be directly compared to our work .",experiment,0,179,18,0,34
question-answering,6,"We will refer to this transformed DSTC2 dataset by "" Task 6 "" of dialog dataset .",experiment,0,180,19,0,17
question-answering,6,model details,experiment,0,181,20,0,2
question-answering,6,input module .,experiment,0,182,21,0,3
question-answering,6,"In the input module , we are given sentences ( previous conversations in dialog ) x t and a question ( most recent user utterance ) q , and we want to obtain their vector representations , x t , q ?",experiment,0,183,22,0,42
question-answering,6,rd .,experiment,0,184,23,0,2
question-answering,6,We use a trainable embedding matrix A ?,experiment,0,185,24,0,8
question-answering,6,R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ?,experiment,0,186,25,0,27
question-answering,6,rd .,experiment,0,187,26,0,2
question-answering,6,Then the sentence representation x t is obtained by Position Encoder .,experiment,0,188,27,0,12
question-answering,6,The same encoder with the same embedding matrix is also used to obtain the question vector q from q.,experiment,0,189,28,0,19
question-answering,6,Output Module for story - based QA .,experiment,0,190,29,0,8
question-answering,6,"In the output module , we are given the vector representation of the predicted answer ?",experiment,0,191,30,0,16
question-answering,6,"and we want to obtain the natural language form of the answer , ?.",experiment,0,192,31,0,14
question-answering,6,We use a V - way single - layer softmax classifier to map ?,experiment,0,193,32,0,14
question-answering,6,"to a V - dimensional sparse vector , v = softmax W ( y ) ? ? RV , where W ( y ) ?",experiment,0,194,33,0,25
question-answering,6,RV d is a weight matrix .,experiment,0,195,34,0,7
question-answering,6,then the final answer ?,experiment,0,196,35,0,5
question-answering,6,is simply the argmax word inv .,experiment,0,197,36,0,7
question-answering,6,"To handle questions with multiple - word answers , we consider each of them as a single word that contains punctuations such as space and comma , and put it in the vocabulary .",experiment,0,198,37,0,34
question-answering,6,output module for dialog .,experiment,0,199,38,0,5
question-answering,6,"We use a fixed number single - layer softmax classifiers , each of which is similar to that of the sotry - based QA model , to sequentially output each word of the system 's response .",experiment,0,200,39,0,37
question-answering,6,"While it is similar in spirit to the RNN decoder , our output module does not have a recurrent hidden state or gating mechanism .",experiment,0,201,40,0,25
question-answering,6,"Instead , it solely uses the final ouptut of the QRN , ? , and the current word output to influence the prediction of the next word among possible candidates .",experiment,0,202,41,0,31
question-answering,6,training .,experiment,0,203,42,0,2
question-answering,6,We withhold 10 % of the training for development .,experiment,1,204,43,0,10
question-answering,6,We use the hidden state size of 50 by deafult .,experiment,1,205,44,0,11
question-answering,6,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",experiment,1,206,45,0,29
question-answering,6,The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,experiment,0,207,46,0,22
question-answering,6,"Weights in the QRN unit are initialized using techniques by , and are tied across the layers .",experiment,0,208,47,0,18
question-answering,6,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,experiment,0,209,48,0,17
question-answering,6,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,experiment,1,210,49,0,18
question-answering,6,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,experiment,1,211,50,0,19
question-answering,6,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",experiment,1,212,51,0,32
question-answering,6,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,experiment,1,213,52,0,22
question-answering,6,"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .",experiment,0,214,53,0,49
question-answering,6,results .,result,0,215,1,0,2
question-answering,6,We compare our model with baselines and previous state - of - the - art models on story - based and dialog tasks .,result,0,216,2,0,24
question-answering,6,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .",result,0,217,3,0,45
question-answering,6,story - based qa .,result,1,218,4,0,5
question-answering,6,Table 1 ( top ) reports the summary of results of our model ( QRN ) and previous work on b AbI QA ( task - wise results are shown in in Appendix ) .,result,0,219,5,0,35
question-answering,6,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .",result,1,220,6,0,35
question-answering,6,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .",result,1,221,7,0,49
question-answering,6,Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .,result,1,222,8,0,41
question-answering,6,"As done in previous work , we also report results when we use ' Match ' for dialogs .",result,0,223,9,0,19
question-answering,6,' Match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on Appendix ) .,result,0,224,10,0,28
question-answering,6,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,result,1,225,11,0,17
question-answering,6,ablations .,result,0,226,12,0,2
question-answering,6,"We test four types of ablations ( also discussed in Section 2.2 ) : number of layers ( 1 , 2 , 3 , or 6 ) , reset gate ( r ) , and gate vectorization ( v ) and the dimension of the hidden vector ( 50 , 100 ) .",result,0,227,13,0,53
question-answering,6,We show a subset of combinations of the ablations for bAbI QA in ; other combinations performed poorly and / or did not give interesting observations .,result,0,228,14,0,27
question-answering,6,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .",result,1,229,15,0,28
question-answering,6,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .",result,0,230,16,0,28
question-answering,6,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .",result,0,231,17,0,32
question-answering,6,( b ) Adding the reset gate helps .,result,1,232,18,0,9
question-answering,6,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .",result,1,233,19,0,27
question-answering,6,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",result,1,234,20,0,19
question-answering,6,"( d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .",result,0,235,21,0,38
question-answering,6,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,result,0,236,22,0,17
question-answering,6,We implement QRN with and without parallelization in TensorFlow ) on a single Titan X GPU to qunaitify the computational gain of the parallelization .,result,0,237,23,0,25
question-answering,6,"For QRN without parallelization , we use the RNN library provided by TensorFlow .",result,0,238,24,0,14
question-answering,6,QRN with parallelization gives 6.2 times faster training and inference than QRN without parallelization on average .,result,0,239,25,0,17
question-answering,6,We expect that the speedup can be even higher for datasets with larger context .,result,0,240,26,0,15
question-answering,6,interpretations .,result,0,241,27,0,2
question-answering,6,An advantage of QRN is that the intermediate query updates are interpretable .,result,0,242,28,0,13
question-answering,6,"shows intermediate local queries ( q kt ) interpreted in natural language , such as "" Where is Sandra ? "" .",result,0,243,29,0,22
question-answering,6,"In order to obtain these , we place a decoder on the input question embedding q and add it s loss for recovering the question to the classification loss ( similarly to ) .",result,0,244,30,0,34
question-answering,6,We then use the same decoder to decode the intermediate queries .,result,0,245,31,0,12
question-answering,6,This helps us understand the flow of information in the networks .,result,0,246,32,0,12
question-answering,6,"In , the question Where is apple ?",result,0,247,33,0,8
question-answering,6,is transformed into,result,0,248,34,0,3
question-answering,6,"Where is Sandra ? at t = 1 . At t = 2 , as Sandra dropped the apple , the apple is no more relevant to Sandra .",result,0,249,35,0,29
question-answering,6,We obtain Where is Daniel ?,result,0,250,36,0,6
question-answering,6,"at time t = 3 , and it is propagated until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query .",result,0,251,37,0,32
question-answering,6,visualization .,result,0,252,38,0,2
question-answering,6,shows vizualization of the ( scalar ) magnitudes of update and reset gates on story sentences and dialog utterances .,result,0,253,39,0,20
question-answering,6,More visualizations are shown in Appendices : and .,result,0,254,40,0,9
question-answering,6,"In , we observe high values on facts that provide information to answer question ( the system 's next utterance for dialog ) .",result,0,255,41,0,24
question-answering,6,"In QA Task 2 example ( top left ) , we observe high update gate values in the first layer on facts that state who has the apple , and in the second layer , the high update gate values are on those that inform where that person went to .",result,0,256,42,0,51
question-answering,6,"We also observe that the forward reset gate at t = 2 in the first layer ( ? ? r 1 2 ) is low , which is signifying that apple no more belongs to Sandra .",result,0,257,43,0,37
question-answering,6,"In dialog Task 3 ( bottom left ) , the model is able to infer that three restaurants are already recommended so that it can recommend another one .",result,0,258,44,0,29
question-answering,6,"In dialog Task 6 ( bottom ) , the model focuses on the sentences containing Spanish , and does not concentrate much on other facts such as I do n't care .",result,0,259,45,0,32
question-answering,6,conclusion,result,0,260,46,0,1
question-answering,6,"In this paper , we introduce Query - Reduction Network ( QRN ) to answer context - based questions and carry out conversations with users that require multi-hop reasoning .",result,0,261,47,0,30
question-answering,6,We show the state - of - theart results in the three datasets of story - based QA and dialog .,result,0,262,48,0,21
question-answering,6,We model a story or a dialog as a sequence of state - changing triggers and compute the final answer to the question or the system 's next utterance by recurrently updating ( or reducing ) the query .,result,0,263,49,0,39
question-answering,6,"QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",result,0,264,50,0,26
question-answering,6,"It addresses the long - term dependency problem of most RNNs by simplifying the recurrent update , in which the candidate hidden state ( reduced query ) does not depend on the previous state .",result,0,265,51,0,35
question-answering,6,"Moreover , QRN can be parallelized and can address the well - known problem of RNN 's vanishing gradients .",result,0,266,52,0,20
question-answering,6,a task - wise results,result,0,267,1,0,5
question-answering,6,"Here we provide detailed per- task breakdown of our results in QA ) and dialog datasets . error rates ( % ) of QRN and previous work : LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMemN2N ) .",result,0,268,2,0,61
question-answering,6,Results within each task of Differentiable Neural Computer ( DNC ) were not provided in its paper ) .,result,0,269,3,0,19
question-answering,6,"For QRN , a number in the front ( 1 , 2 , 3 , 6 ) indicates the number of layers .",result,0,270,4,0,23
question-answering,6,"A number in the back indicates the dimension of hidden vector , while the default value is 50 .",result,0,271,5,0,19
question-answering,6,"' r ' indicates that the reset gate is used , and ' v ' indicates that the gates were vectorized .",result,0,272,6,0,22
question-answering,6,'*' indicates joint training . ) and Gated End - to - end Memory Networks ( GMem N2N ) .,result,0,273,7,0,20
question-answering,6,"For QRN , a number in the front ( 1 , 2 , 3 , 6 ) indicates the number of layers and a number in the back ( 100 ) indicates the dimension of hidden vector , while the default value is 50 .",result,0,274,8,0,45
question-answering,6,"' r ' indicates that the reset gate is used , ' v ' indicates that the gates were vectorized , and '+ ' indicates that ' match ' was used .",result,0,275,9,0,32
question-answering,6,b vector gate parallelization,result,0,276,10,0,4
question-answering,6,"For vector gates , we have z t ?",result,0,277,11,0,9
question-answering,6,Rd instead of z t ?,result,0,278,12,0,6
question-answering,6,r .,result,0,279,13,0,2
question-answering,6,Therefore the following equation replaces Equation :,result,0,280,14,0,7
question-answering,6,where z j k is the k - th column vector of z j .,result,0,281,15,0,15
question-answering,6,Let b ij = log ( 1 ?,result,0,282,16,0,8
question-answering,6,z i j ) for brevity .,result,0,283,17,0,7
question-answering,6,"Then , we can rewrite Equation 8 as following :",result,0,284,18,0,10
question-answering,6,"where L , L ? R T T are lower and strictly lower triangular matrices of 1's are tiled across the column .",result,0,285,19,0,23
question-answering,6,"Z = [ z 1 , . . . , z d ] ?",result,0,286,20,0,14
question-answering,6,r t d .,result,0,287,21,0,4
question-answering,6,c model details,result,0,288,22,0,3
question-answering,6,match .,result,0,289,23,0,2
question-answering,6,"While similar in spirit , our ' Match ' model is slightly different from previous work ( Bordes and .",result,0,290,24,0,20
question-answering,6,"We use answer candidate embedding matrix , and add 2 dimension of 0 - 1 matrix which expresses whether the answer candidate matches with any word in the paragraph and the question .",result,0,291,25,0,33
question-answering,6,"In other words , the softmax is computed b? v = softmax W [ W ( y ) ; M ( y ) ]? ? RV , where W ?",result,0,292,26,0,30
question-answering,6,"R dd and W ( y ) ? RV ( d?2 ) are trainable weight matrices , and M ( y ) ?",result,0,293,27,0,23
question-answering,6,RV 2 is the 0 - 1 match matrix .,result,0,294,28,0,10
question-answering,6,d visualizations,result,0,295,29,0,2
question-answering,6,Visualization of Story - based QA . shows visualization of models for story - based QA tasks .,result,0,296,30,0,18
question-answering,6,"In the task 3 ( left ) , the model focuses on the facts that contain ' football ' in the first layer , and found out where Mary journeyed to before the bathroom in the second layer .",result,0,297,31,0,39
question-answering,6,"In task 7 ( right ) , the model focuses on the facts that provide information about the location of Sandra . 0.00 0.87 1.00 1.00 I 'm on it .",result,0,298,32,0,31
question-answering,6,0.73 0.97 0.38 0.00,result,0,299,33,0,4
question-answering,6,How many people would you in your party .,result,0,300,34,0,9
question-answering,6,1.00 1.00 0.00 0.41,result,0,301,35,0,4
question-answering,6,for four people please .,result,0,302,36,0,5
question-answering,6,Which price range are you looking for .,result,0,303,37,0,8
question-answering,6,Layer 1 Layer 2 Task 1 Issuing API calls z 1 ? ? r 1 ? ? r 1 z,result,0,304,38,0,20
question-answering,6,2 Can you make a restaurant reservation for eight in a cheap price range in madrid 0.00 1.00 0.93 1.00 I 'm on it .,result,0,305,39,0,25
question-answering,6,0.00 1.00 0.74 0.00,result,0,306,40,0,4
question-answering,6,Any preference on a type of cuisine .,result,0,307,41,0,8
question-answering,6,0.00 0.11 1.00 0.01,result,0,308,42,0,4
question-answering,6,i love british food .,result,0,309,43,0,5
question-answering,6,0.00 0.99 0.99 0.57,result,0,310,44,0,4
question-answering,6,Okay let me look into some options for you .,result,0,311,45,0,10
question-answering,6,1.00 0.00 0.00 0.02 < SILENCE > API CALL british madrid eight cheap Layer 1 Layer 2 Task 4 Providing extra-information z 1 ? ? r 1 ? ? r 1 z 2 resto-paris-expen-spanish-8stars,result,0,312,46,0,34
question-answering,6,R-phone resto-paris -expen-spanish-8stars-phone 0.71 0.84 0.99 0.36 resto-paris-expen-spanish-8stars,result,0,313,47,0,8
question-answering,6,R-address resto-paris - expen-spanish-8stars-address 1.00 0.99 1.00 1.00 resto-paris-expen-spanish-8stars R-location paris 0.05 0.01 1.00 0.00 resto-paris-expen-spanish-8stars R-number four 0.02 0.95 0.97 0.00 resto-paris-expen-spanish-8stars,result,0,314,48,0,23
question-answering,6,R-price expensive 0.00 0.05 0.92 0.00 resto-paris-expen-spanish-8stars,result,0,315,49,0,7
question-answering,6,R-rating 8 0.38 0.91 1.00 0.10,result,0,316,50,0,6
question-answering,6,What do you think of this option : resto-paris-expen-spanish-8stars 0.90 0.93 0.99 1.00,result,0,317,51,0,13
question-answering,6,let 's do it .,result,0,318,52,0,5
question-answering,6,0.00 0.00 1.00 0.00,result,0,319,53,0,4
question-answering,6,Great let me do the reservation .,result,0,320,54,0,7
question-answering,6,0.98 0.99 0.97 0.00,result,0,321,55,0,4
question-answering,6,Do you have its address .,result,0,322,56,0,6
question-answering,6,Here it is : resto - paris - expen-spanish -8stars - address : Visualization of update and reset gates in QRN ' 2 r ' model for on several tasks of bAbI dialog and DSTC2 dialog .,result,0,323,57,0,37
question-answering,6,We do not put reset gate in the last layer .,result,0,324,58,0,11
question-answering,6,"Note that we only show some of recent sentences here , even the dialog has more sentences .",result,0,325,59,0,18
question-answering,6,visualization of dialog .,result,0,326,60,0,4
question-answering,6,shows visualization of models for dialog tasks .,result,0,327,61,0,8
question-answering,6,"In the first dialog of task 1 , the model focuses on the user utterance that mentions the user 's desired cuisine and location , and the current query ( user 's last utterance ) informs the system of the number of people , so the system is able to learn that it now needs to ask the user about the desired price range .",result,0,328,62,0,65
question-answering,6,"In the second dialog of task 1 , the model focuses on the facts that provide information about the requests of the user .",result,0,329,63,0,24
question-answering,6,"In task 4 ( third ) , the model focuses on what restaurant a user is talking about and the information about the restaurant .",result,0,330,64,0,25
question-answering,2,Large - scale Simple Question Answering with Memory Networks,title,1,2,1,0,9
question-answering,2,abstract,abstract,0,3,1,0,1
question-answering,2,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,abstract,1,4,2,0,24
question-answering,2,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",abstract,1,5,3,0,51
question-answering,2,"To this end , we introduce a new dataset of 100 k questions that we use in conjunction with existing benchmarks .",abstract,0,6,4,0,22
question-answering,2,"We conduct our study within the framework of Memory Networks ( Weston et al. , 2015 ) because this perspective allows us to eventually scale up to more complex reasoning , and show that Memory Networks can be successfully trained to achieve excellent performance .",abstract,0,7,5,1,45
question-answering,2,introduction,introduction,0,8,1,0,1
question-answering,2,"Open-domain Question Answering ( QA ) systems aim at providing the exact answer ( s ) to questions formulated in natural language , without restriction of domain .",introduction,0,9,2,0,28
question-answering,2,"While there is along history of QA systems that search for textual documents or on the Web and extract answers from them ( see e.g. ) , recent progress has been made with the release of large Knowledge Bases ( KBs ) such as Freebase , which contain consolidated knowledge stored as atomic facts , and extracted from different sources , such as free text , tables in webpages or collaborative input .",introduction,0,10,3,0,73
question-answering,2,Existing approaches for QA from KBs use learnable components to either transform the question into a structured KB query or learn to embed questions and facts in a low dimensional vector space and retrieve the answer by computing similarities in this embedding space .,introduction,0,11,4,0,44
question-answering,2,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",introduction,0,12,5,0,59
question-answering,2,"Hence , existing benchmarks are small ; they mostly cover the head of the distributions of facts , and are restricted in their question types and their syntactic and lexical variations .",introduction,0,13,6,0,32
question-answering,2,"As such , it is still unknown how much the existing systems perform outside the range of the specific question templates of a few , small benchmark datasets , and it is also unknown whether learning on a single dataset transfers well on other ones , and whether such systems can learn from different training sources , which we believe is necessary to capture the whole range of possible questions .",introduction,0,14,7,0,71
question-answering,2,"Besides , the actual need for reasoning , i.e. constructing the answer from more than a single fact from the KB , depends on the actual structure of the KB .",introduction,0,15,8,0,31
question-answering,2,"As we shall see , for instance , a simple preprocessing of Freebase tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact , including list questions that expect more than a single answer .",introduction,0,16,9,0,45
question-answering,2,"In fact , the task of simple QA itself might already cover a wide range of practical usages , if the KB is properly organized .",introduction,0,17,10,0,26
question-answering,2,This paper presents two contributions .,introduction,0,18,11,0,6
question-answering,2,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",introduction,1,19,12,0,45
question-answering,2,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?",introduction,0,20,13,0,29
question-answering,2,Which forest is Fires Creek in ?,introduction,0,21,14,0,7
question-answering,2,What is an active ingredient in childrens earache relief ?,introduction,0,22,15,0,10
question-answering,2,"tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction,0,23,16,0,29
question-answering,2,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",introduction,1,24,17,0,28
question-answering,2,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",introduction,0,25,18,0,63
question-answering,2,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,introduction,1,26,19,0,20
question-answering,2,"While our model bares similarity with previous embedding models for QA , using the framework of MemNNs opens the perspective to more involved inference schemes in future work , since MemNNs were shown to perform well on complex reasoning toy QA tasks .",introduction,0,27,20,0,43
question-answering,2,We discuss related work in Section 5 .,introduction,0,28,21,0,8
question-answering,2,"We report experimental results in Section 6 , where we show that our model achieves excellent results on the benchmark WebQuestions .",introduction,0,29,22,0,22
question-answering,2,We also show that it can learn from two different QA datasets to improve its performance on both .,introduction,0,30,23,0,19
question-answering,2,We also present the first successful application of transfer learning for QA .,introduction,0,31,24,0,13
question-answering,2,"Using the Reverb KB and QA datasets , we show that Reverb facts can be added to the memory and used to answer without retraining , and that MemNNs achieve better results than some systems designed on this dataset .",introduction,0,32,25,0,40
question-answering,2,simple question answering,introduction,0,33,26,0,3
question-answering,2,Knowledge Bases contain facts expressed as triples where subject and object are entities and relationship describes the type of ( directed ) link between these entities .,introduction,0,34,27,0,27
question-answering,2,"The simple QA prob - lem we address here consist in finding the answer to questions that can be rephrased as queries of the form , asking for all objects linked to subject by relationship .",introduction,0,35,28,0,36
question-answering,2,the question,introduction,0,36,29,0,2
question-answering,2,"What do Jamaican people speak ? , for instance , could be rephrased as the Freebase query ( jamaica , language spoken , ? ) .",introduction,0,37,30,0,26
question-answering,2,"In other words , fetching a single fact from a KB is sufficient to answer correctly .",introduction,0,38,31,0,17
question-answering,2,"The term simple QA refers to the simplicity of the reasoning process needed to answer questions , since it involves a single fact .",introduction,0,39,32,0,24
question-answering,2,"However , this does not mean that the QA problem is easy per se , since retrieving this single supporting fact can be very challenging as it involves to search over millions of alternatives given a query expressed in natural language .",introduction,0,40,33,0,42
question-answering,2,"shows that , with a KB with many types of relationships like",introduction,0,41,34,0,12
question-answering,2,"Freebase , the range of questions that can be answered with a single fact is already very broad .",introduction,0,42,35,0,19
question-answering,2,"Besides , as we shall see , modiying slightly the structure of the KB can make some QA problems simpler by adding direct connections between entities and hence allow to bypass the need for more complex reasoning .",introduction,0,43,36,0,38
question-answering,2,knowledge bases,introduction,0,44,37,0,2
question-answering,2,"We use the KB Freebase 1 as the basis of our QA system , our source of facts and answers .",introduction,0,45,38,0,21
question-answering,2,All Freebase entities and relationships are typed and the lexicon for types and relationships is closed .,introduction,0,46,39,0,17
question-answering,2,"Freebase data is collaboratively collected and curated , to ensure a high reliability of the facts .",introduction,0,47,40,0,17
question-answering,2,"Each entity has an internal identifier and a set of strings thatare usually used to refer to that entity in text , termed aliases .",introduction,0,48,41,0,25
question-answering,2,"We consider two extracts of Freebase , whose statistics are given in .",introduction,0,49,42,0,13
question-answering,2,"FB2M , which was used in , contains about 2 M entities and 5 k relationships .",introduction,0,50,43,0,17
question-answering,2,"FB5M , is much larger with about 5 M entities and more than 7.5 k relationships .",introduction,0,51,44,0,17
question-answering,2,"We also use the KB Reverb as a secondary source of facts to study how well a model trained to answer questions using Freebase facts could be used to answer using Reverb 's as well , without being trained on Reverb data .",introduction,0,52,45,0,43
question-answering,2,This is a pure setting of transfer learning .,introduction,0,53,46,0,9
question-answering,2,Reverb is interesting for this experiment because it differs a lot from Freebase .,introduction,0,54,47,0,14
question-answering,2,Its data was extracted automatically from text with minimal human intervention and is highly unstructured : entities are unique strings and the lexicon for relationships is open .,introduction,0,55,48,0,28
question-answering,2,"This leads to many more relationships , but entities with multiple references are not deduplicated , ambiguous referents are not resolved , and the reliability of the stored facts is much lower than in Freebase .",introduction,0,56,49,0,36
question-answering,2,"We used the full extraction from , which contains 2 M entities and 600 k relationships .",introduction,0,57,50,0,17
question-answering,2,the simplequestions dataset,introduction,0,58,51,0,3
question-answering,2,"Existing resources for QA such as WebQuestions are rather small ( few thousands questions ) and hence do not provide a very thorough coverage of the variety of questions that could be answered using a KB like Freebase , even in the context of simple QA .",introduction,0,59,52,0,47
question-answering,2,"Hence , in this paper , we introduce a new dataset of much larger scale for the task of simple QA called SimpleQuestions .",introduction,0,60,53,0,24
question-answering,2,2,introduction,0,61,54,0,1
question-answering,2,"This dataset consists of a total of 108,442 questions written in natural language by human English - speaking annotators each paired with a corresponding fact from FB2M that provides the answer and explains it .",introduction,0,62,55,0,35
question-answering,2,"We randomly shuffle these questions and use 70 % of them ( 75910 ) as training set , 10 % as validation set , and the remaining 20 % as test set .",introduction,0,63,56,0,33
question-answering,2,Examples of questions and facts are given in .,introduction,0,64,57,0,9
question-answering,2,We collected SimpleQuestions in two phases .,introduction,0,65,58,0,7
question-answering,2,The first phase consisted of shortlisting the set of facts from Freebase to be annotated with questions .,introduction,0,66,59,0,18
question-answering,2,We used FB2M as background KB and removed all facts with undefined relationship type i.e. containing the word freebase .,introduction,0,67,60,0,20
question-answering,2,"We also removed all facts for which the ( subject , relationship ) pair had more than a threshold number of objects .",introduction,0,68,61,0,23
question-answering,2,This filtering step is crucial to remove facts,introduction,0,69,62,0,8
question-answering,2,"2 The dataset is available from http://fb.ai/babi. which would result in trivial uninformative questions , such as , Name a person who is an actor ?.",introduction,0,70,63,0,26
question-answering,2,The threshold was set to 10 .,introduction,0,71,64,0,7
question-answering,2,"In the second phase , these selected facts were sampled and delivered to human annotators to generate questions from them .",introduction,0,72,65,0,21
question-answering,2,"For the sampling , each fact was associated with a probability which defined as a function of its relationship frequency in the KB : to favor variability , facts with relationship appearing more frequently were given lower probabilities .",introduction,0,73,66,0,39
question-answering,2,"For each sampled facts , annotators were shown the facts along with hyperlinks to freebase.com to provide some context while framing the question .",introduction,0,74,67,0,24
question-answering,2,"Given this information , annotators were asked to phrase a question involving the subject and the relationship of the fact , with the answer being the object .",introduction,0,75,68,0,28
question-answering,2,"The annotators were explicitly instructed to phrase the question differently as much as possible , if they encounter multiple facts with similar relationship .",introduction,0,76,69,0,24
question-answering,2,They were also given the option of skipping facts if they wish to do so .,introduction,0,77,70,0,16
question-answering,2,This was very important to avoid the annotators to write a boilerplate questions when they had no background knowledge about some facts .,introduction,0,78,71,0,23
question-answering,2,Memory Networks for Simple QA,introduction,0,79,72,0,5
question-answering,2,A Memory Network consists of a memory ( an indexed array of objects ) and a neural network that is trained to query it given some inputs ( usually questions ) .,introduction,0,80,73,0,32
question-answering,2,"It has four components : Input map ( I ) , Generalization ( G ) , Output map ( O ) and Response ( R ) which we detail below .",introduction,0,81,74,0,31
question-answering,2,"But first , we describe the MemNNs workflow used to setup a model for simple QA .",introduction,0,82,75,0,17
question-answering,2,This proceeds in three steps :,introduction,0,83,76,0,6
question-answering,2,Storing Freebase : this first phase parses,introduction,0,84,77,0,7
question-answering,2,Freebase ( either FB2M or FB5M depending on the setting ) and stores it in memory .,introduction,0,85,78,0,17
question-answering,2,It uses the Input module to preprocess the data .,introduction,0,86,79,0,10
question-answering,2,2 .,introduction,0,87,80,0,2
question-answering,2,Training : this second phase trains the Mem NN to answer question .,introduction,0,88,81,0,13
question-answering,2,"This uses Input , Output and Response modules , the training concerns mainly the parameters of the embedding model at the core of the Output module .",introduction,0,89,82,0,27
question-answering,2,Connecting Reverb : this third phase adds new facts coming from,introduction,0,90,83,0,11
question-answering,2,reverb to the memory .,introduction,0,91,84,0,5
question-answering,2,This is done after training to test the ability of MemNNs to handle new facts without having to be re-trained .,introduction,0,92,85,0,21
question-answering,2,It uses the Input module to preprocess Reverb facts and the Generalization module to connect them to the facts already stored .,introduction,0,93,86,0,22
question-answering,2,"After these three stages , the MemNN is ready to answer any question by running the I , O and R modules in turn .",introduction,0,94,87,0,25
question-answering,2,We now detail the implementation of the four modules .,introduction,0,95,88,0,10
question-answering,2,input module,introduction,0,96,89,0,2
question-answering,2,This module preprocesses the three types of data thatare input to the network :,introduction,0,97,90,0,14
question-answering,2,"Freebase facts thatare used to populate the memory , questions that the system need to answer , and Reverb facts that we use , in a second phase , to extend the memory .",introduction,0,98,91,0,34
question-answering,2,preprocessing freebase,introduction,0,99,92,0,2
question-answering,2,"The Freebase data is initially stored as atomic facts involving single entities as subject and object , plus a relationship between them .",introduction,0,100,93,0,23
question-answering,2,"However , this storage needs to be adapted to the QA task in two aspects .",introduction,0,101,94,0,16
question-answering,2,"First , in order to answer list questions , which expect more than one answer , we redefine a fact as being a triple containing a subject , a relationship , and the set of all objects linked to the subject by the relationship .",introduction,0,102,95,0,45
question-answering,2,"This grouping process transforms atomic facts into grouped facts , which we simply refer to as facts in the following .",introduction,0,103,96,0,21
question-answering,2,"shows the impact of this grouping : on FB2M , this decreases the number of facts from 14 M to 11 M and , on FB5 M , from 22 M to 12M .",introduction,0,104,97,0,34
question-answering,2,"Second , the underlying structure of Freebase is a hypergraph , in which more than two entities can be linked .",introduction,0,105,98,0,21
question-answering,2,For instance dates can be linked together with two entities to specify the time period over which the link was valid .,introduction,0,106,99,0,22
question-answering,2,"The underlying triple storage involves mediator nodes for each such fact , effectively making entities linked through paths of length 2 , instead of 1 .",introduction,0,107,100,0,26
question-answering,2,"To obtain direct links between entities in such cases , we created a single fact for these facts by removing the intermediate node and using the second relationship as the relationship for the new condensed fact .",introduction,0,108,101,0,37
question-answering,2,"This step reduces the need for searching the answer outside the immediate neighborhood of the subject referred to in the question , widely increasing the scope of the simple QA task on Freebase .",introduction,0,109,102,0,34
question-answering,2,"On WebQuestions , a benchmark not primarily designed for simple QA , removing mediator nodes allows to jump from around 65 % to 86 % of questions that can be answered with a single fact .",introduction,0,110,103,0,36
question-answering,2,preprocessing freebase facts,introduction,0,111,104,0,3
question-answering,2,"A fact with k objects y = ( s , r , {o 1 , ... , o k } ) is represented by a bag - of - symbol vector f ( y ) in RN S , where NS is the number of entities and relationships .",introduction,0,112,105,0,49
question-answering,2,Each dimension off ( y ) corresponds to a relationship or an entity ( independent of whether it appears as subject or object ) .,introduction,0,113,106,0,25
question-answering,2,"The entries of the subject and of the relationship have value 1 , and the entries of the objects are set to 1 / k .",introduction,0,114,107,0,26
question-answering,2,All other entries are 0 .,introduction,0,115,108,0,6
question-answering,2,preprocessing questions,introduction,0,116,109,0,2
question-answering,2,A question q is mapped to a bag - of - ngrams representation g ( q ) of dimension RN V where NV is the size of the vocabulary .,introduction,0,117,110,0,30
question-answering,2,"The vocabulary contains all individual words that appear in the questions of our datasets , together with the aliases of Freebase entities , each alias being a single n-gram .",introduction,0,118,111,0,30
question-answering,2,"The entries of g ( q ) that correspond to words and n-grams of q are equal to 1 , all other ones are set to 0 .",introduction,0,119,112,0,28
question-answering,2,preprocessing reverb facts,introduction,0,120,113,0,3
question-answering,2,"In our experiments with Reverb , each fact y = ( s , r , o) is represented as a vector h ( y ) ?",introduction,0,121,114,0,26
question-answering,2,rn s +n v .,introduction,0,122,115,0,5
question-answering,2,"This vector is a bagof - symbol for the subject sand the object o , and a bag - of - words for the relationship r.",introduction,0,123,116,0,26
question-answering,2,"The exact composition of h is provided by the Generalization module , which we describe now .",introduction,0,124,117,0,17
question-answering,2,generalization module,introduction,0,125,118,0,2
question-answering,2,This module is responsible for adding new elements to the memory .,introduction,0,126,119,0,12
question-answering,2,"In our case , the memory has a multigraph structure where each node is a Freebase entity and labeled arcs in the multigraph are Freebase relationships : after their preprocessing , all Freebase facts are stored using this structure .",introduction,0,127,120,0,40
question-answering,2,"We also consider the case where new facts , with a different structure ( i.e. new kinds of relationship ) , are provided to the MemNNs by using Reverb .",introduction,0,128,121,0,30
question-answering,2,"In this case , the generalization module is then used to connect Reverb facts to the Freebase - based memory structure , in order to make them usable and searchable by the MemNN .",introduction,0,129,122,0,34
question-answering,2,"To link the subject and the object of a Reverb fact to Freebase entities , we use precomputed entity links .",introduction,0,130,123,0,21
question-answering,2,"If such links do not give any result for an entity , we search for Freebase entities with at least one alias that matches the Reverb entity string .",introduction,0,131,124,0,29
question-answering,2,These two processes allowed to match 17 % of Reverb entities to Freebase ones .,introduction,0,132,125,0,15
question-answering,2,"The remainder of entities were encoded using bag - of - words representation of their strings , since we had no other way of matching them to Freebase entities .",introduction,0,133,126,0,30
question-answering,2,All Reverb relationships were encoded using bag - of - words of their strings .,introduction,0,134,127,0,15
question-answering,2,"Using this approximate process , we are able to store each Reverb fact as a bag - of - symbols ( words or Freebase entities ) all already seen by the MemNN during its training phase based on Freebase .",introduction,0,135,128,0,40
question-answering,2,We can then hope that what had been learned there could also be successfully used to query Reverb facts .,introduction,0,136,129,0,20
question-answering,2,output module,introduction,0,137,130,0,2
question-answering,2,The output module performs the memory lookups given the input to return the supporting facts destined to eventually provide the answer given a question .,introduction,0,138,131,0,25
question-answering,2,"In our case of simple QA , this module only returns a single supporting fact .",introduction,0,139,132,0,16
question-answering,2,"To avoid scoring all the stored facts , we first perform an approximate entity linking step to generate a small set of candidate facts .",introduction,0,140,133,0,25
question-answering,2,The supporting fact is the candidate fact that is most similar to the question according to an embedding model .,introduction,0,141,134,0,20
question-answering,2,candidate generation,introduction,0,142,135,0,2
question-answering,2,"To generate candidate facts , we match n-grams of words of the question to aliases of Freebase entities and select a few matching entities .",introduction,0,143,136,0,25
question-answering,2,All facts having one of these entities as subject are scored in a second step .,introduction,0,144,137,0,16
question-answering,2,"We first generate all possible n-grams from the question , removing those that contain an interrogative pronoun or 1 - grams that belong to a list of stopwords .",introduction,0,145,138,0,29
question-answering,2,"We only keep the n-grams which are an alias of an entity , and then discard all n-grams thatare a subsequence of another n-gram , except if the longer n-gram only differs by in , of , for or the at the beginning .",introduction,0,146,139,0,44
question-answering,2,We finally keep the two entities with the most links in Freebase retrieved for each of the five longest matched n-grams .,introduction,0,147,140,0,22
question-answering,2,Scoring Scoring is performed using an embedding model .,introduction,0,148,141,0,9
question-answering,2,Given two embedding matrices WV ?,introduction,0,149,142,0,6
question-answering,2,R dN V and W S ?,introduction,0,150,143,0,7
question-answering,2,"R dN S , which respectively contain , in columns , the d-dimensional embeddings of the words / n - grams of the vocabulary and the embeddings of the Freebase entities and relationships , the similarity between question q and a Freebase candidate fact y is computed as :",introduction,0,151,144,0,49
question-answering,2,with cos ( ) the cosine similarity .,introduction,0,152,145,0,8
question-answering,2,"When scoring a fact y from Reverb , we use the same embeddings and build the matrix WV S ? R d( N V +N S ) , which contains the concatenation in columns of WV and W S , and also compute the cosine similarity :",introduction,0,153,146,0,47
question-answering,2,"S RV B ( q , y ) = cos ( W V g ( q ) , WV S h ( y ) ) .",introduction,0,154,147,0,26
question-answering,2,"The dimension dis a hyperparameter , and the embedding matrices WV and W S are the parameters learned with the training algorithm of Section 4 .",introduction,0,155,148,0,26
question-answering,2,response module,introduction,0,156,149,0,2
question-answering,2,"In Memory Networks , the Response module postprocesses the result of the Output module to compute the intended answer .",introduction,0,157,150,0,20
question-answering,2,"In our case , it returns the set of objects of the selected supporting fact .",introduction,0,158,151,0,16
question-answering,2,training,introduction,0,159,152,0,1
question-answering,2,This section details how we trained the scoring function of the Output module using a multitask training process on four different sources of data .,introduction,0,160,153,0,25
question-answering,2,"First , in addition to the new SimpleQuestions dataset described in Section 2 , we also used We-bQuestions , a benchmark for QA introduced in : questions are labeled with answer strings from aliases of Freebase entities , and many questions expect multiple answers .",introduction,0,161,154,0,45
question-answering,2,Table 3 details the statistics of both datasets .,introduction,0,162,155,0,9
question-answering,2,"We also train on automatic questions generated from the KB , that is FB2M or FB5M depending on the setting , which are essential to learn embeddings for the entities not appearing in either WebQuestions or SimpleQuestions .",introduction,0,163,156,0,38
question-answering,2,Statistics of FB2M or FB5M are given in ; we generated one training question per fact following the same process as that used in .,introduction,0,164,157,0,25
question-answering,2,"Following previous work such as , we also use the indirect supervision signal of pairs of question paraphrases .",introduction,0,165,158,0,19
question-answering,2,We used a subset of the large set of paraphrases extracted from WIKIANSWERS and introduced in .,introduction,0,166,159,0,17
question-answering,2,Our Paraphrases dataset is made of 15M clusters containing 2 or more paraphrases each .,introduction,0,167,160,0,15
question-answering,2,multitask training,introduction,0,168,161,0,2
question-answering,2,"As in previous work on embedding models and Memory Networks , the embeddings are trained with a ranking criterion .",introduction,0,169,162,0,20
question-answering,2,"For QA datasets the goal is that in the embedding space , a supporting fact is more similar to the question than any other non-supporting fact .",introduction,0,170,163,0,27
question-answering,2,"For the paraphrase dataset , a question should be more similar to one of its paraphrases than to any another question .",introduction,0,171,164,0,22
question-answering,2,The multitask learning of the embedding matrices WV and W S is performed by alternating stochastic gradient descent ( SGD ) steps over the loss function on the different datasets .,introduction,0,172,165,0,31
question-answering,2,"For the QA datasets , given a question / supporting fact pair ( q , y) and a non-supporting fact y ? , we perform a step to minimize the loss function ? QA ( q , y , y ? ) = ? ? S QA ( q , y ) + S QA ( q , y ? ) + , where [. ] + is the positive part and ?",introduction,0,173,166,0,73
question-answering,2,is a margin hyperparameter .,introduction,0,174,167,0,5
question-answering,2,"For the paraphrase dataset , the similarity score between two questions q and q ?",introduction,0,175,168,0,15
question-answering,2,"is also the cosine between their embeddings , i.e. S QQ ( q , q ? ) = cos ( W V g ( q ) , WV g ( q ? ) ) , and given a paraphrase pair ( q , q ? ) and another question q ??",introduction,0,176,169,0,51
question-answering,2,", the loss is :",introduction,0,177,170,0,5
question-answering,2,The embeddings ( i.e. the columns of WV and W S ) are projected onto the L 2 unit ball after each update .,introduction,0,178,171,0,24
question-answering,2,"At each time step , a sample from the paraphrase dataset is drawn with probability 0.2 ( this probability is arbitrary ) .",introduction,0,179,172,0,23
question-answering,2,"Otherwise , a sample from one of the three QA datasets , chosen uniformly at random , is taken .",introduction,0,180,173,0,20
question-answering,2,We use the WARP loss,introduction,0,181,174,0,5
question-answering,2,distant supervision,introduction,0,182,175,0,2
question-answering,2,"Unlike for SimpleQuestions or the synthetic QA data generated from Freebase , for WebQuestions only answer strings are provided for questions : the supporting facts are unknown .",introduction,0,183,176,0,28
question-answering,2,"In order to generate the supervision , we use the candidate fact generation algorithm of Section 3.3 .",introduction,0,184,177,0,18
question-answering,2,"For each candidate fact , the aliases of its objects are compared to the set of provided answer strings .",introduction,0,185,178,0,20
question-answering,2,The fact ( s ) which can generate the maximum number of answer strings from their objects ' aliases are then kept .,introduction,0,186,179,0,23
question-answering,2,"If multiple facts are obtained for the same question , the ones with the minimal number of objects are considered as supervision facts .",introduction,0,187,180,0,24
question-answering,2,This last selection avoids favoring irrelevant relationships that would be kept only because they point to many objects but would not be specific enough .,introduction,0,188,181,0,25
question-answering,2,"If no answer string could be found from the objects of the initial candidates , the question is discarded from the training set .",introduction,0,189,182,0,24
question-answering,2,Future work should investigate the process of weak supervised training of MemNNs recently introduced in that allows to train them without any supervision coming from the supporting facts . :,introduction,0,190,183,0,30
question-answering,2,training and evaluation datasets .,introduction,0,191,184,0,5
question-answering,2,Questions automatically generated from the KB and paraphrases can also be used in training .,introduction,0,192,185,0,15
question-answering,2,webquestions simplequestions,introduction,0,193,186,0,2
question-answering,2,reverb,introduction,0,194,187,0,1
question-answering,2,generating negative examples,introduction,0,195,188,0,3
question-answering,2,"As in , learning is performed with gradient descent , so that negative examples ( non - supporting facts or non-paraphrases ) are generated according to a randomized policy during training .",introduction,0,196,189,0,32
question-answering,2,"For paraphrases , given a pair ( q , q ? ) , a nonparaphrase pair is generated as ( q , q ?? )",introduction,0,197,190,0,25
question-answering,2,where q ??,introduction,0,198,191,0,3
question-answering,2,"is a random question of the dataset , not belonging to the cluser of q .",introduction,0,199,192,0,16
question-answering,2,"For question / supporting fact pairs , we use two policies .",introduction,0,200,193,0,12
question-answering,2,"The default policy to obtain a non-supporting fact is to corrupt the answer fact by exchanging it s subject , its relationship or its object ( s ) with that of another fact chosen uniformly at random from the KB .",introduction,0,201,194,0,41
question-answering,2,"In this policy , the element of the fact to corrupt is chosen randomly , with a small probability ( 0.3 ) of corrupting more than one element of the answer fact .",introduction,0,202,195,0,33
question-answering,2,"The second policy we propose , called candidates as negatives , is to take as non-supporting fact a randomly chosen fact from the set of candidate facts .",introduction,0,203,196,0,28
question-answering,2,"While the first policy is standard in learning embeddings , the second one is more original , and , as we see in the experiments , gives slightly better performance .",introduction,0,204,197,0,31
question-answering,2,related work,related work,0,205,1,0,2
question-answering,2,"The first approaches to open - domain QA were search engine - based systems , where keywords extracted from the question are sent to a search engine , and the answer is extracted from the top results .",related work,0,206,2,0,38
question-answering,2,"This method has been adapted to KB - based QA , and obtained competitive results with respect to semantic parsing and embedding - based approaches .",related work,0,207,3,0,26
question-answering,2,Semantic parsing approaches perform a functional parse of the sentence that can be interpreted as a KB query .,related work,0,208,4,0,19
question-answering,2,"Even though these approaches are difficult to train at scale because of the complexity of their inference , their advantage is to provide a deep interpretation of the question .",related work,0,209,5,0,30
question-answering,2,"Some of these approaches require little to no question - answer pairs , relying on simple rules to tranform the semantic interpretation to a KB query .",related work,0,210,6,0,27
question-answering,2,"Like our work , embedding - based methods for QA can be seen as simple MemNNs .",related work,0,211,7,0,17
question-answering,2,"The algorithms of use an approach similar to ours but are based on Reverb rather than Freebase , and relied purely on bag - of - word for both questions and facts .",related work,0,212,8,0,33
question-answering,2,"The approach of ( Yang et al. , 2014 ) uses a different representation of questions , in which recognized entities are replaced by an entity token , and a different training data using entity mentions from WIKIPEDIA .",related work,0,213,9,1,39
question-answering,2,"Our model is closest to the one presented in , which is discussed in more details in the experiments .",related work,0,214,10,0,20
question-answering,2,experiments,experiment,0,215,1,0,1
question-answering,2,This section provides an extensive evaluation of our MemNNs implementation against state - of the - art QA methods as well as an empirical study of the impact of using multiple training sources on the prediction performance .,experiment,0,216,2,0,38
question-answering,2,"details the dimensions of the test sets of WebQuestions , SimpleQuestions and Reverb which we used for evaluation .",experiment,0,217,3,0,19
question-answering,2,evaluation and baselines,experiment,0,218,4,0,3
question-answering,2,"On WebQuestions , we evaluate against previous results on this benchmark in terms of F1 - score as defined in , which is the average , over all test questions , of the F1 - score of the sets of predicted answers .",experiment,0,219,5,0,43
question-answering,2,"Since no previous result was published on SimpleQuestions , we only compare different versions of MemNNs .",experiment,0,220,6,0,17
question-answering,2,"SimpleQuestions questions are labeled with their entire Freebase fact , so we evaluate in terms of path - level accuracy , in which a prediction is correct if the subject and the relationship were correctly retrieved by the system .",experiment,0,221,7,0,40
question-answering,2,"The Reverb test set , based on the KB of the same name and introduced in is used for evaluation only .",experiment,0,222,8,0,22
question-answering,2,it contains 691 questions .,experiment,0,223,9,0,5
question-answering,2,"We consider the task of re-ranking a small set of candidate answers , which are Reverb facts and are labeled as corrector incorrect .",experiment,0,224,10,0,24
question-answering,2,"We compare our approach to the original system , to and to the original MemNNs , in terms of accuracy , which is the percentage of questions for which the top - ranked candidate fact is correct .",experiment,0,225,11,0,38
question-answering,2,experimental setup,experiment,0,226,1,0,2
question-answering,2,All models were trained with at least the dataset made of synthetic questions created from the KB .,experiment,0,227,2,0,18
question-answering,2,"The hyperparameters were chosen to maximize the F1-score on WebQuestions validation set , independently of the testing dataset .",experiment,0,228,3,0,19
question-answering,2,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?",experiment,1,229,4,0,35
question-answering,2,was set to 0.1 .,experiment,1,230,5,0,5
question-answering,2,"For each configuration of hyperparameters , the F1score on the validation set was computed regularly during learning to perform early stopping .",experiment,0,231,6,0,22
question-answering,2,We tested additional configurations for our algorithm .,experiment,0,232,7,0,8
question-answering,2,"First , in the Candidates as Negatives setting ( negative facts are sampled from the candidate set , see Section 4 ) , abbreviated CANDS AS NEGS , the experimental protocol is the same as in the default setting but the embeddings are initialized with the best configuration of the default setup .",experiment,0,233,8,0,53
question-answering,2,"Second , our model shares some similarities with an approach studied in , in which the authors noticed important gains using a subgraph representation of answers .",experiment,0,234,9,0,27
question-answering,2,"For completeness , we also added such a subgraph representation of objects .",experiment,0,235,10,0,13
question-answering,2,"In that setting , called Subgraph , each object o of a fact is itself represented as a bag - of - entities that encodes the immediate neighborhood of o .",experiment,0,236,11,0,31
question-answering,2,This Subgraph model is trained similarly as our main approach and only the results of a post -hoc ensemble combination of the two models ( where the scores are added ) are presented .,experiment,0,237,12,0,34
question-answering,2,We also report the results obtained by an ensemble of the 5 best models on validation ( subgraph excepted ) ; this is denoted 5 models .,experiment,0,238,13,0,27
question-answering,2,results,result,0,239,1,0,1
question-answering,2,comparative results,result,0,240,1,0,2
question-answering,2,The results of the comparative experiments are given in .,result,0,241,2,0,10
question-answering,2,"On the main benchmark WebQuestions , our best results use all data sources , the bigger extract from Freebase and the CANDS AS NEGS setting .",result,1,242,3,0,26
question-answering,2,"The two ensembles achieve excellent results , with F1 -",result,0,243,4,0,10
question-answering,2,webquestions simplequestions,result,0,244,5,0,2
question-answering,2,Reverb F1-SCORE ( % ) ACCURACY ( % ) ACCURACY ( % ) BASELINES Random guess 1.9 4.9 35 31.3 n / a n / a n / a n / a 54 29.7 n / a 73 ) - using path 35.3 n/ a n / a ) - using path + subgraph 39.2 n / a n / a 39.9 n/ a n/ a 41.3 n/ a n / a of SimpleQuestions questions .,result,0,245,6,0,76
question-answering,2,"This shows that MemNNs are effective at re-ranking the candidates , but also that simple QA is still not solved .",result,0,246,7,0,21
question-answering,2,Our approach bares similarity to ) - using path .,result,0,247,8,0,10
question-answering,2,"They use FB2M , and so their result ( 35.3 % F1 - score on WebQuestions ) should be compared to our 36.2 % .",result,0,248,9,0,25
question-answering,2,"The models are slightly different in that they replace the entity string with the subject entity in the question representation and that we use the cosine similarity instead of the dot product , which gave consistent improvements .",result,0,249,10,0,38
question-answering,2,"Still , the major differences come from how we use Freebase .",result,0,250,11,0,12
question-answering,2,"First , the removal of the mediator nodes allows us to restrict ourselves to single supporting facts , while they search in paths of length 2 with a heuristic to select the paths to follow ( otherwise , inference is too costly ) , which makes our inference simpler and more efficient .",result,0,251,12,0,53
question-answering,2,"Second , using grouped facts , we integrate multiple answers during learning ( through the distant supervision ) , while they use a grouping heuristic at test time .",result,0,252,13,0,29
question-answering,2,Grouping facts also allows us to scale much better and to train on FB5M .,result,0,253,14,0,15
question-answering,2,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .",result,0,254,15,0,41
question-answering,2,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .",result,0,255,16,0,37
question-answering,2,transfer learning on reverb,result,1,256,17,0,4
question-answering,2,"In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .",result,1,257,18,0,33
question-answering,2,"Thus , ( last column ) presents the result of our model without training on Reverb against methods specifically developed on that dataset .",result,0,258,19,0,24
question-answering,2,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .",result,1,259,20,0,43
question-answering,2,These results show that the Memory Network approach can integrate and use new entities and links .,result,0,260,21,0,17
question-answering,2,presents the results on the three datasets when our model is trained with different data sources .,result,0,261,22,0,17
question-answering,2,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",result,1,262,23,0,50
question-answering,2,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .",result,1,263,24,0,38
question-answering,2,importance of data sources,result,0,264,25,0,4
question-answering,2,the bottom half of,result,0,265,26,0,4
question-answering,2,"While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .",result,0,266,27,0,32
question-answering,2,"This is because WebQuestions and SimpleQuestions questions follow simple patterns and are well formed , while Reverb questions have more syntactic and lexical variability .",result,0,267,28,0,25
question-answering,2,"Thus , paraphrases are important to avoid overfitting on specific question patterns of the training sets .",result,0,268,29,0,17
question-answering,2,conclusion,result,0,269,30,0,1
question-answering,2,This paper presents an implementation of MemNNs for the task of large - scale simple QA .,result,0,270,31,0,17
question-answering,2,"Our results demonstrate that , if properly trained , MemNNs are able to handle natural language and a very large memory ( millions of entries ) , and hence can reach state - of - the - art on the popular benchmark WebQuestions .",result,0,271,32,0,44
question-answering,2,"We want to emphasize that many of our findings , especially those regarding how to format the KB , do not only concern MemNNs but potentially any QA system .",result,0,272,33,0,30
question-answering,2,"This paper also introduced the new dataset SimpleQuestions , which , with 100 k examples , is one order of magnitude bigger than WebQuestions : we hope that it will foster interesting new research in QA , simple or not .",result,0,273,34,0,41
question-answering,3,Sentence Similarity Learning by Lexical Decomposition and Composition,title,1,2,1,0,8
question-answering,3,abstract,abstract,0,3,1,0,1
question-answering,3,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",abstract,1,4,2,0,35
question-answering,3,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",abstract,0,5,3,0,26
question-answering,3,"The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence .",abstract,0,6,4,0,27
question-answering,3,"Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector .",abstract,0,7,5,0,22
question-answering,3,"After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components .",abstract,0,8,6,0,22
question-answering,3,"Finally , a similarity score is estimated over the composed feature vectors .",abstract,0,9,7,0,13
question-answering,3,"Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,0,10,8,0,34
question-answering,3,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,abstract,0,11,9,0,18
question-answering,3,It plays an important role for a variety of tasks in both NLP and IR communities .,abstract,0,12,10,0,17
question-answering,3,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,0,13,11,1,35
question-answering,3,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",abstract,0,14,12,1,41
question-answering,3,"However , sentence similarity learning has following challenges :",abstract,0,15,13,0,9
question-answering,3,1 .,abstract,0,16,14,0,2
question-answering,3,There is a lexical gap between semantically equivalent sentences .,abstract,0,17,15,0,10
question-answering,3,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",abstract,0,18,16,0,23
question-answering,3,"2 . Semantic similarity should be measured at different levels of granularity ( word - level , phrase - level and syntax - level ) .",abstract,0,19,17,0,26
question-answering,3,"E.g. , "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",abstract,0,20,18,0,29
question-answering,3,3 .,abstract,0,21,19,0,2
question-answering,3,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",abstract,0,22,20,1,24
question-answering,3,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",abstract,0,23,21,0,48
question-answering,3,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",abstract,0,24,22,0,45
question-answering,3,How we can extract and utilize those information becomes another challenge .,abstract,0,25,23,0,12
question-answering,3,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms for a longtime .",abstract,0,26,24,0,20
question-answering,3,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",abstract,0,27,25,0,24
question-answering,3,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,0,28,26,1,36
question-answering,3,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",abstract,0,29,27,1,49
question-answering,3,The third challenge did not get much,abstract,0,30,28,0,7
question-answering,3,narrative,abstract,0,31,29,0,1
question-answering,3,e1,abstract,0,32,30,0,1
question-answering,3,The research is to sockeye .,abstract,0,33,31,0,6
question-answering,3,e2,abstract,0,34,32,0,1
question-answering,3,The study is [ not related ] to salmon .,abstract,0,35,33,0,10
question-answering,3,e3,abstract,0,36,34,0,1
question-answering,3,The research is relevant to salmon .,abstract,0,37,35,0,7
question-answering,3,e4,abstract,0,38,36,0,1
question-answering,3,"The study is relevant to sockeye , instead of coho .",abstract,0,39,37,0,11
question-answering,3,e5,abstract,0,40,38,0,1
question-answering,3,"The study is relevant to sockeye , rather than flounder .:",abstract,0,41,39,0,11
question-answering,3,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",abstract,0,42,40,0,22
question-answering,3,""" coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",abstract,0,43,41,0,20
question-answering,3,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",abstract,0,44,42,0,47
question-answering,3,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",abstract,1,45,43,0,24
question-answering,3,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",abstract,1,46,44,0,42
question-answering,3,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",abstract,1,47,45,0,29
question-answering,3,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",abstract,1,48,46,0,31
question-answering,3,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",abstract,1,49,47,0,29
question-answering,3,"Finally , the composed feature vector is utilized to predict the sentence similarity .",abstract,1,50,48,0,14
question-answering,3,"Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,0,51,49,0,37
question-answering,3,"In following parts , we start with a brief overview of our model ( Section 2 ) , followed by the details of our end - to - end implementation ( Section 3 ) .",abstract,0,52,50,0,35
question-answering,3,Then we evaluate our model on answer sentence selection and paraphrase identifications tasks ( Section 4 ) .,abstract,0,53,51,0,18
question-answering,3,model overview,abstract,0,54,52,0,2
question-answering,3,"In this section , we propose a sentence similarity learning model to tackle all three challenges ( mentioned in Section 1 ) .",abstract,0,55,53,0,23
question-answering,3,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",abstract,0,56,54,0,30
question-answering,3,"To tackle the second challenge , we assume that each word can be semantically matched by several words in the other sentence , and we calculate a semantic matching vector for each word vector based on all the word vectors in the other side .",abstract,0,57,55,0,45
question-answering,3,"To cope with the third challenge , we assume that each semantic unit ( word ) can be partially matched , and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector .",abstract,0,58,56,0,40
question-answering,3,shows an overview of our sentence similarity model .,abstract,0,59,57,0,9
question-answering,3,"Given a pair of sentences Sand T , our task is to calculate a similarity score sim ( S , T ) in following steps :",abstract,0,60,58,0,26
question-answering,3,word representation .,abstract,0,61,59,0,3
question-answering,3,"Word embedding of is an effective way to handle the lexical gap challenge in the sentence similarity task , as it represents each word with a distributed vector , and words appearing in similar contexts tend to have similar meanings .",abstract,0,62,60,0,41
question-answering,3,"With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .",abstract,0,63,61,0,70
question-answering,3,semantic matching .,abstract,0,64,62,0,3
question-answering,3,"In order to judge the similarity between two sentences , we need to check whether each semantic unit in one sentence is covered by the other sentence , or vice versa .",abstract,0,65,63,0,32
question-answering,3,"For example , in , to check whether E 2 is a paraphrase of E 1 , we need to know the single word "" irrelevant "" in E 1 is matched or covered by the phrase "" not related "" in E 2 .",abstract,0,66,64,0,45
question-answering,3,"In our model , we treat each word as a primitive semantic unit , and calculate a semantic matching vector ?",abstract,0,67,65,0,21
question-answering,3,i for each word s i by composing part or full word vectors in the other sentence T .,abstract,0,68,66,0,19
question-answering,3,"In this way , we can match a word s i to a word or phrase in T . Similarly , for the reverse direction , we also calculate all semantic matching vectorst j in T .",abstract,0,69,67,0,37
question-answering,3,We explore different f match functions later in Section 3 .,abstract,0,70,68,0,11
question-answering,3,decomposition .,abstract,0,71,69,0,2
question-answering,3,"After the semantic matching phase , we have the semantic matching vectors of ?",abstract,0,72,70,0,14
question-answering,3,i and t j .,abstract,0,73,71,0,5
question-answering,3,we interpret ?,abstract,0,74,72,0,3
question-answering,3,i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,abstract,0,75,73,0,27
question-answering,3,"However , it is not necessary that all the semantics of s i ( or t j ) are fully covered by ?",abstract,0,76,74,0,23
question-answering,3,i ( ort j ) .,abstract,0,77,75,0,6
question-answering,3,"Take the E 1 and E 2 in for example , the word "" sockeye "" in E 1 is only partially matched by the word "" salmon "" ( the similar part ) in E 2 , as the full meaning of "" sockeye "" is "" red salmon "" ( the semantic meaning of "" red "" is the dissimilar part ) .",abstract,0,78,76,0,65
question-answering,3,"Motivated by this phenomenon , our model further decomposes word s i ( or t j ) , based on its semantic matching vector ?",abstract,0,79,77,0,25
question-answering,3,"i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",abstract,0,80,78,0,34
question-answering,3,"Formally , we define the decomposition function as :",abstract,0,81,79,0,9
question-answering,3,", our goal in this step is how to utilize those information .",abstract,0,82,80,0,13
question-answering,3,"Besides the suggestion from that the significance of the dissimilar parts alone between two sentences has a great effect of their similarity , we also think that the dissimilar and similar components have strong connections .",abstract,0,83,81,0,36
question-answering,3,"For example , in , if we only look at the dissimilar or similar part alone , it is hard to judge which one between E 4 and E 5 is more similar to E 3 .",abstract,0,84,82,0,37
question-answering,3,"We can easily identify that E 5 is more similar to E 3 , when we consider both the similar and dissimilar parts .",abstract,0,85,83,0,24
question-answering,3,"Therefore , our model composes the similar component matrix and dissimilar component matrix into a feature vector S ( or T ) with the composition function :",abstract,0,86,84,0,27
question-answering,3,similarity assessing .,abstract,0,87,85,0,3
question-answering,3,"In the final stage , we concatenate the two feature vectors ( Sand T ) and predict the final similarity score :",abstract,0,88,86,0,22
question-answering,3,3 An End - to - End Implementation Section 2 gives us a glance of our model .,abstract,0,89,87,0,18
question-answering,3,"In this section , we describe details of each phase .",abstract,0,90,88,0,11
question-answering,3,semantic matching functions,abstract,0,91,89,0,3
question-answering,3,This subsection describes our specifications for the semantic matching function f match in Eq.,abstract,0,92,90,0,14
question-answering,3,( 1 ) .,abstract,0,93,91,0,4
question-answering,3,The goal off match is to generate a semantic matching vector ?,abstract,0,94,92,0,12
question-answering,3,i for s i by composing the vectors from T .,abstract,0,95,93,0,11
question-answering,3,"For a sentence pair Sand T , we first calculate a similarity matrix A mn , where each element a i , j ?",abstract,0,96,94,0,24
question-answering,3,A mn computes the cosine similarity between words s i and t j as,abstract,0,97,95,0,14
question-answering,3,"Then , we define three different semantic matching functions over A mn :",abstract,0,98,96,0,13
question-answering,3,"where k = argmax j a i , j .",abstract,0,99,97,0,10
question-answering,3,The idea of the global function is to consider all word vectors t j in T .,abstract,0,100,98,0,17
question-answering,3,a semantic matching vector ?,abstract,0,101,99,0,5
question-answering,3,"i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",abstract,0,102,100,0,27
question-answering,3,The max function moves to the other extreme .,abstract,0,103,101,0,9
question-answering,3,It generates the semantic matching vector by selecting the most similar word vector t k from T .,abstract,0,104,102,0,18
question-answering,3,"The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .",abstract,0,105,103,0,33
question-answering,3,So the semantic matching vector is a weighted average vector from t k?w tot k+w .,abstract,0,106,104,0,16
question-answering,3,decomposition functions,abstract,0,107,105,0,2
question-answering,3,This subsection describes the implementations for the decomposition function f decomp in Eq.,abstract,0,108,106,0,13
question-answering,3,( 2 ) .,abstract,0,109,107,0,4
question-answering,3,The intention off decomp is to decompose a word vector s j based on its semantic matching vector ?,abstract,0,110,108,0,19
question-answering,3,j into a similar component s + i and a dissimilar component s ?,abstract,0,111,109,0,14
question-answering,3,"i , where s +",abstract,0,112,110,0,5
question-answering,3,i indicates the semantics of s i covered by ?,abstract,0,113,111,0,10
question-answering,3,i and s ?,abstract,0,114,112,0,4
question-answering,3,i indicates the uncovered part .,abstract,0,115,113,0,6
question-answering,3,"We implement three types of decomposition function : rigid , linear and orthogonal .",abstract,0,116,114,0,14
question-answering,3,The rigid decomposition only adapts to the max version off match .,abstract,0,117,115,0,12
question-answering,3,"First , it detects whether there is an exactly matched word in the other sentence , or s i equal to ?",abstract,0,118,116,0,22
question-answering,3,i .,abstract,0,119,117,0,2
question-answering,3,"If yes , the vector s i is dispatched to the similar component s + i , and the dissimilar component is assigned with a zero vector",abstract,0,120,118,0,27
question-answering,3,0 .,abstract,0,121,119,0,2
question-answering,3,"Otherwise , the vector s i is assigned to the dissimilar component s ?",abstract,0,122,120,0,14
question-answering,3,i .,abstract,0,123,121,0,2
question-answering,3,Eq. gives the formal definition :,abstract,0,124,122,0,6
question-answering,3,The motivation for the linear decomposition is that the more similar between s i and ?,abstract,0,125,123,0,16
question-answering,3,"i , the higher proportion of s i should be assigned to the similar component .",abstract,0,126,124,0,16
question-answering,3,"First , we calculate the cosine similarity ?",abstract,0,127,125,0,8
question-answering,3,between s i and ?,abstract,0,128,126,0,5
question-answering,3,i .,abstract,0,129,127,0,2
question-answering,3,"Then , we decompose s i linearly based on ?.",abstract,0,130,128,0,10
question-answering,3,Eq. gives the corresponding definition :,abstract,0,131,129,0,6
question-answering,3,The orthogonal decomposition is to decompose a vector in the geometric space .,abstract,0,132,130,0,13
question-answering,3,Based on the semantic matching vector ?,abstract,0,133,131,0,7
question-answering,3,"i , our model decomposes s i into a parallel component and a perpendicular component .",abstract,0,134,132,0,16
question-answering,3,"Then , the parallel component is viewed as the similar component s + i , and perpendicular component is taken as the dissimilar component s ?",abstract,0,135,133,0,26
question-answering,3,i .,abstract,0,136,134,0,2
question-answering,3,Eq. gives the concrete definitions .,abstract,0,137,135,0,6
question-answering,3,composition functions,abstract,0,138,136,0,2
question-answering,3,The aim of composition function f comp in Eq. is to extract features from both the similar component matrix and the dissimilar component matrix .,abstract,0,139,137,0,25
question-answering,3,We also want to acquire similarities and dissimilarities of various granularity during the composition phase .,abstract,0,140,138,0,16
question-answering,3,"Inspired from Kim , we utilize a two - channel convolutional neural networks ( CNN ) and design filters based on various order of n-grams , e.g. , unigram , bigram and trigram .",abstract,0,141,139,0,34
question-answering,3,The CNN model involves two sequential operations : convolution and max - pooling .,abstract,0,142,140,0,14
question-answering,3,"For the convolution operation , we define a list of filters {w o }.",abstract,0,143,141,0,14
question-answering,3,"The shape of each filter is d h , where d is the dimension of word vectors and h is the window size .",abstract,0,144,142,0,24
question-answering,3,"Each filter is applied to two patches ( a window size h of vectors ) from both similar and dissimilar channels , and generates a feature .",abstract,0,145,143,0,27
question-answering,3,Eq. ( 10 ) expresses this process .,abstract,0,146,144,0,8
question-answering,3,"To deal with variable feature size , we perform a max - pooling operation over co by selecting the maximum value co = max co .",abstract,0,147,145,0,26
question-answering,3,"Therefore , after these two operations , each filter generates only one feature .",abstract,0,148,146,0,14
question-answering,3,We define several filters by varying the window size and the initial values .,abstract,0,149,147,0,14
question-answering,3,"Eventually , a vector of features is captured by composing the two component matrixes , and the feature dimension is equal to the number of filters .",abstract,0,150,148,0,27
question-answering,3,similarity assessment function,abstract,0,151,149,0,3
question-answering,3,The similarity assessment function f sim in Eq. ( 4 ) predicts a similarity score by taking two feature vectors as input .,abstract,0,152,150,0,23
question-answering,3,"We employ a linear function to sum up all the features and apply a sigmoid function to constrain the similarity within the range [ 0 , 1 ] .",abstract,0,153,151,0,29
question-answering,3,training,abstract,0,154,152,0,1
question-answering,3,We train our sentence similariy model by maximizing the likelihood on a training set .,abstract,0,155,153,0,15
question-answering,3,"Each training instance in the training set is represented as a triple ( S i , Ti , Li ) , where Si and Ti are a pair of sentences , and Li ? { 0 , 1 } indicates the similarity between them .",abstract,0,156,154,0,45
question-answering,3,"We assign Li = 1 if Ti is a paraphrase of Si for the paraphrase identification task , or Ti is a correct answer for Si for the answer sentence selection task .",abstract,0,157,155,0,33
question-answering,3,"Otherwise , we assign Li = 0 .",abstract,0,158,156,0,8
question-answering,3,We implement the mathematical expressions with Theano and use Adam for optimization .,abstract,0,159,157,0,13
question-answering,3,experiment,experiment,0,160,1,0,1
question-answering,3,experimental setting,experiment,0,161,1,0,2
question-answering,3,We evaluate our model on two tasks : answer sentence selection and paraphrase identification .,experiment,0,162,2,0,15
question-answering,3,"The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence , and the performance is measured by mean average precision ( MAP ) and mean reciprocal rank ( MRR ) .",experiment,0,163,3,0,42
question-answering,3,We experiment on two datasets : QASent and Wiki QA .,experiment,0,164,4,0,11
question-answering,3,"The statistics of the two datasets can be found in , where QASent was created from the TREC QA track , and WikiQA ( Yang et al. , 2015 ) is constructed from real queries of Bing and Wikipedia .",experiment,0,165,5,1,40
question-answering,3,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,experiment,0,166,6,0,19
question-answering,3,The metrics include the accuracy and the positive class F 1 score .,experiment,0,167,7,0,13
question-answering,3,"We experiment on the Microsoft Research Paraphrase corpus ( MSRP ) , which includes 2753 true and 1323 false instances in the training set , and 1147 true and 578 false instances in the test set .",experiment,0,168,8,0,37
question-answering,3,We build a development set by randomly selecting 100 true and 100 false instances from the training set .,experiment,0,169,9,0,19
question-answering,3,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .",experiment,0,170,10,1,34
question-answering,3,model properties,experiment,0,171,11,0,2
question-answering,3,"There are several alternative options in our model , e.g. , the semantic matching functions , the decomposition operations , and the filter types .",experiment,0,172,12,0,25
question-answering,3,The choice of these options may affect the final performance .,experiment,0,173,13,0,11
question-answering,3,"In this subsection , we present some experiments to demonstrate the properties of our model , and find a good configuration that we use to evaluate our final model .",experiment,0,174,14,0,30
question-answering,3,All the experiments in this subsection were performed on the QASent dataset and evaluated on the development set .,experiment,0,175,15,0,19
question-answering,3,"First , we evaluated the effectiveness of various semantic matching functions .",experiment,0,176,16,0,12
question-answering,3,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",experiment,1,177,17,0,58
question-answering,3,presents the results .,result,0,178,1,0,4
question-answering,3,We found that the max function worked better than the global function on both MAP and MRR .,result,1,179,2,0,18
question-answering,3,"By increasing the window size , the local -l function acquired progressive improvements when the window size is smaller than 4 .",result,0,180,3,0,22
question-answering,3,"But after we enlarged the window size to 4 , the performance dropped .",result,0,181,4,0,14
question-answering,3,"The local - 3 function worked better than the max function in term of the MAP , and also got a comparable MRR .",result,0,182,5,0,24
question-answering,3,"Therefore , we use the local - 3 function in the following experiments .",result,0,183,6,0,14
question-answering,3,"Second , we studied the effect of various decomposition operations .",result,0,184,7,0,11
question-answering,3,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .",result,1,185,8,0,21
question-answering,3,shows the performance .,result,0,186,9,0,4
question-answering,3,We found that the rigid operation got the worst result .,result,0,187,10,0,11
question-answering,3,"This is reasonable , because the rigid operation decomposes word vectors by exactly matching words .",result,0,188,11,0,16
question-answering,3,"The orthogonal operation got a similar MAP as the linear operation , and it worked better in term of MRR .",result,0,189,12,0,21
question-answering,3,"Therefore , we choose the orthogonal operation in the following experiments .",result,0,190,13,0,12
question-answering,3,"Third , we tested the influence of various filter types .",result,1,191,14,0,11
question-answering,3,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .",result,1,192,15,0,70
question-answering,3,We generate 500 filters for each filter type ( with different initial values ) .,result,0,193,16,0,15
question-answering,3,Experimental results are shown in .,result,0,194,17,0,6
question-answering,3,"At the beginning , adding higher - order ngram filters was helpful for the performance .",result,0,195,18,0,16
question-answering,3,"The performance reached to the peak , when we used the win - 3 filters .",result,0,196,19,0,16
question-answering,3,"After that , adding more complex filters decreased the performance .",result,0,197,20,0,11
question-answering,3,"Therefore , the trigram is the best granularity for our model .",result,0,198,21,0,12
question-answering,3,"In the following experiments , we utilize filter types in win - 3 .",result,0,199,22,0,14
question-answering,3,Comparing with State - of - the - art Models,result,0,200,23,0,10
question-answering,3,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .",result,0,201,24,0,19
question-answering,3,qasent dataset .,result,1,202,25,0,3
question-answering,3,"presents the performances of the state - of - the - art systems and our model , where the performances were evaluated with the standard trec eval - 8.1 script",result,0,203,26,0,30
question-answering,3,1 .,result,0,204,27,0,2
question-answering,3,"Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",result,0,205,28,1,37
question-answering,3,"Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .",result,0,206,29,0,20
question-answering,3,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .",result,0,207,30,0,23
question-answering,3,"Therefore , the lower - level granularity is an indispensable factor for a good performance .",result,0,208,31,0,16
question-answering,3,"conducted word alignment for a sentence pair based on word vectors , and measured the sentence similarity based on a couple of word alignment features .",result,0,209,32,0,26
question-answering,3,"They got a slightly better performance ( the fourth row of ) , which indicates that the vector representation for words is helpful to bridging the lexical gap problem .",result,0,210,33,0,30
question-answering,3,"dos introduced the attention mechanism into the CNN model , and learnt sentence representation by considering the influence of the other sentence .",result,0,211,34,0,23
question-answering,3,They got better performance than all the other previous work .,result,0,212,35,0,11
question-answering,3,Our model makes use of all these useful factors and also considers the dissimilarities of a sentence pair .,result,0,213,36,0,19
question-answering,3,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",result,1,214,37,0,28
question-answering,3,wiki qa dataset .,result,1,215,38,0,4
question-answering,3,presents the results of our model and several state - of - the - art models .,result,0,216,39,0,17
question-answering,3,constructed the dataset and reimplemented several baseline models .,result,0,217,40,0,9
question-answering,3,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,result,0,218,41,0,25
question-answering,3,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,result,0,219,42,0,14
question-answering,3,The corresponding performance is given at the fourth row of .,result,0,220,43,0,11
question-answering,3,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",result,0,221,44,0,21
question-answering,3,The semantic matching phase in our model is similar to the attention mechanism .,result,0,222,45,0,14
question-answering,3,"But different from the previous models , our model utilizes both the similarity and dissimilarity simultaneously .",result,0,223,46,0,17
question-answering,3,The last row of shows that our model is more effective than the other models .,result,1,224,47,0,16
question-answering,3,msrp dataset .,result,1,225,48,0,3
question-answering,3,granularity and modeled interaction features at each level for a pair of sentences .,result,0,226,49,0,14
question-answering,3,They obtained their best performance by pretraining the model on a language modeling task ( the 3rd row of ) .,result,0,227,50,0,21
question-answering,3,"However , their model heavily depends on the pretraining strategy .",result,0,228,51,0,11
question-answering,3,"Without pretraining , they got a much worse performance ( the second row of ) .",result,0,229,52,0,16
question-answering,3,proposed a similar model to .,result,0,230,53,0,6
question-answering,3,"Similarly , they also used a CNN model to extract features at multiple levels of granularity .",result,0,231,54,0,17
question-answering,3,"Differently , they utilized some extra annotated resources , e.g. , embeddings from part - of - speech ( POS ) tags and PARAGRAM vectors trained from the Paraphrase Data base .",result,0,232,55,0,32
question-answering,3,Their model outperformed without the need of pretraining ( the sixth row of ) .,result,0,233,56,0,15
question-answering,3,"However , the performance was reduced after removing the extra resources ( the fourth and fifth rows of ) .",result,0,234,57,0,20
question-answering,3,applied their attention - based CNN model on this dataset .,result,0,235,58,0,11
question-answering,3,"By adding a couple of sparse features and using a layerwise training strategy , they got a pretty good performance .",result,0,236,59,0,21
question-answering,3,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",result,1,237,60,0,34
question-answering,3,"However , the best performance so far on this dataset is obtained by .",result,0,238,61,0,14
question-answering,3,"In their model , they just utilized several hand - crafted features in a Support Vector Machine ( SVM ) model .",result,0,239,62,0,22
question-answering,3,"Therefore , the deep learning methods still have along way to go for this task .",result,0,240,63,0,16
question-answering,3,related work,related work,0,241,1,0,2
question-answering,3,The semantic matching functions in subsection 3.1 are inspired from the attention - based neural machine translation .,related work,0,242,2,0,18
question-answering,3,"However , most of the previous work using the attention mechanism in only LSTM models .",related work,0,243,3,0,16
question-answering,3,Whereas our model introduces the attention mechanism into the CNN model .,related work,0,244,4,0,12
question-answering,3,A similar work is the attention - based CNN model proposed by .,related work,0,245,5,0,13
question-answering,3,"They first build an attention matrix for a sentence pair , and then directly take the attention matrix as a new channel of the CNN model .",related work,0,246,6,0,27
question-answering,3,"Differently , our model uses the attention matrix ( or similarity matrix ) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix , and then feeds these two matrixes into a two - channel CNN model .",related work,0,247,7,0,44
question-answering,3,The model can then focus much on the interactions between similar and dissimilar parts of a sentence pair .,related work,0,248,8,0,19
question-answering,3,conclusion,related work,0,249,9,0,1
question-answering,3,"In this work , we proposed a model to assess sentence similarity by decomposing and composing lexical semantics .",related work,0,250,10,0,19
question-answering,3,"To bridge the lexical gap problem , our model represents each word with its context vector .",related work,0,251,11,0,17
question-answering,3,"To extract features from both the similarity and dissimilarity of a sentence pair , we designed several methods to decompose the word vector into a similar component and a dissimilar component .",related work,0,252,12,0,32
question-answering,3,"To extract features at multiple levels of granularity , we employed a two - channel CNN model and equipped it with multiple types of ngram filters .",related work,0,253,13,0,27
question-answering,3,Experimental results show that our model is quite effective on both the answer sentence selection task and the paraphrase identification task .,related work,0,254,14,0,22
question-answering,0,Open Question Answering with Weakly Supervised Embedding Models,title,1,2,1,0,8
question-answering,0,abstract,abstract,0,3,1,0,1
question-answering,0,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,abstract,1,4,2,0,17
question-answering,0,Promising progress has recently been achieved by methods that learn to map questions to logical forms or data base queries .,abstract,0,5,3,0,21
question-answering,0,Such approaches can be effective but at the cost of either large amounts of human - labeled data or by defining lexicons and grammars tailored by practitioners .,abstract,0,6,4,0,28
question-answering,0,"In this paper , we instead take the radical approach of learning to map questions to vectorial feature representations .",abstract,0,7,5,0,20
question-answering,0,"By mapping answers into the same space one can query any knowledge base independent of its schema , without requiring any grammar or lexicon .",abstract,0,8,6,0,25
question-answering,0,Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine - tuning step using the weak supervision provided by blending automatically and collaboratively generated resources .,abstract,0,9,7,0,33
question-answering,0,"We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex , the only existing method able to be trained on similar weakly labeled data .",abstract,0,10,8,0,35
question-answering,0,introduction,introduction,0,11,1,0,1
question-answering,0,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",introduction,1,12,2,0,26
question-answering,0,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge .,introduction,0,13,3,0,20
question-answering,0,"An important development in this are a has been the creation of large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia which store huge amounts of general - purpose information .",introduction,0,14,4,0,36
question-answering,0,"They are organized as data bases of triples connecting pairs of entities by various relationships and of the form ( left entity , relationship , right entity ) .",introduction,0,15,5,0,29
question-answering,0,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,introduction,0,16,6,0,31
question-answering,0,The use of KBs simplifies the problem by separating the issue of collecting and organizing information ( i.e. information extraction ) from the one of searching through it ( i.e. question answering or natural language interfacing ) .,introduction,0,17,7,0,38
question-answering,0,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",introduction,0,18,8,0,36
question-answering,0,Recent progress has been made by tackling this problem with semantic parsers .,introduction,0,19,9,0,13
question-answering,0,These methods convert questions into logical forms or data base queries ( e.g. in SPARQL ) which are then subsequently used to query KBs for answers .,introduction,0,20,10,0,27
question-answering,0,"Even if such systems have shown the ability to handle large - scale KBs , they require practitioners to hand - craft lexicons , grammars , and KB schema for the parsing to be effective .",introduction,0,21,11,0,36
question-answering,0,"This nonnegligible human intervention might not be generic enough to conveniently scale up to new data bases with other schema , broader vocabularies or other languages than English .",introduction,0,22,12,0,29
question-answering,0,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",introduction,1,23,13,0,35
question-answering,0,"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .",introduction,0,24,14,0,41
question-answering,0,"For example , ( parrotfish.e , live - in.r , southern - water .e ) stands for",introduction,0,25,15,0,17
question-answering,0,What is parrotfish 's habitat ?,introduction,0,26,16,0,6
question-answering,0,"and southern - water.e and ( cantonese.e , be-major - language - in.r , hong - kong.e ) for",introduction,0,27,17,0,19
question-answering,0,What is the main language of Hong - Kong ?,introduction,0,28,18,0,10
question-answering,0,and cantonese.e.,introduction,0,29,19,0,2
question-answering,0,"In this task , the main difficulties come from lexical variability rather than from complex syntax , having multiple answers per question , and the absence of a supervised training signal .",introduction,0,30,20,0,32
question-answering,0,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,introduction,1,31,21,0,34
question-answering,0,"Unfortunately , we do not have access to any human labeled ( query , answer ) supervision for this task .",introduction,0,32,22,0,21
question-answering,0,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .",introduction,1,33,23,0,27
question-answering,0,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,introduction,0,34,24,0,55
question-answering,0,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,introduction,1,35,25,0,33
question-answering,0,Our method strongly outperforms previous results on the WikiAnswers + ReVerb evaluation data set introduced by .,introduction,0,36,26,0,17
question-answering,0,"Even if the embeddings obtained after training are of good quality , the scale of the optimization problem makes it hard to control and to lead to convergence .",introduction,0,37,27,0,29
question-answering,0,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .",introduction,1,38,28,0,36
question-answering,0,The rest of the paper is organized as follows .,introduction,0,39,29,0,10
question-answering,0,Section 2 discusses some previous work and Section 3 introduces the problem of open question answering .,introduction,0,40,30,0,17
question-answering,0,"Then , Section 4 presents our model and Section 5 our experimental results .",introduction,0,41,31,0,14
question-answering,0,related work,related work,0,42,1,0,2
question-answering,0,"Large - scale question answering has along history , mostly initiated via the TREC tracks .",related work,0,43,2,0,16
question-answering,0,"The first successful systems transformed the questions into queries which were fed to web search engines , the answer being subsequently extracted from top returned pages or snippets .",related work,0,44,3,0,29
question-answering,0,Such approaches require significant engineering to hand - craft queries and then parse and search over results .,related work,0,45,4,0,18
question-answering,0,"The emergence of large - scale KBs , such as Freebase or DBpedia , changed the setting by transforming open question answering into a problem of querying a KB using natural language .",related work,0,46,5,0,33
question-answering,0,"This is a challenging problem , which would require huge amount of labeled data to be tackled properly by purely supervised machine learning methods because of the great variability of language and of the large scale of KBs .",related work,0,47,6,0,39
question-answering,0,"The earliest methods for open question - answering with KBs , based on hand - written templates , were not robust enough to such variability over possibly evolving KBs ( addition / deletion of triples and entities ) .",related work,0,48,7,0,39
question-answering,0,The solution to gain more expressiveness via machine learning comes from distant or indirect supervision to circumvent the issue of labeled data .,related work,0,49,8,0,23
question-answering,0,Initial works attempting to learn to connect KBs and natural language with less supervision have actually been tackling the information extraction problem .,related work,0,50,9,0,23
question-answering,0,"Recently , new systems for learning question answering systems with few labeled data have been introduced based on semantic parsers .",related work,0,51,10,0,21
question-answering,0,"Such works tend to require realistic amounts of manual intervention via labeled examples , but still need vast efforts to carefully design lexicons , grammars and the KB .",related work,0,52,11,0,29
question-answering,0,"In contrast , proposed a framework for open question answering requiring little human annotation .",related work,0,53,12,0,15
question-answering,0,"Their system , Paralex , answers questions with more limited semantics than those introduced in , but does so at a very large scale in an open - domain manner .",related work,0,54,13,0,31
question-answering,0,It is trained using automatically and collaboratively generated data and using the KB ReVerb .,related work,0,55,14,0,15
question-answering,0,"In this work , we follow this trend by proposing an embedding - based model for question answering that is also trained under weak and cheap supervision .",related work,0,56,15,0,28
question-answering,0,Embedding - based models are getting more and more popular in natural language processing .,related work,0,57,16,0,15
question-answering,0,"Starting from the neural network language model of , these methods have now reached near state - of - the - art performance on many standard tasks while usually requiring less hand - crafted features .",related work,0,58,17,0,36
question-answering,0,"Recently , some embedding models have been proposed to perform a connection between natural language and KBs for word - sense dis ambiguation and for information extraction .",related work,0,59,18,0,28
question-answering,0,"Our work builds on these approaches to instead learn to perform open question answering under weak supervision , which to our knowledge has not been attempted before .",related work,0,60,19,0,28
question-answering,0,open-domain question answering,related work,0,61,20,0,3
question-answering,0,"In this paper , we follow the question answering framework of and use the same data .",related work,0,62,21,0,17
question-answering,0,"Hence , relatively little labeling or feature engineering has been used .",related work,0,63,22,0,12
question-answering,0,task definition,related work,0,64,23,0,2
question-answering,0,"Our work considers the task of question answering as in : given a question q , the corresponding answer is given by a triplet from a KB .",related work,0,65,24,0,28
question-answering,0,"This means that we consider questions for which a set of triples t provide an interpretation of the question and it s answer , such as : Here , we only give a singlet per question , but many can exist .",related work,0,66,25,0,42
question-answering,0,"In the remainder , the KB is denoted K and its set of entities and relationships is E .",related work,0,67,26,0,19
question-answering,0,The word vocabulary for questions is termed V. n v and n e are the sizes of V and E respectively .,related work,0,68,27,0,22
question-answering,0,"Our model consists in learning a function S ( ) , which can score questionanswer triple pairs ( q , t ) .",related work,0,69,28,0,23
question-answering,0,"Hence , finding the top - ranked answert ( q ) to a question q is directly carried out by : t",related work,0,70,29,0,22
question-answering,0,"To handle multiple answer , we instead present the results as a ranked list , rather than taking the top prediction , and evaluate that instead .",related work,0,71,30,0,27
question-answering,0,Using the scoring function S ( ) allows to directly query the KB without needing to define an intermediate structured logical representation for questions as in semantic parsing systems .,related work,0,72,31,0,30
question-answering,0,"We aim at learning S ( ) , with no human - labeled supervised data in the form ( question , answer ) pairs , but only by indirect supervision , generated either automatically or collaboratively .",related work,0,73,32,0,37
question-answering,0,We detail in the rest of this section our process for creating training data .,related work,0,74,33,0,15
question-answering,0,training data,related work,0,75,34,0,2
question-answering,0,"Our training data consists of two sources : an automatically created KB , Re - Verb , from which we generate questions and a set of pairs of questions collaboratively labeled as paraphrases from the website WikiAnswers .",related work,0,76,35,0,38
question-answering,0,knowledge base,related work,0,77,36,0,2
question-answering,0,The set of potential answers K is given by the KB ReVerb .,related work,0,78,37,0,13
question-answering,0,"ReVerb is an open - source data base composed of more than 14M triples , made of more than 2 M entities and 600 k relationships , which have been automatically extracted from the ClueWeb09 corpus .",related work,0,79,38,0,37
question-answering,0,"In the following , entities are denoted with a .e suffix and relationships with a .r suffix .",related work,0,80,39,0,18
question-answering,0,"ReVerb contains broad and general knowledge harvested with very little human intervention , which suits the realistically supervised setting .",related work,0,81,40,0,20
question-answering,0,"But , as a result , ReVerb is ambiguous and noisy with many useless triples and entities as well as numerous duplicates .",related work,0,82,41,0,23
question-answering,0,"For instance , winston - churchill.e , churchill.e and even roosevelt - and - churchill.e are all distinct entities .. 2 presents some examples of triples : some make sense , some others are completely unclear or useless .",related work,0,83,42,0,39
question-answering,0,"In contrast to highly curated data bases such Freebase , ReVerb has more noise but also many more relation types ( Freebase has around 20 k ) .",related work,0,84,43,0,28
question-answering,0,"So for some types of triple it has much better coverage , despite the larger size of Freebase ; for example Freebase does not cover verbs like afraid - of or suffer - from .",related work,0,85,44,0,35
question-answering,0,questions generation,related work,0,86,45,0,2
question-answering,0,"We have no available data of questions q labeled with their answers , i.e. with the corresponding triples t ?",related work,0,87,46,0,20
question-answering,0,k .,related work,0,88,47,0,2
question-answering,0,"Following , we hence decided to create such question - triple pairs automatically .",related work,0,89,48,0,14
question-answering,0,These pairs are generated using the 16 seed questions displayed in .,related work,0,90,49,0,12
question-answering,0,"At each round , we pick a triple at random and then generate randomly one of the seed questions .",related work,0,91,50,0,20
question-answering,0,"Note only triples with a *-in.r relation ( denoted r- in in ) can generate from the pattern where did er ? , for example , and similar for other constraints .",related work,0,92,51,0,32
question-answering,0,"Otherwise , the pattern is chosen randomly .",related work,0,93,52,0,8
question-answering,0,"Except for these exceptions , we used all 16 seed questions for all triples hence generating approximately 16 14M questions stored in a training set we denote D.",related work,0,94,53,0,28
question-answering,0,The generated questions are imperfect and noisy and create a weak training signal .,related work,0,95,54,0,14
question-answering,0,"Firstly , their syntactic structure is rather simplistic , and real questions as posed by humans ( such as in our actual test ) can look quite different to them .",related work,0,96,55,0,31
question-answering,0,"Secondly , many generated questions do not correspond to semantically valid English sentences .",related work,0,97,56,0,14
question-answering,0,"For instance , since the type of entities in ReVerb is unknown , a pattern like who does er ?",related work,0,98,57,0,20
question-answering,0,"can be chosen for a triple where the type of ? in ( ? , r , e ) is not a person , and similar for other types ( e.g. when ) .",related work,0,99,58,0,34
question-answering,0,"Besides , for the strings representing entities and relationships in the questions , we simply used their names in ReVerb , replacingby spaces and stripping off .",related work,0,100,59,0,27
question-answering,0,Patterns for generating questions from ReVerb triples following .,related work,0,101,60,0,9
question-answering,0,"their suffixes , i.e. the string representing winston - churchill.e is simply winston churchill .",related work,0,102,61,0,15
question-answering,0,"While this is often fine , this is also very limited and caused many incoherences in the data .",related work,0,103,62,0,19
question-answering,0,"Generating questions with a richer KB than ReVerb , such as Freebase or DBpedia , would lead to better quality because typing and better lexicons could be used .",related work,0,104,63,0,29
question-answering,0,"However , this would contradict one of our motivations which is to train a system with as little human intervention as possible ( and hence choosing ReVerb over hand - curated KBs ) .",related work,0,105,64,0,34
question-answering,0,paraphrases,related work,0,106,65,0,1
question-answering,0,The automatically generated examples are useful to connect KB triples and natural language .,related work,0,107,66,0,14
question-answering,0,"However , they do not allow for a satisfactory modeling of English language because of their poor wording .",related work,0,108,67,0,19
question-answering,0,"To overcome this issue , we again follow and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website .",related work,0,109,68,0,30
question-answering,0,"On WikiAnswers , users can tag pairs of questions as rephrasing of each other .",related work,0,110,69,0,15
question-answering,0,"harvested a set of 18 M of these question - paraphrase pairs , with 2.4 M distinct questions in the corpus .",related work,0,111,70,0,22
question-answering,0,These pairs have been labeled collaboratively .,related work,0,112,71,0,7
question-answering,0,This is cheap but also causes the data to be noisy .,related work,0,113,72,0,12
question-answering,0,"Hence , estimated that only 55 % of the pairs were actual paraphrases .",related work,0,114,73,0,14
question-answering,0,The set of paraphrases is denoted P in the following .,related work,0,115,74,0,11
question-answering,0,"By considering all words and tokens appearing in P and D , we end up with a size for the vocabulary V of more than 800k .",related work,0,116,75,0,27
question-answering,0,embedding - based model,related work,0,117,76,0,4
question-answering,0,"Our model ends up learning vector embeddings of symbols , either for entities or relationships from ReVerb , or for each word of the vocabulary .",related work,0,118,77,0,26
question-answering,0,question - kb triple scoring,related work,0,119,78,0,5
question-answering,0,architecture,related work,0,120,79,0,1
question-answering,0,"Our framework concerns the learning of a function S ( q , t ) , based on embeddings , that is designed to score the similarity of a question q and a triplet from K.",related work,0,121,80,0,35
question-answering,0,"Our scoring approach is inspired by previous work for labeling images with words , which we adapted , replacing images and labels by questions and triples .",related work,0,122,81,0,27
question-answering,0,"Intuitively , it consists of projecting questions , treated as a bag of words ( and possibly n-grams as well ) , on the one hand , and triples on the other hand , into a shared embedding space and then computing a similarity measure ( the dot product in this paper ) between both projections .",related work,0,123,82,0,57
question-answering,0,The scoring function is then :,related work,0,124,83,0,6
question-answering,0,"with f ( ) a function mapping words from questions into Contrary to previous work modeling KBs with embeddings ( e.g. ) , in our model , an entity does not have the same embedding when appearing in the lefthand or in the right - hand side of a triple .",related work,0,125,84,0,51
question-answering,0,"Since , g ( ) sums embeddings of all constituents of a triple , we need to use 2 embeddings per entity to encode for the fact that relationships in the KB are not symmetric and so that appearing as a left - hand or right - hand entity is different .",related work,0,126,85,0,52
question-answering,0,"This approach can be easily applied at test time to score any ( question , triple ) pairs .",related work,0,127,86,0,19
question-answering,0,"Given a question q , one can predict the corresponding answer ( a triple ) t ( q ) wit h:t ( q ) = arg max",related work,0,128,87,0,27
question-answering,0,Training by Ranking Previous work has shown that this kind of model can be conveniently trained using a ranking loss .,related work,0,129,88,0,21
question-answering,0,"Hence , given our data set D = { ( q i , ti ) , i = 1 , . . . , | D |} consisting of ( question , answer triple ) training pairs , one could learn the embeddings using constraints of the form :",related work,0,130,89,0,49
question-answering,0,where 0.1 is the margin .,related work,0,131,90,0,6
question-answering,0,"That is , we want the triple that labels a given question to be scored higher than other triples in K by a margin of 0.1 .",related work,0,132,91,0,27
question-answering,0,"We also enforce a constraint on the norms of the columns of V and W , i.e. ?",related work,0,133,92,0,18
question-answering,0,"i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ?",related work,0,134,93,0,19
question-answering,0,1 .,related work,0,135,94,0,2
question-answering,0,"To train our model , we need positive and negative examples of ( q , t ) pairs .",related work,0,136,95,0,19
question-answering,0,"However , D only contains positive samples , for which the triple actually corresponds to the question .",related work,0,137,96,0,18
question-answering,0,"Hence , during training , we use a procedure to corrupt triples .",related work,0,138,97,0,13
question-answering,0,"given ( q , t ) ?",related work,0,139,98,0,7
question-answering,0,"D , we create a corrupted triplet with the following method : pick another random triplet tmp from K , and then , replace with 66 % chance each member oft ( left entity , relationship and right entity ) by the corresponding element int tmp .",related work,0,140,99,0,47
question-answering,0,"This heuristic creates negative triples t somewhat similar to their positive counterpart t , and is similar to schemes of previous work ( e.g. in ) .",related work,0,141,100,0,27
question-answering,0,"Training the embedding model is carried out by stochastic gradient descent ( SGD ) , updating W and V at each step .",related work,0,142,101,0,23
question-answering,0,At the start of training the parameters off ( ) and g ( ) ( the n v k word embeddings in V and then e k entities and rel .,related work,0,143,102,0,31
question-answering,0,"embeddings in W ) are initialized to random weights ( mean 0 , standard deviation 1 k ) .",related work,0,144,103,0,19
question-answering,0,"Then , we iterate the following steps to train them :",related work,0,145,104,0,11
question-answering,0,"1 . Sample a positive training pair ( q i , ti ) from D .",related work,0,146,105,0,16
question-answering,0,2 .,related work,0,147,106,0,2
question-answering,0,Create a corrupted triplet i ensuring that ti = ti .,related work,0,148,107,0,11
question-answering,0,3 .,related work,0,149,108,0,2
question-answering,0,Make a stochastic gradient step to minimize 0.1 ? f ( q i ) g (t i ) +f ( q i ) g (t i ) + .,related work,0,150,109,0,29
question-answering,0,4 . Enforce the constraint that each embedding vector is normalized .,related work,0,151,110,0,12
question-answering,0,The learning rate of SGD is updated during the course of learning using adagrad .,related work,0,152,111,0,15
question-answering,0,x + is the positive part of x .,related work,0,153,112,0,9
question-answering,0,Multitask Training with Paraphrases Pairs,related work,0,154,113,0,5
question-answering,0,"We multitask the training of our model by training on pairs of paraphrases of questions ( q 1 , q 2 ) from P as well as training on the pseudolabeled data constructed in D .",related work,0,155,114,0,36
question-answering,0,We use the same architecture simply replacing g ( ) by a copy off ( ) .,related work,0,156,115,0,17
question-answering,0,This leads to the following function that scores the similarity between two questions :,related work,0,157,116,0,14
question-answering,0,"The matrix W containing embeddings of words is shared between Sand S prp , allowing it to encode information from examples from both D and P. Training of S prp is also conducted with SGD ( and adagrad ) as for S , but , in this case , negative examples are created by replacing one of the questions from the pair by another question chosen at random in P.",related work,0,158,117,0,70
question-answering,0,"During our experiments , W and V were learned by alternating training steps using Sand S prp , switching from one to another at each step .",related work,0,159,118,0,27
question-answering,0,The initial learning rate was set to 0.1 and the dimension k of the embedding space to 64 .,related work,0,160,119,0,19
question-answering,0,Training ran for 1 day on a 16 core machine using hogwild .,related work,0,161,120,0,13
question-answering,0,Fine - tuning the Similarity between Embeddings,related work,0,162,121,0,7
question-answering,0,"The scale of the problem forced us to keep our architecture simple : with n e ? 3.5M ( with 2 embeddings for each entity ) and n v ? 800 k , we have to learn around 4.3 M embeddings .",related work,0,163,122,0,42
question-answering,0,"With an embedding space of dimension k = 64 , this leads to around 275M parameters to learn .",related work,0,164,123,0,19
question-answering,0,The training algorithm must also stay simple to scale on a training set of around 250 M of examples ( D and P combined ) ; SGD appears as the only viable option .,related work,0,165,124,0,34
question-answering,0,"SGD , combined with adagrad for adapting the learning rate on the course of training , is a powerful algorithm .",related work,0,166,125,0,21
question-answering,0,"However , the scale of the optimization problem makes it very hard to control and conduct properly until convergence .",related work,0,167,126,0,20
question-answering,0,"When SGD stops after a pre-defined number of epochs , we are almost certain that the problem is not fully solved and that some room for improvement remains : we observed that embeddings were able to often rank correct answers near the top of the candidates list , but not always in the first place .",related work,0,168,127,0,56
question-answering,0,"In this paper , we introduce away to fine - tune our embedding - based model so that correct answers might end up more often at the top of the list .",related work,0,169,128,0,32
question-answering,0,"Updating the embeddings involves working on too many parameters , but ultimately , these embeddings are meant to be used in a dot -product that computes the similarity between q and t.",related work,0,170,129,0,32
question-answering,0,We propose to learn a matrix M ?,related work,0,171,130,0,8
question-answering,0,R kk parameterizing the similarity between words and triples embeddings .,related work,0,172,131,0,11
question-answering,0,the scoring function becomes :,related work,0,173,132,0,5
question-answering,0,M has only k 2 parameters and can be efficiently determined by solving the following convex problem ( fixing the embedding matrices W and V ) :,related work,0,174,133,0,27
question-answering,0,where X F is the Frobenius norm of X .,related work,0,175,134,0,10
question-answering,0,We solve this problem in a few minutes using L - BFGS on a subset of m = 10 M examples from D .,related work,0,176,135,0,24
question-answering,0,We first use 4 M examples to train and 6M as validation set to determine the value of the regularization parameter ?.,related work,0,177,136,0,22
question-answering,0,"We then retrain the model on the whole 10M examples using the selected value , which happened to be ? = 1.7 10 ?5 .",related work,0,178,137,0,25
question-answering,0,"This fine - tuning is related to learning a new metric in the embedding space , but since the resulting M is not symmetric , it does not define a dot-product .",related work,0,179,138,0,32
question-answering,0,"Still , M is close to a constant factor times identity ( as in the original score S ( ) ) .",related work,0,180,139,0,22
question-answering,0,"The fine - tuning does not deeply alter the ranking , but , as expected , allows for a slight change in the triples ranking , which ends in consistent improvement in performance , as we show in the experiments .",related work,0,181,140,0,41
question-answering,0,experiments,experiment,0,182,1,0,1
question-answering,0,evaluation protocols,experiment,0,183,2,0,2
question-answering,0,We first detail the data and metrics which were chosen to assess the quality of our embedding model .,experiment,0,184,3,0,19
question-answering,0,test set,experiment,0,185,4,0,2
question-answering,0,The data set WikiAnswers + ReVerb contains no labeled examples but some are needed for evaluating models .,experiment,0,186,5,0,18
question-answering,0,"We used the test set which has been created by in the following way : ( 1 ) they identified 37 questions from a heldout portion of WikiAnswers which were likely to have at least one answer in ReVerb , ( 2 ) they added all valid paraphrases of these questions to obtain a set of 691 questions , ( 3 ) they ran various versions of their paralex system on them to gather candidate triples ( for a total of 48 k ) , which they finally hand - labeled .",experiment,0,187,6,0,92
question-answering,0,reranking,experiment,1,188,7,0,1
question-answering,0,We first evaluated different versions of our model against the paralex system in a reranking setting .,experiment,0,189,8,0,17
question-answering,0,"For each question q from the WikiAn - swers + ReVerb test set , we take the provided candidate triples t and rerank them by sorting by the score S ( q , t ) or S ft ( q , t ) of our model , depending whether we use fine - tuning or not .",experiment,0,190,9,0,57
question-answering,0,"As in , we then compute the precision , recall and F1 -score of the highest ranked answer as well as the mean average precision ( MAP ) of the whole output , which measures the average precision over all levels of recall .",experiment,0,191,10,0,44
question-answering,0,full ranking,experiment,0,192,11,0,2
question-answering,0,"We hence decided to filter out some candidates before ranking by using a simple string matching strategy : after pos-tagging the question , we construct a set of candidate strings containing ( i ) all noun phrases that appear less than 1,000 .",experiment,0,193,12,0,43
question-answering,0,Examples of nearest neighboring entities and relationships from REVERB for some words from our vocabulary .,experiment,0,194,13,0,16
question-answering,0,"the prefix l : , resp.",experiment,0,195,14,0,6
question-answering,0,"R : , indicates the embedding of an entity when appearing in left - hand side , resp. right - hand side , of triples .",experiment,0,196,15,0,26
question-answering,0,results,result,0,197,1,0,1
question-answering,0,This section now discusses our empirical performance .,result,0,198,2,0,8
question-answering,0,Reranking and present the results of the reranking experiments .,result,0,199,3,0,10
question-answering,0,"We compare various versions of our model against two versions of paralex , whose results were given in .",result,0,200,4,0,19
question-answering,0,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .",result,1,201,5,0,21
question-answering,0,"Paraphrases allow for the embeddings to encode a richer connection between KB constituents and words , as well as between words themselves .",result,0,202,6,0,23
question-answering,0,"Note that the WikiAnswers data provides word alignment between paraphrases , which we did not use , unlike paralex .",result,0,203,7,0,20
question-answering,0,"We also tried to use n-grams ( 2.5 M most frequent ) as well as the words to represent the question , but this did not bring any improvement , which might at first seem counter - intuitive .",result,0,204,8,0,39
question-answering,0,"We believe this is due to two factors : it is hard to learn good embeddings for n-grams since their frequency is usually very low and ( 2 ) our automatically generated questions have a poor syntax and hence , many n-grams in this data set do not make sense .",result,0,205,9,0,51
question-answering,0,"We actually conducted experiments with several variants of our model , which tried to take the word ordering into account ( e.g. with convolutions ) , and they all failed to outperform our best performance without word order , once again perhaps because the supervision is not clean enough to allow for such elaborated language modeling .",result,0,206,10,0,57
question-answering,0,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,result,1,207,11,0,35
question-answering,0,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .",result,1,208,12,0,38
question-answering,0,paralex works by starting with an initial lexicon mapping from the KB to language and then gradually increasing its coverage by iterating on the WikiAnswers + ReVerb data .,result,0,209,13,0,29
question-answering,0,Most of its predictions come from automatically acquired templates and rules : this allows for a good precision but it is not flexible enough across language variations to grant a satisfying recall .,result,0,210,14,0,33
question-answering,0,Most of our improvement comes from a much better recall .,result,0,211,15,0,11
question-answering,0,"However , as we said earlier , this reranking setting is detrimental for paralex because paralex was evaluated on the task of reranking some of its own predictions .",result,0,212,16,0,29
question-answering,0,"The results provided for paralex , while not corresponding to those of a full ranking among all triples from ReVerb ( it is still reranking among a subset of candidates ) , concerns an evaluation setting more complicated than for our model .",result,0,213,17,0,43
question-answering,0,"Hence , we also display the results of a full ranking by our system in the following .",result,0,214,18,0,18
question-answering,0,and display the results of our model to rank all 14 M triples from ReVerb .,result,0,215,19,0,16
question-answering,0,The performance of the plain models is not good ( F1 = 0.22 only for S ft ) because the ranking is degraded by too many candidates .,result,0,216,20,0,28
question-answering,0,But most of these can be discarded beforehand .,result,0,217,21,0,9
question-answering,0,word,result,0,218,22,0,1
question-answering,0,Closest entities or relationships from ReVerb in the embedding space get rid of get -rid - of.r be -get - rid - of.r rid-of.r can -get - rid - of.r will - get - rid - of.r should - get - rid - of.r have - to - get - rid - of.r want - to - get- rid- of.r will - not - get - rid-of.r,result,0,219,23,0,67
question-answering,0,help- get-rid-of.r useful be-useful - for.r be-useful - in.r,result,0,220,24,0,9
question-answering,0,R:wide-range-of-application.e can-be-useful-for.r,result,0,221,25,0,2
question-answering,0,be-use-extensively-for. r be-not-very-useful-for.r,result,0,222,26,0,3
question-answering,0,R:plex-or-technical-algorithm.e,result,0,223,27,0,1
question-answering,0,R:internal-and-external-use.e,result,0,224,28,0,1
question-answering,0,r:authoring.e,result,0,225,29,0,1
question-answering,0,R:good- or- bad- purpose.e radiation R:radiation.e,result,0,226,30,0,6
question-answering,0,l:radiation.e,result,0,227,31,0,1
question-answering,0,r:gamma-radiation.e,result,0,228,32,0,1
question-answering,0,l:gamma- radiation.e,result,0,229,33,0,2
question-answering,0,L:x - ray.e L:gamma - ray .,result,0,230,34,0,7
question-answering,0,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .",result,0,231,35,0,22
question-answering,0,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .",result,0,232,36,0,38
question-answering,0,Embeddings displays some examples of nearest neighboring entities from ReVerb for some words from our vocabulary .,result,0,233,37,0,17
question-answering,0,"As expected , we can see that verbs or adverbs tend to correspond to relationships while nouns refer to entities .",result,0,234,38,0,21
question-answering,0,"Interestingly , the model learns some synonymy and hyper / hyponymy .",result,0,235,39,0,12
question-answering,0,"For instance , radiation is close to x - ray.e and iphone to smartphone .e.",result,0,236,40,0,15
question-answering,0,"This happens thanks to the multitasking with paraphrase data , since in our automatically generated ( q , t ) pairs , the words radiation and iphone are only used for entities with the strings radiation and iphone respectively in their names .",result,0,237,41,0,43
question-answering,0,evaluation on webquestions,result,0,238,42,0,3
question-answering,0,Our initial objective was to be able to perform open - domain question answering .,result,0,239,43,0,15
question-answering,0,"In this last experimental section , we tend to evaluate how generic our learned system is .",result,0,240,44,0,17
question-answering,0,"To this end , we propose to ask our model to answer questions coming from another dataset from the literature , but without retraining it with labeled data , just by directly using the parameters learned on WikiAnswers + ReVerb .",result,0,241,45,0,41
question-answering,0,"We chose the data set WebQuestions , which consists of natural language questions matched with answers corresponding to entities of Freebase : in this case , no triple has to be returned , only a single entity .",result,0,242,46,0,38
question-answering,0,"We used exact string matching to find the ReVerb entities corresponding to the Freebase answers from the test set of WebQuestions and obtained 1,538 questions labeled with ReVerb out of the original 2,034 .",result,0,243,47,0,34
question-answering,0,Results of different versions of our model are displayed in .,result,0,244,48,0,11
question-answering,0,"For each test question , we record the rank of the first ReVerb triple containing the answer entity .",result,0,245,49,0,19
question-answering,0,"Top - 1 and Top - 10 are computed on questions for which the system returned at least one answer ( around 1,000 questions using string matching ) , while F1 is computed for all questions .",result,0,246,50,0,37
question-answering,0,"Of course , performance is not great and can not be directly compared with that of the best system reported in ( more than 0.30 of F1 ) .",result,0,247,51,0,29
question-answering,0,"One of the main reasons is that most questions of WebQuestions , such as Who was vice - president after Kennedy died ? , should be represented by multiple triples , a setting for which our system has not been designed .",result,0,248,52,0,42
question-answering,0,"Still , for a system trained with almost no manual annotation nor prior information on another dataset , with another - very noisy - KB , the results can be seen as particularly promising .",result,0,249,53,0,35
question-answering,0,"Besides , evaluation is broad since , in ReVerb , most entities actually appear many times under different names as explained in Section 3 .",result,0,250,54,0,25
question-answering,0,"Hence , there might be higher ranked answers but they are missed by our evaluation script .",result,0,251,55,0,17
question-answering,0,conclusion,result,0,252,56,0,1
question-answering,0,This paper introduces a new framework for learning to perform open question answering with very little supervision .,result,0,253,57,0,18
question-answering,0,"Using embeddings as its core , our approach can be successfully trained on imperfect labeled data and indirect supervision and significantly outperforms previous work for answering simple factual questions .",result,0,254,58,0,30
question-answering,0,"Besides , we introduce a new way to fine - tune embedding models for cases where their optimization problem can not be completely solved .",result,0,255,59,0,25
question-answering,0,"In spite of these promising results , some exciting challenges remain , especially in order to scale up this model to questions with more complex semantics .",result,0,256,60,0,27
question-answering,0,"Due to the very low supervision signal , our work can only answer satisfactorily simple factual questions , and does not even take into account the word ordering when modeling them .",result,0,257,61,0,32
question-answering,0,"Further , much more work has to be carried out to encode the semantics of more complex questions into the embedding space .",result,0,258,62,0,23
relation-classification,8,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,title,1,2,1,0,11
relation-classification,8,abstract,abstract,0,3,1,0,1
relation-classification,8,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,abstract,1,4,2,0,30
relation-classification,8,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",abstract,1,5,3,0,47
relation-classification,8,Our solution is built on top of the pre-trained self - attentive models ( Transformer ) .,abstract,0,6,4,0,17
relation-classification,8,"Since our method uses a single - pass to compute all relations at once , it scales to larger datasets easily ; which makes it more usable in real - world applications .",abstract,0,7,5,0,33
relation-classification,8,1,abstract,0,8,6,0,1
relation-classification,8,introduction,introduction,0,9,1,0,1
relation-classification,8,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,introduction,1,10,2,0,22
relation-classification,8,"A solution to this task is essential for many downstream NLP applications such as automatic knowledge - base completion , knowledge base question answering , and symbolic approaches for visual question answering , etc .",introduction,0,11,3,0,35
relation-classification,8,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,introduction,1,12,4,0,29
relation-classification,8,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",introduction,0,13,5,0,32
relation-classification,8,"However , nearly all existing approaches for MRE tasks 2014 ; adopt some variations of the singlerelation extraction ( SRE ) approach , which treats each pair of entity mentions as an independent instance , and requires multiple passes of encoding for the multiple pairs of entities .",introduction,0,14,6,0,48
relation-classification,8,"The drawback of this approach is obvious - it is computationally expensive and this issue becomes more severe when the input paragraph is large , making this solution impossible to implement when the encoding step involves deep models .",introduction,0,15,7,0,39
relation-classification,8,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .",introduction,0,16,8,0,34
relation-classification,8,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .",introduction,0,17,9,0,23
relation-classification,8,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .",introduction,0,18,10,0,31
relation-classification,8,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,introduction,0,19,11,0,50
relation-classification,8,"To the best of our knowledge , this work is the first promising solution that can solve MRE tasks with such high efficiency ( encoding the input in one - pass ) and effectiveness ( achieve a new state - of - the - art performance ) , as proved on the ACE 2005 benchmark .",introduction,0,20,12,0,56
relation-classification,8,background,introduction,0,21,13,0,1
relation-classification,8,MRE is an important task as it is an essential prior step for many downstream tasks such as automatic knowledge - base completion and questionanswering .,introduction,0,22,14,0,26
relation-classification,8,Popular MRE benchmarks include ACE and ERE .,introduction,0,23,15,0,8
relation-classification,8,"In MRE , given as a text paragraph x = {x 1 , . . . , x N } and M mentions e = {e 1 , . . . , e M } as input , the goal is to predict the relation r ij for each mention pair ( e i , e j ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation .",introduction,0,24,16,0,81
relation-classification,8,"This paper uses "" entity mention "" , "" mention "" and "" entity "" interchangeably .",introduction,0,25,17,0,17
relation-classification,8,"Existing MRE approaches are based on either feature and model architecture selection techniques , or domain adaptations approaches .",introduction,0,26,18,0,19
relation-classification,8,"But these approaches require multiple passes of encoding over the paragraph , as they treat a MRE task as multiple passes of a SRE task .",introduction,0,27,19,0,26
relation-classification,8,proposed approach,introduction,0,28,20,0,2
relation-classification,8,This section describes the proposed one - pass encoding MRE solution .,introduction,1,29,21,0,12
relation-classification,8,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .",introduction,1,30,22,0,48
relation-classification,8,The framework is illustrated in 1 .,introduction,0,31,23,0,7
relation-classification,8,"It is worth mentioning that our solution can easily use other transformer - based encoders besides BERT , e.g..",introduction,0,32,24,0,19
relation-classification,8,Structured Prediction with BERT for MRE,introduction,0,33,25,0,6
relation-classification,8,The BERT model has been successfully applied to various NLP tasks .,introduction,0,34,26,0,12
relation-classification,8,"However , the final prediction layers used in the original model is not applicable to MRE tasks .",introduction,0,35,27,0,18
relation-classification,8,The MRE task essentially requires to perform edge predictions over a graph with entities as nodes .,introduction,0,36,28,0,17
relation-classification,8,"Inspired by , we propose that we can first encode the input paragraph using BERT .",introduction,0,37,29,0,16
relation-classification,8,"Thus , the representation for a pair of entity mentions ( e i , e j ) can be denoted as oi and o j respectively .",introduction,0,38,30,0,27
relation-classification,8,"In the case of a mention e i consist of multiple hidden states ( due to the byte pair encoding ) , oi is aggregated via average - pooling over the hidden states of the corresponding tokens in the last BERT layer .",introduction,0,39,31,0,43
relation-classification,8,"We then concatenate oi and o j denoted as [ o i : o j ] , and pass it to a linear classifier 2 to predict the relation",introduction,0,40,32,0,29
relation-classification,8,where w l ?,introduction,0,41,33,0,4
relation-classification,8,r 2 dzl .,introduction,0,42,34,0,4
relation-classification,8,"dz is the dimension of BERT embedding at each token position , and l is the number of relation labels .",introduction,0,43,35,0,21
relation-classification,8,Entity - Aware Self - Attention based on Relative Distance,introduction,0,44,36,0,10
relation-classification,8,This section describes how we encode multiplerelations information into the model .,introduction,0,45,37,0,12
relation-classification,8,The key concept is to use the relative distances between words and entities to encode the positional information for each entity .,introduction,0,46,38,0,22
relation-classification,8,This information is propagated through different layers via attention computations .,introduction,0,47,39,0,11
relation-classification,8,"Following , for each pair of word tokens ( x i , x j ) with the input representations from the previous layer ash i and h j , we extend the computation of self - attention z i as :",introduction,0,48,40,0,41
relation-classification,8,"are the parameters of the model , and dz is the dimension of the output from the self - attention layer .",introduction,0,49,41,0,22
relation-classification,8,"Compared to standard BERT 's self - attention , a V ij , a K ij ?",introduction,0,50,42,0,17
relation-classification,8,"R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",introduction,0,51,43,0,23
relation-classification,8,"Specifically , we devise a V ij and a K ij to encourage each token to be aware of the relative distance to different entity mentions , and vice versa .",introduction,0,52,44,0,31
relation-classification,8,"Adapted from , we argue that the relative distance information will not help if the distance is beyond a certain threshold .",introduction,0,53,45,0,22
relation-classification,8,Hence we first define the distance function as :,introduction,0,54,46,0,9
relation-classification,8,"This distance definition clips all distances to a region [?k , k].",introduction,0,55,47,0,12
relation-classification,8,k is a hyper - parameter to be tuned on the development set .,introduction,0,56,48,0,14
relation-classification,8,We can now define a V ij and a K ij formally as :,introduction,0,57,49,0,14
relation-classification,8,"As defined above , if either token xi or x j belongs to an entity , we will introduce a relative positional representation according to their distance .",introduction,0,58,50,0,28
relation-classification,8,The distance is defined in an entity - centric way as we always compute the distance from the entity mention to the other token .,introduction,0,59,51,0,25
relation-classification,8,"If neither xi nor x j are entity mentions , we explicitly assign a zero vector to a K ij and a V ij .",introduction,0,60,52,0,25
relation-classification,8,"When both xi and x j are inside entity mentions , we take the distance as d ( i , j ) to make row - wise attention computation coherent as depicted in .",introduction,0,61,53,0,34
relation-classification,8,"During the model fine - tuning , the newly introduced parameters {w K ?k , ... , w K k } and {w V ?k , ... , w V k } are trained from scratch .",introduction,0,62,54,0,37
relation-classification,8,experiments,experiment,0,63,1,0,1
relation-classification,8,"We demonstrate the advantage of our method on a popular MRE benchmark , ACE 2005 , and a more recent MRE benchmark , SemEval 2018 Task 7 .",experiment,0,64,2,0,28
relation-classification,8,"We also evaluate on a commonly used SRE benchmark SemEval 2010 task 8 , and achieve state - of - the - art performance .",experiment,0,65,3,0,25
relation-classification,8,settings,experiment,0,66,4,0,1
relation-classification,8,"Data For ACE 2005 , we adopt the multi-domain setting and split the data following : we train on the union of news domain ( nw and bn ) , tune hyperparameters on half of the broadcast conversation ( bc ) domain , and evaluate on the remainder of broadcast conversation ( bc ) , the telephone speech ( cts ) , usenet newsgroups ( un ) , and weblogs ( wl ) domains .",experiment,0,67,5,0,75
relation-classification,8,for se-m,experiment,0,68,6,0,2
relation-classification,8,eval 2018,experiment,0,69,7,0,2
relation-classification,8,"Task 7 , we evaluate on its sub-task 1.1 .",experiment,0,70,8,0,10
relation-classification,8,We use the same data split in the shared task .,experiment,0,71,9,0,11
relation-classification,8,The passages in this task is usually much longer compared to ACE .,experiment,0,72,10,0,13
relation-classification,8,"Therefore we adopt the following pre-processing step - for the entity pair in each relation , we assume the tokens related to their relation labeling are always within a range from the fifth token ahead of the pair to the fifth token after it .",experiment,0,73,11,0,45
relation-classification,8,"Therefore , the tokens in the original passage thatare not covered by the range of ANY input relations , will be removed from the input .",experiment,0,74,12,0,26
relation-classification,8,methods,method,0,75,1,0,1
relation-classification,8,"We compare our solution with previous works that predict a single relation per pass , our model that predicts single relation per pass for MRE , and with the following naive modifications of BERT that could achieve MRE in one - pass .",method,1,76,2,0,43
relation-classification,8,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .",method,1,77,3,0,16
relation-classification,8,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .",method,1,78,4,0,19
relation-classification,8,BERT SP with position embedding on the final attention layer .,method,1,79,5,0,11
relation-classification,8,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,method,0,80,6,0,21
relation-classification,8,"In this method , the BERT model encode the paragraph to the last attention - layer .",method,1,81,7,0,17
relation-classification,8,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .",method,1,82,8,0,34
relation-classification,8,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )",method,0,83,9,0,26
relation-classification,8,results on ace 2005,result,1,84,1,0,4
relation-classification,8,Main Results gives the over all results on ACE 2005 .,result,0,85,2,0,11
relation-classification,8,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,result,1,86,3,0,25
relation-classification,8,"Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .",result,0,87,4,0,20
relation-classification,8,This result further demonstrates its effectiveness .,result,0,88,5,0,7
relation-classification,8,"Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .",result,0,89,6,0,34
relation-classification,8,"The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .",result,0,90,7,0,56
relation-classification,8,Summing up the vectors confuses this information .,result,0,91,8,0,8
relation-classification,8,"It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .",result,0,92,9,0,23
relation-classification,8,prior state - of - the - art level of the methods without domain adaptation .,result,0,93,10,0,16
relation-classification,8,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .",result,1,94,11,0,51
relation-classification,8,It also beats the other two methods on BERT in Multi- Relation per Pass .,result,0,95,12,0,15
relation-classification,8,Performance Gap between MRE in One - Pass and Multi - Pass,method,0,96,1,0,12
relation-classification,8,The MRE - in - one - pass models can also be used to train and test with one entity pair per pass ( Single - Relation per Pass results in ) .,method,0,97,2,0,33
relation-classification,8,"Therefore , we compare the same methods when applied to the multi-relation and singlerelation settings .",method,0,98,3,0,16
relation-classification,8,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .",method,0,99,4,0,33
relation-classification,8,A 2 % gap is observed as expected .,method,0,100,5,0,9
relation-classification,8,"By comparison , our full model has a much smaller performance gap between two different settings ( and no consistent performance drop over different domains ) .",method,0,101,6,0,27
relation-classification,8,The BERT SP is not expected to have a gap as shown in the table .,method,0,102,7,0,16
relation-classification,8,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .",method,0,103,8,0,36
relation-classification,8,training and inference time,method,0,104,9,0,4
relation-classification,8,"Through our experiment , 4 we verify that the full model with MRE is significantly faster compared to all other methods for both training and inference .",method,0,105,10,0,27
relation-classification,8,The training time for full model with MRE is 3.5 x faster than it with SRE .,method,0,106,11,0,17
relation-classification,8,"As for inference speed , the former could reach 126 relation per second compared the later at 23 relation per second .",method,0,107,12,0,22
relation-classification,8,"It is also much faster when compared to the second best performing approach , BERT SP w / pos-emb on final attlayer , which is at 76 relation per second , as it runs the last layer for every entity pair .",method,0,108,13,0,42
relation-classification,8,"evaluates the usage of different prediction layers , including replacing our linear layer in Eq .",method,0,109,14,0,16
relation-classification,8,prediction module selection,method,0,110,15,0,3
relation-classification,8,( 1 ) with MLP or Biaff .,method,0,111,16,0,8
relation-classification,8,Results show that the usage of the linear predictor gives better results .,method,0,112,17,0,13
relation-classification,8,This is consistent with the motivation of the pre-trained encoders : by unsupervised pre-training the encoders are expected to be sufficiently powerful thus adding more complex layers on top does not improve the capacity but leads to more free parameters and higher risk of over-fitting .,method,0,113,18,0,46
relation-classification,8,results on semeval 2018,result,0,114,1,0,4
relation-classification,8,task 7,result,0,115,2,0,2
relation-classification,8,The results on SemEval 2018 Task 7 are shown in .,result,1,116,3,0,11
relation-classification,8,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .",result,1,117,4,0,43
relation-classification,8,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .",result,1,118,5,0,30
relation-classification,8,"Note that the system ( Rotsztejn et al. , 2018 ) integrates many techniques like feature - engineering , model combination , pretraining embeddings on in - domain data , and artificial data generation , while our model is almost a direct adaption from the ACE architecture .",result,0,119,6,1,48
relation-classification,8,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .",result,1,120,7,0,38
relation-classification,8,additional sre,result,0,121,8,0,2
relation-classification,8,results,result,0,122,1,0,1
relation-classification,8,"We conduct additional experiments on the relation classification task , SemEval 2010 Task 8 , to com -",result,0,123,2,0,18
relation-classification,8,method,method,0,124,1,0,1
relation-classification,8,averaged f1 macro micro,method,0,125,2,0,4
relation-classification,8,Top 3 in the Shared Task 81.7 82.8 78.9 - 76 . pare with models developed on this benchmark .,method,0,126,3,0,20
relation-classification,8,"From the results in , our proposed techniques also outperforms the state - of - the - art on this single - relation benchmark .",method,0,127,4,0,25
relation-classification,8,"On this single relation task , the out - of - box BERT achieves a reasonable result after finetuning .",method,0,128,5,0,20
relation-classification,8,"Adding the entity - aware attention gives about 8 % improvement , due to the availability of the entity information during encoding .",method,0,129,6,0,23
relation-classification,8,"Adding structured prediction layer to BERT ( i.e. , BERT SP ) also leads to a similar amount of improvement .",method,0,130,7,0,21
relation-classification,8,"However , the gap between BERT SP method with and without entity - aware attention is small .",method,0,131,8,0,18
relation-classification,8,"This is likely because of the bias of data distribution : the assumption that only two target entities exist , makes the two techniques have similar effects .",method,0,132,9,0,28
relation-classification,8,conclusion,method,0,133,10,0,1
relation-classification,8,"In summary , we propose a first - of - its - kind solution that can simultaneously extract multiple relations with one - pass encoding of an input paragraph for MRE tasks .",method,0,134,11,0,33
relation-classification,8,"With the proposed structured prediction and entity - aware self - attention layers on top of BERT , we achieve a new state - of - the - art results with high efficiency on the ACE 2005 benchmark .",method,0,135,12,0,39
relation-classification,8,"Our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction , e.g. , entity - centric passage encoding in question answering .",method,0,136,13,0,28
relation-classification,8,"In the future work , we will explore the usage of this method with other applications .",method,0,137,14,0,17
relation-classification,9,SCIBERT : A Pretrained Language Model for Scientific Text,title,1,2,1,0,9
relation-classification,9,abstract,abstract,0,3,1,0,1
relation-classification,9,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,1,4,2,0,18
relation-classification,9,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",abstract,1,5,3,1,34
relation-classification,9,SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks .,abstract,0,6,4,0,21
relation-classification,9,"We evaluate on a suite of tasks including sequence tagging , sentence classification and dependency parsing , with datasets from a variety of scientific domains .",abstract,0,7,5,0,26
relation-classification,9,We demonstrate statistically significant improvements over BERT and achieve new state - of - theart results on several of these tasks .,abstract,0,8,6,0,22
relation-classification,9,The code and pretrained models are available at https://github.com/allenai/scibert/.,abstract,1,9,7,0,9
relation-classification,9,introduction,introduction,0,10,1,0,1
relation-classification,9,The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these documents .,introduction,0,11,2,0,32
relation-classification,9,"Recent progress in NLP has been driven by the adoption of deep neural models , but training such models often requires large amounts of labeled data .",introduction,0,12,3,0,27
relation-classification,9,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",introduction,1,13,4,0,39
relation-classification,9,"As shown through ELMo , and BERT , unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks .",introduction,0,14,5,0,24
relation-classification,9,These models return contextualized embeddings for each token which can be passed into minimal task - specific neural architectures .,introduction,0,15,6,0,20
relation-classification,9,"Leveraging the success of unsupervised pretraining has become especially important especially when task - specific annotations are difficult to obtain , like in scientific NLP .",introduction,0,16,7,0,26
relation-classification,9,"Yet while both BERT and ELMo have released pretrained models , they are still trained on general domain corpora such as news articles and Wikipedia .",introduction,0,17,8,0,26
relation-classification,9,"In this work , we make the following contributions :",introduction,0,18,9,0,10
relation-classification,9,"( i ) We release SCIBERT , a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain .",introduction,0,19,10,0,25
relation-classification,9,SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text .,introduction,0,20,11,0,19
relation-classification,9,"( ii ) We perform extensive experimentation to investigate the performance of finetuning versus task - specific architectures atop frozen embeddings , and the effect of having an in - domain vocabulary .",introduction,0,21,12,0,33
relation-classification,9,"( iii ) We evaluate SCIBERT on a suite of tasks in the scientific domain , and achieve new state - of the - art ( SOTA ) results on many of these tasks .",introduction,0,22,13,0,35
relation-classification,9,methods,method,0,23,1,0,1
relation-classification,9,background,method,0,24,2,0,1
relation-classification,9,The BERT model architecture is based on a multilayer bidirectional Transformer .,method,0,25,3,0,12
relation-classification,9,"Instead of the traditional left - to - right language modeling objective , BERT is trained on two tasks : predicting randomly masked tokens and predicting whether two sentences follow each other .",method,0,26,4,0,33
relation-classification,9,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,method,1,27,5,0,17
relation-classification,9,Vocabulary BERT uses WordPiece for unsupervised tokenization of the input text .,method,0,28,6,0,12
relation-classification,9,The vocabulary is built such that it contains the most frequently used words or subword units .,method,0,29,7,0,17
relation-classification,9,We refer to the original vocabulary released with BERT as BASEVOCAB .,method,0,30,8,0,12
relation-classification,9,"We construct SCIVOCAB , a new WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .",method,1,31,9,0,20
relation-classification,9,We produce both cased and uncased vocabularies and set the vocabulary size to 30 K to match the size of BASEVOCAB .,method,0,32,10,0,22
relation-classification,9,"The resulting token overlap between BASEVOCAB and SCIVOCAB is 42 % , illustrating a substantial difference in frequently used words between scientific and general domain texts .",method,0,33,11,0,27
relation-classification,9,corpus,method,1,34,12,0,1
relation-classification,9,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,method,1,35,13,0,15
relation-classification,9,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,method,1,36,14,0,21
relation-classification,9,"We use the full text of the papers , not just the abstracts .",method,0,37,15,0,14
relation-classification,9,"The average paper length is 154 sentences ( 2,769 tokens ) resulting in a corpus size of 3.17B tokens , similar to the 3.3B tokens on which BERT was trained .",method,0,38,16,0,31
relation-classification,9,"We split sentences using Scispa Cy , 2 which is optimized for scientific text .",method,0,39,17,0,15
relation-classification,9,3 experimental setup,experiment,0,40,1,0,3
relation-classification,9,tasks,experiment,0,41,2,0,1
relation-classification,9,We experiment on the following core NLP tasks :,experiment,0,42,3,0,9
relation-classification,9,1 .,experiment,0,43,4,0,2
relation-classification,9,named entity recognition ( ner ),experiment,1,44,5,0,6
relation-classification,9,2 . pico extraction ( pico ),experiment,1,45,6,0,7
relation-classification,9,3 . text classification ( cls ),experiment,1,46,7,0,7
relation-classification,9,4 . relation classification ( rel ),experiment,1,47,8,0,7
relation-classification,9,5 . dependency parsing ( dep ),experiment,1,48,9,0,7
relation-classification,9,"PICO , like NER , is a sequence labeling task where the model extracts spans describing the Participants , Interventions , Comparisons , and Outcomes in a clinical trial paper .",experiment,0,49,10,0,31
relation-classification,9,"REL is a special case of text classification where the model predicts the type of relation expressed between two entities , which are encapsulated in the sentence by inserted special tokens .",experiment,0,50,11,0,32
relation-classification,9,datasets,experiment,0,51,12,0,1
relation-classification,9,"For brevity , we only describe the newer datasets here , and refer the reader to the references in Table 1 for the older datasets .",experiment,0,52,13,0,26
relation-classification,9,EBM - NLP annotates PICO spans in clinical trial abstracts .,experiment,0,53,14,0,11
relation-classification,9,SciERC annotates entities and relations from computer science ab - 1 https://github.com/google/sentencepiece,experiment,0,54,15,0,12
relation-classification,9,"2 https://github.com/allenai/SciSpaCy stracts . ACL - ARC and Sci -Cite assign intent labels ( e.g. Comparison , Extension , etc. ) to sentences from scientific papers that cite other papers .",experiment,0,55,16,0,31
relation-classification,9,The Paper Field dataset is built from the Microsoft Academic Graph 3 and maps paper titles to one of 7 fields of study .,experiment,0,56,17,0,24
relation-classification,9,"Each field of study ( i.e. geography , politics , economics , business , sociology , medicine , and psychology ) has approximately 12 K training examples .",experiment,0,57,18,0,28
relation-classification,9,pretrained bert,experiment,0,58,19,0,2
relation-classification,9,variants,experiment,0,59,20,0,1
relation-classification,9,bert - base,experiment,0,60,21,0,3
relation-classification,9,We use the pretrained weights for BERT - Base released with the original BERT code .,experiment,0,61,22,0,16
relation-classification,9,The vocabulary is BASE - VOCAB .,experiment,0,62,23,0,7
relation-classification,9,We evaluate both cased and uncased versions of this model .,experiment,0,63,24,0,11
relation-classification,9,scibert,experiment,0,64,25,0,1
relation-classification,9,We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT - Base .,experiment,0,65,26,0,23
relation-classification,9,We train 4 different versions of SCIBERT : ( i ) cased or uncased and ( ii ) BASEVOCAB or SCIVOCAB .,experiment,0,66,27,0,22
relation-classification,9,The two models that use BASEVOCAB are finetuned from the corresponding BERT - Base models .,experiment,0,67,28,0,16
relation-classification,9,The other two models that use the new SCIVOCAB are trained from scratch .,experiment,0,68,29,0,14
relation-classification,9,Pretraining BERT for long sentences can be slow .,experiment,0,69,30,0,9
relation-classification,9,"Following the original BERT code , we set a maximum sentence length of 128 tokens , and train the model until the training loss stops decreasing .",experiment,0,70,31,0,27
relation-classification,9,We then continue training the model allowing sentence lengths up to 512 tokens .,experiment,0,71,32,0,14
relation-classification,9,We use a single TPU v 3 with 8 cores .,experiment,0,72,33,0,11
relation-classification,9,"Training the SCIVOCAB models from scratch on our corpus takes 1 week 5 ( 5 days with max length 128 , then 2 days with max length 512 ) .",experiment,0,73,34,0,30
relation-classification,9,The BASEVOCAB models take 2 fewer days of training because they are n't trained from scratch .,experiment,0,74,35,0,17
relation-classification,9,All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library .,experiment,0,75,36,0,16
relation-classification,9,All our models ( Sections 3.4 and 3.5 ) are implemented in PyTorch using AllenNLP .,experiment,0,76,37,0,16
relation-classification,9,casing,experiment,0,77,38,0,1
relation-classification,9,We follow in using the cased models for NER and the uncased models for all other tasks .,experiment,0,78,39,0,18
relation-classification,9,We also use the cased models for parsing .,experiment,0,79,40,0,9
relation-classification,9,Some light experimentation showed that the uncased models perform slightly better ( even sometimes on NER ) than cased models .,experiment,0,80,41,0,21
relation-classification,9,finetuning bert,experiment,0,81,42,0,2
relation-classification,9,"We mostly follow the same architecture , optimization , and hyperparameter choices used in .",experiment,0,82,43,0,15
relation-classification,9,"For text classification ( i.e. CLS and REL ) , we feed the final BERT vector for the [ CLS ] token into a linear classification layer .",experiment,0,83,44,0,28
relation-classification,9,"For sequence labeling ( i.e. NER and PICO ) , we feed the final BERT vector for each token into a linear classification layer with softmax output .",experiment,0,84,45,0,28
relation-classification,9,"We differ slightly in using an additional conditional random field , which made evaluation easier by guaranteeing well - formed entities .",experiment,0,85,46,0,22
relation-classification,9,"For DEP , we use the model from with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs .",experiment,0,86,47,0,29
relation-classification,9,"In all settings , we apply a dropout of 0.1 and optimize cross entropy loss using Adam .",experiment,0,87,48,0,18
relation-classification,9,"We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5 e - 6 , 1 e - 5 , 2 e - 5 , or 5 e - 5 with a slanted triangular schedule which is equivalent to the linear warmup followed by linear decay .",experiment,0,88,49,0,55
relation-classification,9,"For each dataset and BERT variant , we pick the best learning rate and number of epochs on the development set and report the corresponding test results .",experiment,0,89,50,0,28
relation-classification,9,We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2 e - 5 .,experiment,0,90,51,0,27
relation-classification,9,"While task - dependent , optimal hyperparameters for each task are often the same across BERT variants .",experiment,0,91,52,0,18
relation-classification,9,frozen bert,experiment,0,92,53,0,2
relation-classification,9,embeddings,experiment,0,93,54,0,1
relation-classification,9,"We also explore the usage of BERT as pretrained contextualized word embeddings , like ELMo ) , by training simple task - specific models atop frozen BERT embeddings .",experiment,0,94,55,0,29
relation-classification,9,"For text classification , we feed each sentence of BERT vectors into a 2 - layer BiLSTM of size 200 and apply a multilayer perceptron ( with hidden size 200 ) on the concatenated first and last BiLSTM vectors .",experiment,0,95,56,0,40
relation-classification,9,"For sequence labeling , we use the same BiLSTM layers and use a conditional random field to guarantee well - formed predictions .",experiment,0,96,57,0,23
relation-classification,9,"For DEP , we use the full model from with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks .",experiment,0,97,58,0,27
relation-classification,9,We did not find changing the depth or size of the BiLSTMs to significantly impact results .,experiment,0,98,59,0,17
relation-classification,9,"We optimize cross entropy loss using Adam , but holding BERT weights frozen and applying a dropout of 0.5 .",experiment,0,99,60,0,20
relation-classification,9,We train with early stopping on the development set ( patience of 10 ) using a batch size of 32 and a learning rate of 0.001 .,experiment,0,100,61,0,27
relation-classification,9,"We did not perform extensive hyperparameter search , but while optimal hyperparameters are going to be task - dependent , some light experimentation showed these settings work fairly well across most tasks and BERT variants .",experiment,0,101,62,0,36
relation-classification,9,summarizes the experimental results .,experiment,0,102,1,0,5
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),experiment,1,103,2,0,23
relation-classification,9,8 . We also achieve new SOTA results on many of these tasks using SCIBERT .,experiment,0,104,3,0,16
relation-classification,9,results,result,0,105,1,0,1
relation-classification,9,biomedical domain,result,1,106,2,0,2
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,result,1,107,3,0,24
relation-classification,9,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .",result,1,108,4,0,21
relation-classification,9,SCIBERT performs slightly worse than SOTA on 3 datasets .,result,0,109,5,0,10
relation-classification,9,The SOTA model for JNLPBA is a BiLSTM - CRF ensemble trained on multiple NER datasets not just JNLPBA .,result,0,110,6,0,20
relation-classification,9,"The SOTA model for NCBI - disease is BIOBERT , which is BERT - Base finetuned on 18B tokens from biomedical papers .",result,0,111,7,0,23
relation-classification,9,"The SOTA result for GENIA is in Nguyen and Verspoor ( 2019 ) which uses the model from with partof - speech ( POS ) features , which we do not use .",result,0,112,8,1,33
relation-classification,9,"In , we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in .",result,0,113,9,0,18
relation-classification,9,"Interesting , SCIBERT outperforms BIOBERT results on 7 The SOTA paper did not report a single score .",result,0,114,10,0,18
relation-classification,9,We compute the average of the reported results for each class weighted by number of examples in each class .,result,0,115,11,0,20
relation-classification,9,"8 Forrest of this paper , all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS .",result,0,116,12,0,26
relation-classification,9,"BC5 CDR and ChemProt , and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus .",result,0,117,13,0,20
relation-classification,9,computer science domain,result,1,118,14,0,3
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,result,1,119,15,0,25
relation-classification,9,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .",result,1,120,16,0,20
relation-classification,9,"For relations in Sci - ERC , our results are not comparable with those in because we are performing relation classification given gold entities , while they perform joint entity and relation extraction .",result,0,121,17,0,34
relation-classification,9,multiple domains,result,1,122,18,0,2
relation-classification,9,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,result,1,123,19,0,25
relation-classification,9,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .",result,1,124,20,0,12
relation-classification,9,No prior published SOTA results exist for the Paper Field dataset .,result,0,125,21,0,12
relation-classification,9,discussion,result,0,126,22,0,1
relation-classification,9,effect of finetuning,result,0,127,23,0,3
relation-classification,9,"We observe improved results via BERT finetuning rather than task - specific architectures atop frozen embeddings ( + 3.25 F1 with SCIBERT and + 3.58 with BERT - Base , on average ) .",result,0,128,24,0,34
relation-classification,9,"For each scientific domain , we observe the largest effects of finetuning on the computer science ( + 5.59 F1 with SCIB - ERT and + 3.17 F1 with BERT - Base ) and biomedical tasks ( + 2.94 F1 with SCIBERT and + 4.61 F1 with BERT - Base ) , and the smallest effect on multidomain tasks ( + 0.7 F1 with SCIBERT and + 1.14 F1 with BERT - Base ) .",result,0,129,25,0,75
relation-classification,9,"On every dataset except BC5 CDR and SciCite , BERT - Base with finetuning outperforms ( or performs similarly to ) a model using frozen SCIBERT embeddings .",result,0,130,26,0,28
relation-classification,9,effect of scivocab,result,0,131,27,0,3
relation-classification,9,We assess the importance of an in - domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB .,result,0,132,28,0,21
relation-classification,9,We find the optimal hyperparameters for SCIBERT - BASEVOCAB often coincide with those of SCIB - ERT - SCIVOCAB .,result,0,133,29,0,20
relation-classification,9,"Averaged across datasets , we observe + 0.60 F1 when using SCIVOCAB .",result,0,134,30,0,13
relation-classification,9,"For each scientific do - main , we observe + 0.76 F1 for biomedical tasks , + 0.61 F1 for computer science tasks , and + 0.11 F1 for multidomain tasks .",result,0,135,31,0,32
relation-classification,9,"Given the disjoint vocabularies ( Section 2 ) and the magnitude of improvement over BERT - Base ( Section 4 ) , we suspect that while an in - domain vocabulary is helpful , SCIBERT benefits most from the scientific corpus pretraining .",result,0,136,32,0,43
relation-classification,9,related work,related work,0,137,1,0,2
relation-classification,9,Recent work on domain adaptation of BERT includes BIOBERT and CLIN - ICALBERT .,related work,0,138,2,0,14
relation-classification,9,"BIOBERT is trained on PubMed abstracts and PMC full text articles , and CLIN - ICALBERT is trained on clinical text from the MIMIC - III data base .",related work,0,139,3,0,29
relation-classification,9,"In contrast , SCIBERT is trained on the full text of 1.14 M biomedical and computer science papers from the Semantic Scholar corpus .",related work,0,140,4,0,24
relation-classification,9,"Furthermore , SCIBERT uses an in - domain vocabulary ( SCIVOCAB ) while the other abovementioned models use the original BERT vocabulary ( BASEVOCAB ) .",related work,0,141,5,0,26
relation-classification,9,conclusion and future work,related work,0,142,6,0,4
relation-classification,9,"We released SCIBERT , a pretrained language model for scientific text based on BERT .",related work,0,143,7,0,15
relation-classification,9,We evaluated SCIBERT on a suite of tasks and datasets from scientific domains .,related work,0,144,8,0,14
relation-classification,9,"SCIBERT significantly outperformed BERT - Base and achieves new SOTA results on several of these tasks , even compared to some reported BIOBERT ) results on biomedical tasks .",related work,0,145,9,0,29
relation-classification,9,"For future work , we will release a version of SCIBERT analogous to BERT - Large , as well as experiment with different proportions of papers from each domain .",related work,0,146,10,0,30
relation-classification,9,"Because these language models are costly to train , we aim to build a single resource that 's useful across multiple domains .",related work,0,147,11,0,23
relation-classification,1,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,title,1,2,1,0,12
relation-classification,1,abstract,abstract,0,3,1,0,1
relation-classification,1,Joint extraction of entities and relations is an important task in information extraction .,abstract,0,4,2,0,14
relation-classification,1,"To tackle this problem , we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem .",abstract,0,5,3,0,24
relation-classification,1,"Then , based on our tagging scheme , we study different end - toend models to extract entities and their relations directly , without identifying entities and relations separately .",abstract,0,6,4,0,30
relation-classification,1,We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods .,abstract,0,7,5,0,35
relation-classification,1,"What 's more , the end - to - end model proposed in this paper , achieves the best results on the public dataset .",abstract,0,8,6,0,25
relation-classification,1,introduction,introduction,0,9,1,0,1
relation-classification,1,"Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text , as shows .",introduction,1,10,2,0,24
relation-classification,1,"Different from open information extraction ( Open IE ) ) whose relation words are extracted from the given sentence , in this task , relation words are extracted from a predefined relation set which may not appear in the given sentence .",introduction,0,11,3,0,42
relation-classification,1,It is an important issue in knowledge extraction and automatic construction of knowledge base .,introduction,0,12,4,0,15
relation-classification,1,"Traditional methods handle this task in a pipelined manner , i.e. , extracting the entities first and then recognizing their relations .",introduction,0,13,5,0,22
relation-classification,1,"This separated framework makes the task easy to deal with , and each component can be more flexible .",introduction,0,14,6,0,19
relation-classification,1,But it neglects the relevance between these two sub - tasks and each subtask is an independent model .,introduction,0,15,7,0,19
relation-classification,1,The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery .,introduction,0,16,8,0,18
relation-classification,1,"Different from the pipelined methods , joint learning framework is to extract entities together with relations using a single model .",introduction,1,17,9,0,21
relation-classification,1,"It can effectively integrate the information of entities and relations , and it has been shown to achieve better results in this task .",introduction,0,18,10,0,24
relation-classification,1,"However , most existing joint methods are feature - based structured systems .",introduction,0,19,11,0,13
relation-classification,1,"They need complicated feature engineering and heavily rely on the other NLP toolkits , which might also lead to error propagation .",introduction,0,20,12,0,22
relation-classification,1,"In order to reduce the manual work in feature extraction , recently , presents a neural network - based method for the end - to - end entities and relations extraction .",introduction,0,21,13,0,32
relation-classification,1,"Although the joint models can represent both entities and relations with shared parameters in a single model , they also extract the entities and relations separately and produce redundant information .",introduction,0,22,14,0,31
relation-classification,1,"For instance , the sentence in contains three entities : "" United States "" , "" Trump "" and "" Apple Inc "" .",introduction,0,23,15,0,24
relation-classification,1,"But only "" United States "" and "" Trump "" hold a fix relation "" Country - President "" .",introduction,0,24,16,0,20
relation-classification,1,"Entity "" Apple Inc "" has no obvious relationship with the other entities in this sentence .",introduction,0,25,17,0,17
relation-classification,1,"Hence , the extracted result from this sentence is { United States e 1 , Country - President r , Trump e 2 } , which called triplet here .",introduction,0,26,18,0,30
relation-classification,1,"In this paper , we focus on the extraction of triplets thatare composed of two entities and one relation between these two entities .",introduction,0,27,19,0,24
relation-classification,1,"Therefore , we can model the triplets directly , rather than extracting the entities and relations separately .",introduction,0,28,20,0,18
relation-classification,1,"Based on the motivations , we propose a tagging scheme accompanied with the end - to - end model to settle this problem .",introduction,0,29,21,0,24
relation-classification,1,We design a kind of novel tags which contain the information of entities and the relationships they hold .,introduction,0,30,22,0,19
relation-classification,1,"Based on this tagging scheme , the joint extraction of entities and relations can be transformed into a tagging problem .",introduction,0,31,23,0,21
relation-classification,1,"In this way , we can also easily use neural networks to model the task without complicated feature engineering .",introduction,0,32,24,0,20
relation-classification,1,"Recently , end - to - end models based on LSTM have been successfully applied to various tagging tasks :",introduction,0,33,25,0,20
relation-classification,1,"named entity recognition , ccg",introduction,0,34,26,0,5
relation-classification,1,"supertagging , chunking et al .",introduction,0,35,27,1,6
relation-classification,1,"LSTM is capable of learning long - term dependencies , which is beneficial to sequence modeling tasks .",introduction,0,36,28,0,18
relation-classification,1,"Therefore , based on our tagging scheme , we investigate different kinds of LSTM - based end - to - end models to jointly extract the entities and relations .",introduction,0,37,29,0,30
relation-classification,1,We also modify the decoding method by adding a biased loss to make it more suitable for our special tags .,introduction,0,38,30,0,21
relation-classification,1,The method we proposed is a supervised learning algorithm .,introduction,0,39,31,0,10
relation-classification,1,"In reality , however , the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone .",introduction,0,40,32,0,27
relation-classification,1,"Therefore , we conduct experiments on a public dataset 1 which is produced by distant supervision method to validate our approach .",introduction,0,41,33,0,22
relation-classification,1,The experimental results show that our tagging scheme is effective in this task .,introduction,0,42,34,0,14
relation-classification,1,"In addition , our end - to - end model can achieve the best results on the public dataset .",introduction,0,43,35,0,20
relation-classification,1,"The major contributions of this paper are : ( 1 ) A novel tagging scheme is proposed to jointly extract entities and relations , which can easily transform the extraction problem into a tagging task .",introduction,0,44,36,0,36
relation-classification,1,"( 2 ) Based on our tagging scheme , we study different kinds of end - to - end models to settle the problem .",introduction,0,45,37,0,25
relation-classification,1,The tagging - based methods are better than most of the existing pipelined and joint learning methods .,introduction,0,46,38,0,18
relation-classification,1,"( 3 ) Furthermore , we also develop an end - to - 1 https://github.com/shanzhenren/Co",introduction,0,47,39,0,15
relation-classification,1,Type end model with biased loss function to suit for the novel tags .,introduction,0,48,40,0,14
relation-classification,1,It can enhance the association between related entities .,introduction,0,49,41,0,9
relation-classification,1,related works,related work,0,50,1,0,2
relation-classification,1,"Entities and relations extraction is an important step to construct a knowledge base , which can be benefit for many NLP tasks .",related work,0,51,2,0,23
relation-classification,1,Two main frameworks have been widely used to solve the problem of extracting entity and their relationships .,related work,0,52,3,0,18
relation-classification,1,One is the pipelined method and the other is the joint learning method .,related work,0,53,4,0,14
relation-classification,1,"The pipelined method treats this task as two separated tasks , i.e. , named entity recognition ( NER ) and relation classification ( RC ) .",related work,0,54,5,0,26
relation-classification,1,"Classical NER models are linear statistical models , such as Hidden Markov Models ( HMM ) and Conditional Random Fields ( CRF ) .",related work,0,55,6,0,24
relation-classification,1,"Recently , several neural network architectures have been successfully applied to NER , which is regarded as a sequential token tagging task .",related work,0,56,7,0,23
relation-classification,1,Existing methods for relation classification can also be divided into handcrafted feature based methods and neural network based methods .,related work,0,57,8,0,20
relation-classification,1,While joint models extract entities and relations using a single model .,related work,0,58,9,0,12
relation-classification,1,Most of the joint methods are feature - based structured systems .,related work,0,59,10,0,12
relation-classification,1,"Recently , uses a LSTMbased model to extract entities and relations , which can reduce the manual work .",related work,0,60,11,0,19
relation-classification,1,"Different from the above methods , the method proposed in this paper is based on a special tagging manner , so that we can easily use end - toend model to extract results without NER and RC .",related work,0,61,12,0,38
relation-classification,1,end - to - end method is to map the input sentence into meaningful vectors and then back to produce a sequence .,related work,0,62,13,0,23
relation-classification,1,It is widely used in machine translation and sequence tagging tasks .,related work,0,63,14,0,12
relation-classification,1,"Most methods apply bidirectional LSTM to encode the input sentences , but the decoding methods are always different .",related work,0,64,15,0,19
relation-classification,1,"For examples , use a CRF layers to decode the tag sequence , while apply LSTM layer to produce the tag sequence .",related work,0,65,16,0,23
relation-classification,1,method,method,0,66,1,0,1
relation-classification,1,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,method,1,67,2,0,23
relation-classification,1,"In this section , we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method .",method,0,68,3,0,23
relation-classification,1,Then we detail the model we used to extract results .,method,0,69,4,0,11
relation-classification,1,is an example of how the results are tagged .,method,0,70,5,0,10
relation-classification,1,Each word is assigned a label that contributes to extract the results .,method,0,71,6,0,13
relation-classification,1,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .",method,1,72,7,0,24
relation-classification,1,"In addition to "" O "" , the other tags consist of three parts : the word position in the entity , the relation type , and the relation role .",method,0,73,8,0,31
relation-classification,1,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .",method,1,74,9,0,28
relation-classification,1,"The relation type information is obtained from a predefined set of relations and the relation role information is represented by the numbers "" 1 "" and "" 2 "" .",method,0,75,10,0,30
relation-classification,1,"An extracted result is represented by a triplet : ( Entity 1 , Relation T ype , Entity 2 ) .",method,0,76,11,0,21
relation-classification,1,""" 1 "" means that the word belongs to the first entity in the triplet , while "" 2 "" belongs to second entity that behind the relation type .",method,0,77,12,0,30
relation-classification,1,"Thus , the total number of tags is N t = 2 * 4 * | R | + 1 , where | R | is the size of the predefined relation set .",method,0,78,13,0,34
relation-classification,1,is an example illustrating our tagging method .,method,0,79,14,0,8
relation-classification,1,"The input sentence contains two triplets : { United States , Country - President , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } , where "" Country - President "" and "" Company - Founder "" are the predefined relation types .",method,0,80,15,0,49
relation-classification,1,the tagging scheme,method,0,81,16,0,3
relation-classification,1,"The words "" United "" , "" States "" , "" Trump "" , "" Apple "" , "" Inc "" , "" Steven "" , "" Paul "" and "" Jobs "" are all related to the final extracted results .",method,0,82,17,0,42
relation-classification,1,Thus they are tagged based on our special tags .,method,0,83,18,0,10
relation-classification,1,"For example , the word of "" United "" is the first word of entity "" United States "" and is related to the relation "" Country - President "" , so its tag is "" B - CP - 1 "" .",method,0,84,19,0,43
relation-classification,1,"The other entity "" Trump "" , which is corresponding to "" United States "" , is labeled as "" S - CP - 2 "" .",method,0,85,20,0,27
relation-classification,1,"Besides , the other words irrelevant to the final result are labeled as "" O "" .",method,0,86,21,0,17
relation-classification,1,From Tag Sequence To Extracted Results,method,0,87,22,0,6
relation-classification,1,"From the tag sequence in , we know that "" Trump "" and "" United States "" share the same relation type "" Country - President "" , "" Apple Inc "" and "" Steven Paul Jobs "" share the same relation type "" Company - Founder "" .",method,0,88,23,0,49
relation-classification,1,We combine entities with the same relation type into a triplet to get the final result .,method,0,89,24,0,17
relation-classification,1,"Accordingly , "" Trump "" and "" United States "" can be combined into a triplet whose relation type is "" Country - President "" .",method,0,90,25,0,26
relation-classification,1,"Because , the relation role of "" Trump "" is "" 2 "" and "" United States "" is "" 1 "" , the final result is { United States , Country - President , Trump } .",method,0,91,26,0,38
relation-classification,1,"The same applies to { Apple Inc , Company - Founder , Steven Paul Jobs } .",method,0,92,27,0,17
relation-classification,1,"Besides , if a sentence contains two or more triplets with the same relation type , we combine every two entities into a triplet based on the nearest principle .",method,0,93,28,0,30
relation-classification,1,"For example , if the relation type "" Country - President "" in is "" Company - Founder "" , then there will be four entities in the given sentence with the same relation type .",method,0,94,29,0,36
relation-classification,1,""" United States "" is closest to entity "" Trump "" and the "" Apple Inc "" is closest to "" Jobs "" , so the results will be { United States , Company - Founder , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } .",method,0,95,30,0,53
relation-classification,1,"In this paper , we only consider the situation where an entity belongs to a triplet , and we leave identification of overlapping relations for future work .",method,0,96,31,0,28
relation-classification,1,The End - to - end Model,method,1,97,32,0,7
relation-classification,1,"In recent years , end - to - end model based on neural network is been widely used in sequence tagging task .",method,0,98,33,0,23
relation-classification,1,"In this paper , we investigate an end - toend model to produce the tags sequence as shows .",method,1,99,34,0,19
relation-classification,1,It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .,method,1,100,35,0,30
relation-classification,1,The biased loss can enhance the relevance of entity tags .,method,1,101,36,0,11
relation-classification,1,The Bi - LSTM Encoding Layer .,method,0,102,37,0,7
relation-classification,1,"In sequence tagging problems , the Bi - LSTM encoding layer has been shown the effectiveness to capture the semantic information of each word .",method,0,103,38,0,25
relation-classification,1,"It contains forward lstm layer , backward lstm layer and the concatenate layer .",method,0,104,39,0,14
relation-classification,1,The word embedding layer converts the word with 1 - hot representation to an embedding vector .,method,0,105,40,0,17
relation-classification,1,"Hence , a sequence of words can be represented as : Gold standard annotation for an example sentence based on our tagging scheme , where "" CP "" is short for "" Country - President "" and "" CF "" is short for "" Company - Founder "" .",method,0,106,41,0,49
relation-classification,1,corresponding to the t - th word in the sentence and n is the length of the given sentence .,method,0,107,42,0,20
relation-classification,1,"After word embedding layer , there are two parallel LSTM layers : forward LSTM layer and backward LSTM layer .",method,0,108,43,0,20
relation-classification,1,"The LSTM architecture consists of a set of recurrently connected subnets , known as memory blocks .",method,0,109,44,0,17
relation-classification,1,Each time - step is a LSTM memory block .,method,0,110,45,0,10
relation-classification,1,"The LSTM memory block in Bi - LSTM encoding layer is used to compute current hidden vector ht based on the previous hidden vector h t?1 , the previous cell vector c t?1 and the current input word embedding wt .",method,0,111,46,0,41
relation-classification,1,"It s structure diagram is shown in , and detail operations are defined as follows :",method,0,112,47,0,16
relation-classification,1,"where i , f and o are the input gate , forget gate and output gate respectively , b is the bias term , c is the cell memory , and W ( . ) are the parameters .",method,0,113,48,0,39
relation-classification,1,"For each word wt , the forward LSTM layer will encode wt by considering the contextual information from word w 1 tow t , which is marked as ? ? ht .",method,0,114,49,0,32
relation-classification,1,"In the similar way , the backward LSTM layer will encode wt based on the contextual information from w n tow t , which is marked as ? ? ht .",method,0,115,50,0,31
relation-classification,1,"finally , we concatenate ? ?",method,0,116,51,0,6
relation-classification,1,ht and ? ?,method,0,117,52,0,4
relation-classification,1,"ht to represent word t 's encoding information , denoted as",method,0,118,53,0,11
relation-classification,1,the lstm decoding layer .,method,0,119,54,0,5
relation-classification,1,We also adopt a LSTM structure to produce the tag sequence .,method,0,120,55,0,12
relation-classification,1,"When detecting the tag of word wt , the inputs of decoding layer are : ht obtained from Bi - LSTM encoding layer , former predicted tag embedding T t?1 , former cell value c ( 2 ) t? 1 , and the former hidden vector in decoding layer h ( 2 ) t?1 .",method,0,121,56,0,55
relation-classification,1,"The structure diagram of the memory block in LSTM dis shown in , and detail operations are defined as follows :",method,0,122,57,0,21
relation-classification,1,The final softmax layer computes normalized entity tag probabilities based on the tag predicted vector T t :,method,0,123,58,0,18
relation-classification,1,"where W y is the softmax matrix , N t is the total number of tags .",method,0,124,59,0,17
relation-classification,1,"Because T is similar to tag embedding and LSTM is capable of learning long - term dependencies , the decoding manner can model tag interactions .",method,0,125,60,0,26
relation-classification,1,the bias objective function .,method,0,126,61,0,5
relation-classification,1,We train our model to maximize the log-likelihood of the data and the optimization method we used is RM - Sprop proposed by Hinton in .,method,0,127,62,0,26
relation-classification,1,The objective function can be defined as :,method,0,128,63,0,8
relation-classification,1,"where | D| is the size of training set , L j is the length of sentence x j , y ( j ) t is the label of word tin sentence x j and p ( j ) t is the normalized probabilities of tags which defined in Formula 15 .",method,0,129,64,0,52
relation-classification,1,"Besides , I ( O ) is a switching function to distinguish the loss of tag ' O ' and relational tags that can indicate the results .",method,0,130,65,0,28
relation-classification,1,It is defined as follows :,method,0,131,66,0,6
relation-classification,1,?,method,0,132,67,0,1
relation-classification,1,is the bias weight .,method,0,133,68,0,5
relation-classification,1,the larger ?,method,0,134,69,0,3
relation-classification,1,"is , the greater influence of relational tags on the model .",method,0,135,70,0,12
relation-classification,1,experiments,experiment,0,136,1,0,1
relation-classification,1,experimental setting,experiment,0,137,1,0,2
relation-classification,1,dataset,experiment,0,138,2,0,1
relation-classification,1,"To evaluate the performance of our methods , we use the public dataset NYT 2 which is produced by distant supervision method .",experiment,0,139,3,0,23
relation-classification,1,A large amount of training data can be obtained by means of distant supervision methods without manually labeling .,experiment,0,140,4,0,19
relation-classification,1,While the test set is manually labeled to ensure its quality .,experiment,0,141,5,0,12
relation-classification,1,"In total , the training data contains 353 k triplets , and the test set contains 3 , 880 triplets .",experiment,0,142,6,0,21
relation-classification,1,"Besides , the size of relation set is 24 .",experiment,0,143,7,0,10
relation-classification,1,evaluation,experiment,0,144,8,0,1
relation-classification,1,"We adopt standard Precision ( Prec ) , Recall ( Rec ) and F 1 score to evaluate the results .",experiment,0,145,9,0,21
relation-classification,1,"Different from classical methods , our method can extract triplets without knowing the information of entity types .",experiment,0,146,10,0,18
relation-classification,1,"In other words , we did not use the label of entity types to train the model , therefore we do not need to consider the entity types in the evaluation .",experiment,0,147,11,0,32
relation-classification,1,A triplet is regarded as correct when its relation type and the head offsets of two corresponding entities are both correct .,experiment,0,148,12,0,22
relation-classification,1,"Besides , the groundtruth relation mentions are given and "" None "" label is excluded as did .",experiment,0,149,13,0,18
relation-classification,1,We create a validation set by randomly sampling 10 % data from test set and use the remaining data as evaluation based on ) 's suggestion .,experiment,0,150,14,0,27
relation-classification,1,We run 10 times for each experiment then report the average results and their standard deviation as shows .,experiment,0,151,15,0,19
relation-classification,1,hyperparameters,experiment,0,152,16,0,1
relation-classification,1,Our model consists of a Bi - LSTM encoding layer and a LSTM decoding layer with bias objective function .,experiment,0,153,17,0,20
relation-classification,1,The word embeddings used in the encoding part are initialed by running word2vec 3 on NYT training corpus .,experiment,1,154,18,0,19
relation-classification,1,The dimension of the word embeddings is d = 300 .,experiment,1,155,19,0,11
relation-classification,1,We regularize our network using dropout on embedding layer and the dropout ratio is 0.5 .,experiment,1,156,20,0,16
relation-classification,1,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,experiment,1,157,21,0,19
relation-classification,1,the bias parameter ?,experiment,1,158,22,0,4
relation-classification,1,corresponding to the results in is 10 .,experiment,1,159,23,0,8
relation-classification,1,2,experiment,0,160,24,0,1
relation-classification,1,The dataset can be downloaded at : https://github.com/shanzhenren/CoType.,experiment,0,161,25,0,8
relation-classification,1,There are three data sets in the public resource and we only use the NYT dataset .,experiment,0,162,26,0,17
relation-classification,1,Because more than 50 % of the data in BioInfer has overlapping relations which is beyond the scope of this paper .,experiment,0,163,27,0,22
relation-classification,1,"As for dataset Wiki - KBP , the number of relation type in the test set is more than that of the train set , which is also not suitable for a supervised training method .",experiment,0,164,28,0,36
relation-classification,1,Details of the data can be found in is the pipelined methods and the second part ( row 4 to 6 ) is the jointly extracting methods .,experiment,0,165,29,0,28
relation-classification,1,Our tagging methods are shown in part three ( row 7 to 9 ) .,experiment,0,166,30,0,15
relation-classification,1,"In this part , we not only report the results of precision , recall and F1 , we also compute their standard deviation .",experiment,0,167,31,0,24
relation-classification,1,baselines,experiment,0,168,32,0,1
relation-classification,1,"We compare our method with several classical triplet extraction methods , which can be divided into the following categories : the pipelined methods , the jointly extracting methods and the end - to - end methods based our tagging scheme .",experiment,1,169,33,0,41
relation-classification,1,"For the pipelined methods , we follow ) 's settings :",experiment,1,170,34,0,11
relation-classification,1,The NER results are obtained by CoType then several classical relation classification methods are applied to detect the relations .,experiment,0,171,35,0,20
relation-classification,1,these methods are :,method,0,172,1,0,4
relation-classification,1,"( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;",method,1,173,2,0,46
relation-classification,1,( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .,method,1,174,3,0,21
relation-classification,1,"The jointly extracting methods used in this paper are listed as follows : ( 4 ) DS - Joint ) is a supervised method , which jointly extracts entities and relations using structured perceptron on human - annotated dataset ; ( 5 ) MultiR is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data ; ( 6 ) CoType ) is a domain independent framework by jointly embedding entity mentions , relation mentions , text features and type labels into meaningful representations .",method,1,175,4,0,90
relation-classification,1,"In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .",method,1,176,5,0,26
relation-classification,1,LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .,method,1,177,6,0,29
relation-classification,1,"Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .",method,1,178,7,0,22
relation-classification,1,They are used for the first time to jointly extract entities and relations based on our tagging scheme .,method,0,179,8,0,19
relation-classification,1,experimental results,experiment,0,180,1,0,2
relation-classification,1,We report the results of different methods as shown in .,experiment,0,181,2,0,11
relation-classification,1,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .",experiment,1,182,3,0,37
relation-classification,1,It shows the effectiveness of our proposed method .,experiment,0,183,4,0,9
relation-classification,1,"Furthermore , from , we also can see that the jointly extracting methods are better than pipelined methods , and the tagging methods are better than most of the jointly extracting methods .",experiment,0,184,5,0,33
relation-classification,1,It also validates the validity of our tagging scheme for the task of jointly extracting entities and relations .,experiment,0,185,6,0,19
relation-classification,1,"When compared with the traditional methods , the precisions of the end - to - end models are significantly improved .",experiment,1,186,7,0,21
relation-classification,1,But only LSTM - LSTM - Bias can be better to balance the precision and recall .,experiment,0,187,8,0,17
relation-classification,1,The reason maybe that these end - to - end models all use a Bi - LSTM encoding input sentence and different neural networks to decode the results .,experiment,0,188,9,0,29
relation-classification,1,The methods based on neural networks can well fit the data .,experiment,0,189,10,0,12
relation-classification,1,"Therefore , they can learn the common features of the training set well and may lead to the lower expansibility .",experiment,0,190,11,0,21
relation-classification,1,We also find that the LSTM - LSTM model is better than LSTM - CRF model based on our tagging scheme .,experiment,1,191,12,0,22
relation-classification,1,"Because , LSTM is capable of learning long - term dependencies and CRF is good at capturing the joint probability of the entire sequence of labels .",experiment,0,192,13,0,27
relation-classification,1,The related tags may have along distance from each other .,experiment,0,193,14,0,11
relation-classification,1,"hence ,",experiment,0,194,15,0,2
relation-classification,1,LSTM decoding manner is a little better than CRF .,experiment,0,195,16,0,10
relation-classification,1,LSTM - LSTM - Bias adds a bias weight to enhance the effect of entity tags and weaken the effect of invalid tag .,experiment,0,196,17,0,24
relation-classification,1,"Therefore , in this tagging scheme , our method can be better than the common LSTM - decoding methods .",experiment,0,197,18,0,20
relation-classification,1,analysis and discussion,experiment,0,198,19,0,3
relation-classification,1,error analysis,experiment,0,199,20,0,2
relation-classification,1,"In this paper , we focus on extracting triplets composed of two entities and a relation .",experiment,0,200,21,0,17
relation-classification,1,has shown the predict results of the task .,experiment,0,201,22,0,9
relation-classification,1,It treats an triplet is correct only when the relation type and the head offsets of two corresponding entities are both correct .,experiment,0,202,23,0,23
relation-classification,1,"In order to find out the factors that affect the results of end - to - end models , we analyze the performance on predicting each element in the triplet as shows .",experiment,0,203,24,0,33
relation-classification,1,"E1 and E2 represent the performance on predicting each entity , respectively .",experiment,0,204,25,0,13
relation-classification,1,"If the head offset of the first entity is correct , then the instance of E1 is correct , the same to E2 .",experiment,0,205,26,0,24
relation-classification,1,"Regardless of relation type , if the head offsets of two corresponding entities are both correct , the instance of ( E1 , E2 ) is correct .",experiment,0,206,27,0,28
relation-classification,1,"As shown in , ( E1 , E2 ) has higher precision when compared with E1 and E2 .",experiment,0,207,28,0,19
relation-classification,1,But its recall result is lower than E1 and E2 .,experiment,0,208,29,0,11
relation-classification,1,It means that some of the predicted entities do not form a pair .,experiment,0,209,30,0,14
relation-classification,1,"They only obtain E1 and do not find its corresponding E2 , or obtain E2 and do not find its corresponding E1 .",experiment,0,210,31,0,23
relation-classification,1,"Thus it leads to the prediction of more single E and less ( E1 , E2 ) pairs .",experiment,0,211,32,0,19
relation-classification,1,"Therefore , entity pair ( E1 , E2 ) has higher precision and lower recall than single E. Besides , the predicted results of ( E1 , E2 ) in have about 3 % improvement when compared predicted results in Table 1 , which means that 3 % of the test data is pre-dicted to be wrong because the relation type is predicted to be wrong .",experiment,0,212,33,0,67
relation-classification,1,analysis of biased loss,experiment,0,213,34,0,4
relation-classification,1,"Different from LSTM - CRF and LSTM - LSTM , our approach is biased towards relational labels to enhance links between entities .",experiment,0,214,35,0,23
relation-classification,1,"In order to further analyze the effect of the bias objective function , we visualize the ratio of predicted single entities for each end - to - end method as .",experiment,0,215,36,0,31
relation-classification,1,The single entities refer to those who can not find their corresponding entities .,experiment,0,216,37,0,14
relation-classification,1,"shows whether it is E1 or E2 , our method can get a relatively low ratio on the single entities .",experiment,0,217,38,0,21
relation-classification,1,It means that our method can effectively associate two entities when compared LSTM - CRF and LSTM - LSTM which pay little attention to the relational tags .,experiment,0,218,39,0,28
relation-classification,1,single e1,experiment,0,219,40,0,2
relation-classification,1,"Single Besides , we also change the Bias Parameter ?",experiment,0,220,41,0,10
relation-classification,1,"from 1 to 20 , and the predicted results are shown in .",experiment,0,221,42,0,13
relation-classification,1,if ?,experiment,0,222,43,0,2
relation-classification,1,"is too large , it will affect the accuracy of prediction and if ?",experiment,0,223,44,0,14
relation-classification,1,"is too small , the recall will decline .",experiment,0,224,45,0,9
relation-classification,1,"When ? = 10 , LSTM - LSTM - Bias can balance the precision and recall , and can achieve the best F 1 scores .",experiment,0,225,46,0,26
relation-classification,1,case study,experiment,0,226,47,0,2
relation-classification,1,"In this section , we observe the prediction results of end - to - end methods , and then select several representative examples to illustrate the advantages and dis advantages of the methods as shows .",experiment,0,227,48,0,36
relation-classification,1,"Each example contains three row , the first row is the gold standard , the second and the third rows are the extracted results of model LSTM - LSTM and LSTM - LSTM - Bias respectively .",experiment,0,228,49,0,37
relation-classification,1,"S1 represents the situation that the distance between the two interrelated entities is faraway from each other , which is more difficult to detect their relationships .",experiment,0,229,50,0,27
relation-classification,1,"When compared with LSTM - LSTM , LSTM - LSTM - Bias uses a bias objective function which enhance the relevance between entities .",experiment,0,230,51,0,24
relation-classification,1,"Therefore , in this example , LSTM - LSTM - Bias can extract two related entities , while LSTM - LSTM can only extract one entity of "" Florida "" and can not detect entity "" Panama City Beach "" .",experiment,0,231,52,0,41
relation-classification,1,S2 is a negative example that shows these methods may mistakenly predict one of the entity .,experiment,0,232,53,0,17
relation-classification,1,There are no indicative words between entities Nuremberg and Germany .,experiment,0,233,54,0,11
relation-classification,1,"Besides , the patten "" a * of * "" between Germany and M iddle Ages maybe easy to mislead the models that there exists a relation of "" Contains "" between them .",experiment,0,234,55,0,34
relation-classification,1,The problem can be solved by adding some samples of this kind of expression patterns to the training data .,experiment,0,235,56,0,20
relation-classification,1,"S3 is a case that models can predict the entities ' head offset right , but the relational role is wrong .",experiment,0,236,57,0,22
relation-classification,1,"LSTM - LSTM treats both "" Stephen A. Schwarzman "" and "" Blackstone Group "" as entity E1 , and can not find its corresponding E2 .",experiment,0,237,58,0,27
relation-classification,1,"Although , LSTM - LSMT - Bias can find the entities pair ( E1 , E2 ) , it reverses the roles of "" Stephen A. Schwarzman "" and "" Blackstone Group "" .",experiment,0,238,59,0,34
relation-classification,1,"It shows that LSTM - LSTM - Bias is able to better on pre-dicting entities pair , but it remains to be improved in distinguishing the relationship between the two entities .",experiment,0,239,60,0,32
relation-classification,1,conclusion,experiment,0,240,61,0,1
relation-classification,1,"In this paper , we propose a novel tagging scheme and investigate the end - to - end models to jointly extract entities and relations .",experiment,0,241,62,0,26
relation-classification,1,The experimental results show the effectiveness of our proposed method .,experiment,0,242,63,0,11
relation-classification,1,But it still has shortcoming on the identification of the overlapping relations .,experiment,0,243,64,0,13
relation-classification,1,"In the future work , we will replace the softmax function in the output layer with multiple classifier , so that a word can has multiple tags .",experiment,0,244,65,0,28
relation-classification,1,"In this way , a word can appear in multiple triplet results , which can solve the problem of overlapping relations .",experiment,0,245,66,0,22
relation-classification,1,"Although , our model can enhance the effect of entity tags , the association between two corresponding entities still requires refinement in next works .",experiment,0,246,67,0,25
relation-classification,5,End - to - end neural relation extraction using deep biaffine attention,title,1,2,1,0,12
relation-classification,5,abstract,abstract,0,3,1,0,1
relation-classification,5,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",abstract,1,4,2,0,24
relation-classification,5,"The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship .",abstract,0,5,3,0,50
relation-classification,5,"On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",abstract,0,6,4,0,33
relation-classification,5,introduction,introduction,0,7,1,0,1
relation-classification,5,Extracting entities and their semantic relations from raw text is a key information extraction task .,introduction,1,8,2,0,16
relation-classification,5,"For example , given the sentence "" David Foster is the AP 's Northwest regional reporter , based in Seattle "" in the CoNLL04 dataset , our goal is to recognize "" David Foster "" as person , "" AP "" as organization , and "" Northwest "" and "" Seattle "" as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .",introduction,0,9,3,0,90
relation-classification,5,Such information is useful in many other NLP tasks .,introduction,0,10,4,0,10
relation-classification,5,"Especially in IR applications such as entity search , structured search and question answering , it helps provide end users with significantly better search experience .",introduction,0,11,5,0,26
relation-classification,5,A common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification .,introduction,0,12,6,0,25
relation-classification,5,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",introduction,1,13,7,0,28
relation-classification,5,Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .,introduction,0,14,8,0,29
relation-classification,5,State - of - the - art relation extraction performance has been obtained by end - to - end models based on neural networks .,introduction,0,15,9,0,25
relation-classification,5,"Specifically , proposed a RNNbased model which achieved top results on the CoNLL04 dataset .",introduction,0,16,10,0,15
relation-classification,5,Their approach relies on various manually extracted features .,introduction,0,17,11,0,9
relation-classification,5,Other neural models employ dependency parsing - based information .,introduction,0,18,12,0,10
relation-classification,5,"In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .",introduction,0,19,13,0,22
relation-classification,5,integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM - based dependency parser .,introduction,0,20,14,0,19
relation-classification,5,"entity recognition , and a CNN on top of the BiLSTM for classifying relations .",introduction,0,21,15,0,15
relation-classification,5,"Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .",introduction,0,22,16,1,35
relation-classification,5,"Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .",introduction,0,23,17,0,45
relation-classification,5,"In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .",introduction,0,24,18,0,29
relation-classification,5,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",introduction,1,25,19,0,22
relation-classification,5,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .",introduction,1,26,20,0,31
relation-classification,5,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,introduction,1,27,21,0,17
relation-classification,5,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .",introduction,1,28,22,0,27
relation-classification,5,"In most previous neural joint models , the relation classification part relies on a common "" linear "" concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .",introduction,0,29,23,0,56
relation-classification,5,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .",introduction,1,30,24,0,21
relation-classification,5,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .",introduction,1,31,25,0,25
relation-classification,5,"Experimental results on the benchmark "" relation and entity recognition "" dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .",introduction,0,32,26,0,30
relation-classification,5,"In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .",introduction,0,33,27,0,18
relation-classification,5,We also provide an ablation study to investigate effects of different contributing factors in our model .,introduction,0,34,28,0,17
relation-classification,5,our proposed model,introduction,0,35,29,0,3
relation-classification,5,This section details our end - to - end relation extraction model .,introduction,0,36,30,0,13
relation-classification,5,"Given an input sequence of n word tokens w 1 , w 2 , ... , w n , we use a vector vi to represent each i th word w i by concatenating word embedding e",introduction,0,37,31,0,37
relation-classification,5,"Here , for each word type w , we use a one - layer BiLSTM ( BiLSTM char ) to learn its character - level word embedding e ( C ) w.",introduction,0,38,32,0,32
relation-classification,5,named entity recognition ( ner ) :,introduction,0,39,33,0,7
relation-classification,5,"The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a "" latent "" feature vector representing the i th word token .",introduction,0,40,34,0,39
relation-classification,5,Then the NER component performs linear transformation of each latent feature vector by using a single - layer feed - forward network ( FFNN NER ) :,introduction,0,41,35,0,27
relation-classification,5,The output layer size of FFNN NER is the number of BIOLU - based NER labels .,introduction,0,42,36,0,17
relation-classification,5,The NER component feeds the output vectors h 1:n into a linear - chain CRF layer for NER label prediction .,introduction,0,43,37,0,21
relation-classification,5,"A cross-entropy loss L NER is computed during training , while the Viterbi algorithm is used for decoding .",introduction,0,44,38,0,19
relation-classification,5,Our NER component thus is the BiLSTM - CRF model with additional LSTM - based character - level word embeddings .,introduction,0,45,39,0,21
relation-classification,5,"Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .",introduction,0,46,40,0,30
relation-classification,5,We represent each i th predicted label by a vector embedding e ti .,introduction,0,47,41,0,14
relation-classification,5,We create a sequence of vectors x 1:n in which each x i is computed as :,introduction,0,48,42,0,17
relation-classification,5,"As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :",introduction,0,49,43,0,31
relation-classification,5,The RC component further uses these latent vectors r i for relation classification .,introduction,0,50,44,0,14
relation-classification,5,We propose a novel use of the deep biaffine attention mechanism for relation classification .,introduction,0,51,45,0,15
relation-classification,5,"The biaffine attention mechanism was proposed for dependency parsing , helping to produce the best reported parsing performance to date .",introduction,0,52,46,0,21
relation-classification,5,"First , to encode the directionality of a relation , we use two single - layer feed - forward networks to project each r i into head and tail vector representations which correspond to whether the i th word serves as the head or tail argument of the relation :",introduction,0,53,47,0,50
relation-classification,5,"Following , our RC component incrementally constructs relation candidates using all possible combinations of the last word tokens of predicted entities , i.e. words with L or U labels .",introduction,0,54,48,0,30
relation-classification,5,We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .,introduction,0,55,49,0,28
relation-classification,5,"For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .",introduction,0,56,50,0,27
relation-classification,5,"Then for each head - tail candidate pair ( w j , wk ) , we apply the biaffine attention operator :",introduction,0,57,51,0,22
relation-classification,5,3 experiments,experiment,0,58,1,0,2
relation-classification,5,experimental setup,experiment,0,59,1,0,2
relation-classification,5,evaluation scenarios :,experiment,0,60,2,0,3
relation-classification,5,We evaluate our joint model on two evaluation setup scenarios :,experiment,0,61,3,0,11
relation-classification,5,( 1 ) ner&rc :,experiment,0,62,4,0,5
relation-classification,5,A realistic scenario where entity boundaries are not given .,experiment,0,63,5,0,10
relation-classification,5,( 2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .,experiment,0,64,6,0,17
relation-classification,5,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,experiment,0,65,7,0,21
relation-classification,5,"Following , we encode the gold entity boundaries in the BILOU scheme .",experiment,0,66,8,0,13
relation-classification,5,"Then we represent each B , I , O , L or U boundary tag as a vector embedding .",experiment,0,67,9,0,20
relation-classification,5,"As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .",experiment,0,68,10,0,30
relation-classification,5,"Dataset : We use the benchmark "" entity and relation recognition "" dataset CoNLL04 from .",experiment,0,69,11,0,16
relation-classification,5,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .",experiment,0,70,12,1,33
relation-classification,5,implementation :,experiment,0,71,13,0,2
relation-classification,5,Our model is implemented using DYNET v 2.0 .,experiment,1,72,14,0,9
relation-classification,5,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",experiment,1,73,15,0,16
relation-classification,5,We compute the average of NER / EC score and RC score after each training epoch .,experiment,0,74,16,0,17
relation-classification,5,"We choose the model with the highest average score on the development set , which is then applied to the test set for the final evaluation phase .",experiment,0,75,17,0,28
relation-classification,5,More details of the implementation as well as optimal hyper - parameters are in the Appendix .,experiment,0,76,18,0,17
relation-classification,5,Our code is available at : https : //github.com/datquocnguyen/jointRE,experiment,1,77,19,0,9
relation-classification,5,"Metric : Similar to previous works in , we use the macro -averaged F1 - score over the entity classes to score NER / EC and over the relation classes to score RC .",experiment,0,78,20,0,34
relation-classification,5,More details of the metric are also in the Appendix .,experiment,0,79,21,0,11
relation-classification,5,"Unlike previous neural models , we report results as mean and standard deviation of the scores over 10 runs with 10 random seeds .",experiment,0,80,22,0,24
relation-classification,5,main results,result,0,81,1,0,2
relation-classification,5,end - to - end results :,method,0,82,1,0,7
relation-classification,5,The first six rows in compare our results with previous state - of - the - art published results on the same test set .,method,0,83,2,0,25
relation-classification,5,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .",method,0,84,3,0,30
relation-classification,5,We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .,method,0,85,4,1,23
relation-classification,5,"Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",method,0,86,5,1,67
relation-classification,5,"These results show that our model performs better than previous state - of - the - art models , using the same setup .",method,0,87,6,0,24
relation-classification,5,"In , the last two rows present results reported in and on the dataset CoNLL04 .",method,0,88,7,0,16
relation-classification,5,"However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .",method,0,89,8,0,25
relation-classification,5,Both and employ additional extra features based on external NLP tools and use larger training sets than ours .,method,0,90,9,0,19
relation-classification,5,"Specifically , integrate syntactic features by using a pre-trained BiLSTM - based dependency parser to extract BiLSTM - based latent feature representations for words in the input sentence , and then using these latent representations directly as part of the input embeddings in their model .",method,0,91,10,0,46
relation-classification,5,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,method,0,92,11,0,21
relation-classification,5,ablation analysis :,method,0,93,12,0,3
relation-classification,5,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",method,1,94,13,0,27
relation-classification,5,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",method,0,95,14,0,24
relation-classification,5,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .",method,1,96,15,0,20
relation-classification,5,"Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .",method,0,97,16,0,40
relation-classification,5,differences are not significant .,method,0,98,17,0,5
relation-classification,5,A similar observation is also found in .,method,0,99,18,0,8
relation-classification,5,"Also , in preliminary experiments , we do not find any significant difference in performance of our joint model when feeding gold NER labels instead of predicted NER labels into the RC component during training .",method,0,100,19,0,36
relation-classification,5,This is not surprising as the training NER score is at 99 +% .,method,0,101,20,0,14
relation-classification,5,also presents ablation tests over 5 factors of our joint model on the development set .,method,0,102,21,0,16
relation-classification,5,"In particular , Setup 1 performances significantly degrade by 4 + % absolutely , when not using the character - level word embeddings .",method,0,103,22,0,24
relation-classification,5,"The performances also decrease when using a softmax classifier for NER label prediction rather than a CRF layer ( here , the decrease is significant ) .",method,0,104,23,0,27
relation-classification,5,"In contrast , we do not find any significant difference in Setup 2 scores when not using either the character - level embeddings or the CRF layer , clearly showing the usefulness of the given gold entity boundaries .",method,0,105,24,0,39
relation-classification,5,"The 3 remaining factors , including removing NER label embeddings and not taking either the Bilinear or Linear part ( in Equation 8 ) into the Biaffine attention layer , do not affect the NER / EC score .",method,0,106,25,0,39
relation-classification,5,"However , they significantly decrease the RC score .",method,0,107,26,0,9
relation-classification,5,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",method,0,108,27,0,21
relation-classification,5,"More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , "" w / o Bilinear "" results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .",method,0,109,28,0,66
relation-classification,5,conclusion,method,0,110,29,0,1
relation-classification,5,"In this paper , we have presented an end - to - end neural network - based relation extraction model .",method,0,111,30,0,21
relation-classification,5,Our model employs a BiLSTM - CRF architecture for entity recognition and a biaffine attention mechanism for relation classification .,method,0,112,31,0,20
relation-classification,5,"On the benchmark CoNLL04 dataset , our model produces new state - of - the - art performance .",method,0,113,32,0,19
relation-classification,7,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,1,2,1,0,16
relation-classification,7,abstract,abstract,0,3,1,0,1
relation-classification,7,motivation :,abstract,0,4,2,0,2
relation-classification,7,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,0,5,3,0,16
relation-classification,7,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,1,6,4,0,37
relation-classification,7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,1,7,5,0,30
relation-classification,7,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,1,8,6,0,21
relation-classification,7,results :,result,0,9,1,0,2
relation-classification,7,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",result,0,10,2,0,32
relation-classification,7,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",result,0,11,3,0,36
relation-classification,7,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",result,0,12,4,0,65
relation-classification,7,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,result,0,13,5,0,18
relation-classification,7,availability and implementation :,result,0,14,6,0,4
relation-classification,7,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",result,1,15,7,0,21
relation-classification,7,introduction,introduction,0,16,1,0,1
relation-classification,7,The volume of biomedical literature continues to rapidly increase .,introduction,0,17,2,0,10
relation-classification,7,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",introduction,0,18,3,0,29
relation-classification,7,PubMed alone has a total of 29M articles as of January 2019 .,introduction,0,19,4,0,13
relation-classification,7,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,introduction,0,20,5,0,21
relation-classification,7,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",introduction,0,21,6,0,20
relation-classification,7,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,introduction,0,22,7,0,26
relation-classification,7,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",introduction,0,23,8,0,36
relation-classification,7,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,introduction,0,24,9,0,27
relation-classification,7,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",method,0,25,1,0,20
relation-classification,7,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",method,0,26,2,0,44
relation-classification,7,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",method,1,27,3,0,26
relation-classification,7,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",method,0,28,4,0,19
relation-classification,7,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",method,0,29,5,0,37
relation-classification,7,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions thatare usually not included in a general domain corpus .",method,0,30,6,0,39
relation-classification,7,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",method,0,31,7,0,32
relation-classification,7,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",method,0,32,8,0,34
relation-classification,7,approach,method,0,33,9,0,1
relation-classification,7,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",method,1,34,10,0,20
relation-classification,7,The over all process of pre-training and fine - tuning BioBERT is illustrated in .,method,0,35,11,0,15
relation-classification,7,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",method,1,36,12,0,25
relation-classification,7,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",method,1,37,13,0,20
relation-classification,7,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",method,1,38,14,0,27
relation-classification,7,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",method,0,39,15,0,28
relation-classification,7,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,method,0,40,16,0,20
relation-classification,7,The contributions of our paper are as follows :,method,0,41,17,0,9
relation-classification,7,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,method,0,42,18,0,23
relation-classification,7,We show that pre-training BERT on biomedical corpora largely improves its performance .,method,0,43,19,0,13
relation-classification,7,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",method,0,44,20,0,41
relation-classification,7,"Compared with most previous biomedical text mining models thatare mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",method,0,45,21,0,47
relation-classification,7,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",method,0,46,22,0,23
relation-classification,7,materials and methods,method,0,47,1,0,3
relation-classification,7,BioBERT basically has the same structure as BERT .,method,0,48,2,0,9
relation-classification,7,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",method,0,49,3,0,24
relation-classification,7,BERT : bidirectional encoder representations from transformers,method,0,50,4,0,7
relation-classification,7,Learning word representations from a large amount of unannotated text is a long - established method .,method,0,51,5,0,17
relation-classification,7,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",method,0,52,6,0,29
relation-classification,7,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",method,0,53,7,0,22
relation-classification,7,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,method,0,54,8,0,21
relation-classification,7,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",method,0,55,9,0,41
relation-classification,7,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",method,0,56,10,0,25
relation-classification,7,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",method,0,57,11,0,26
relation-classification,7,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",method,0,58,12,0,27
relation-classification,7,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,method,0,59,13,0,26
relation-classification,7,"Due to the space limitations , we refer readers to for a more detailed description of BERT .",method,0,60,14,0,18
relation-classification,7,pre-training biobert,method,0,61,15,0,2
relation-classification,7,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",method,0,62,1,0,18
relation-classification,7,"However , biomedical domain texts contain a considerable number of domain - specific .",method,0,63,2,0,14
relation-classification,7,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",method,0,64,3,0,37
relation-classification,7,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",method,0,65,4,0,22
relation-classification,7,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",method,0,66,5,0,24
relation-classification,7,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",method,0,67,6,0,23
relation-classification,7,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",method,0,68,7,0,25
relation-classification,7,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,method,0,69,8,0,22
relation-classification,7,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",method,0,70,9,0,18
relation-classification,7,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",method,0,71,10,0,17
relation-classification,7,I ##mm ##uno ##g ##lo # #bul # #in ) .,method,0,72,11,0,11
relation-classification,7,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,method,0,73,12,0,19
relation-classification,7,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",method,0,74,13,0,88
relation-classification,7,fine-tuning biobert,method,0,75,14,0,2
relation-classification,7,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",method,0,76,15,0,16
relation-classification,7,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",method,0,77,16,0,21
relation-classification,7,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",method,0,78,17,0,28
relation-classification,7,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",method,0,79,18,0,24
relation-classification,7,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,method,0,80,19,0,22
relation-classification,7,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",method,0,81,20,0,31
relation-classification,7,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",method,0,82,21,0,18
relation-classification,7,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,method,0,83,22,0,16
relation-classification,7,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",method,0,84,23,0,25
relation-classification,7,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,method,0,85,24,0,20
relation-classification,7,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,method,0,86,25,0,20
relation-classification,7,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,method,0,87,26,0,39
relation-classification,7,"The precision , recall and F 1 scores on the RE task are reported .",method,0,88,27,0,15
relation-classification,7,Question answering is a task of answering questions posed in natural language given related passages .,method,0,89,28,0,16
relation-classification,7,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",method,0,90,29,0,18
relation-classification,7,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,method,0,91,30,0,16
relation-classification,7,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,method,0,92,31,0,20
relation-classification,7,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",method,0,93,32,0,32
relation-classification,7,"Like , we excluded the samples with unanswerable questions from the training sets .",method,0,94,33,0,14
relation-classification,7,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",method,0,95,34,0,26
relation-classification,7,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",method,0,96,35,0,19
relation-classification,7,results,result,0,97,1,0,1
relation-classification,7,datasets,result,0,98,2,0,1
relation-classification,7,The statistics of biomedical NER datasets are listed in .,result,0,99,3,0,10
relation-classification,7,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",result,0,100,4,0,28
relation-classification,7,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,result,0,101,5,0,24
relation-classification,7,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,result,0,102,6,0,22
relation-classification,7,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,result,0,103,7,0,20
relation-classification,7,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",result,0,104,8,0,25
relation-classification,7,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets thatare frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",result,0,105,9,0,39
relation-classification,7,The RE datasets contain gene - disease relations and protein - chemical relations ) .,result,0,106,10,0,15
relation-classification,7,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,result,0,107,11,0,14
relation-classification,7,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",result,0,108,12,0,14
relation-classification,7,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",result,0,109,13,0,21
relation-classification,7,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,result,0,110,14,0,18
relation-classification,7,We have made the pre-processed BioASQ datasets publicly available .,result,0,111,15,0,10
relation-classification,7,"For all the datasets , we used the same dataset splits used in previous works ) for a fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",result,0,112,16,0,40
relation-classification,7,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",result,0,113,17,0,31
relation-classification,7,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,result,0,114,18,0,19
relation-classification,7,Note that the state - of - the - art models each have a different architecture and training procedure .,result,0,115,19,0,20
relation-classification,7,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",result,0,116,20,0,69
relation-classification,7,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",result,0,117,21,0,27
relation-classification,7,experimental setups,experiment,0,118,1,0,2
relation-classification,7,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,experiment,1,119,2,0,18
relation-classification,7,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,experiment,1,120,3,0,21
relation-classification,7,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",experiment,0,121,4,0,26
relation-classification,7,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",experiment,0,122,5,0,42
relation-classification,7,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",experiment,0,123,6,0,33
relation-classification,7,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,experiment,0,124,7,0,27
relation-classification,7,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",experiment,0,125,8,0,29
relation-classification,7,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,experiment,1,126,9,0,13
relation-classification,7,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",experiment,1,127,10,0,25
relation-classification,7,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,experiment,0,128,11,0,29
relation-classification,7,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",experiment,0,129,12,0,23
relation-classification,7,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,experiment,1,130,13,0,20
relation-classification,7,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,experiment,0,131,14,0,15
relation-classification,7,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",experiment,0,132,15,0,30
relation-classification,7,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,experiment,0,133,16,0,33
relation-classification,7,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",experiment,0,134,17,0,23
relation-classification,7,experimental results,experiment,0,135,1,0,2
relation-classification,7,The results of NER are shown in .,experiment,1,136,2,0,8
relation-classification,7,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",experiment,0,137,3,0,48
relation-classification,7,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",experiment,1,138,4,0,16
relation-classification,7,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",experiment,1,139,5,0,45
relation-classification,7,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",experiment,0,140,6,0,56
relation-classification,7,The RE results of each model are shown in .,experiment,1,141,7,0,10
relation-classification,7,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",experiment,0,142,8,0,26
relation-classification,7,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",experiment,1,143,9,0,31
relation-classification,7,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",experiment,1,144,10,0,17
relation-classification,7,The QA results are shown in .,experiment,1,145,11,0,7
relation-classification,7,We micro averaged the best scores of the state - of - the - art models from each batch .,experiment,0,146,12,0,20
relation-classification,7,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,experiment,0,147,13,0,23
relation-classification,7,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",experiment,1,148,14,0,55
relation-classification,7,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",experiment,1,149,15,0,23
relation-classification,7,discussion,experiment,0,150,16,0,1
relation-classification,7,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,experiment,0,151,17,0,16
relation-classification,7,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",experiment,0,152,18,0,25
relation-classification,7,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",experiment,0,153,19,0,34
relation-classification,7,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",experiment,0,154,20,0,22
relation-classification,7,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,experiment,0,155,21,0,34
relation-classification,7,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,experiment,0,156,22,0,26
relation-classification,7,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,experiment,0,157,23,0,19
relation-classification,7,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",experiment,0,158,24,0,22
relation-classification,7,"F1 scores were used for NER / RE , and MRR scores were used for QA .",experiment,0,159,25,0,17
relation-classification,7,BioBERT significantly improves performance on most of the datasets .,experiment,0,160,26,0,10
relation-classification,7,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to see the effect of pre-training on downstream tasks .",experiment,0,161,27,0,26
relation-classification,7,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,experiment,0,162,28,0,59
relation-classification,7,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",experiment,0,163,29,0,30
relation-classification,7,entities .,experiment,0,164,30,0,2
relation-classification,7,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",experiment,0,165,31,0,19
relation-classification,7,"Also , BioBERT can provide longer named entities as answers .",experiment,0,166,32,0,11
relation-classification,7,conclusion,experiment,0,167,33,0,1
relation-classification,7,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",experiment,0,168,34,0,20
relation-classification,7,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,experiment,0,169,35,0,18
relation-classification,7,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",experiment,0,170,36,0,25
relation-classification,7,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",experiment,0,171,37,1,40
relation-classification,7,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,experiment,0,172,38,0,53
relation-classification,7,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",experiment,0,173,39,0,23
relation-classification,7,"The best scores are in bold , and the second best scores are underlined .",experiment,0,174,40,0,15
relation-classification,7,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",experiment,0,175,41,1,52
relation-classification,7,"The best scores are in bold , and the second best scores are underlined .",experiment,0,176,42,0,15
relation-classification,7,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",experiment,0,177,43,0,21
relation-classification,7,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",experiment,0,178,44,0,27
relation-classification,7,"The best scores are in bold , and the second best scores are underlined .",experiment,0,179,45,0,15
relation-classification,7,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-are a.bioasq.org ) .,experiment,0,180,46,0,23
relation-classification,7,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",experiment,0,181,47,0,19
relation-classification,7,biobert,experiment,0,182,48,0,1
relation-classification,7,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,experiment,0,183,49,0,15
relation-classification,7,bc2gm,experiment,0,184,50,0,1
relation-classification,7,bert,experiment,0,185,51,0,1
relation-classification,7,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",experiment,0,186,52,0,25
relation-classification,7,qa,experiment,0,187,53,0,1
relation-classification,7,bioasq 6 b - factoid,experiment,0,188,54,0,5
relation-classification,7,Q : Which type of urinary incontinence is diagnosed with the Q tip test ?,experiment,0,189,55,0,15
relation-classification,7,bert,experiment,0,190,56,0,1
relation-classification,7,A total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,experiment,0,191,57,0,17
relation-classification,7,after undergoing ( . . .),experiment,0,192,58,0,6
relation-classification,7,"q-tip test , . . .",experiment,0,193,59,0,6
relation-classification,7,Q : Which bacteria causes erythrasma ?,experiment,0,194,60,0,7
relation-classification,7,bert,experiment,0,195,61,0,1
relation-classification,7,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,experiment,0,196,62,0,15
relation-classification,7,note :,experiment,0,197,63,0,2
relation-classification,7,Predicted named entities for NER and predicted answers for QA are in bold .,experiment,0,198,64,0,14
relation-classification,7,funding,experiment,0,199,65,0,1
relation-classification,4,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,title,1,2,1,0,9
relation-classification,4,abstract,abstract,0,3,1,0,1
relation-classification,4,Dependency trees help relation extraction models capture long - range relations between words .,abstract,1,4,2,0,14
relation-classification,4,"However , existing dependency - based models either neglect crucial information ( e.g. , negation ) by pruning the dependency trees too aggressively , or are computationally inefficient because it is difficult to parallelize over different tree structures .",abstract,0,5,3,0,39
relation-classification,4,"We propose an extension of graph convolutional networks that is tailored for relation extraction , which pools information over arbitrary dependency structures efficiently in parallel .",abstract,0,6,4,0,26
relation-classification,4,"To incorporate relevant information while maximally removing irrelevant content , we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold .",abstract,0,7,5,0,40
relation-classification,4,"The resulting model achieves state - of - the - art performance on the large - scale TACRED dataset , outperforming existing sequence and dependency - based neural models .",abstract,0,8,6,0,30
relation-classification,4,"We also show through detailed analysis that this model has complementary strengths to sequence models , and combining them further improves the state of the art .",abstract,0,9,7,0,27
relation-classification,4,* equal contribution .,abstract,0,10,8,0,4
relation-classification,4,The order of authorship was decided by a tossed coin .,abstract,0,11,9,0,11
relation-classification,4,introduction,introduction,0,12,1,0,1
relation-classification,4,"Relation extraction involves discerning whether a relation exists between two entities in a sentence ( often termed subject and object , respectively ) .",introduction,0,13,2,0,24
relation-classification,4,"Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale , such as question answering , knowledge base population , and biomedical knowledge discovery .",introduction,0,14,3,0,33
relation-classification,4,"Models making use of dependency parses of the input sentences , or dependency - based models , : An example modified from the TAC KBP challenge corpus .",introduction,0,15,4,0,28
relation-classification,4,"A subtree of the original UD dependency tree between the subject ( "" he "" ) and object ( "" Mike Cane "" ) is also shown , where the shortest dependency path between the entities is highlighted in bold .",introduction,0,16,5,0,41
relation-classification,4,"Note that negation ( "" not "" ) is off the dependency path .",introduction,0,17,6,0,14
relation-classification,4,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",introduction,1,18,7,0,39
relation-classification,4,Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,introduction,0,19,8,0,22
relation-classification,4,"However , these models face the challenge of sparse feature spaces and are brittle to lexical variations .",introduction,0,20,9,0,18
relation-classification,4,More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees .,introduction,0,21,10,0,20
relation-classification,4,One common approach to leverage dependency information is to perform bottom - up or top - down computation along the parse tree or the subtree below the lowest common ancestor ( LCA ) of the entities .,introduction,0,22,11,0,37
relation-classification,4,"Another popular approach , inspired by , is to reduce the parse tree to the shortest dependency path between the entities .",introduction,0,23,12,0,22
relation-classification,4,"However , these models suffer from several drawbacks .",introduction,0,24,13,0,9
relation-classification,4,"Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient , because aligning trees for efficient batch training is usually nontrivial .",introduction,0,25,14,0,28
relation-classification,4,"Models based on the shortest dependency path between the subject and object are computationally more efficient , but this simplifying assumption has major limitations as well .",introduction,0,26,15,0,27
relation-classification,4,"shows a real - world example where crucial information ( i.e. , negation ) would be excluded when the model is restricted to only considering the dependency path .",introduction,0,27,16,0,29
relation-classification,4,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .",introduction,0,28,17,0,22
relation-classification,4,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",introduction,1,29,18,0,28
relation-classification,4,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",introduction,1,30,19,0,35
relation-classification,4,"We test our model on the popular SemEval 2010 Task 8 dataset and the more recent , larger TAC - RED dataset .",introduction,0,31,20,0,23
relation-classification,4,"On both datasets , our model not only outperforms existing dependency - based neural models by a significant margin when combined with the new pruning technique , but also achieves a 10 - 100x speedup over existing tree - based models .",introduction,0,32,21,0,42
relation-classification,4,"On TACRED , our model further achieves the state - of - the - art performance , surpassing a competitive neural sequence model baseline .",introduction,0,33,22,0,25
relation-classification,4,"This model also exhibits complementary strengths to sequence models on TACRED , and combining these two model types through simple prediction interpolation further improves the state of the art .",introduction,0,34,23,0,30
relation-classification,4,"To recap , our main contributions are : ( i ) we propose a neural model for relation extraction based on graph convolutional networks , which allows it to efficiently pool information over arbitrary dependency structures ; ( ii ) we present a new pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness ; ( iii ) we present detailed analysis on the model and the pruning technique , and show that dependency - based models have complementary strengths with sequence models .",introduction,0,35,24,0,93
relation-classification,4,models,introduction,0,36,25,0,1
relation-classification,4,"In this section , we first describe graph convolutional networks ( GCNs ) over dependency tree structures , and then we introduce an architecture that uses GCNs at its core for relation extraction .",introduction,0,37,26,0,34
relation-classification,4,Graph Convolutional Networks over Dependency Trees,introduction,0,38,27,0,6
relation-classification,4,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs .,introduction,0,39,28,0,16
relation-classification,4,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",introduction,0,40,29,0,37
relation-classification,4,"In an L-layer GCN , if we denote by h ( l?1 ) i the input vector and h ( l ) i the output vector of node i at the l - th layer , a graph convolution operation can be written as",introduction,0,41,30,0,44
relation-classification,4,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ?",introduction,0,42,31,0,20
relation-classification,4,"a nonlinear function ( e.g. , ReLU ) .",introduction,0,43,32,0,9
relation-classification,4,"Intuitively , during each graph convolution , each node gathers and summarizes information from its neighboring nodes in the graph .",introduction,0,44,33,0,21
relation-classification,4,"We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A , where A ij = 1 if there is a dependency edge between tokens i and j .",introduction,0,45,34,0,38
relation-classification,4,"However , naively applying the graph convolution operation in Equation ( 1 ) could lead to node representations with drastically different magnitudes , since the degree of a token varies a lot .",introduction,0,46,35,0,33
relation-classification,4,This could bias our sentence representation towards favoring high - degree nodes regardless of the information carried in the node ( see details in Section 2.2 ) .,introduction,0,47,36,0,28
relation-classification,4,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .",introduction,0,48,37,0,28
relation-classification,4,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",introduction,0,49,38,0,31
relation-classification,4,"where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",introduction,0,50,39,0,32
relation-classification,4,Relation extraction with a graph convolutional network .,introduction,0,51,40,0,8
relation-classification,4,"The left side shows the over all architecture , while on the right side , we only show the detailed graph convolution computation for the word "" relative "" for clarity .",introduction,0,52,41,0,32
relation-classification,4,A full unlabeled dependency parse of the sentence is also provided for reference .,introduction,0,53,42,0,14
relation-classification,4,stacking this operation over,introduction,0,54,43,0,4
relation-classification,4,"L layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .",introduction,0,55,44,0,39
relation-classification,4,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,introduction,0,56,45,0,19
relation-classification,4,"We also experimented with : ( 1 ) using different transformation matrices W for topdown , bottom - up , and self - loop edges ; and ( 2 ) adding dependency relation - specific parameters for edge - wise gating , similar to .",introduction,0,57,46,0,45
relation-classification,4,"We found that modeling directions does not lead to improvement , 1 and adding edgewise gating further hurts performance .",introduction,0,58,47,0,20
relation-classification,4,"We hypothesize that this is because the presented GCN model is usually already able to capture dependency edge patterns that are informative for classifying relations , and modeling edge directions and types does not offer additional discriminative power to the network before it leads to overfitting .",introduction,0,59,48,0,47
relation-classification,4,"For example , the relations entailed by "" A 's son , B "" and "" B 's son , A "" can be readily distinguished with "" 's "" attached to different entities , even when edge directionality is not considered .",introduction,0,60,49,0,43
relation-classification,4,encoding relations with gcn,introduction,0,61,50,0,4
relation-classification,4,We now formally define the task of relation extraction .,introduction,0,62,51,0,10
relation-classification,4,"Let X = [ x 1 , ... , x n ] denote a sentence , where xi is the i th token .",introduction,0,63,52,0,24
relation-classification,4,A subject entity and an object entity are identified and correspond to two spans in the sentence :,introduction,0,64,53,0,18
relation-classification,4,.,introduction,0,65,54,0,1
relation-classification,4,"Given X , X s , and X o , the goal of relation extraction is to predict a relation r ?",introduction,0,66,55,0,22
relation-classification,4,"R ( a predefined relation set ) that holds between the entities or "" no relation "" otherwise .",introduction,0,67,56,0,19
relation-classification,4,"After applying an L-layer GCN over word vectors , we obtain hidden representations of each token that are directly influenced by its neighbors no more than L edges apart in the dependency tree .",introduction,0,68,57,0,34
relation-classification,4,"To make use of these word representations for relation extraction , we first obtain a sentence representation as follows ( see also left ) :",introduction,0,69,58,0,25
relation-classification,4,"where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ?",introduction,0,70,59,0,23
relation-classification,4,Rd is a max pooling function that maps from n output vectors to the sentence vector .,introduction,0,71,60,0,17
relation-classification,4,We also observe that information close to entity tokens in the dependency tree is often central to relation classification .,introduction,0,72,61,0,20
relation-classification,4,"Therefore , we also obtain a subject representation h s from h ( L ) as follows",introduction,0,73,62,0,17
relation-classification,4,as well as an object representation ho similarly .,introduction,0,74,63,0,9
relation-classification,4,"Inspired by recent work on relational learning between entities , we obtain the final representation used for classification by concatenating the sentence and the entity representations , and feeding them through a feed - forward neural network ( FFNN ) :",introduction,0,75,64,0,41
relation-classification,4,This h final representation is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over relations .,introduction,0,76,65,0,24
relation-classification,4,contextualized gcn,introduction,0,77,66,0,2
relation-classification,4,"The network architecture introduced so far learns effective representations for relation extraction , but it also leaves a few issues inadequately addressed .",introduction,0,78,67,0,23
relation-classification,4,"First , the input word vectors do not contain contextual information about word order or dis ambiguation .",introduction,0,79,68,0,18
relation-classification,4,"Second , the GCN highly depends on a correct parse tree to extract crucial information from the sentence ( especially when pruning is performed ) , while existing parsing algorithms produce imperfect trees in many cases .",introduction,0,80,69,0,37
relation-classification,4,"To resolve these issues , we further apply a Contextualized GCN ( C - GCN ) model , where the input word vectors are first fed into a bi-directional long short - term memory ( LSTM ) network to generate contextualized representations , which are then used ash ( 0 ) in the original model .",introduction,0,81,70,0,56
relation-classification,4,This BiL - STM contextualization layer is trained jointly with the rest of the network .,introduction,0,82,71,0,16
relation-classification,4,We show empirically in Section 5 that this augmentation substantially improves the performance over the original model .,introduction,0,83,72,0,18
relation-classification,4,"We note that this relation extraction model is conceptually similar to graph kernel - based models , in that it aims to utilize local dependency tree patterns to inform relation classification .",introduction,0,84,73,0,32
relation-classification,4,"Our model also incorporates crucial off - path information , which greatly improves its robustness compared to shortest dependency pathbased approaches .",introduction,0,85,74,0,22
relation-classification,4,"Compared to tree - structured models ( e.g. , Tree - LSTM",introduction,0,86,75,0,12
relation-classification,4,"( Tai et al. , 2015 ) ) , it not only is able to capture more global information through the use of pooling functions , but also achieves substantial speedup by not requiring recursive operations that are difficult to parallelize .",introduction,0,87,76,1,42
relation-classification,4,"For example , we observe that on a Titan Xp GPU , training a Tree - LSTM model over a minibatch of 50 examples takes 6.54 seconds on average , while training the original GCN model takes only 0.07 seconds , and the C - GCN model 0.08 seconds .",introduction,0,88,77,0,50
relation-classification,4,Incorporating Off - path Information with Path - centric Pruning,introduction,0,89,78,0,10
relation-classification,4,"Dependency trees provide rich structures that one can exploit in relation extraction , but most of the information pertinent to relations is usually contained within the subtree rooted at the lowest common ancestor ( LCA ) of the two entities .",introduction,0,90,79,0,41
relation-classification,4,Previous studies have shown that removing tokens outside this scope helps relation extraction by eliminating irrelevant information from the sentence .,introduction,0,91,80,0,21
relation-classification,4,It is therefore desirable to combine our GCN models with tree pruning strategies to further improve performance .,introduction,0,92,81,0,18
relation-classification,4,"However , pruning too aggressively ( e.g. , keeping only the dependency path ) could lead to loss of crucial information and conversely hurt robustness .",introduction,0,93,82,0,26
relation-classification,4,"For instance , the negation in is neglected when a model is restricted to only looking at the dependency path between the entities .",introduction,0,94,83,0,24
relation-classification,4,"Similarly , in the sentence "" She was diagnosed with cancer last year , and succumbed this June "" , the dependency path She?diagnosed ?",introduction,0,95,84,0,25
relation-classification,4,cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction dependency to succumbed is also present .,introduction,0,96,85,0,26
relation-classification,4,"Motivated by these observations , we propose path - centric pruning , a novel technique to incorporate information off the dependency path .",introduction,0,97,86,0,23
relation-classification,4,This is achieved by including tokens that are up to distance K away from the dependency path in the LCA subtree .,introduction,0,98,87,0,22
relation-classification,4,"K = 0 , corresponds to pruning the tree down to the path , K = 1 keeps all nodes that are directly attached to the path , and K = ?",introduction,0,99,88,0,32
relation-classification,4,retains the entire LCA subtree .,introduction,0,100,89,0,6
relation-classification,4,"We combine this pruning strategy with our GCN model , by directly feeding the pruned trees into the graph convolutional layers .",introduction,0,101,90,0,22
relation-classification,4,"We show that pruning with K = 1 achieves the best balance between including relevant information ( e.g. , negation and conjunction ) and keeping irrelevant content out of the resulting pruned tree as much as possible .",introduction,0,102,91,0,38
relation-classification,4,related work,related work,0,103,1,0,2
relation-classification,4,"At the core of fully - supervised and distantlysupervised relation extraction approaches are statistical classifiers , many of which find syntactic information beneficial .",related work,0,104,2,0,24
relation-classification,4,"For example , explored adding syntactic features to a statistical classifier and found them to be useful when sentences are long .",related work,0,105,3,0,22
relation-classification,4,"Various kernel - based approaches also leverage syntactic information to measure similarity between training and test examples to predict the relation , finding that tree - based kernels and dependency path - based kernels ( Bunescu and Mooney , 2005 ) are effective for this task .",related work,0,106,4,1,47
relation-classification,4,Recent studies have found neural models effective in relation extraction .,related work,0,107,5,0,11
relation-classification,4,first applied a one - dimensional convolutional neural network ( CNN ) with manual features to encode relations .,related work,0,108,6,0,19
relation-classification,4,showed that combining a CNN with a recurrent neural network ( RNN ) through a voting scheme can further improve performance .,related work,0,109,7,0,22
relation-classification,4,and proposed to use attention mechanisms over RNN and CNN architectures for this task .,related work,0,110,8,0,15
relation-classification,4,"Apart from neural models over word sequences , incorporating dependency trees into neural models has also been shown to improve relation extraction performance by capturing long - distance relations .",related work,0,111,9,0,30
relation-classification,4,generalized the idea of dependency path kernels by applying a LSTM network over the shortest dependency path between entities .,related work,0,112,10,0,20
relation-classification,4,Liu et al. first applied a recursive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path .,related work,0,113,11,1,28
relation-classification,4,"Miwa and Bansal ( 2016 ) applied a Tree - LSTM , a generalized form of LSTM over dependency trees , in a joint entity and relation extraction setting .",related work,0,114,12,1,30
relation-classification,4,They found it to be most effective when applied to the subtree rooted at the LCA of the two entities .,related work,0,115,13,0,21
relation-classification,4,"More recently , and have shown that relatively simple neural models ( CNN and augmented LSTM , respectively ) can achieve comparable or superior performance to dependency - based models when trained on larger datasets .",related work,0,116,14,0,36
relation-classification,4,"In this paper , we study dependency - based models in depth and show that with a properly designed architecture , they can outperform and have complementary advantages to sequence models , even in a large - scale setting .",related work,0,117,15,0,40
relation-classification,4,"Finally , we note that a technique similar to pathcentric pruning has been applied to reduce the space of possible arguments in semantic role labeling .",related work,0,118,16,0,26
relation-classification,4,"The authors showed pruning words too faraway from the path between the predicate and the root to be beneficial , but reported the best pruning distance to be 10 , which almost always retains the entire tree .",related work,0,119,17,0,38
relation-classification,4,"Our method differs in that it is applied to the shortest dependency path between entities , and we show that in our technique the best pruning distance is 1 for several dependency - based relation extraction models .",related work,0,120,18,0,38
relation-classification,4,experiments,experiment,0,121,1,0,1
relation-classification,4,baseline models,experiment,0,122,2,0,2
relation-classification,4,We compare our models with several competitive dependency - based and neural sequence models .,experiment,0,123,3,0,15
relation-classification,4,dependency - based models .,experiment,1,124,4,0,5
relation-classification,4,In our main experiments we compare with three types of dependency - based models .,experiment,0,125,5,0,15
relation-classification,4,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,experiment,1,126,6,0,19
relation-classification,4,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .",experiment,1,127,7,0,34
relation-classification,4,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",experiment,1,128,8,0,18
relation-classification,4,"We investigate the child - sum variant of Tree - LSTM , and apply it to the dependency tree ( or part of it ) .",experiment,0,129,9,0,26
relation-classification,4,"In practice , we find that modifying this model by concatenating dependency label embeddings to the input of forget gates improves its performance on relation extraction , and therefore use this variant in our experiments .",experiment,0,130,10,0,36
relation-classification,4,"Earlier , our group compared and with sequence models , and we report these results ; for ( 3 ) we report results with our own implementation .",experiment,0,131,11,0,28
relation-classification,4,neural sequence model .,experiment,1,132,12,0,4
relation-classification,4,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .",experiment,1,133,13,0,41
relation-classification,4,"We compare with this strong baseline , and use its open implementation in further analysis .",experiment,0,134,14,0,16
relation-classification,4,3,experiment,0,135,15,0,1
relation-classification,4,experimental setup,experiment,0,136,1,0,2
relation-classification,4,We conduct experiments on two relation extraction datasets :,experiment,0,137,2,0,9
relation-classification,4,"( 1 ) TACRED : Introduced in , TACRED contains over 106 k mention pairs drawn from the yearly TAC KBP 4 challenge .",experiment,0,138,3,0,24
relation-classification,4,It represents 41 relation types and a special no relation class when the mention pair does not have a relation between them within these categories .,experiment,0,139,4,0,26
relation-classification,4,"Mentions in TACRED are typed , with subjects categorized into person and organization , and objects into 16 fine - grained types ( e.g. , date and location ) .",experiment,0,140,5,0,30
relation-classification,4,We report micro-averaged F 1 scores on this dataset as is conventional .,experiment,0,141,6,0,13
relation-classification,4,"For fair comparisons on the TACRED dataset , we follow the evaluation protocol used in by selecting the model with the median dev F 1 from 5 independent runs and reporting its test F 1 .",experiment,0,142,7,0,36
relation-classification,4,"We also use the same "" entity mask "" strategy where we replace each subject ( and object similarly ) entity with a special SUBJ - < NER > token .",experiment,0,143,8,0,31
relation-classification,4,"For all models , we also adopt the "" multichannel "" strategy by concatenating the input word embeddings with POS and NER embeddings .",experiment,0,144,9,0,24
relation-classification,4,"Traditionally , evaluation on SemEval is conducted without entity mentions masked .",experiment,0,145,10,0,12
relation-classification,4,"However , as we will discuss in Section 6.4 , this method encourages models to overfit to these mentions and fails to test their actual ability to generalize .",experiment,0,146,11,0,29
relation-classification,4,"We therefore report results with two evaluation protocols : ( 1 ) with- mention , where mentions are kept for comparison with previous work ; and ( 2 ) maskmention , where they are masked to test the generalization of our model in a more realistic setting .",experiment,0,147,12,0,48
relation-classification,4,"Due to space limitations , we report model training details in the supplementary material .",experiment,0,148,13,0,15
relation-classification,4,Results on the TACRED Dataset,experiment,1,149,14,0,5
relation-classification,4,We present our main results on the TACRED test set in .,experiment,0,150,15,0,12
relation-classification,4,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,experiment,1,151,16,0,30
relation-classification,4,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .",experiment,1,152,17,0,33
relation-classification,4,"In addition , we find our model improves upon other dependencybased models in both precision and recall .",experiment,1,153,18,0,18
relation-classification,4,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .",experiment,1,154,19,0,22
relation-classification,4,We hypothesize that this is because the C - GCN is more robust to parse errors by capturing local word patterns ( see also Section 6.2 ) .,experiment,0,155,20,0,28
relation-classification,4,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .",experiment,1,156,21,0,25
relation-classification,4,"To leverage this result , we experiment with a simple interpolation strategy to combine these models .",experiment,0,157,22,0,17
relation-classification,4,"Given the output probabilities PG ( r|x ) from a GCN model and PS ( r|x ) from the sequence model for any relation r , we calculate the interpolated probability as",experiment,0,158,23,0,32
relation-classification,4,where ? ?,experiment,0,159,24,0,3
relation-classification,4,"[ 0 , 1 ] is chosen on the dev set and set to 0.6 .",experiment,0,160,25,0,16
relation-classification,4,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .",experiment,0,161,26,0,30
relation-classification,4,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,experiment,0,162,27,0,19
relation-classification,4,Results on the SemEval Dataset,experiment,1,163,28,0,5
relation-classification,4,"To study the generalizability of our proposed model , we also trained and evaluated our best C - GCN model on the SemEval test set ) .",experiment,0,164,29,0,27
relation-classification,4,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .",experiment,1,165,30,0,30
relation-classification,4,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .",experiment,1,166,31,0,27
relation-classification,4,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .",experiment,1,167,32,0,32
relation-classification,4,Effect of Path - centric Pruning,experiment,1,168,33,0,6
relation-classification,4,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .",experiment,1,169,34,0,29
relation-classification,4,"We experimented with K ? { 0 , 1 , 2 , ?} on the TACRED dev set , and also include results when the full tree is used .",experiment,0,170,35,0,30
relation-classification,4,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .",experiment,1,171,36,0,30
relation-classification,4,This confirms our hypothesis in Section 3 that incorporating off - path information is crucial to relation extraction .,experiment,0,172,37,0,19
relation-classification,4,Miwa and Bansal ( 2016 ) reported that a Tree - LSTM achieves similar performance when the dependency path and the LCA subtree are used respectively .,experiment,0,173,38,1,27
relation-classification,4,"Our experiments confirm this , and further show that the result can be improved by path - centric pruning with K = 1 .",experiment,0,174,39,0,24
relation-classification,4,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .",experiment,1,175,40,0,25
relation-classification,4,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .",experiment,1,176,41,0,48
relation-classification,4,analysis & discussion,experiment,0,177,42,0,3
relation-classification,4,ablation study,experiment,0,178,43,0,2
relation-classification,4,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",experiment,1,179,44,0,26
relation-classification,4,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,experiment,1,180,45,0,15
relation-classification,4,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .",experiment,1,181,46,0,25
relation-classification,4,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",experiment,1,182,47,0,24
relation-classification,4,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",experiment,1,183,48,0,25
relation-classification,4,Complementary Strengths of GCNs and PA - LSTMs,experiment,0,184,49,0,8
relation-classification,4,"To understand what the GCN models are capturing and how they differ from a sequence model such as the PA - LSTM , we compared their performance :",experiment,0,185,50,0,28
relation-classification,4,The three dependency edges that contribute the most to the classification of different relations in the TACRED dev set .,experiment,0,186,51,0,20
relation-classification,4,"For clarity , we removed edges which 1 ) connect to common punctuation ( i.e. , commas , periods , and quotation marks ) , 2 ) connect to common prepositions ( i.e. , of , to , by ) , and 3 ) connect between tokens within the same entity .",experiment,0,187,52,0,52
relation-classification,4,"We use PER , ORG for entity types of PERSON , ORGANIZATION .",experiment,0,188,53,0,13
relation-classification,4,"We use S - and O - to denote subject and object entities , respectively .",experiment,0,189,54,0,16
relation-classification,4,We also include edges for more relations in the supplementary material .,experiment,0,190,55,0,12
relation-classification,4,over examples in the TACRED dev set .,experiment,0,191,56,0,8
relation-classification,4,"Specifically , for each model , we trained it for 5 independent runs with different seeds , and for each example we evaluated the model 's accuracy over these 5 runs .",experiment,0,192,57,0,32
relation-classification,4,"For instance , if a model correctly classifies an example for 3 out of 5 times , it achieves an accuracy of 60 % on this example .",experiment,0,193,58,0,28
relation-classification,4,"We observe that on 847 ( 3.7 % ) dev examples , our C - GCN model achieves an accuracy at least 60 % higher than that of the PA - LSTM , while on 629 ( 2.8 % ) examples the PA - LSTM achieves 60 % higher .",experiment,0,194,59,0,50
relation-classification,4,This complementary performance explains the gain we see in when the two models are combined .,experiment,0,195,60,0,16
relation-classification,4,"We further show that this difference is due to each model 's competitive advantage : dependency - based models are better at handling sentences with entities farther apart , while sequence models can better leverage local word patterns regardless of parsing quality ( see also .",experiment,0,196,61,0,46
relation-classification,4,We include further analysis in the supplementary material .,experiment,0,197,62,0,9
relation-classification,4,understanding model behavior,experiment,0,198,63,0,3
relation-classification,4,"To gain more insights into the C - GCN model 's behavior , we visualized the partial dependency tree it is processing and how much each token 's final representation contributed to h sent ( ) .",experiment,0,199,64,0,37
relation-classification,4,"We find that the model often focuses on the dependency path , but sometimes also incorporates offpath information to help reinforce its prediction .",experiment,0,200,65,0,24
relation-classification,4,"The model also learns to ignore determiners ( e.g. , "" the "" ) as they rarely affect relation prediction .",experiment,0,201,66,0,21
relation-classification,4,"To further understand what dependency edges contribute most to the classification of different relations , we scored each dependency edge by summing up the number of dimensions each of its connected nodes contributed to h sent .",experiment,0,202,67,0,37
relation-classification,4,We present the top scoring edges in .,experiment,0,203,68,0,8
relation-classification,4,"As can be seen in the table , most of these edges are associated with indicative nouns or verbs of each relation .",experiment,0,204,69,0,23
relation-classification,4,5,experiment,0,205,70,0,1
relation-classification,4,Entity Bias in the SemEval Dataset,experiment,0,206,71,0,6
relation-classification,4,"In our study , we observed a high correlation between the entity mentions in a sentence and its relation label in the SemEval dataset .",experiment,0,207,72,0,25
relation-classification,4,"We experimented with PA - LSTM models to analyze this dependency tree corresponding to K = 1 in path-centric pruning is shown , and the shortest dependency path is thickened .",experiment,0,208,73,0,31
relation-classification,4,We omit edges to punctuation for clarity .,experiment,0,209,74,0,8
relation-classification,4,"The first example shows that the C - GCN is effective at leveraging long - range dependencies while reducing noise with the help of pruning ( while the PA - LSTM predicts no relation twice , org : alternate names twice , and org : parents once in this case ) .",experiment,0,210,75,0,52
relation-classification,4,"The second example shows that the PA - LSTM is better at leveraging the proximity of the word "" migrated "" regardless of attachment errors in the parse ( while the C - GCN is misled to predict per :country of birth three times , and no relation twice ) .",experiment,0,211,76,0,51
relation-classification,4,phenomenon .,experiment,0,212,77,0,2
relation-classification,4,"We started by simplifying every sentence in the SemEval training and dev sets to "" subject and object "" , where subject and object are the actual entities in the sentence .",experiment,0,213,78,0,32
relation-classification,4,"Surprisingly , a trained PA - LSTM model on this data is able to achieve 65.1 F 1 on the dev set if Glo Ve is used to initialize word vectors , and 47.9 dev F 1 even without GloVe initialization .",experiment,0,214,79,0,42
relation-classification,4,"To further evaluate the model in a more realistic setting , we trained one model with the original SemEval training set ( unmasked ) and one with mentions masked in the training set , following what we have done for TACRED ( masked ) .",experiment,0,215,80,0,45
relation-classification,4,"While the unmasked model achieves a 83.6 F 1 on the original SemEval dev set , F 1 drops drastically to 62.4 if we replace dev set entity mentions with a special < UNK > token to simulate the presence of unseen entities .",experiment,0,216,81,0,44
relation-classification,4,"In contrast , the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74.7 .",experiment,0,217,82,0,22
relation-classification,4,This suggests that models trained without entities masked generalize poorly to new examples with unseen entities .,experiment,0,218,83,0,17
relation-classification,4,Our findings call for more careful evaluation that takes dataset biases into account in future relation extraction studies .,experiment,0,219,84,0,19
relation-classification,4,conclusion,experiment,0,220,85,0,1
relation-classification,4,We showed the success of a neural architecture based on a graph convolutional network for relation extraction .,experiment,0,221,86,0,18
relation-classification,4,We also proposed path - centric pruning to improve the robustness of dependencybased models by removing irrelevant content without ignoring crucial information .,experiment,0,222,87,0,23
relation-classification,4,"We showed through detailed analysis that our model has complementary strengths to sequence models , and that the proposed pruning technique can be effectively applied to other dependency - based models .",experiment,0,223,88,0,32
relation-classification,4,a experimental details,experiment,0,224,1,0,3
relation-classification,4,a.1 hyperparameters tacred,experiment,0,225,2,0,3
relation-classification,4,We set LSTM hidden size to 200 in all neural models .,experiment,0,226,3,0,12
relation-classification,4,We also use hidden size 200 for the output feedforward layers in the GCN model .,experiment,0,227,4,0,16
relation-classification,4,We use 2 GCN layers and 2 feedforward ( FFNN ) layers in our experiments .,experiment,0,228,5,0,16
relation-classification,4,We employ the ReLU function for all nonlinearities in the GCN layers and the standard max pooling operations in all pooling layers .,experiment,0,229,6,0,23
relation-classification,4,"For the Tree - LSTM model , we find a 2 - layer architecture works substantially better than the vanilla 1 - layer model , and use it in all our experiments .",experiment,0,230,7,0,33
relation-classification,4,"For both the Tree - LSTM and our models , we apply path - centric pruning with K = 1 , as we find that this generates best results for all models ( also see ) .",experiment,0,231,8,0,37
relation-classification,4,"We use the pretrained 300 - dimensional Glo Ve vectors to initialize word embeddings , and we use embedding size of 30 for all other embeddings ( i.e. , POS , NER ) .",experiment,0,232,9,0,34
relation-classification,4,"We use the dependency parse trees , POS and NER sequences as included in the original release of the dataset , which was generated with Stanford CoreNLP .",experiment,0,233,10,0,28
relation-classification,4,For regularization we apply dropout with p = 0.5 to all LSTM layers and all but the last GCN layers .,experiment,0,234,11,0,21
relation-classification,4,sem eval,experiment,0,235,12,0,2
relation-classification,4,We use LSTM hidden size of 100 and use 1 GCN layer for the SemEval dataset .,experiment,0,236,13,0,17
relation-classification,4,"We preprocess the dataset with Stanford CoreNLP to generate the dependency parse trees , POS and NER annotations .",experiment,0,237,14,0,19
relation-classification,4,All other hyperparameters are set to be the same .,experiment,0,238,15,0,10
relation-classification,4,"For both datasets , we work with the Universal Dependencies v 1 formalism .",experiment,0,239,16,0,14
relation-classification,4,a.2 training,experiment,0,240,17,0,2
relation-classification,4,For training we use Stochastic Gradient Descent with an initial learning rate of 1.0 .,experiment,0,241,18,0,15
relation-classification,4,We use a cutoff of 5 for gradient clipping .,experiment,0,242,19,0,10
relation-classification,4,"For GCN models , we train every model for 100 epochs on the TAC - RED dataset , and from epoch 5 we start to anneal the learning rate by a factor of 0.9 every time the F 1 score on the dev set does not increase after an epoch .",experiment,0,243,20,0,51
relation-classification,4,For Tree - LSTM models we find 30 total epochs to be enough .,experiment,0,244,21,0,14
relation-classification,4,"Due to the small size of the SemEval dataset , we train all models for 150 epochs , and use an initial learning rate of 0.5 with a decay rate of 0.95 .",experiment,0,245,22,0,33
relation-classification,4,"In our experiments we found that the output vector h sent tends to have large magnitude , and : Aggregated 5 - run difference compared to PA - LSTM on the TACRED dev set .",experiment,0,246,23,0,35
relation-classification,4,"For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ?",experiment,0,247,24,0,32
relation-classification,4,y .,experiment,0,248,25,0,2
relation-classification,4,""" 0 "" is omitted due to redundancy .",experiment,0,249,26,0,9
relation-classification,4,therefore adding the following regularization term to the cross entropy loss of each example improves the results :,experiment,0,250,27,0,18
relation-classification,4,"Here , reg functions as an l 2 regularization on the learned sentence representations .",experiment,0,251,28,0,15
relation-classification,4,?,experiment,0,252,29,0,1
relation-classification,4,controls the regularization strength and we set ? = 0.003 .,experiment,0,253,30,0,11
relation-classification,4,We empirically found this to be more effective than applying l 2 regularization on the convolutional weights .,experiment,0,254,31,0,18
relation-classification,4,B Comparing GCN models and PA - LSTM on TACRED,experiment,0,255,32,0,10
relation-classification,4,We compared the performance of both GCN models with the PA - LSTM on the TACRED dev set .,experiment,0,256,33,0,19
relation-classification,4,"To minimize randomness that is not inherent to these models , we accumulate statistics over 5 independent runs of each model , and report them in .",experiment,0,257,34,0,27
relation-classification,4,"As is shown in the figure , both GCN models capture very different examples from the PA - LSTM model .",experiment,0,258,35,0,21
relation-classification,4,"In the entire dev set of 22,631 examples , 1,450 had at least 3 more GCN models predicting the label correctly compared to the PA - LSTM , and 1,550 saw an improvement from using the PA - LSTM .",experiment,0,259,36,0,40
relation-classification,4,"The C - GCN , on the other hand , outperformed the PA - LSTM by at least 3 models on a total of 847 examples , and lost by a margin of at least 3 on another 629 examples , as reported in the main text .",experiment,0,260,37,0,48
relation-classification,4,This smaller difference is also reflected in the diminished gain from ensembling with the PA - LSTM shown in .,experiment,0,261,38,0,20
relation-classification,4,we hypoth -,experiment,0,262,39,0,3
relation-classification,6,Semantic Relation Classification via Bidirectional LSTM Networks with Entity - aware Attention using Latent Entity Typing,title,1,2,1,0,16
relation-classification,6,abstract,abstract,0,3,1,0,1
relation-classification,6,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,abstract,1,4,2,0,20
relation-classification,6,"Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) .",abstract,0,5,3,0,43
relation-classification,6,"In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification .",abstract,0,6,4,0,33
relation-classification,6,"To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method .",abstract,0,7,5,0,35
relation-classification,6,Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET .,abstract,0,8,6,0,31
relation-classification,6,"Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",abstract,0,9,7,0,39
relation-classification,6,introduction,introduction,0,10,1,0,1
relation-classification,6,"Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks , such as information extraction , question answering and knowledge base population .",introduction,0,11,2,0,29
relation-classification,6,A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,introduction,1,12,3,0,20
relation-classification,6,"For example , given a sentence with tagged entity pair , crash and attack , this sentence is classified into the re-lation Cause - Effect ( e1 , e2 ) 1 between the entity pair like .",introduction,0,13,4,0,37
relation-classification,6,"A first entity is surrounded by e 1 and / e 1 , and a second entity is surrounded by e 2 and / e 2 .",introduction,0,14,5,0,27
relation-classification,6,"Most previous relation classification models rely heavily on high - level lexical and syntactic features obtained from NLP tools such as WordNet , dependency parser , part - of - speech ( POS ) tagger , and named entity recognizer ( NER ) .",introduction,0,15,6,0,44
relation-classification,6,The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive .,introduction,0,16,7,0,22
relation-classification,6,"Recently , many studies therefore propose end - toend neural models without the high - level features .",introduction,0,17,8,0,18
relation-classification,6,"Among them , attention - based models , which focus to the most important semantic information in a sentence , show state - of - the - art results in a lot of NLP tasks .",introduction,0,18,9,0,36
relation-classification,6,"Since these models are mainly proposed for solving translation and language modeling tasks , they could not fully utilize the information of tagged entities in relation classification task .",introduction,0,19,10,0,29
relation-classification,6,"However , tagged entity pairs could be powerful hints for solving relation classification task .",introduction,0,20,11,0,15
relation-classification,6,"For example , even if we do not consider other words except the crash and attack , we intuitively know that the entity pair has a relation Cause - Effect ( e1 , e2 ) 1 better than Component - Whole ( e1 , e2 ) 1 in To address these issues , We propose a novel endto - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) .",introduction,0,21,12,0,80
relation-classification,6,"To capture the context of sentences , We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short - Term Memory ( LSTM ) networks .",introduction,0,22,13,0,33
relation-classification,6,Entity - aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET .,introduction,0,23,14,0,28
relation-classification,6,The contributions of our work are summarized as follows :,introduction,0,24,15,0,10
relation-classification,6,We propose an novel end - to - end recurrent neural model and an entity - aware attention mechanism with a LET which focuses to semantic information of entities and their latent types ; ( 2 ) Our model obtains 85.2 % F1 - score in SemEval- 2010 Task 8 and it outper - forms existing state - of - the - art models without any highlevel features ;,introduction,0,25,16,0,69
relation-classification,6,"We show that our model is more interpretable since it 's decision making process could be visualized with self attention , entity - aware attention , and LET .",introduction,0,26,17,0,29
relation-classification,6,related work,related work,0,27,1,0,2
relation-classification,6,There are several studies for solving relation classification task .,related work,0,28,2,0,10
relation-classification,6,Early methods used handcrafted features through a series of NLP tools or manually designing kernels .,related work,0,29,3,0,16
relation-classification,6,"These approaches use high - level lexical and syntactic features obtained from NLP tools and manually designing kernels , but the classification models relying on such features suffer from propagation of implicit error of the tools .",related work,0,30,4,0,37
relation-classification,6,"On the other hands , deep neural networks have shown outperform previous models using handcraft features .",related work,0,31,5,0,17
relation-classification,6,"Especially , many researches tried to solve the problem based on end - to - end models using only raw sentences and pre-trained word representations learned by Skip - gram and Continuous Bag - of - Words .",related work,0,32,6,0,38
relation-classification,6,zeng et al .,related work,0,33,7,1,4
relation-classification,6,employed a deep convolutional neural network ( CNN ) for extracting lexical and sentence level features .,related work,0,34,8,0,17
relation-classification,6,dos santos et al.,related work,0,35,9,1,4
relation-classification,6,proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes .,related work,0,36,10,0,20
relation-classification,6,Zhang and Wang used bidirectional recurrent neural network ( RNN ) to learn long - term dependency between entity pairs .,related work,0,37,11,0,21
relation-classification,6,"Fur-thermore , Zhang et al. proposed bidirectional LSTM network ( BLSTM ) utilizing position of words , POS tags , named entity information , dependency parse .",related work,0,38,12,1,27
relation-classification,6,This model resolved vanishing gradient problem appeared in RNNs by using BLSTM .,related work,0,39,13,0,13
relation-classification,6,"Recently , some researcher have proposed attentionbased models which can focus to the most important semantic information in a sentence .",related work,0,40,14,0,21
relation-classification,6,Zhou et al. combined attention mechanisms with BLSTM .,related work,0,41,15,1,9
relation-classification,6,Xiao and Liu split the sentence into two entities and used two attention - based BLSTM hierarchically .,related work,0,42,16,0,18
relation-classification,6,Shen and Huang proposed attention - based CNN using word level attention mechanism that is able to better determine which parts of the sentence are more influential .,related work,0,43,17,0,28
relation-classification,6,"In contrast with end - to - end model , several works proposed models utilizing the shortest dependency path ( SDP ) between entity pairs of dependency parse trees .",related work,0,44,18,0,30
relation-classification,6,SDP - LSTM model proposed by Yan et al .,related work,0,45,19,1,10
relation-classification,6,and deep recurrent neural networks ( DRNNs ) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP .,related work,0,46,20,1,32
relation-classification,6,model,related work,0,47,21,0,1
relation-classification,6,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .",related work,1,48,22,0,26
relation-classification,6,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .",related work,1,49,23,0,92
relation-classification,6,"After that , the features are averaged along the time steps to produce the sentencelevel features .",related work,0,50,24,0,17
relation-classification,6,word representation,related work,0,51,25,0,2
relation-classification,6,Let a input sentence is denoted by,related work,0,52,26,0,7
relation-classification,6,where n is the number of words .,related work,0,53,27,0,8
relation-classification,6,We transform each word into vector representations by looking up word embedding matrix W word ?,related work,0,54,28,0,16
relation-classification,6,"R dw |V | , where d w is the dimension of the vector and | V | is the size of vocabulary .",related work,0,55,29,0,24
relation-classification,6,"Then the word representations X = {x 1 , x 2 , ... , x n } are obtained by mapping w i , the i - th word , to a column vector xi ?",related work,0,56,30,0,36
relation-classification,6,R dw are fed into the next layer .,related work,0,57,31,0,9
relation-classification,6,self attention,related work,0,58,32,0,2
relation-classification,6,We can obtain the richer word representations by using self attentions .,related work,0,59,33,0,12
relation-classification,6,These word representations are considered the context based on correlation between words in a sentence .,related work,0,60,34,0,16
relation-classification,6,"The illustrates the results of the self attention in the sentence , "" the ?e1 ? pollution ? / e1 ?was caused by the ?e2 ? shipwrek ?/e2 ? "" , which is labeled Cause - Effect ( e1 , e2 ) .",related work,0,61,35,0,43
relation-classification,6,There are visualizations of the two heads in the multi-head attention applied for self attention .,related work,0,62,36,0,16
relation-classification,6,"The color density indicates the attention values , results of Equation 3.1 , which means how much an entity focuses on each word in a sentence .",related work,0,63,37,0,27
relation-classification,6,"In , the left represents the words that pollution , the first entity , focuses on and the right represents the words that shipwreck , the second entity , focuses on .",related work,0,64,38,0,32
relation-classification,6,"We can recognize that the entity pair is commonly concentrated on was , caused , and each other .",related work,0,65,39,0,19
relation-classification,6,"Actually , these words play the most important role in semantically predicting the Cause - Effect ( e1 , e2 ) , which is the relation class of this entity pair .",related work,0,66,40,0,32
relation-classification,6,"shows where the model focuses on the sentence to compute relations between entity pairs , which is the result of visualizing the alpha vectors in Equation 3.9 .",related work,0,67,41,0,28
relation-classification,6,"The important words in sentence are highlighted in yellow , which means that the more clearly the color is , the more important it is .",related work,0,68,42,0,26
relation-classification,6,"For example , in the first sentence , the inside is strongly highlighted , which is actually the best word representing the relation Component - whole ( e 1 , e2 ) between the given entity pair .",related work,0,69,43,0,38
relation-classification,6,"As another example , in the third sentence , the highlighted assess and using represent the relation , Instrument - Agency ( e2 , e1 ) between entity pair , analysts and frequency , well .",related work,0,70,44,0,36
relation-classification,6,"We can see that the using is more highlighted than the assess , because the former represents the relation better .",related work,0,71,45,0,21
relation-classification,6,"visualizes latent type representation t j? { 1 , 2 } in Equation 3.12",related work,0,72,46,0,14
relation-classification,6,"Since the dimensionality of representation vectors are too large to visualize , we applied the t - SNE , one of the most popular dimensionality reduction methods .",related work,0,73,47,0,28
relation-classification,6,"In , the red points represent latent type vectors c i?K and the rests are latent type representations t j , where the colors of points are determined by the closest of the latent type vectors in the vector space of the original dimensionality .",related work,0,74,48,0,45
relation-classification,6,The points are generally well divided and are almost uniformly distributed without being biased to one side .,related work,0,75,49,0,18
relation-classification,6,summarizes the results of extracting 50 entities in close order with each latent type vector .,related work,0,76,50,0,16
relation-classification,6,This allows us to roughly understand what latent types of entities are .,related work,0,77,51,0,13
relation-classification,6,We use a total of three types and find that similar characteristics appear in words grouped by together .,related work,0,78,52,0,19
relation-classification,6,"In the type 1 , the words are related to human 's jobs and foods .",related work,0,79,53,0,16
relation-classification,6,"The type2 has a lot of entities related to machines and engineering like engine , woofer , and motor .",related work,0,80,54,0,20
relation-classification,6,"Finally , in type3 , there are many words with bad meanings related associated with dis asters and :",related work,0,81,55,0,19
relation-classification,6,Sets of Entities grouped by Latent Types drugs .,related work,0,82,56,0,9
relation-classification,6,"As a result , each type has a set of words with similar characteristics , which can prove that LET works effectively .",related work,0,83,57,0,23
relation-classification,6,bidirectional lstm,related work,0,84,58,0,2
relation-classification,6,network,related work,0,85,59,0,1
relation-classification,6,"For sequentially encoding the output of self attention layer , we use a BLSTM that consists of two sub LSTM networks : a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence .",related work,0,86,60,0,48
relation-classification,6,"More formally , BLSTM works as follows :",related work,0,87,61,0,8
relation-classification,6,The representation vectors M obtained from self attention layer are forwarded into to the network step by step .,related work,0,88,62,0,19
relation-classification,6,"At the time step t , the hidden state",related work,0,89,63,0,9
relation-classification,6,entity - aware attention mechanism,related work,0,90,64,0,5
relation-classification,6,Although many models with attention mechanism achieved state - of - the - art performance in many NLP tasks .,related work,0,91,65,0,20
relation-classification,6,"However , for the relation classification task , these models lack of prior knowledge forgiven entity pairs , which could be powerful hints for solving the task .",related work,0,92,66,0,28
relation-classification,6,Relation classification differs from sentence classification in that information about entities is given along with sentences .,related work,0,93,67,0,17
relation-classification,6,We propose a novel entity - aware attention mechanism for fully utilizing informative factors in given entity pairs .,related work,0,94,68,0,19
relation-classification,6,"Entity - aware attention utilizes the two additional features except H = {h 1 , h 2 , ... , h n } , ( 1 ) relative position features , ( 2 ) entity features with LET , and the final sentence representation z , result of the attention , is computed as follows :",related work,0,95,69,0,56
relation-classification,6,relative position features,related work,0,96,70,0,3
relation-classification,6,"In relation classification , the position of each word relative to entities has been widely used for word representations .",related work,0,97,71,0,20
relation-classification,6,"Recently , position - aware attention is published as away to use the relative position features more effectively .",related work,0,98,72,0,19
relation-classification,6,"It is a variant of attention mechanisms , which use not only outputs of BLSTM but also the relative position features when calculating attention weights .",related work,0,99,73,0,26
relation-classification,6,We adopt this method with slightly modification as shown in Equation 3.8 .,related work,0,100,74,0,13
relation-classification,6,"In the equation , p e 1 i ?",related work,0,101,75,0,9
relation-classification,6,R dp and p e 2 i ?,related work,0,102,76,0,8
relation-classification,6,"R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",related work,0,103,77,0,54
relation-classification,6,"Similar to word embeddings , the relative positions are converted to vector representations by looking up learnable embedding matrix W pos ?",related work,0,104,78,0,22
relation-classification,6,"R dp ( 2L?1 ) , where d p is the dimension of the relative position vectors and L is the maximum sentence length .",related work,0,105,79,0,25
relation-classification,6,"Finally , the representations of BLSTM layer take into account the context and the positional relationship with entities by concatenating hi , p e 1 i , and p e 2 i .",related work,0,106,80,0,33
relation-classification,6,The representation is linearly transformed by W H ?,related work,0,107,81,0,9
relation-classification,6,R da ( 2 d h + 2 dp ) as in the Equation 3.8 .,related work,0,108,82,0,16
relation-classification,6,Entity Features with Latent Type,related work,0,109,83,0,5
relation-classification,6,"Since entity pairs are powerful hints for solving relation classification task , we involve the entity pairs and their types in the attention mechanism to effectively train relations between entity pairs and other words in a sentence .",related work,0,110,84,0,38
relation-classification,6,We employ the two entity - aware features .,related work,0,111,85,0,9
relation-classification,6,"The first is the hidden states of BLSTM corresponding to positions of entity pairs , which are high - level features representing entities .",related work,0,112,86,0,24
relation-classification,6,These are denoted by h ei ?,related work,0,113,87,0,7
relation-classification,6,"R 2d h , where e i is index of i - th entity .",related work,0,114,88,0,15
relation-classification,6,"In addition , latent types of the entities obtained by LET , our proposed novel method , are the second one .",related work,0,115,89,0,22
relation-classification,6,"Using types as features can be a great way to improve performance , since the types of entities alone can be inferred the approximate relations .",related work,0,116,90,0,26
relation-classification,6,"Because the annotated types are not given , we use the latent type representations by applying the LET inspired by latent topic clustering , a method for predicting latent topic of texts in question answering task .",related work,0,117,91,0,37
relation-classification,6,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms .,related work,0,118,92,0,17
relation-classification,6,The mathematical formulation is the follows :,related work,0,119,93,0,7
relation-classification,6,where c i is the i - th latent type vector and K is the number of latent entity types .,related work,0,120,94,0,21
relation-classification,6,"As a result , entity features are constructed by concatenating the hidden states corresponding entity positions and types of entity pairs .",related work,0,121,95,0,22
relation-classification,6,"After linear transformation of the entity features , they add up with the representations of BLSTM layer as in Equation 3.8 , and the representation of sentence z ?",related work,0,122,96,0,29
relation-classification,6,R 2 d h is computed by Equations from 3.8 to 3.10 .,related work,0,123,97,0,13
relation-classification,6,classification and training,related work,0,124,98,0,3
relation-classification,6,The sentence representation obtained from the entity - aware attention z is fed into a fully connected softmax layer for classification .,related work,0,125,99,0,22
relation-classification,6,"It produces the conditional probability p ( y|S , ? ) over all relation types :",related work,0,126,100,0,16
relation-classification,6,where y is a target relation class and S is the input sentence .,related work,0,127,101,0,14
relation-classification,6,the ?,related work,0,128,102,0,2
relation-classification,6,is whole learnable parameters in the whole network including,related work,0,129,103,0,9
relation-classification,6,where | R| is the number of relation classes .,related work,0,130,104,0,10
relation-classification,6,"A loss function L is the cross entropy between the predictions and the ground truths , which is defined as :",related work,0,131,105,0,21
relation-classification,6,"where | D| is the size of training dataset and ( S ( i ) , y ( i ) ) is the i - th sample in the dataset .",related work,0,132,106,0,31
relation-classification,6,We minimize the loss L using AdaDelta optimizer to compute the parameters ? of our model .,related work,0,133,107,0,17
relation-classification,6,"To alleviate overfitting , we constrain the L2 regularization with the coefficient ?.",related work,0,134,108,0,13
relation-classification,6,"In addition , the dropout method is applied afterword embedding , LSTM network , and entity - aware attention to prevent co-adaptation of hidden units by randomly omitting feature detectors .",related work,0,135,109,0,31
relation-classification,6,experiments,experiment,0,136,1,0,1
relation-classification,6,dataset and evaluation metrics,experiment,0,137,2,0,4
relation-classification,6,"We evaluate our model on the SemEval - 2010 Task 8 dataset , which is an commonly used benchmark for relation classification and compare the results with the state - of - the - art models in this are a .",experiment,0,138,3,0,41
relation-classification,6,"The dataset contains 10 distinguished relations , Cause - Effect , Instrument - Agency , Product - Producer , Content - Container , Entity - Origin , Entity - Destination , Component - Whole , Member - Collection , Message - Topic , and Other .",experiment,0,139,4,0,46
relation-classification,6,"The former 9 relations have two directions , whereas Other is not directional , so the total number of relations is 19 .",experiment,0,140,5,0,23
relation-classification,6,"There are 10,717 annotated sentences which consist of 8,000 samples for training and 2,717 samples for testing .",experiment,0,141,6,0,18
relation-classification,6,"We adopt the official evaluation metric of SemEval - 2010 Task 8 , which is based on the macro -averaged F1 - score ( excluding Other ) , and takes into consideration the directionality .",experiment,0,142,7,0,35
relation-classification,6,implementation details,experiment,0,143,8,0,2
relation-classification,6,We tune the hyperparameters for our model on the development set randomly sampled 800 sentences for validation .,experiment,0,144,9,0,18
relation-classification,6,The best hyperparameters in our proposed model are shown in following .,experiment,0,145,10,0,12
relation-classification,6,hyperparameter,experiment,0,146,11,0,1
relation-classification,6,"Description Value We use pre-trained weights of the publicly available Glo Ve model to initialize word embeddings in our model , and other weights are randomly initialized from zero-mean Gaussian distribution .",experiment,0,147,12,0,32
relation-classification,6,compares our Entity - aware Attention LSTM model with state - of - theart models on this relation classification dataset .,experiment,0,148,13,0,21
relation-classification,6,"We divide the models into three groups , Non-Neural Model , SDP - based Model , and End - to - End Model .",experiment,0,149,14,0,24
relation-classification,6,"First , the SVM , Non-Neural Model , was top of the SemEval - 2010 task , during the official competition period .",experiment,0,150,15,0,23
relation-classification,6,They used many handcraft feature and SVM classifier .,experiment,0,151,16,0,9
relation-classification,6,"As a result , they achieved an F1-score of 82.2 % .",experiment,0,152,17,0,12
relation-classification,6,"The second is SDP - based Model such as MVRNN , FCM , DepNN , de pLCNN + NS , SDP - LSTM , and DRNNs .",experiment,0,153,18,0,27
relation-classification,6,The SDP is reasonable features for detecting semantic structure of sentences .,experiment,0,154,19,0,12
relation-classification,6,"Actually , the SDP - based models show high performance , but SDP may not always be accurate and the parsing time is exponentially increased by long sentences .",experiment,0,155,20,0,29
relation-classification,6,The last model is End - to - End Model automatically learned internal representations can occur between the original inputs and the final outputs in deep learning .,experiment,0,156,21,0,28
relation-classification,6,"There are CNN - based models such as CNN , CR - CNN , and Attention - CNN and RNN - based models such as BLSTM , Attention - BLSTM , and Hierarchical - BLSTM ( Hier - BLSTM ) for this task .",experiment,0,157,22,0,44
relation-classification,6,experimental results,experiment,0,158,1,0,2
relation-classification,6,model f1,experiment,0,159,2,0,2
relation-classification,6,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .",experiment,1,160,3,0,32
relation-classification,6,"However , they rely on high - level lexical features such as WordNet , dependency parse trees , POS tags , and NER tags from NLP tools .",experiment,0,161,4,0,28
relation-classification,6,The experimental results show that the LET is effective for relation classification .,experiment,0,162,5,0,13
relation-classification,6,The LET improve a performance of 0.5 % than the model not applied it .,experiment,0,163,6,0,15
relation-classification,6,The model showed the best performance with three types .,experiment,0,164,7,0,10
relation-classification,6,visualization,experiment,0,165,8,0,1
relation-classification,6,There are three different visualization to demonstrate that our model is more interpretable .,experiment,0,166,9,0,14
relation-classification,6,"First , the visualization of self attention shows where each word focus on parts of a sentence .",experiment,0,167,10,0,18
relation-classification,6,"By showing the words that the entity pair attends , we can find the words that well represent the relation between them .",experiment,0,168,11,0,23
relation-classification,6,"Next , the entity - aware attention visualization shows where the model pays attend to a sentence .",experiment,0,169,12,0,18
relation-classification,6,"This visualization result highlights important words in a sentence , which are usually important keywords for classification .",experiment,0,170,13,0,18
relation-classification,6,"Finally , we visualize representation of type in LET by using t- SNE , a method for dimensionality reduction , and group the whole entities in the dataset by the its latent types .",experiment,0,171,14,0,34
relation-classification,6,entity - aware attention,experiment,0,172,15,0,4
relation-classification,6,latent entity type,experiment,0,173,16,0,3
relation-classification,6,conclusion,experiment,0,174,17,0,1
relation-classification,6,"In this paper , we proposed entity - aware attention mechanism with latent entity typing and a novel end - to - end recurrent neural model which incorporates this mechanism for relation classification .",experiment,0,175,18,0,34
relation-classification,6,Our model achieves 85.2 % F1 - score in SemEval- 2010,experiment,0,176,19,0,11
relation-classification,6,Task 8 using only raw sentence and word embeddings without any high - level features from NLP tools and it outperforms existing state - of - the - art methods .,experiment,0,177,20,0,31
relation-classification,6,"In addition , our three visualizations of attention mechanisms applied to the model demonstrate that our model is more interpretable than previous models .",experiment,0,178,21,0,24
relation-classification,6,We expect our model to be extended not only the relation classification task but also other tasks that entity plays an important role .,experiment,0,179,22,0,24
relation-classification,6,"Especially , latent entity typing can be effectively applied to sequence modeling task using entity information without NER .",experiment,0,180,23,0,19
relation-classification,6,"In the future , we will propose a new method in question answering or knowledge base population based on relations between entities extracted from our model .",experiment,0,181,24,0,27
relation-classification,2,Joint entity recognition and relation extraction as a multi-head selection problem,title,1,2,1,0,11
relation-classification,2,abstract,abstract,0,3,1,0,1
relation-classification,2,State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers .,abstract,0,4,2,0,41
relation-classification,2,"Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools .",abstract,0,5,3,0,21
relation-classification,2,"However , these features are not always accurate for various languages and contexts .",abstract,0,6,4,0,14
relation-classification,2,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .",abstract,1,7,5,0,35
relation-classification,2,"Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) .",abstract,0,8,6,0,39
relation-classification,2,"We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",abstract,0,9,7,0,39
relation-classification,2,"Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",abstract,0,10,8,0,32
relation-classification,2,introduction,introduction,0,11,1,0,1
relation-classification,2,The goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts .,introduction,0,12,2,0,21
relation-classification,2,It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering .,introduction,0,13,3,0,23
relation-classification,2,"The problem is traditionally approached as two separate subtasks , namely ( i ) named entity recognition ( NER ) and ( ii ) relation extraction ( RE ) , in a pipeline setting .",introduction,0,14,4,0,35
relation-classification,2,"The main limitations of the pipeline models are : ( i ) error propagation between the components ( i.e. , NER and RE ) and ( ii ) possible useful information from the one task is not exploited by the other ( e.g. , identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities , i.e. , PER , ORG and vice versa ) .",introduction,0,15,5,0,75
relation-classification,2,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",introduction,1,16,6,0,34
relation-classification,2,The previous joint models heavily rely on hand - crafted features .,introduction,0,17,7,0,12
relation-classification,2,"Recent advances in neural networks alleviate the issue of manual feature engineering , but some of them still depend on NLP tools ( e.g. , POS taggers , dependency parsers ) .",introduction,0,18,8,0,32
relation-classification,2,propose a Recurrent Neural Network ( RNN ) - based joint model that uses a bidirectional sequential LSTM ( Long Short Term Memory ) to model the entities and a tree - LSTM that takes into account dependency tree information to model the relations between the entities .,introduction,0,19,9,0,48
relation-classification,2,The dependency information is extracted using an external dependency parser .,introduction,0,20,10,0,11
relation-classification,2,"Similarly , in the work of for entity and relation extraction from biomedical text , a model which also uses tree - LSTMs is applied to extract dependency information .",introduction,0,21,11,0,30
relation-classification,2,"propose a method that relies on RNNs but uses a lot of hand - crafted features and additional NLP tools to extract features such as POS - tags , etc. replicate the context around the entities with Convolutional Neural Networks ( CNNs ) .",introduction,0,22,12,0,44
relation-classification,2,"Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .",introduction,0,23,13,0,21
relation-classification,2,This means that relations of other pairs of entities in the same sentence - which could be helpful in deciding on the relation type for a particular pair - are not taken into account .,introduction,0,24,14,0,35
relation-classification,2,"propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .",introduction,0,25,15,0,32
relation-classification,2,introduce a quadratic scoring layer to model the two tasks simultaneously .,introduction,0,26,16,0,12
relation-classification,2,"The limitation of this approach is that only a single relation can be assigned to a token , while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity .",introduction,0,27,17,0,38
relation-classification,2,"In this work , we focus on a new general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .",introduction,1,28,18,0,34
relation-classification,2,"Our model achieves state - of - the - art performance in a number of different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) without relying on any manually engineered features nor additional NLP tools .",introduction,0,29,19,0,48
relation-classification,2,"In summary , our proposed model ( which will be detailed next in Section 3 ) solves several shortcomings that we identified in related works ( Section 2 ) for joint entity recognition and relation extraction : ( i ) our model does not rely on external NLP tools nor hand - crafted features , ( ii ) entities and relations within the same text fragment ( typically a sentence ) are extracted simultaneously , where ( iii ) an entity can be involved in multiple relations at once .",introduction,0,30,20,0,90
relation-classification,2,"Specifically , the model of depends on dependency parsers , which perform particularly well on specific languages ( i.e. , English ) and contexts ( i.e. , news ) .",introduction,0,31,21,0,30
relation-classification,2,"Yet , our ambition is to develop a model that generalizes well in various setups , therefore using only automatically extracted features thatare learned during training .",introduction,0,32,22,0,27
relation-classification,2,"For instance , and use exactly the same model in different contexts , i.e. , news ( ACE04 ) and biomedical data ( ADE ) , respectively .",introduction,0,33,23,0,28
relation-classification,2,"Comparing our results to the ADE dataset , we obtain a 1.8 % improvement on the NER task and ? 3 % on the RE task .",introduction,0,34,24,0,27
relation-classification,2,"On the other hand , our model performs within a reasonable margin ( ? 0.6 % in the NER task and ? 1 % on the RE task ) on the ACE04 dataset without the use of pre-calculated features .",introduction,0,35,25,0,40
relation-classification,2,This shows that the model of strongly relies on the features extracted by the dependency parsers and can not generalize well into different contexts where dependency parser features are weak .,introduction,0,36,26,0,31
relation-classification,2,"Comparing to , we train our model by modeling all the entities and the relations of the sentence at once .",introduction,0,37,27,0,21
relation-classification,2,This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time .,introduction,0,38,28,0,25
relation-classification,2,"Finally , we solve the underlying problem of the models proposed by and , who essentially assume classes ( i.e. , relations ) to be mutually exclusive : we solve this by phrasing the relation extraction component as a multi-label prediction problem .",introduction,0,39,29,0,43
relation-classification,2,"To demonstrate the effectiveness of the proposed method , we conduct the largest experimental evaluation to date ( to the best of our knowledge ) in jointly performing both entity recognition and relation extraction ( see Section 4 and Section 5 ) , using different datasets from various domains ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",introduction,0,40,30,0,69
relation-classification,2,"Specifically , we apply our method to four datasets , namely ACE04 ( news ) , Adverse Drug Events ( ADE ) , Dutch Real Estate Classifieds ( DREC ) and CoNLL'04 ( news ) .",introduction,0,41,31,0,36
relation-classification,2,"Our method outperforms all state - of - the - art methods that do not rely on any additional features or tools , while performance is very close ( or even better in the biomedical dataset ) compared to methods that do exploit hand - engineered features or NLP tools .",introduction,0,42,32,0,51
relation-classification,2,related work,related work,0,43,1,0,2
relation-classification,2,The tasks of entity recognition and relation extraction can be applied either one by one in a pipeline setting or in a joint model .,related work,0,44,2,0,25
relation-classification,2,"In this section , we present related work for each task ( i.e. , named entity recognition and relation extraction ) as well as prior work into joint entity and relation extraction .",related work,0,45,3,0,33
relation-classification,2,1,related work,0,46,4,0,1
relation-classification,2,"Note that another difference is that we use a CRF layer for the NER part , while Katiyar & Cardie ( 2017 ) uses a softmax and uses a quadratic scoring layer ; see further , when we discuss performance comparison results in Section 5 .",related work,0,47,5,1,46
relation-classification,2,named entity recognition,related work,0,48,6,0,3
relation-classification,2,"We formulate the entity identification task as a sequence labeling problem , similar to previous work on joint learning models and named entity recognition using the BIO ( Beginning , Inside , Outside ) encoding scheme .",related work,0,49,7,0,37
relation-classification,2,Each entity consists of multiple sequential tokens within the sentence and we should assign a tag for every token in the sentence .,related work,0,50,8,0,23
relation-classification,2,"That way we are able to identify the entity arguments ( start and end position ) and its type ( e.g. , ORG ) .",related work,0,51,9,0,25
relation-classification,2,"To do so , we assign the B - type ( beginning ) to the first token of the entity , the I - type ( inside ) to every other token within the entity and the O tag ( outside ) if a token is not part of an entity .",related work,0,52,10,0,52
relation-classification,2,shows an example of the BIO encoding tags assigned to the tokens of the sentence .,related work,0,53,11,0,16
relation-classification,2,"In the CRF layer , one can observe that we assign the B - ORG and I - ORG tags to indicate the beginning and the inside tokens of the entity "" Disease Control Center "" , respectively .",related work,0,54,12,0,39
relation-classification,2,"On top of the BiLSTM layer , we employ either a softmax or a CRF layer to calculate the most probable entity tag for each token .",related work,0,55,13,0,27
relation-classification,2,We calculate the score of each token w i for each entity tag :,related work,0,56,14,0,14
relation-classification,2,"where the superscript ( e ) is used for the notation of the NER task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) ,",related work,0,57,15,0,32
relation-classification,2,"with d as the hidden size of the LSTM , p the number of NER tags ( e.g. , B - ORG ) and",related work,0,58,16,0,24
relation-classification,2,l the layer width .,related work,0,59,17,0,5
relation-classification,2,We calculate the probabilities of all the candidate tags for a give n,related work,0,60,18,0,13
relation-classification,2,"we employ the softmax approach only for the entity classification ( EC ) task ( which is similar to NER ) where we need to predict only the entity types ( e.g. , PER ) for each token assuming boundaries are given .",related work,0,61,19,0,43
relation-classification,2,The CRF approach is used for the NER task which includes both entity type and boundaries recognition .,related work,0,62,20,0,18
relation-classification,2,"In the softmax approach , we assign entity types to tokens in a greedy way at prediction time ( i.e. , the selected tag is just the highest scoring tag over all possible set of tags ) .",related work,0,63,21,0,38
relation-classification,2,"Although assuming an independent tag distribution is beneficial for entity classification tasks ( e.g. , POS tagging ) , this is not the case when there are strong de-pendencies between the tags .",related work,0,64,22,0,33
relation-classification,2,"Specifically , in NER , the BIO tagging scheme forces several restrictions ( e.g. , B - LOC can not be followed by I - PER ) .",related work,0,65,23,0,28
relation-classification,2,"The softmax method allows local decisions ( i.e. , for the tag of each token w i ) even though the BiLSTM captures information about the neighboring words .",related work,0,66,24,0,29
relation-classification,2,"Still , the neighboring tags are not taken into account for the tag decision of a specific token .",related work,0,67,25,0,19
relation-classification,2,"For example , in the entity "" John Smith "" , tagging "" Smith "" as PER is useful for deciding that "" John "" is B - PER .",related work,0,68,26,0,30
relation-classification,2,"To this end , for NER , we use a linear - chain CRF , similar to Lample et al .",related work,0,69,27,1,21
relation-classification,2,where an improvement of ? 1 % F 1 NER points is reported when using CRF .,related work,0,70,28,0,17
relation-classification,2,"In our case , with the use of CRF we also report a ? 1 % over all performance improvement as observed in",related work,0,71,29,0,23
relation-classification,2,"is the score of the predicted tag for token w i , T is a square transition matrix in which each entry represents transition scores from one tag to another .",related work,0,72,30,0,31
relation-classification,2,t ?,related work,0,73,31,0,2
relation-classification,2,R ( p+2 ) ( p + 2 ) because y,related work,0,74,32,0,11
relation-classification,2,We apply Viterbi to obtain the tag sequence ? ( e ) with the highest score .,related work,0,75,33,0,17
relation-classification,2,We train both the softmax ( for the EC task ) and the CRF layer ( for NER ) by minimizing the cross - entropy loss L NER .,related work,0,76,34,0,29
relation-classification,2,"We also use the entity tags as input to our relation extraction layer by learning label embeddings , motivated by where an improvement of 2 % F 1 is reported ( with the use of label embeddings ) .",related work,0,77,35,0,39
relation-classification,2,"In our case , label embeddings lead to an increase of 1 % F 1 score as reported in ( see Section 5.2 ) .",related work,0,78,36,0,25
relation-classification,2,"The input to the next layer is twofold : the output states of the LSTM and the learned label embedding representation , encoding the intuition that knowledge of named enti-ties can be useful for relation extraction .",related work,0,79,37,0,37
relation-classification,2,"During training , we use the gold entity tags , while at prediction time we use the predicted entity tags as input to the next layer .",related work,0,80,38,0,27
relation-classification,2,The input to the next layer is the concatenation of the hidden LSTM state hi with the label embedding g i for token w i :,related work,0,81,39,0,26
relation-classification,2,relation extraction,related work,0,82,40,0,2
relation-classification,2,We consider relation extraction as the second task of our joint model .,method,0,83,1,0,13
relation-classification,2,The main approaches for relation extraction rely either on hand - crafted features or neural networks .,method,0,84,2,0,17
relation-classification,2,"Feature - based methods focus on obtaining effective hand - crafted features , for instance defining kernel functions and designing lexical , syntactic , semantic features , etc . .",method,0,85,3,0,30
relation-classification,2,Neural network models have been proposed to overcome the issue of manually designing hand - crafted features leading to improved performance .,method,0,86,4,0,22
relation-classification,2,CNN - and models have been introduced to automatically extract lexical and sentence level features leading to a deeper language understanding .,method,0,87,5,0,22
relation-classification,2,combine CNNs and RNNs using an ensemble scheme to achieve state - of - the - art results .,method,0,88,6,0,19
relation-classification,2,Joint entity and relation extraction,method,0,89,1,0,5
relation-classification,2,Entity and relation extraction includes the task of ( i ) identifying the entities ( described in Section 2.1 ) and ( ii ) extracting the relations among them ( described in Section 2.2 ) .,method,0,90,2,0,36
relation-classification,2,Feature - based joint models have been proposed to simultaneously solve the entity recognition and relation extraction ( RE ) subtasks .,method,0,91,3,0,22
relation-classification,2,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features and thus ( i ) require additional effort for the data preprocessing , ( ii ) perform poorly in different application and language settings where the NLP tools are not reliable , and ( iii ) increase the computational complexity .",method,0,92,4,0,60
relation-classification,2,"In this paper , we introduce a joint neural network model to overcome the aforementioned issues and to automatically perform end - to - end relation extraction without the need of any manual feature engineering or the use of additional NLP components .",method,0,93,5,0,43
relation-classification,2,Neural network approaches have been considered to address the problem in a joint setting ( end - to - end relation extraction ) and typically include the use of RNNs and CNNs .,method,0,94,6,0,33
relation-classification,2,"Specifically , propose the use of bidirectional tree - structured RNNs to capture dependency tree information ( where parse trees are extracted using state - of - the - art dependency parsers ) which has been proven beneficial for relation extraction .",method,0,95,7,0,42
relation-classification,2,"apply the work of to biomedical text , reporting state - of - the - art performance for two biomedical datasets .",method,0,96,8,0,22
relation-classification,2,propose the use of a lot of hand - crafted features along with RNNs .,method,0,97,9,0,15
relation-classification,2,"solve the entity classification task ( which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted ) and relation extraction problems using an approximation of a global normalization objective ( i.e. , CRF ) : they replicate the context of the sentence ( left and right part of the entities ) to feed one entity pair at a time to a CNN for relation extraction .",method,0,98,10,0,82
relation-classification,2,"Thus , they do not simultaneously infer other potential entities and relations within the same sentence .",method,0,99,11,0,17
relation-classification,2,"and The input of our model is the words of the sentence which are then represented as word vectors ( i.e. , embeddings ) .",method,0,100,12,0,25
relation-classification,2,The BiLSTM layer extracts a more complex representation for each word .,method,0,101,13,0,12
relation-classification,2,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,method,0,102,14,0,18
relation-classification,2,"The outputs for each token ( e.g. , Smith ) are : ( i ) an entity recognition label ( e.g. , I - PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",method,0,103,15,0,67
relation-classification,2,"tional complexity described by , by dividing the loss functions into a NER and a relation extraction component .",method,0,104,16,0,19
relation-classification,2,"Moreover , we are able to handle multiple relations instead of just predicting single ones , as was described for the application of structured real estate advertisements of .",method,0,105,17,0,29
relation-classification,2,joint model,method,0,106,18,0,2
relation-classification,2,"In this section , we present our multi-head joint model illustrated in .",method,1,107,19,0,13
relation-classification,2,"The model is able to simultaneously identify the entities ( i.e. , types and boundaries ) and all the possible relations between them at once .",method,0,108,20,0,26
relation-classification,2,We formulate the problem as a multi-head selection problem extending previous work as described in Section 2.3 .,method,0,109,21,0,18
relation-classification,2,"By multi-head , we mean that any particular entity maybe the CoNLL04 dataset is presented .",method,0,110,22,0,16
relation-classification,2,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .",method,1,111,23,0,32
relation-classification,2,The BiLSTM layer is able to extract a more complex representation for each word that incorporates the context via the RNN structure .,method,0,112,24,0,23
relation-classification,2,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,method,0,113,25,0,18
relation-classification,2,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",method,1,114,26,0,80
relation-classification,2,"Since we assume token - based encoding , we consider only the last token of the entity as head of another token , eliminating redundant relations .",method,0,115,27,0,27
relation-classification,2,"For instance , there is a Works for relation between entities "" John Smith "" and "" Disease Control Center "" .",method,0,116,28,0,22
relation-classification,2,"Instead of connecting all tokens of the entities , we connect only "" Smith "" with "" Center "" .",method,0,117,29,0,20
relation-classification,2,"Also , for the case of no relation , we introduce the "" N "" label and we predict the token itself as the head .",method,0,118,30,0,26
relation-classification,2,embedding layer,method,0,119,31,0,2
relation-classification,2,"Given a sentence w = w 1 , ... , w n as a sequence of tokens , the word embedding layer is responsible to map each token to a word vector ( w word2vec ) .",method,0,120,32,0,37
relation-classification,2,We use pre-trained word embeddings using the Skip - Gram word2vec model .,method,0,121,33,0,13
relation-classification,2,"In this work , we also use character embeddings since they are commonly applied to neural NER .",method,0,122,34,0,18
relation-classification,2,This type of embeddings is able to capture morphological features such as prefixes and suffixes .,method,0,123,35,0,16
relation-classification,2,"For instance , in the Adverse Drug Events ( ADE ) dataset , the suffix "" toxicity "" can specify an adverse drug event entity such as "" neurotoxicity "" or "" hepatotoxicity "" and thus it is very informative .",method,0,124,36,0,41
relation-classification,2,"Another example might be the Dutch suffix "" kamer "" ( "" room "" in English ) in the Dutch Real Estate Classifieds ( DREC ) dataset which is used to specify the space entities "" badkamer "" ( "" bathroom "" in English ) and "" slaapkamer "" ( "" bedroom "" in English ) .",method,0,125,37,0,57
relation-classification,2,"Character - level embeddings are learned during training , similar to Ma & Hovy and .",method,0,126,38,0,16
relation-classification,2,"In the work of Lample et al. , character embeddings lead to a performance improvement of up to 3 % in terms of NER F 1 score .",method,0,127,39,1,28
relation-classification,2,"In our work , by incorporating character embeddings , we report in an increase of ? 2 %",method,0,128,40,0,18
relation-classification,2,over all F 1 scoring points .,method,0,129,41,0,7
relation-classification,2,"For more details , see Section 5.2 .",method,0,130,42,0,8
relation-classification,2,bidirectional lstm encoding layer,method,0,131,43,0,4
relation-classification,2,RNNs are commonly used in modeling sequential data and have been successfully applied in various NLP tasks .,method,0,132,44,0,18
relation-classification,2,"In this work , we use multi - layer LSTMs , a specific kind of RNNs which are able to capture long term dependencies well .",method,0,133,45,0,26
relation-classification,2,We employ a BiLSTM which is able to encode information from left to right ( past to future ) and right to left ( future to past ) .,method,0,134,46,0,29
relation-classification,2,"This way , we can combine bidirectional information for each word by concatenating the forward ( hi ) and the backward ( hi ) output at timestep i .",method,0,135,47,0,29
relation-classification,2,The BiLSTM output at timestep i can be written as :,method,0,136,48,0,11
relation-classification,2,Relation extraction as multi-head selection,method,0,137,1,0,5
relation-classification,2,"In this subsection , we describe the relation extraction task , formulated as a multi-head selection problem .",method,0,138,2,0,18
relation-classification,2,"In the general formulation of our method , each token w i can have multiple heads ( i.e. , multiple relations with other tokens ) .",method,0,139,3,0,26
relation-classification,2,"We predict the tuple (? i ,? i ) where ?",method,0,140,4,0,11
relation-classification,2,i is the vector of heads and ?,method,0,141,5,0,8
relation-classification,2,i is the vector of the corresponding relations for each token w i .,method,0,142,6,0,14
relation-classification,2,"This is different for the previous standard head selection for dependency parsing method since ( i ) it is extended to predict multiple heads and ( ii ) the decisions for the heads and the relations are jointly taken ( i.e. , instead of first predicting the heads and then in a next step the relations by using an additional classifier ) .",method,0,143,7,0,63
relation-classification,2,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ?",method,0,144,8,0,27
relation-classification,2,"{ 0 , ... , n} the vector of the most probable heads ?",method,0,145,9,0,14
relation-classification,2,i ? wand the vector of the most probable corresponding relation labelsr i ?,method,0,146,10,0,14
relation-classification,2,r .,method,0,147,11,0,2
relation-classification,2,We calculate the score between tokens w i and w j given a label r k as follows :,method,0,148,12,0,19
relation-classification,2,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) ,",method,0,149,13,0,32
relation-classification,2,"is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",method,0,150,14,0,22
relation-classification,2,we define,method,0,151,15,0,2
relation-classification,2,"to be the probability of token w j to be selected as the head of token w i with the relation label r k between them , where ?( . ) stands for the sigmoid function .",method,0,152,16,0,37
relation-classification,2,We minimize the cross - entropy loss L rel during training :,method,0,153,17,0,12
relation-classification,2,where y i ?,method,0,154,18,0,4
relation-classification,2,wand r i ?,method,0,155,19,0,4
relation-classification,2,R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,method,0,156,20,0,29
relation-classification,2,"After training , we keep the combination of heads ?",method,0,157,21,0,10
relation-classification,2,i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq..,method,0,158,22,0,18
relation-classification,2,"Unlike previous work on joint models ( Katiyar & Cardie , 2017 ) , we are able to predict multiple relations considering the classes as independent and not mutually exclusive ( the probabilities do not necessarily sum to 1 for different classes ) .",method,0,159,23,1,44
relation-classification,2,"For the joint entity and relation extraction task , we calculate the final objective as L NER + L rel .",method,0,160,24,0,21
relation-classification,2,edmonds ' algorithm,method,0,161,25,0,3
relation-classification,2,Our model is able to simultaneously extract entity mentions and the relations between them .,method,0,162,26,0,15
relation-classification,2,"To demonstrate the effectiveness and the general purpose nature of our model , we also test it on the recently introduced Dutch real estate classifieds ( DREC ) dataset where the entities need to form a tree structure .",method,0,163,27,0,39
relation-classification,2,"By using thresholded inference , a tree structure of relations is not guaranteed .",method,0,164,28,0,14
relation-classification,2,Thus we should enforce tree structure constraints to our model .,method,0,165,29,0,11
relation-classification,2,"To this end , we post-process the output of our system with Edmonds ' maximum spanning tree algorithm for directed graphs .",method,0,166,30,0,22
relation-classification,2,"A fully connected directed graph G = ( V , E ) is constructed ,",method,0,167,31,0,15
relation-classification,2,where the vertices,method,0,168,32,0,3
relation-classification,2,V represent the last tokens of the identified entities ( as predicted by NER ) and the edges E represent the highest scoring relations with their scores as weights .,method,0,169,33,0,30
relation-classification,2,Edmonds ' algorithm is applied in cases a tree is not already formed by thresholded inference .,method,0,170,34,0,17
relation-classification,2,experimental setup,experiment,0,171,1,0,2
relation-classification,2,datasets and evaluation metrics,experiment,0,172,2,0,4
relation-classification,2,"We conduct experiments on four datasets : ( i ) Automatic Content Extraction , ACE04 ( Dod - We follow the cross -validation setting of Li & Ji and .",experiment,0,173,3,0,30
relation-classification,2,We removed DISC and did 5 - fold cross -validation on the bnews and nwire subsets ( 348 documents ) .,experiment,0,174,4,0,21
relation-classification,2,We obtained the preprocessing script from Miwa 's github codebase .,experiment,0,175,5,0,11
relation-classification,2,"We measure the performance of our system using micro F 1 scores , Precision and Recall on both entities and relations .",experiment,0,176,6,0,22
relation-classification,2,We treat an entity as correct when the entity type and the region of its head are correct .,experiment,0,177,7,0,19
relation-classification,2,"We treat a relation as correct when it s type and argument entities are correct , similar to and .",experiment,0,178,8,0,20
relation-classification,2,We refer to this type of evaluation as strict .,experiment,0,179,9,0,10
relation-classification,2,"We select the best hyperparameter values on a randomly selected validation set for each fold , selected from the training set ( 15 % of the data ) since there are no official train and validation splits in the work of .",experiment,0,180,10,0,42
relation-classification,2,"CoNLL04 : There are four entity types in the dataset ( Location , Organization , Person ,",experiment,0,181,11,0,17
relation-classification,2,"and Other ) and five relation types ( Kill , Live in , Located in , OrgBased in and Work for ) .",experiment,0,182,12,0,23
relation-classification,2,We use the splits defined by Gupta et al. and .,experiment,0,183,13,1,11
relation-classification,2,"The dataset consists of 910 training instances , 243 for validation and 288 for testing .",experiment,0,184,14,0,16
relation-classification,2,We measure the performance by computing the F 1 score on the test set .,experiment,0,185,15,0,15
relation-classification,2,We adopt two evaluation settings to compare to previous work .,experiment,0,186,16,0,11
relation-classification,2,"Specifically , we perform an EC task assuming the entity boundaries are given similar to Gupta et al. and .",experiment,0,187,17,1,20
relation-classification,2,"To obtain comparable results , we omit the entity class "" Other "" when computing the EC score .",experiment,0,188,18,0,19
relation-classification,2,We score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .,experiment,0,189,19,0,43
relation-classification,2,We report macro-average F 1 scores for EC and RE to obtain comparable results to previous studies .,experiment,0,190,20,0,18
relation-classification,2,"Moreover , we perform actual NER evaluation instead of just EC , reporting results using the strict evaluation metric .",experiment,0,191,21,0,20
relation-classification,2,We measure the performance by computing the F 1 score on the test set .,experiment,0,192,22,0,15
relation-classification,2,"To compare our results with previous work , we use the boundaries evaluation setting .",experiment,0,193,23,0,15
relation-classification,2,"In this setting , we count an entity as correct if the boundaries of the entity are correct .",experiment,0,194,24,0,19
relation-classification,2,A relation is correct when the relation is correct and the argument entities are both correct .,experiment,0,195,25,0,17
relation-classification,2,"Also , we report results using the strict evaluation for future reference .",experiment,0,196,26,0,13
relation-classification,2,ADE : There are two types of entities ( drugs and diseases ) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease ( adverse drug events ) .,experiment,0,197,27,0,42
relation-classification,2,"There are 6,821 sentences in total and similar to previous work , we remove ? 130 relations with overlapping entities ( e.g. , "" lithium "" is a drug which is related to "" lithium intoxication "" ) .",experiment,0,198,28,0,39
relation-classification,2,"Since there are no official sets , we evaluate our model using 10 - fold cross- validation where 10 % of the data was used as validation and 10 % for test set similar to .",experiment,0,199,29,0,36
relation-classification,2,The final results are displayed in F 1 metric as a macro -average across the folds .,experiment,0,200,30,0,17
relation-classification,2,"The dataset consists of 10,652 entities and 6,682 relations .",experiment,0,201,31,0,10
relation-classification,2,We report results similar to previous work on this dataset using the strict evaluation metric .,experiment,0,202,32,0,16
relation-classification,2,word embeddings,experiment,0,203,33,0,2
relation-classification,2,"We use pre-trained word2vec embeddings used in previous work , so as to retain the same inputs for our model and to obtain comparable results thatare not affected by the input embeddings .",experiment,0,204,34,0,33
relation-classification,2,"Specifically , we use the 200 - dimensional word embeddings used in the work of for the ACE04 dataset 6 trained on Wikipedia .",experiment,0,205,35,0,24
relation-classification,2,We obtained the 50 - dimensional word embeddings used by,experiment,0,206,36,0,10
relation-classification,2,hyperparameters and implementation details,experiment,0,207,37,0,4
relation-classification,2,We have developed our joint model by using Python with the TensorFlow machine learning library .,experiment,1,208,38,0,16
relation-classification,2,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .",experiment,1,209,39,1,22
relation-classification,2,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,experiment,1,210,40,0,35
relation-classification,2,We use dropout to regularize our network .,experiment,1,211,41,0,8
relation-classification,2,Dropout is applied in the input embeddings and in between the hidden layers for both tasks .,experiment,0,212,42,0,17
relation-classification,2,Different dropout rates have been applied but the best dropout values ( 0.2 to 0.4 ) for each dataset have been used .,experiment,0,213,43,0,23
relation-classification,2,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,experiment,1,214,44,0,15
relation-classification,2,We also fixed our label embeddings to be of size b = 25 for all the datasets except for CoNLL04 where the label embeddings were not beneficial and thus were not used .,experiment,0,215,45,0,33
relation-classification,2,We experimented with tanh and relu activation functions ( recall that this is the function f ( ) from the model description relu activation only in the ACE04 and tanh in all other datasets .,experiment,0,216,46,0,35
relation-classification,2,We employ the technique of early stopping based on the validation set .,experiment,1,217,47,0,13
relation-classification,2,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .",experiment,1,218,48,0,27
relation-classification,2,We select the best epoch according to the results in the validation set .,experiment,0,219,49,0,14
relation-classification,2,For more details about the effect of each hyperparameter to the model performance see the Appendix .,experiment,0,220,50,0,17
relation-classification,2,results and discussion,result,0,221,1,0,3
relation-classification,2,results,result,0,222,1,0,1
relation-classification,2,"In , we present the results of our analysis .",result,0,223,2,0,10
relation-classification,2,The first column indicates the considered dataset .,result,0,224,3,0,8
relation-classification,2,"In the second column , we denote the model which is applied ( i.e. , previous work and the proposed models ) .",result,0,225,4,0,23
relation-classification,2,The proposed models are the following :,result,0,226,5,0,7
relation-classification,2,"( i ) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss for multiple head prediction , ( ii ) multi-head +",result,0,227,6,0,28
relation-classification,2,"E is the proposed model with addition of Edmonds ' algorithm to guarantee a tree - structured output for the DREC dataset , ( iii ) single - head is the proposed method but it predicts only one head per token using a softmax loss instead of a sigmoid , and ( iv ) multi-head EC is the proposed method with a softmax to predict the entity classes assuming that the boundaries are given , and the sigmoid loss for multiple head selection . ( iii ) Relaxed : we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .",result,0,228,7,0,132
relation-classification,2,"In the next three columns , we present the results for the entity identification task ( Precision , Recall , F 1 ) and then ( in the subsequent three columns ) the results of the relation extraction task ( Precision , Recall , F 1 ) .",result,0,229,8,0,48
relation-classification,2,"Finally , in the last column , we report an additional F 1 measure which is the average F 1 performance of the two subtasks .",result,0,230,9,0,26
relation-classification,2,"We mark with bold font in , the class probabilities do not necessarily sum up to one since the classes are considered independent .",result,0,231,10,0,24
relation-classification,2,"Moreover , we use a CRF - layer to model the NER task to capture dependencies between sequential tokens .",result,0,232,11,0,20
relation-classification,2,"Finally , we obtain more effective word representations by using character - level embeddings .",result,0,233,12,0,15
relation-classification,2,"On the other hand , our model performs within a reasonable margin ( ? 0.5 % for the NER task and ? 1 %",result,0,234,13,0,24
relation-classification,2,"for the RE task ) compared to For the CoNLL04 dataset , there are two different evaluation settings , namely relaxed and strict .",result,0,235,14,0,24
relation-classification,2,"In the relaxed setting , we perform an EC task instead of NER assuming that the boundaries of the entities are given .",result,0,236,15,0,23
relation-classification,2,We adopt this setting to produce comparable results with previous studies ) .,result,0,237,16,0,13
relation-classification,2,"Similar to , we present results of single models and no ensembles .",result,0,238,17,0,13
relation-classification,2,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,result,1,239,18,0,32
relation-classification,2,"Unlike these previous studies that consider pairs of entities to obtain the entity types and the corresponding relations , we model the whole sentence at once .",result,0,240,19,0,27
relation-classification,2,"That way , our method is able to directly infer all entities and relations of a sentence and benefit from their possible interactions that can not be modeled when training is performed for each entity pair individually , one at a time .",result,0,241,20,0,43
relation-classification,2,"In the same setting , we also report the results of Gupta et al. in which they use multiple complicated hand - crafted features coming from NLP tools .",result,0,242,21,1,29
relation-classification,2,Our model performs slightly better for the EC task and within a margin of 1 % in terms of over all F 1 score .,result,0,243,22,0,25
relation-classification,2,The difference in the over all performance is due to the fact that our model uses only automatically generated features .,result,0,244,23,0,21
relation-classification,2,"We also report re-sults on the same dataset conducting NER ( i.e. , predicting entity types and boundaries ) and evaluating using the strict evaluation measure , similar to .",result,0,245,24,0,30
relation-classification,2,Our results are not directly comparable to the work of because we use the splits provided by .,result,0,246,25,0,18
relation-classification,2,"However , in this setting we present the results from as reference .",result,0,247,26,0,13
relation-classification,2,"We report an improvement of ? 2 % over all F 1 score , which suggests that our neural model is able to extract more informative representations compared to feature - based approaches .",result,0,248,27,0,34
relation-classification,2,"We also report results for the DREC dataset , with two different evaluation settings .",result,1,249,28,0,15
relation-classification,2,"Specifically , we use the boundaries and the strict settings .",result,1,250,29,0,11
relation-classification,2,"We transform the previous results from to the boundaries setting to make them comparable to our model since in their work , they report token - based F 1 score , which is not a common evaluation metric in relation extraction problems .",result,0,251,30,0,43
relation-classification,2,"Also , in their work , they focus on identifying only the boundaries of the entities and not the types ( e.g. , Floor , Space ) .",result,0,252,31,0,28
relation-classification,2,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .",result,1,253,32,0,15
relation-classification,2,"This is due to the fact that their quadratic scoring layer is beneficial for the RE task , yet complicates NER , which is usually modeled as a sequence labeling task .",result,0,254,33,0,32
relation-classification,2,"Moreover , we report results using the strict evaluation which is used in most related works .",result,0,255,34,0,17
relation-classification,2,"Using the prior knowledge that each entity has only one head , we can simplify our model and predict only one head each time ( i.e. , using a softmax loss ) .",result,0,256,35,0,33
relation-classification,2,The difference between the single and the multi-head models is marginal ( < 0.1 % for both tasks ) .,result,0,257,36,0,20
relation-classification,2,"This shows that our model ( multi-head ) can adapt to various environments , even if the setting is single head ( in terms of the application , and thus also in both training and test data ) .",result,0,258,37,0,39
relation-classification,2,"Finally , we compare our model with previous work",result,0,259,38,0,9
relation-classification,2,analysis of feature contribution,result,0,260,39,0,4
relation-classification,2,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,result,1,261,40,0,23
relation-classification,2,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,result,1,262,41,0,39
relation-classification,2,"This shows that the NER labels , as expected , provide meaningful information for the RE component .",result,0,263,42,0,18
relation-classification,2,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,result,1,264,43,0,29
relation-classification,2,"This illustrates that composing words by the representation of characters is effective , and our method benefits from additional information such as capital letters , suffixes and prefixes within the token ( i.e. , its character sequences ) .",result,0,265,44,0,39
relation-classification,2,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .",result,1,266,45,0,22
relation-classification,2,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .",result,1,267,46,0,38
relation-classification,2,"This happens because the CRF loss is able to capture the strong tag dependencies ( e.g. , I - LOC can not follow B - PER ) thatare present in the dataset instead of just assuming that the tag decision for each token is independent from tag decisions of neighboring tokens .",result,0,268,47,0,52
relation-classification,2,conclusion,result,0,269,48,0,1
relation-classification,2,"In this work , we present a joint neural model to simultaneously extract entities and relations from textual data .",result,0,270,49,0,20
relation-classification,2,Our model comprises a CRF layer for the entity recognition task and a sigmoid layer for the relation extraction task .,result,0,271,50,0,21
relation-classification,2,"Specifically , we model the relation extraction task as a multi-head selection problem since one entity can have multiple relations .",result,0,272,51,0,21
relation-classification,2,"Previous models on this task rely heavily on external NLP tools ( i.e. , POS taggers , dependency parsers ) .",result,0,273,52,0,21
relation-classification,2,"Thus , the performance of these models is affected by the accuracy of the extracted features .",result,0,274,53,0,17
relation-classification,2,"Unlike previous studies , our model produces automatically generated features rather than relying on hand - crafted ones , or existing NLP tools .",result,0,275,54,0,24
relation-classification,2,"Given its independence from such NLP or other feature generating tools , our approach can be easily adopted for any language and context .",result,0,276,55,0,24
relation-classification,2,We demonstrate the effectiveness of our approach by conducting a large scale experimental study .,result,0,277,56,0,15
relation-classification,2,Our model is able to outperform neural methods that automatically generate features while the results are marginally similar ( or sometimes better ) compared to feature - based neural network approaches .,result,0,278,57,0,32
relation-classification,2,"As future work , we aim to explore the effectiveness of entity pre-training for the entity recognition module .",result,0,279,58,0,19
relation-classification,2,This approach has been proven beneficial in the work of Miwa & Bansal ( 2016 ) for both the entity and the relation extraction modules .,result,0,280,59,1,26
relation-classification,2,"in addition ,",result,0,281,60,0,3
relation-classification,2,we are planning to explore away to reduce the calculations in the quadratic relation scoring layer .,result,0,282,61,0,17
relation-classification,2,"For instance , a straightforward way to do so is to use in the sigmoid layer only the tokens that have been identified as entities .",result,0,283,62,0,26
relation-classification,2,"Gupta , P. , . optimize only over the NER task ) , ( ii ) explore several hyperparameters of the network ( e.g. , dropout , LSTM size , character embeddings size ) , and ( iii ) report F 1 score using different word embeddings compared to the embeddings used in previous works .",result,0,284,63,0,56
relation-classification,2,"In of the main paper , we focused on comparing our model against other joint models thatare able to solve the two tasks ( i.e. , NER and relation extraction ) simultaneously , mainly demonstrating superiority of phrasing the relation extraction as a multi-head selection problem ( enabling the extraction of multiple relations at once ) .",result,0,285,64,0,57
relation-classification,2,"Here , in and vice versa ) .",result,0,286,65,0,8
relation-classification,2,"Note that improving NER in isolation was not the objective of our multi-head model , but we rather aimed to compare our model against other joint models that solve the task of entity recognition and relation identification simultaneously .",result,0,287,66,0,39
relation-classification,2,We thus did not envision to claim or achieve state - of - the - art performance in each of the individual building blocks of our joint model .,result,0,288,67,0,29
relation-classification,2,"and A4 show the performance of our model on the test set for different values of the embedding dropout , LSTM layer dropout and the LSTM output dropout hyperparameters , respectively .",result,0,289,68,0,32
relation-classification,2,"Note that the hyperparameter values used for the results in Section 5 were obtained by tuning over the development set , and these are indicated in boldface in the tables below .",result,0,290,69,0,32
relation-classification,2,We vary one hyperparameter at a time in order to assess the effect of a particular hyperparameter .,result,0,291,70,0,18
relation-classification,2,"The main outcomes from these tables are twofold : ( i ) low dropout values ( e.g. , 0 , 0.1 ) lead to a performance decrease in the over all F 1 score ( see where a ? 3 % F 1 decrease is reported on the ACE04 dataset ) and ( ii ) average dropout values ( i.e. , 0.2 - 0.4 ) lead to consistently similar results .",result,0,292,71,0,71
relation-classification,2,"In In the main results ( see Section 5 ) , to guarantee a fair comparison to previous work and to obtain comparable results thatare not affected by the input embeddings , we use embeddings used also in prior studies .",result,0,293,72,0,41
relation-classification,2,"To assess the performance of our system to input variations , we also report results using different word embeddings ( see ) ( i.e. , ; Li et al. ) on the ACE04 dataset .",result,0,294,73,1,35
relation-classification,2,"Our results showcase that our model , even when using different word embeddings , is still performing better compared to other works that , like ours , do not rely on additional NLP tools .",result,0,295,74,0,35
relation-classification,3,Adversarial training for multi-context joint entity and relation extraction,title,1,2,1,0,9
relation-classification,3,abstract,abstract,0,3,1,0,1
relation-classification,3,Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data .,abstract,0,4,2,0,30
relation-classification,3,We show how to use AT for the tasks of entity recognition and relation extraction .,abstract,1,5,3,0,16
relation-classification,3,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",abstract,1,6,4,0,60
relation-classification,3,introduction,introduction,0,7,1,0,1
relation-classification,3,"Many neural network methods have recently been exploited in various natural language processing ( NLP ) tasks , such as parsing , POS tagging , relation extraction , translation , and joint tasks .",introduction,0,8,2,0,34
relation-classification,3,"However , observed that intentional small scale perturbations ( i.e. , adversarial examples ) to the input of such models may lead to incorrect decisions ( with high confidence ) .",introduction,0,9,3,0,31
relation-classification,3,proposed adversarial training ( AT ) ( for image recognition ) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model .,introduction,0,10,4,0,32
relation-classification,3,"Although AT has recently been applied in NLP tasks ( e.g. , text classification ) , this paper - to the best of our knowledge - is the first attempt investigating regularization effects of AT in a joint setting for two related tasks .",introduction,0,11,5,0,44
relation-classification,3,We start from a baseline joint model that performs the tasks of named entity recognition and relation extraction at once .,introduction,0,12,6,0,21
relation-classification,3,"Previously proposed models ( summarized in Section 2 ) exhibit several issues that the neural network - based baseline approach ( detailed in Section 3.1 ) overcomes : ( i ) our model uses automatically extracted features without the need of external parsers nor manually extracted features ( see ; ; ) , ( ii ) all entities and the corresponding relations within the sentence are extracted at once , instead of examining one pair of entities at a time ( see ) , and ( iii ) we model relation extraction in a multi-label setting , allowing multiple relations per entity ( see ; ) .",introduction,0,13,7,0,107
relation-classification,3,The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task ( Section 3.2 ) .,introduction,0,14,8,0,28
relation-classification,3,"To evaluate the proposed AT method , we perform a large scale experimental study in this joint task ( see Section 4 ) , using datasets from different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",introduction,0,15,9,0,49
relation-classification,3,"We use a strong baseline that outperforms all previous models that rely on automatically extracted features , achieving state - of - the - art performance ( Section 5 ) .",introduction,0,16,10,0,31
relation-classification,3,"Compared to the baseline model , applying AT during training leads to a consistent additional increase in joint extraction effectiveness .",introduction,0,17,11,0,21
relation-classification,3,related work,related work,0,18,1,0,2
relation-classification,3,Joint entity and relation extraction :,related work,0,19,2,0,6
relation-classification,3,Joint models thatare based on manually extracted features have been proposed for performing both the named entity recognition ( NER ) and relation extraction subtasks at once .,related work,0,20,3,0,28
relation-classification,3,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features leading to additional complexity .",related work,0,21,4,0,24
relation-classification,3,Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs .,related work,0,22,5,0,19
relation-classification,3,"Specifically , as well as apply bidirectional tree - structured RNNs for different contexts ( i.e. , news , biomedical ) to capture syntactic information ( using external dependency parsers ) .",related work,0,23,6,0,32
relation-classification,3,propose the use of various manually extracted features along with RNNs .,related work,0,24,7,0,12
relation-classification,3,"solve the simpler problem of entity classification ( EC , assuming entity boundaries are given ) , instead of NER , and they replicate the context around the entities , feeding entity pairs to the relation extraction layer .",related work,0,25,8,0,39
relation-classification,3,investigate RNNs with attention without taking into account that relation labels are not mutually exclusive .,related work,0,26,9,0,16
relation-classification,3,"Finally , use LSTMs in a joint model for extracting just one relation at a time , but increase the complexity of the NER part .",related work,0,27,10,0,26
relation-classification,3,Our baseline model enables simultaneous extraction of multiple relations from the same input .,related work,0,28,11,0,14
relation-classification,3,"Then , we further extend this strong baseline using adversarial training .",related work,0,29,12,0,12
relation-classification,3,Adversarial training ( AT ) has been proposed to make classifiers more robust to input perturbations in the context of image recognition .,related work,0,30,13,0,23
relation-classification,3,"In the context of NLP , several variants have been proposed for different tasks such as text classification , relation extraction and POS tagging .",related work,0,31,14,0,25
relation-classification,3,AT is considered as a regularization method .,related work,0,32,15,0,8
relation-classification,3,"Unlike other regularization methods ( i.e. , dropout , word dropout ) that introduce random noise , AT generates perturbations thatare variations of examples easily misclassified by the model .",related work,0,33,16,0,30
relation-classification,3,model,related work,0,34,17,0,1
relation-classification,3,Joint learning as head selection,related work,0,35,18,0,5
relation-classification,3,"The baseline model , described in detail in , is illustrated in .",related work,1,36,19,0,13
relation-classification,3,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,related work,1,37,20,0,24
relation-classification,3,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",related work,1,38,21,0,22
relation-classification,3,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",related work,1,39,22,0,28
relation-classification,3,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,related work,1,40,23,0,23
relation-classification,3,We also use pre-trained word embeddings .,related work,1,41,24,0,7
relation-classification,3,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .",related work,1,42,25,0,26
relation-classification,3,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .",related work,1,43,26,0,19
relation-classification,3,"In , the B - PER tag is assigned to the beginning token of a ' person ' ( PER ) entity .",related work,0,44,27,0,23
relation-classification,3,"For the prediction of the entity tags , we use : ( i ) a softmax approach for the entity classification ( EC ) task ( assuming entity boundaries given ) or ( ii ) a CRF approach where we identify both the type and the boundaries for each entity .",related work,0,45,28,0,51
relation-classification,3,"During decoding , in the softmax setting , we greedily detect the entity types of the tokens ( i.e. , independent prediction ) .",related work,0,46,29,0,24
relation-classification,3,"Although independent distribution of types is reasonable for EC tasks , this is not the case when there are strong correlations between neighboring tags .",related work,0,47,30,0,25
relation-classification,3,"For instance , the BIO encoding scheme imposes several constraints in the NER task ( e.g. , the B - PER and I - LOC tags can not be sequential ) .",related work,0,48,31,0,32
relation-classification,3,"Motivated by this intuition , we use a linear - chain CRF for the NER task .",related work,0,49,32,0,17
relation-classification,3,"For decoding , in the CRF setting , we use the Viterbi algorithm .",related work,0,50,33,0,14
relation-classification,3,"During training , for both EC ( softmax ) and NER tasks ( CRF ) , we minimize the cross - entropy loss L NER .",related work,0,51,34,0,26
relation-classification,3,"The entity tags are later fed into the relation extraction layer as label embeddings ( see ) , assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities .",related work,0,52,35,0,36
relation-classification,3,We model the relation extraction task as a multi-label head selection problem .,related work,1,53,36,0,13
relation-classification,3,"In our model , each word w i can be involved in multiple relations with other words .",related work,0,54,37,0,18
relation-classification,3,"For instance , in the example illustrated in , "" Smith "" could be involved not only in a Lives in relation with the token "" California "" ( head ) but also in other relations simultaneously ( e.g. , Works for , Born In with some corresponding tokens ) .",related work,0,55,38,0,51
relation-classification,3,"The goal of the task is to predict for each word w i , a vector of heads ?",related work,0,56,39,0,19
relation-classification,3,i and the vector of corresponding relationsr i .,related work,0,57,40,0,9
relation-classification,3,"We compute the score s ( w j , w i , r k ) of word w j to be the head of w i given a relation label r k using a single layer neural network .",related work,0,58,41,0,39
relation-classification,3,"The corresponding probability is defined as : P ( w j , r k | w i ; ? ) = ? ( s ( w j , w i , r k ) ) , where ?( . ) is the sigmoid function .",related work,0,59,42,0,45
relation-classification,3,"During training , we minimize the cross - entropy loss L rel as :",related work,0,60,43,0,14
relation-classification,3,where m is the number of associated heads ( and thus relations ) per word w i .,related work,0,61,44,0,18
relation-classification,3,"During decoding , the most probable heads and relations are selected using threshold - based prediction .",related work,0,62,45,0,17
relation-classification,3,The final objective for the joint task is computed as L JOINT ( w ; ? ) = L NER + L rel where ?,related work,0,63,46,0,25
relation-classification,3,is a set of parameters .,related work,0,64,47,0,6
relation-classification,3,"In the case of multi-token entities , only the last token of the entity can serve as head of another token , to eliminate redundant relations .",related work,0,65,48,0,27
relation-classification,3,"If an entity is not involved in any relation , we predict the auxiliary "" N "" relation label and the token itself as head .",related work,0,66,49,0,26
relation-classification,3,adversarial training ( at ),related work,1,67,50,0,5
relation-classification,3,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,related work,1,68,51,0,19
relation-classification,3,"Specifically , we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation .",related work,0,69,52,0,25
relation-classification,3,This is similar to the concept introduced by to improve the robustness of image recognition classifiers .,related work,0,70,53,0,17
relation-classification,3,We generate an adversarial example by adding the worst - case perturbation ?,related work,0,71,54,0,13
relation-classification,3,adv to the original embedding w that maximizes the loss function :,related work,0,72,55,0,12
relation-classification,3,where ?,related work,0,73,56,0,2
relation-classification,3,is a copy of the current model parameters .,related work,0,74,57,0,9
relation-classification,3,since eq.,related work,0,75,58,0,2
relation-classification,3,"( 2 ) is intractable in neural networks , we use the approximation proposed in defined as : ? adv = g/ g , with g = ? w L JOINT ( w ; ? ) , where is a small bounded norm treated as a hyperparameter .",related work,0,76,59,0,48
relation-classification,3,"Similar to , we set to be ? ?",related work,0,77,60,0,9
relation-classification,3,D ( where Dis the dimension of the embeddings ) .,related work,0,78,61,0,11
relation-classification,3,"We train on the mixture of original and adversarial examples , so the final loss is computed as : L JOINT ( w ; ? ) + L JOINT ( w + ? adv ;? ) .",related work,0,79,62,0,37
relation-classification,3,experimental setup,experiment,0,80,1,0,2
relation-classification,3,"We evaluate our models on four datasets , using the code as available from our github codebase .",experiment,1,81,2,0,18
relation-classification,3,1,experiment,0,82,3,0,1
relation-classification,3,"Specifically , we follow the 5 - fold crossvalidation defined by for the ACE04 dataset .",experiment,0,83,4,0,16
relation-classification,3,"For the CoNLL04 ) EC task ( assuming boundaries are given ) , we use the same splits as in ; .",experiment,0,84,5,0,22
relation-classification,3,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,experiment,1,85,6,0,22
relation-classification,3,"For the Dutch Real Estate Classifieds , DREC ( Bekoulis et al. , 2017 ) dataset , we use train - test splits as in .",experiment,0,86,7,1,26
relation-classification,3,"For the Adverse Drug Events , ADE , we perform 10 - fold cross -validation similar to .",experiment,0,87,8,0,18
relation-classification,3,"To obtain comparable results thatare not affected by the input embeddings , we use the embeddings of the previous works .",experiment,0,88,9,0,21
relation-classification,3,We employ early stopping in all of the experiments .,experiment,1,89,10,0,10
relation-classification,3,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .",experiment,1,90,11,0,29
relation-classification,3,the scaling parameter ?,experiment,0,91,12,0,4
relation-classification,3,"is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",experiment,0,92,13,0,20
relation-classification,3,"Larger values of ? ( i.e. , larger perturbations ) lead to consistent performance decrease in our early experiments .",experiment,0,93,14,0,20
relation-classification,3,This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .,experiment,0,94,15,0,23
relation-classification,3,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .",experiment,1,95,16,0,127
relation-classification,3,"The proposed models are : ( i ) baseline , ( ii ) baseline EC ( predicts only entity classes ) and ( iii ) baseline ( EC ) + AT ( regularized by AT ) .",experiment,0,96,17,0,37
relation-classification,3,The and symbols indicate whether the models rely on external NLP tools .,experiment,0,97,18,0,13
relation-classification,3,"We include different evaluation types ( S , Rand B ) .",experiment,0,98,19,0,12
relation-classification,3,"boundaries are known ( CoNLL04 ) , to compare to previous works .",experiment,0,99,20,0,13
relation-classification,3,"In all cases , a relation is considered as correct when both the relation type and the argument entities are correct .",experiment,0,100,21,0,22
relation-classification,3,shows our experimental results .,experiment,0,101,1,0,5
relation-classification,3,The name of the dataset is presented in the first column while the models are listed in the second column .,experiment,0,102,2,0,21
relation-classification,3,"The proposed models are the following : ( i ) baseline : the baseline model shown in with the CRF layer and the sigmoid loss , ( ii ) baseline EC : the proposed model with the softmax layer for EC , ( iii ) baseline ( EC ) + AT : the baseline regularized using AT .",experiment,0,103,3,0,58
relation-classification,3,The final three columns present the F 1 results for the two subtasks and their average performance .,experiment,0,104,4,0,18
relation-classification,3,Bold values indicate the best results among models that use only automatically extracted features .,experiment,0,105,5,0,15
relation-classification,3,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .",experiment,1,106,6,0,14
relation-classification,3,"This improvement can be explained by the use of : ( i ) multi-label head selection , ( ii ) CRF - layer and ( iii ) character level embeddings .",experiment,0,107,7,0,31
relation-classification,3,"Compared to , who rely on NLP tools , the baseline performs within a reasonable margin ( less than 1 % ) on the joint task .",experiment,0,108,8,0,27
relation-classification,3,"On the other hand , use the same model for the ADE biomedical dataset , where we report a 2.5 % over all improvement .",experiment,0,109,9,0,25
relation-classification,3,This indicates that NLP tools are not always accurate for various contexts .,experiment,0,110,10,0,13
relation-classification,3,"For the CoNLL04 dataset , we use two evaluation settings .",experiment,1,111,11,0,11
relation-classification,3,We use the relaxed evaluation similar to ; on the EC task .,experiment,0,112,12,0,13
relation-classification,3,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .",experiment,1,113,13,0,47
relation-classification,3,"Moreover , compared to the model of that relies on complex features , the baseline model performs within a margin of 1 % in terms of over all F 1 score .",experiment,0,114,14,0,32
relation-classification,3,"We also report NER results on the same dataset and improve over all F 1 score with ? 1 % compared to , indicating that our automatically extracted features are more informative than the hand - crafted ones .",experiment,0,115,15,0,39
relation-classification,3,These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model .,experiment,0,116,16,0,33
relation-classification,3,"For the DREC dataset , we use two evaluation methods .",experiment,1,117,17,0,11
relation-classification,3,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .",experiment,1,118,18,0,27
relation-classification,3,and show the effectiveness of the adversarial training on top of the baseline model .,experiment,1,119,19,0,15
relation-classification,3,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .",experiment,1,120,20,0,20
relation-classification,3,"Moreover , as seen in , the performance of the models using AT is closer to maximum even from the early training epochs .",experiment,0,121,21,0,24
relation-classification,3,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the over all F 1 performance ( 0.4 % ) .",experiment,1,122,22,0,27
relation-classification,3,"For CoNLL04 , we note an improvement in the over all F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .",experiment,1,123,23,0,29
relation-classification,3,"For the DREC dataset , in both settings , there is an over all improvement of ? 1 % .",experiment,1,124,24,0,20
relation-classification,3,"shows that from the first epochs , the model obtains its maximum performance on the DREC validation set .",experiment,0,125,25,0,19
relation-classification,3,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .",experiment,1,126,26,0,17
relation-classification,3,results,result,0,127,1,0,1
relation-classification,3,"Our results demonstrate that AT outperforms the neural baseline model consistently , considering our experiments across multiple and more diverse datasets than typical related works .",result,0,128,2,0,26
relation-classification,3,The im - provement of AT over our baseline ( depending on the dataset ) ranges from ? 0.4 % to ? 0.9 % in terms of over all F 1 score .,result,0,129,3,0,33
relation-classification,3,"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component , which is in accordance with the recent advances in NER using neural networks that report similarly small gains ( e.g. , the performance improvement in and on the CoNLL - 2003 test set is 0.01 % and 0.17 % F 1 percentage points , while in the work of , a 0.07 % F 1 improvement on CoNLL - 2000 using AT for NER is reported ) .",result,0,130,4,1,86
relation-classification,3,"However , the relation extraction performance increases by ? 1 % F 1 scoring points , except for the ACE04 dataset .",result,0,131,5,0,22
relation-classification,3,"Further , as seen in , the improvement for CoNLL04 is particularly small on the evaluation set .",result,0,132,6,0,18
relation-classification,3,"This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models , but this needs further investigation in future work .",result,0,133,7,0,31
relation-classification,3,conclusion,result,0,134,8,0,1
relation-classification,3,We proposed to use adversarial training ( AT ) for the joint task of entity recognition and relation extraction .,result,0,135,9,0,20
relation-classification,3,"The contribution of this study is twofold : ( i ) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model , with ( ii ) a large scale experimental evaluation .",result,0,136,10,0,39
relation-classification,3,"Experiments show that AT improves the results for each task separately , as well as the over all performance of the baseline joint model , while reaching high performance already during the first epochs of the training procedure .",result,0,137,11,0,39
relation-classification,0,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,title,1,2,1,0,14
relation-classification,0,abstract,abstract,0,3,1,0,1
relation-classification,0,We present a novel end - to - end neural model to extract entities and relations between them .,abstract,1,4,2,0,19
relation-classification,0,Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM - RNNs on bidirectional sequential LSTM - RNNs .,abstract,0,5,3,0,29
relation-classification,0,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,abstract,1,6,4,0,19
relation-classification,0,We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .,abstract,0,7,5,0,23
relation-classification,0,"Our model improves over the stateof - the - art feature - based model on end -toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1score on ACE2005 and ACE2004 , respectively .",abstract,0,8,6,0,38
relation-classification,0,We also show that our LSTM - RNN based model compares favorably to the state - of - the - art CNN based model ( in F1-score ) on nominal relation classification ( Sem Eval - 2010 Task 8 ) .,abstract,0,9,7,1,41
relation-classification,0,"Finally , we present an extensive ablation analysis of several model components .",abstract,0,10,8,0,13
relation-classification,0,introduction,introduction,0,11,1,0,1
relation-classification,0,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,introduction,1,12,2,0,26
relation-classification,0,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .",introduction,1,13,3,0,57
relation-classification,0,"For instance , to learn that Toefting and Bolton have an Organization - Affiliation ( ORG - AFF ) relation in the sentence Toefting transferred to Bolton , the entity information that Toefting and Bolton are Person and Organization entities is important .",introduction,0,14,4,0,43
relation-classification,0,"Extraction of these entities is in turn encouraged by the presence of the context words transferred to , which indicate an employment relation .",introduction,0,15,5,0,24
relation-classification,0,Previous joint models have employed feature - based structured learning .,introduction,0,16,6,0,11
relation-classification,0,An alternative approach to this end - to - end relation extraction task is to employ automatic feature learning via neural network ( NN ) based models .,introduction,0,17,7,0,28
relation-classification,0,There are two ways to represent relations between entities using neural networks : recurrent / recursive neural networks ( RNNs ) and convolutional neural networks ( CNNs ) .,introduction,0,18,8,0,29
relation-classification,0,"Among these , RNNs can directly represent essential linguistic structures , i.e. , word sequences and constituent / dependency trees .",introduction,0,19,9,0,21
relation-classification,0,"Despite this representation ability , for relation classification tasks , the previously reported performance using long short - term memory ( LSTM ) based RNNs is worse than one using CNNs .",introduction,0,20,10,0,32
relation-classification,0,"These previous LSTM - based systems mostly include limited linguistic structures and neural architectures , and do not model entities and relations jointly .",introduction,0,21,11,0,24
relation-classification,0,We are able to achieve improvements over state - of - the - art models via endto - end modeling of entities and relations based on richer LSTM - RNN architectures that incorporate complementary linguistic structures .,introduction,0,22,12,0,37
relation-classification,0,Word sequence and tree structure are known to be complementary information for extracting relations .,introduction,0,23,13,0,15
relation-classification,0,"For instance , dependencies between words are not enough to predict that source and U.S. have an ORG - AFF relation in the sentence "" This is ... "" , one U.S. source said , and the context word said is required for this prediction .",introduction,0,24,14,0,46
relation-classification,0,"Many traditional , feature - based relation classification models extract features from both sequences and parse trees .",introduction,0,25,15,0,18
relation-classification,0,"However , previous RNNbased models focus on only one of these linguistic structures .",introduction,0,26,16,0,14
relation-classification,0,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,introduction,1,27,17,0,24
relation-classification,0,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,introduction,1,28,18,0,49
relation-classification,0,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .",introduction,1,29,19,0,36
relation-classification,0,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .",introduction,1,30,20,0,49
relation-classification,0,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .",introduction,1,31,21,0,30
relation-classification,0,"On end - to - end relation extraction , we improve over the state - of - the - art feature - based model , with 12.1 % ( ACE2005 ) and 5.7 % ( ACE2004 ) relative error reductions in F1-score .",introduction,0,32,22,1,43
relation-classification,0,"On nominal relation classification ( Sem Eval - 2010 Task 8 ) , our model compares favorably to the state - of - the - art CNNbased model in F1-score .",introduction,0,33,23,1,31
relation-classification,0,"Finally , we also ablate and compare our various model components , which leads to some key findings ( both positive and negative ) about the contribution and effectiveness of different RNN structures , input dependency relation structures , different parsing models , external resources , and joint learning settings .",introduction,0,34,24,0,51
relation-classification,0,related work,related work,0,35,1,0,2
relation-classification,0,"LSTM - RNNs have been widely used for sequential labeling , such as clause identification , phonetic labeling , and NER .",related work,0,36,2,0,22
relation-classification,0,"showed that building a conditional random field ( CRF ) layer on top of bidirectional LSTM - RNNs performs comparably to the state - of - the - art methods in the partof - speech ( POS ) tagging , chunking , and NER .",related work,0,37,3,0,45
relation-classification,0,"For relation classification , in addition to traditional feature / kernel - based approaches , several neural models have been proposed in the , including embedding - based models , , and RNN - based models .",related work,0,38,4,0,37
relation-classification,0,"Recently , and showed that the shortest dependency paths between relation arguments , which were used in feature / kernel - based systems , are also useful in NN - based models .",related work,0,39,5,0,33
relation-classification,0,"also showed that LSTM - RNNs are useful for relation classification , but the performance was worse than CNN - based models .",related work,0,40,6,0,23
relation-classification,0,"compared separate sequence - based and tree - structured LSTM - RNNs on relation classification , using basic RNN model structures .",related work,0,41,7,0,22
relation-classification,0,"Research on tree - structured LSTM - RNNs fixes the direction of information propagation from bottom to top , and also can not handle an arbitrary number of typed children as in a typed dependency tree .",related work,0,42,8,0,37
relation-classification,0,"Furthermore , no RNNbased relation classification model simultaneously uses word sequence and dependency tree information .",related work,0,43,9,0,16
relation-classification,0,"We propose several such novel model structures and training settings , investigating the simultaneous use of bidirectional sequential and bidirectional tree - structured LSTM - RNNs to jointly capture linear and dependency context for end - toend extraction of relations between entities .",related work,0,44,10,0,43
relation-classification,0,"As for end - to - end ( joint ) extraction of relations between entities , all existing models are featurebased systems ( and no NN - based model has been proposed ) .",related work,0,45,11,0,34
relation-classification,0,"Such models include structured prediction , integer linear programming , card - pyramid parsing ( Kate and Mooney , 2010 ) , and global probabilistic graphical models .",related work,0,46,12,1,28
relation-classification,0,"Among these , structured prediction methods are state - of - the - art on several corpora .",related work,0,47,13,0,18
relation-classification,0,"We present an improved , NN - based alternative for the end - to - end relation extraction .",related work,0,48,14,0,19
relation-classification,0,model,related work,0,49,15,0,1
relation-classification,0,"We design our model with LSTM - RNNs that represent both word sequences and dependency tree structures , and perform end - to - end extraction of relations between entities on top of these RNNs.",related work,0,50,16,0,35
relation-classification,0,illustrates the overview of the model .,related work,0,51,17,0,7
relation-classification,0,"The model mainly consists of three representation layers : a word embeddings layer ( embedding layer ) , a word sequence based LSTM - RNN layer ( sequence layer ) , and finally a dependency subtree based LSTM - RNN layer ( dependency layer ) .",related work,0,52,18,0,46
relation-classification,0,"During decoding , we build greedy , left - to - right entity detection on the sequence layer and realize relation classification on the dependency layers , where each subtree based LSTM - RNN corresponds to a relation candidate between two detected entities .",related work,0,53,19,0,44
relation-classification,0,"After decoding the entire model structure , we update the parameters simultaneously via backpropagation through time ( BPTT ) .",related work,0,54,20,0,20
relation-classification,0,"The dependency layers are stacked on the sequence layer , so the embedding and sequence layers are shared by both entity detection and relation classification , and the shared parameters are affected by both entity and relation labels .",related work,0,55,21,0,39
relation-classification,0,embedding layer,related work,0,56,22,0,2
relation-classification,0,The embedding layer handles embedding representations .,related work,0,57,23,0,7
relation-classification,0,"n w , n p , n d and n e - dimensional vectors v , v ( p ) , v and v ( e ) are embedded to words , part - of - speech ( POS ) tags , dependency types , and entity labels , respectively .",related work,0,58,24,0,51
relation-classification,0,sequence layer,related work,0,59,25,0,2
relation-classification,0,The sequence layer represents words in a linear sequence using the representations from the embedding layer .,related work,0,60,26,0,17
relation-classification,0,"This layer represents sentential context information and maintains entities , as shown in bottom - left part of .",related work,0,61,27,0,19
relation-classification,0,We represent the word sequence in a sentence with bidirectional LSTM - RNNs .,related work,0,62,28,0,14
relation-classification,0,"The LSTM unit at t-th word consists of a collection of n ls - dimensional vectors : an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht .",related work,0,63,29,0,44
relation-classification,0,"The unit receives an n-dimensional input vector x t , the previous hidden state h t?1 , and the memory cell c t?1 , and calculates the new vectors using the following equations :",related work,0,64,30,0,34
relation-classification,0,( 1 ),related work,0,65,31,0,3
relation-classification,0,where ?,related work,0,66,32,0,2
relation-classification,0,"denotes the logistic function , denotes element - wise multiplication , W and U are weight matrices , and bare bias vectors .",related work,0,67,33,0,23
relation-classification,0,The LSTM unit at t-th word receives the concatenation of word and POS embeddings as its input vector :,related work,0,68,34,0,19
relation-classification,0,.,related work,0,69,35,0,1
relation-classification,0,We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ?,related work,0,70,36,0,23
relation-classification,0,"ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ?",related work,0,71,37,0,19
relation-classification,0,"ht , and pass it to the subsequent layers .",related work,0,72,38,0,10
relation-classification,0,entity detection,related work,0,73,39,0,2
relation-classification,0,We treat entity detection as a sequence labeling task .,related work,0,74,40,0,10
relation-classification,0,"We assign an entity tag to each word using a commonly used encoding scheme BILOU ( Begin , Inside , Last , Outside , Unit ) ( Ratinov and , where each entity tag represents the entity type and the position of a word in the entity .",related work,0,75,41,0,48
relation-classification,0,"For example , in , we assign B - PER and L - PER ( which denote the beginning and last words of a person entity type , respectively ) to each word in Sidney Yates to represent this phrase as a PER ( person ) entity type .",related work,0,76,42,0,49
relation-classification,0,We perform entity detection on top of the sequence layer .,related work,0,77,43,0,11
relation-classification,0,We employ a two - layered NN with an n he - dimensional hidden layer h ( e ) and a softmax output layer for entity detection .,related work,0,78,44,0,28
relation-classification,0,"Here , Ware weight matrices and bare bias vectors .",related work,0,79,45,0,10
relation-classification,0,"We assign entity labels to words in a greedy , left - to - right manner .",related work,0,80,46,0,17
relation-classification,0,"1 During this decoding , we use the predicted label of a word to predict the label of the next word so as to take label dependencies into account .",related work,0,81,47,0,30
relation-classification,0,The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word ) .,related work,0,82,48,0,24
relation-classification,0,dependency layer,related work,0,83,49,0,2
relation-classification,0,"The dependency layer represents a relation between a pair of two target words ( corresponding to a relation candidate in relation classification ) in the dependency tree , and is in charge of relationspecific representations , as is shown in top - right part of .",related work,0,84,50,0,46
relation-classification,0,"This layer mainly focuses on the shortest path between a pair of target words in the dependency tree ( i.e. , the path between the least common node and the two target words ) since these paths are shown to be effective in relation classification .",related work,0,85,51,0,46
relation-classification,0,"For example , we show the shortest path between Yates and Chicago in the bottom of , and this path well captures the key phrase of their relation , i.e. , born in .",related work,0,86,52,0,34
relation-classification,0,"We employ bidirectional tree - structured LSTM - RNNs ( i.e. , bottom - up and top - down ) to represent a relation candidate by capturing the dependency structure around the target word pair .",related work,0,87,53,0,36
relation-classification,0,This bidirectional structure propagates to each node not only the information from the leaves but also information from the root .,related work,0,88,54,0,21
relation-classification,0,"This is especially important for relation classification , which makes use of argument nodes near the bottom of the tree , and our top - down LSTM - RNN sends information from the top of the tree to such near - leaf nodes ( unlike in standard bottom - up LSTM - RNNs ) .",related work,0,89,55,0,55
relation-classification,0,2 Note that the two variants of tree - structured LSTM - RNNs by are notable to represent our target structures which have a variable number of typed children : the Child - Sum Tree - LSTM does not deal with types and the N - ary Tree assumes a fixed number of children .,related work,0,90,56,0,55
relation-classification,0,We thus propose a new variant of tree - structured LSTM - RNN that shares weight matrices U s for same - type children and also allows variable number of children .,related work,0,91,57,0,32
relation-classification,0,"For this variant , we calculate n lt - dimensional vectors in the LSTM unit at t-th node with C ( t ) children using following equations :",related work,0,92,58,0,28
relation-classification,0,where m ( ) is a type mapping function .,related work,0,93,59,0,10
relation-classification,0,"To investigate appropriate structures to represent relations between two target word pairs , we experiment with three structure options .",related work,0,94,60,0,20
relation-classification,0,"We primarily employ the shortest path structure ( SP - Tree ) , which captures the core dependency path between a target word pair and is widely used in relation classification models , e.g. , .",related work,0,95,61,0,36
relation-classification,0,We also try two other dependency structures : SubTree and Full - Tree .,related work,0,96,62,0,14
relation-classification,0,SubTree is the subtree under the lowest common ancestor of the target word pair .,related work,0,97,63,0,15
relation-classification,0,This provides additional modifier information to the path and the word pair in SPTree .,related work,0,98,64,0,15
relation-classification,0,FullTree is the full dependency tree .,related work,0,99,65,0,7
relation-classification,0,This captures context from the entire sentence .,related work,0,100,66,0,8
relation-classification,0,"While we use one node type for SPTree , we define two node types for SubTree and FullTree , i.e. , one for nodes on shortest paths and one for all other nodes .",related work,0,101,67,0,34
relation-classification,0,We use the type mapping function m ( ) to distinguish these two nodes types .,related work,0,102,68,0,16
relation-classification,0,Stacking Sequence and Dependency Layers,related work,0,103,69,0,5
relation-classification,0,We stack the dependency layers ( corresponding to relation candidates ) on top of the sequence layer to incorporate both word sequence and dependency tree structure information into the output .,related work,0,104,70,0,31
relation-classification,0,The dependency - layer LSTM unit at the t - th word receives as input,related work,0,105,71,0,15
relation-classification,0,"i.e. , the concatenation of its corresponding hidden state vectors st in the sequence layer , dependency type embedding v",related work,0,106,72,0,20
relation-classification,0,relation classification,related work,0,107,73,0,2
relation-classification,0,"We incrementally build relation candidates using all possible combinations of the last words of detected entities , i.e. , words with L or U labels in the BILOU scheme , during decoding .",related work,0,108,74,0,33
relation-classification,0,"For instance , in , we build a relation candidate using Yates with an L - PER label and Chicago with an U - LOC label .",related work,0,109,75,0,27
relation-classification,0,"For each relation candidate , we realize the dependency layer d p ( described above ) corresponding to the path between the word pair pin the relation candidate , and the NN receives a relation candidate vector constructed from the output of the dependency tree layer , and predicts its relation label .",related work,0,110,76,0,53
relation-classification,0,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation .,related work,0,111,77,0,22
relation-classification,0,"We represent relation labels by type and direction , except for negative relations that have no direction .",related work,0,112,78,0,18
relation-classification,0,"The relation candidate vector is constructed as the concatenation d p = [?h p A ; ?h p 1 ; ?h p 2 ] , where ?h p",related work,0,113,79,0,28
relation-classification,0,A is the hidden state vector of the top LSTM,related work,0,114,80,0,10
relation-classification,0,We use the dependency to the parent since the number of children varies .,related work,0,115,81,0,14
relation-classification,0,"Dependency types can also be incorporated into m ( ) , but this did not help in initial experiments .",related work,0,116,82,0,20
relation-classification,0,"unit in the bottom - up LSTM - RNN ( representing the lowest common ancestor of the target word pair p ) , and ?h p 1 , ?h p 2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top - down LSTM - RNN .",related work,0,117,83,0,57
relation-classification,0,All the corresponding arrows are shown in .,related work,0,118,84,0,8
relation-classification,0,"Similarly to the entity detection , we employ a two - layered NN with an n hr -dimensional hidden layer h ( r ) and a softmax output layer ( with weight matrices W , bias vectors b ) .",related work,0,119,85,0,40
relation-classification,0,"We construct the input d p for relation classification from tree - structured LSTM - RNNs stacked on sequential LSTM - RNNs , so the contribution of sequence layer to the input is indirect .",related work,0,120,86,0,35
relation-classification,0,"Furthermore , our model uses words for representing entities , so it can not fully use the entity information .",related work,0,121,87,0,20
relation-classification,0,"To alleviate these problems , we directly concatenate the average of hidden state vectors for each entity from the sequence layer to the input d p to relation classification , i.e. , d p =",related work,0,122,88,0,35
relation-classification,0,", where I p 1 and I p 2 represent sets of word indices in the first and second entities .",related work,0,123,89,0,21
relation-classification,0,"Also , we assign two labels to each word pair in prediction since we consider both left - to - right and right - to - left directions .",related work,0,124,90,0,29
relation-classification,0,"When the predicted labels are inconsistent , we select the positive and more confident label , similar to .",related work,0,125,91,0,19
relation-classification,0,training,related work,0,126,92,0,1
relation-classification,0,"We update the model parameters including weights , biases , and embeddings by BPTT and Adam ( Kingma and Ba , 2015 ) with gradient clipping , parameter averaging , and L2-regularization ( we regularize weights W and U , not the bias terms b ) .",related work,0,127,93,1,47
relation-classification,0,We also apply dropout to the embedding layer and to the final hidden layers for entity detection and relation classification .,related work,0,128,94,0,21
relation-classification,0,"We employ two enhancements , scheduled sampling and entity pretraining , to alleviate the problem of unreliable prediction of entities in the early stage of training , and to encourage building positive relation instances from the detected entities .",related work,0,129,95,0,39
relation-classification,0,"In scheduled sampling , we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal .",related work,0,130,96,0,32
relation-classification,0,"As for i , we choose the inverse sigmoid decay i = k / ( k + exp ( i / k ) ) , where k( ?",related work,0,131,97,0,28
relation-classification,0,1 ) is a hyper - parameter that adjusts how often we use the gold labels as prediction .,related work,0,132,98,0,19
relation-classification,0,"Entity pretraining is inspired by , and we pretrain the entity detection model using the training data before training the entire model parameters .",related work,0,133,99,0,24
relation-classification,0,results and discussion,result,0,134,1,0,3
relation-classification,0,data and task settings,result,0,135,2,0,4
relation-classification,0,"We evaluate on three datasets : ACE05 and ACE04 for end - to - end relation extraction , and SemEval - 2010 Task 8 for relation classification .",result,0,136,3,0,28
relation-classification,0,"We use the first two datasets as our primary target , and use the last one to thoroughly analyze and ablate the relation classification part of our model .",result,0,137,4,0,29
relation-classification,0,ACE05 defines 7 coarse - grained entity types and 6 coarse - grained relation types between entities .,result,0,138,5,0,18
relation-classification,0,"We use the same data splits , preprocessing , and task settings as .",result,0,139,6,0,14
relation-classification,0,We report the primary micro F1 -scores as well as micro precision and recall on both entity and relation extraction to better explain model performance .,result,0,140,7,0,26
relation-classification,0,We treat an entity as correct when it s type and the region of its head are correct .,result,0,141,8,0,19
relation-classification,0,We treat a relation as correct when it s type and argument entities are correct ; we thus treat all non-negative relations on wrong entities as false positives .,result,0,142,9,0,29
relation-classification,0,"ACE04 defines the same 7 coarse - grained entity types as ACE05 , but defines 7 coarse - grained relation types .",result,0,143,10,0,22
relation-classification,0,"We follow the cross-validation setting of Chan and and , and the preprocessing and evaluation metrics of ACE05 .",result,0,144,11,0,19
relation-classification,0,semeval-2010,result,0,145,12,0,1
relation-classification,0,Task 8 defines 9 relation types between nominals and a tenth type,result,0,146,13,0,12
relation-classification,0,Other when two nouns have none of these relations .,result,0,147,14,0,10
relation-classification,0,"We treat this Other type as a negative relation type , and no direction is considered .",result,0,148,15,0,17
relation-classification,0,"The dataset consists of 8,000 training and 2,717 test sentences , and each sentence is annotated with a relation between two given nominals .",result,0,149,16,0,24
relation-classification,0,We randomly selected 800 sentences from the training set as our development set .,result,0,150,17,0,14
relation-classification,0,"We followed the official task setting , and report the official macro -averaged F1 - score ( Macro - F1 ) on the 9 relation types .",result,0,151,18,0,27
relation-classification,0,"For more details of the data and task settings , please refer to the supplementary material .",result,0,152,19,0,17
relation-classification,0,experimental settings,experiment,0,153,1,0,2
relation-classification,0,We implemented our model using the cnn library .,experiment,1,154,2,0,9
relation-classification,0,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .",experiment,1,155,3,1,24
relation-classification,0,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .",experiment,1,156,4,0,53
relation-classification,0,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,experiment,1,157,5,0,18
relation-classification,0,We tuned hyper - parameters using development sets for ACE05 and SemEval - 2010 Task 8 to achieve high primary ( Micro - and Macro - ) F1-scores .,experiment,0,158,6,0,29
relation-classification,0,"9 For ACE04 , we directly employed the best parameters for ACE05 .",experiment,0,159,7,0,13
relation-classification,0,The hyperparameter settings are shown in the supplementary material .,experiment,0,160,8,0,10
relation-classification,0,for semeval-2010,experiment,0,161,9,0,2
relation-classification,0,"Task 8 , we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types .",experiment,0,162,10,0,26
relation-classification,0,Our statistical significance results are based on the Approximate Randomization ( AR ) test .,experiment,0,163,11,0,15
relation-classification,0,End - to - end Relation Extraction Results,method,0,164,1,0,8
relation-classification,0,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .",method,1,165,2,0,40
relation-classification,0,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .",method,1,166,3,0,33
relation-classification,0,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .",method,1,167,4,0,27
relation-classification,0,"This is reasonable because the model can only create relation instances when both of the entities are found and , without these enhancements , it may get too late to find some relations .",method,0,168,5,0,34
relation-classification,0,Removing label embeddings did not affect 6 https://github.com/clab/cnn,method,0,169,6,0,8
relation-classification,0,7 http://nlp.stanford.edu/software/,method,0,170,7,0,2
relation-classification,0,stanford-corenlp-full-2015-04-20.zip,method,0,171,8,0,1
relation-classification,0,8 https://dumps.wikimedia.org/enwiki/ 20150901/,method,0,172,9,0,3
relation-classification,0,9,method,0,173,10,0,1
relation-classification,0,"We did not tune the precision - recall trade - offs , but doing so can specifically improve precision further .",method,0,174,11,0,21
relation-classification,0,"Other work on ACE is not comparable or performs worse than the model by the entity detection performance , but this degraded the recall in relation classification .",method,0,175,12,0,28
relation-classification,0,This indicates that entity label information is helpful in detecting relations .,method,0,176,13,0,12
relation-classification,0,"We also show the performance without sharing parameters , i.e. , embedding and sequence layers , for detecting entities and relations ( ? Shared parameters ) ; we first train the entity detection model , detect entities with the model , and build a separate relation extraction model using the detected entities , i.e. , without entity detection .",method,0,177,14,0,59
relation-classification,0,This setting can be regarded as a pipeline model since two separate models are trained sequentially .,method,0,178,15,0,17
relation-classification,0,"Without the shared parameters , both the performance in entity detection and relation classification drops slightly , although the differences are not significant .",method,0,179,16,0,24
relation-classification,0,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .",method,1,180,17,0,53
relation-classification,0,"Next , we show the performance with different LSTM - RNN structures in .",method,0,181,18,0,14
relation-classification,0,"We first compare the three input dependency structures ( SPTree , SubTree , FullTree ) for tree - structured LSTM - RNNs .",method,0,182,19,0,23
relation-classification,0,"Performances on these three structures are almost same when we distinguish the nodes in the shortest paths from other nodes , but when we do not distinguish them ( - SP ) , the information outside of the shortest path , i.e. , FullTree ( - SP ) , significantly hurts performance ( p < 0.05 ) .",method,0,183,20,0,58
relation-classification,0,We then compare our tree - structured LSTM - RNN ( SPTree ) with the Child - Sum treestructured LSTM - RNN on the shortest path of .,method,0,184,21,0,28
relation-classification,0,"Child - Sum performs worse than our SPTree model , but not with as big of a decrease as above .",method,0,185,22,0,21
relation-classification,0,This maybe because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child .,method,0,186,23,0,30
relation-classification,0,We finally show results with two counterparts of sequence - based LSTM - RNNs using the shortest path ( last two rows in ) .,method,0,187,24,0,25
relation-classification,0,SPSeq is a bidirectional LSTM - RNN on the shortest path .,method,0,188,25,0,12
relation-classification,0,The LSTM unit receives input from the sequence layer concatenated with embeddings for the surrounding dependency types and directions .,method,0,189,26,0,20
relation-classification,0,We concatenate the outputs of the two RNNs for the relation candidate .,method,0,190,27,0,13
relation-classification,0,SPX u is our adaptation of the shortest path LSTM - RNN proposed by to match our sequence - layer based model .,method,0,191,28,0,23
relation-classification,0,11 This has two LSTM - RNNs for the left and right subpaths of the shortest path .,method,0,192,29,0,18
relation-classification,0,"We first calculate the max pooling of the LSTM units for each of these two RNNs , and then concatenate the outputs of the pooling for the relation candidate .",method,0,193,30,0,30
relation-classification,0,The comparison with these sequence - based LSTM - RNNs indicates that a tree - structured LSTM - RNN is comparable to sequence - based ones in representing shortest paths .,method,0,194,31,0,31
relation-classification,0,"Overall , the performance comparison of the LSTM - RNN structures in show that for end - to - end relation extraction , selecting the appropriate tree structure representation of the input ( i.e. , the shortest path ) is more important than the choice of the LSTM - RNN structure on that input ( i.e. , sequential versus tree - based ) .",method,0,195,32,0,64
relation-classification,0,relation classification analysis results,result,0,196,1,0,4
relation-classification,0,"To thoroughly analyze the relation classification part alone , e.g. , comparing different LSTM structures , architecture components such as hidden layers and input information , and classification task settings , we use the SemEval - 2010 Task 8 .",result,0,197,2,0,40
relation-classification,0,"This dataset , often used to evaluate NN models for relation classification , annotates only relation - related nominals ( unlike ACE datasets ) , so we can focus cleanly on the relation classification part .",result,0,198,3,0,36
relation-classification,0,settings,result,0,199,4,0,1
relation-classification,0,Macro - F1 No External Knowledge Resources Our Model ( SPTree ) 0.844 dos 0.841 0.840 + Word,result,0,200,5,0,18
relation-classification,0,Net Our Model ( SPTree + WordNet ) 0.855 0.856 0.837 We first report official test set results in Table 4 .,result,0,201,6,0,22
relation-classification,0,"Our novel LSTM - RNN model is comparable to both the state - of - the - art CNN - based models on this task with or without external sources , i.e. , WordNet , unlike the previous best LSTM - RNN model .",result,0,202,7,0,44
relation-classification,0,"Next , we compare different LSTM - RNN structures in .",result,0,203,8,0,11
relation-classification,0,"As for the three input dependency structures ( SPTree , SubTree , FullTree ) , Full",result,0,204,9,0,16
relation-classification,0,"Tree performs significantly worse than other structures regardless of whether or not we distinguish the nodes in the shortest paths from the other nodes , which hints that the information outside of the shortest path significantly hurts the performance ( p < 0.05 ) .",result,0,205,10,0,45
relation-classification,0,We also compare our treestructured LSTM - RNN ( SPTree ) with sequencebased LSTM - RNNs ( SPSeq and SPXu ) and treestructured LSTM - RNNs ( Child - Sum ) .,result,0,206,11,0,32
relation-classification,0,"All these LSTM - RNNs perform slightly worse than our SP - 12 When incorporating WordNet information into our model , we prepared embeddings for WordNet hypernyms extracted by SuperSenseTagger and concatenated the embeddings to the input vector ( the concatenation of word and POS embeddings ) of the sequence LSTM .",result,0,207,12,0,52
relation-classification,0,We tuned the dimension of the WordNet embeddings and set it to 15 using the development dataset .,result,0,208,13,0,18
relation-classification,0,"0.848 produces different results on FullTree as compared to the results on ACE05 in , the trend still holds that selecting the appropriate tree structure representation of the input is more important than the choice of the LSTM - RNN structure on that input .",result,0,209,14,0,45
relation-classification,0,"Finally , summarizes the contribution of several model components and training settings on SemEval relation classification .",result,0,210,15,0,17
relation-classification,0,"We first remove the hidden layer by directly connecting the LSTM - RNN layers to the softmax layers , and found that this slightly degraded performance , but the difference was small .",result,0,211,16,0,33
relation-classification,0,We then skip the sequence layer and directly use the word and POS embeddings for the dependency layer .,result,0,212,17,0,19
relation-classification,0,"Removing the sequence layer 13 or entity - related information from the sequence layer ( ? Pair ) slightly degraded performance , and , on removing both , the performance dropped significantly ( p < 0.05 ) .",result,0,213,18,0,38
relation-classification,0,This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task .,result,0,214,19,0,25
relation-classification,0,"When we replace the Stanford neural dependency parser with the Stanford lexicalized PCFG parser ( Stanford PCFG ) , the performance slightly dropped , but the difference was small .",result,0,215,20,0,30
relation-classification,0,This indicates that the selection of parsing models is not critical .,result,0,216,21,0,12
relation-classification,0,"We also included WordNet , and this slightly improved the performance ( + WordNet ) , but the difference was small .",result,0,217,22,0,22
relation-classification,0,"Lastly , for the generation of relation candidates , generating only leftto - right candidates slightly degraded the perfor- mance , but the difference was small and hence the creation of right - to - left candidates was not critical .",result,0,218,23,0,41
relation-classification,0,"Treating the inverse relation candidate as a negative instance ( Negative sampling ) also performed comparably to other generation methods in our model , which showed a significance improvement over generating only left - to - right candidates ) .",result,0,219,24,0,40
relation-classification,0,conclusion,result,0,220,25,0,1
relation-classification,0,We presented a novel end - to - end relation extraction model that represents both word sequence and dependency tree structures by using bidirectional sequential and bidirectional tree - structured LSTM - RNNs .,result,0,221,26,0,34
relation-classification,0,"This allowed us to represent both entities and relations in a single model , achieving gains over the state - of - the - art , feature - based system on end - to - end relation extraction ( ACE04 and ACE05 ) , and showing favorably comparable performance to recent state - of - the - art CNNbased models on nominal relation classification ( Sem Eval - 2010 Task 8 ) .",result,0,222,27,1,73
relation-classification,0,Our evaluation and ablation led to three key findings .,result,0,223,28,0,10
relation-classification,0,"First , the use of both word sequence and dependency tree structures is effective .",result,0,224,29,0,15
relation-classification,0,"Second , training with the shared parameters improves relation extraction accuracy , especially when employed with entity pretraining , scheduled sampling , and label embeddings .",result,0,225,30,0,26
relation-classification,0,"Finally , the shortest path , which has been widely used in relation classification , is also appropriate for representing tree structures in neural LSTM models .",result,0,226,31,0,27
named-entity-recognition,8,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,title,1,2,1,0,10
named-entity-recognition,8,abstract,abstract,0,3,1,0,1
named-entity-recognition,8,"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",abstract,1,4,2,0,19
named-entity-recognition,8,"Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .",abstract,0,5,3,1,44
named-entity-recognition,8,"As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .",abstract,0,6,4,0,48
named-entity-recognition,8,BERT is conceptually simple and empirically powerful .,abstract,0,7,5,0,8
named-entity-recognition,8,"It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .",abstract,0,8,6,0,79
named-entity-recognition,8,Jeremy Howard and Sebastian Ruder . 2018 .,abstract,0,9,7,0,8
named-entity-recognition,8,Universal language model fine - tuning for text classification .,abstract,0,10,8,0,10
named-entity-recognition,8,in acl .,abstract,0,11,9,0,3
named-entity-recognition,8,association for computational linguistics .,abstract,0,12,10,0,5
named-entity-recognition,8,introduction,introduction,0,13,1,0,1
named-entity-recognition,8,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,introduction,1,14,2,0,17
named-entity-recognition,8,"These include sentence - level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level .",introduction,0,15,3,0,58
named-entity-recognition,8,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,introduction,0,16,4,0,22
named-entity-recognition,8,"The feature - based approach , such as ELMo , uses task - specific architectures that include the pre-trained representations as additional features .",introduction,0,17,5,0,24
named-entity-recognition,8,"The fine - tuning approach , such as the Generative Pre-trained Transformer ( OpenAI GPT ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",introduction,0,18,6,0,40
named-entity-recognition,8,"The two approaches share the same objective function during pre-training , where they use unidirectional language models to learn general language representations .",introduction,0,19,7,0,23
named-entity-recognition,8,"We argue that current techniques restrict the power of the pre-trained representations , especially for the fine - tuning approaches .",introduction,0,20,8,0,21
named-entity-recognition,8,"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre-training .",introduction,0,21,9,0,25
named-entity-recognition,8,"For example , in Open AI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer .",introduction,0,22,10,0,36
named-entity-recognition,8,"Such restrictions are sub-optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",introduction,0,23,11,0,41
named-entity-recognition,8,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",introduction,1,24,12,0,22
named-entity-recognition,8,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .",introduction,1,25,13,0,27
named-entity-recognition,8,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .",introduction,1,26,14,0,34
named-entity-recognition,8,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",introduction,1,27,15,0,33
named-entity-recognition,8,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .",introduction,1,28,16,0,26
named-entity-recognition,8,The contributions of our paper are as follows :,introduction,0,29,17,0,9
named-entity-recognition,8,We demonstrate the importance of bidirectional pre-training for language representations .,introduction,0,30,18,0,11
named-entity-recognition,8,"Unlike , which uses unidirectional language models for pre-training , BERT uses masked language models to enable pretrained deep bidirectional representations .",introduction,0,31,19,0,22
named-entity-recognition,8,"This is also in contrast to , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs .",introduction,0,32,20,0,28
named-entity-recognition,8,We show that pre-trained representations reduce the need for many heavily - engineered taskspecific architectures .,introduction,0,33,21,0,16
named-entity-recognition,8,"BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .",introduction,0,34,22,0,39
named-entity-recognition,8,BERT advances the state of the art for eleven NLP tasks .,introduction,0,35,23,0,12
named-entity-recognition,8,The code and pre-trained models are available at https://github.com/,introduction,0,36,24,0,9
named-entity-recognition,8,google-research/bert .,introduction,0,37,25,0,2
named-entity-recognition,8,related work,related work,0,38,1,0,2
named-entity-recognition,8,"There is along history of pre-training general language representations , and we briefly review the most widely - used approaches in this section .",related work,0,39,2,0,24
named-entity-recognition,8,unsupervised feature - based approaches,related work,0,40,3,0,5
named-entity-recognition,8,"Learning widely applicable representations of words has been an active are a of research for decades , including non-neural and neural methods .",related work,0,41,4,0,23
named-entity-recognition,8,"Pre-trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch .",related work,0,42,5,0,21
named-entity-recognition,8,"To pretrain word embedding vectors , left - to - right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context .",related work,0,43,6,0,34
named-entity-recognition,8,"These approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .",related work,0,44,7,0,17
named-entity-recognition,8,"To train sentence representations , prior work has used objectives to rank candidate next sentences , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .",related work,0,45,8,0,40
named-entity-recognition,8,ELMo and its predecessor generalize traditional word embedding research along a different dimension .,related work,0,46,9,0,14
named-entity-recognition,8,They extract context - sensitive features from a left - to - right and a right - to - left language model .,related work,0,47,10,0,23
named-entity-recognition,8,The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations .,related work,0,48,11,0,24
named-entity-recognition,8,"When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks including question answering , sentiment analysis , and named entity recognition .",related work,0,49,12,0,36
named-entity-recognition,8,proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs .,related work,0,50,13,0,21
named-entity-recognition,8,"Similar to ELMo , their model is feature - based and not deeply bidirectional .",related work,0,51,14,0,15
named-entity-recognition,8,shows that the cloze task can be used to improve the robustness of text generation models .,related work,0,52,15,0,17
named-entity-recognition,8,unsupervised fine- tuning approaches,related work,0,53,16,0,4
named-entity-recognition,8,"As with the feature - based approaches , the first works in this direction only pre-trained word embedding parameters from unlabeled text .",related work,0,54,17,0,23
named-entity-recognition,8,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned for a supervised downstream task .",related work,0,55,18,0,28
named-entity-recognition,8,The advantage of these approaches is that few parameters need to be learned from scratch .,related work,0,56,19,0,16
named-entity-recognition,8,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",related work,0,57,20,0,28
named-entity-recognition,8,Left - to - right language model - BERT BERT E E 1 E ...,related work,0,58,21,0,15
named-entity-recognition,8,CT 1 T ... E 1 E ...,related work,0,59,22,0,8
named-entity-recognition,8,ct 1 t ... :,related work,0,60,23,0,5
named-entity-recognition,8,Overall pre-training and fine - tuning procedures for BERT .,related work,0,61,24,0,10
named-entity-recognition,8,"Apart from output layers , the same architectures are used in both pre-training and fine - tuning .",related work,0,62,25,0,18
named-entity-recognition,8,The same pre-trained model parameters are used to initialize models for different down - stream tasks .,related work,0,63,26,0,17
named-entity-recognition,8,"During fine - tuning , all parameters are fine - tuned .",related work,0,64,27,0,12
named-entity-recognition,8,"[ CLS ] is a special symbol added in front of every input example , and [ SEP ] is a special separator token ( e.g. separating questions / answers ) .",related work,0,65,28,0,32
named-entity-recognition,8,ing and auto - encoder objectives have been used for pre-training such models .,related work,0,66,29,0,14
named-entity-recognition,8,Transfer Learning from Supervised Data,related work,0,67,30,0,5
named-entity-recognition,8,"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation .",related work,0,68,31,0,24
named-entity-recognition,8,"Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models , where an effective recipe is to fine - tune models pre-trained with I ma - geNet .",related work,0,69,32,0,33
named-entity-recognition,8,bert,related work,0,70,33,0,1
named-entity-recognition,8,We introduce BERT and its detailed implementation in this section .,related work,0,71,34,0,11
named-entity-recognition,8,There are two steps in our framework : pre-training and fine - tuning .,related work,0,72,35,0,14
named-entity-recognition,8,"During pre-training , the model is trained on unlabeled data over different pre-training tasks .",related work,0,73,36,0,15
named-entity-recognition,8,"For finetuning , the BERT model is first initialized with the pre-trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",related work,0,74,37,0,31
named-entity-recognition,8,"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre-trained parameters .",related work,0,75,38,0,21
named-entity-recognition,8,The question - answering example in will serve as a running example for this section .,related work,0,76,39,0,16
named-entity-recognition,8,A distinctive feature of BERT is its unified architecture across different tasks .,related work,0,77,40,0,13
named-entity-recognition,8,There is mini-mal difference between the pre-trained architecture and the final downstream architecture .,related work,0,78,41,0,14
named-entity-recognition,8,model architecture,related work,0,79,42,0,2
named-entity-recognition,8,BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation and released in the tensor2 tensor library .,related work,0,80,43,0,25
named-entity-recognition,8,1,related work,0,81,44,0,1
named-entity-recognition,8,"Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to as well as excellent guides such as "" The Annotated Transformer . """,related work,0,82,45,0,46
named-entity-recognition,8,2,related work,0,83,46,0,1
named-entity-recognition,8,"In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A .",related work,0,84,47,0,36
named-entity-recognition,8,"We primarily report results on two model sizes : BERT BASE ( L=12 , H = 768 , A = 12 , Total Param-eters=110M ) and BERT LARGE ( L=24 , H = 1024 , A = 16 , Total Parameters=340M ) .",related work,0,85,48,0,43
named-entity-recognition,8,BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .,related work,0,86,49,0,17
named-entity-recognition,8,"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",related work,0,87,50,0,34
named-entity-recognition,8,4,related work,0,88,51,0,1
named-entity-recognition,8,input / output representations,related work,0,89,52,0,4
named-entity-recognition,8,"To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .",related work,0,90,53,0,41
named-entity-recognition,8,"Throughout this work , a "" sentence "" can bean arbitrary span of contiguous text , rather than an actual linguistic sentence .",related work,0,91,54,0,23
named-entity-recognition,8,"A "" sequence "" refers to the input token sequence to BERT , which maybe a single sentence or two sentences packed together .",related work,0,92,55,0,24
named-entity-recognition,8,we use wordpiece embeddings,related work,0,93,56,0,4
named-entity-recognition,8,"( Wu et al. , 2016 ) with a 30,000 token vocabulary .",related work,0,94,57,1,13
named-entity-recognition,8,The first token of every sequence is always a special classification token ( [ CLS ] ) .,related work,0,95,58,0,18
named-entity-recognition,8,The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks .,related work,0,96,59,0,19
named-entity-recognition,8,Sentence pairs are packed together into a single sequence .,related work,0,97,60,0,10
named-entity-recognition,8,We differentiate the sentences in two ways .,related work,0,98,61,0,8
named-entity-recognition,8,"First , we separate them with a special token ( [ SEP ] ) .",related work,0,99,62,0,15
named-entity-recognition,8,"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B .",related work,0,100,63,0,21
named-entity-recognition,8,"As shown in , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ? R H , and the final hidden vector for the i th input token as",related work,0,101,64,0,40
named-entity-recognition,8,"For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",related work,0,102,65,0,22
named-entity-recognition,8,"A visualization of this construction can be seen in . ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre-train BERT .",related work,0,103,66,1,36
named-entity-recognition,8,"Instead , we pre-train BERT using two unsupervised tasks , described in this section .",related work,0,104,67,0,15
named-entity-recognition,8,This step is presented in the left part of .,related work,0,105,68,0,10
named-entity-recognition,8,task # 1 : masked lm,related work,0,106,69,0,6
named-entity-recognition,8,"Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",related work,0,107,70,0,43
named-entity-recognition,8,"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly "" see itself "" , and the model could trivially predict the target word in a multi - layered context .",related work,0,108,71,0,52
named-entity-recognition,8,"former is often referred to as a "" Transformer encoder "" while the left - context - only version is referred to as a "" Transformer decoder "" since it can be used for text generation .",related work,0,109,72,0,37
named-entity-recognition,8,"In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens .",related work,0,110,73,0,28
named-entity-recognition,8,"We refer to this procedure as a "" masked LM "" ( MLM ) , although it is often referred to as a Cloze task in the literature .",related work,0,111,74,0,29
named-entity-recognition,8,"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",related work,0,112,75,0,29
named-entity-recognition,8,"In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .",related work,0,113,76,0,20
named-entity-recognition,8,"In contrast to denoising auto - encoders , we only predict the masked words rather than reconstructing the entire input .",related work,0,114,77,0,21
named-entity-recognition,8,"Although this allows us to obtain a bidirectional pre-trained model , a downside is that we are creating a mismatch between pre-training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",related work,0,115,78,0,41
named-entity-recognition,8,"To mitigate this , we do not always replace "" masked "" words with the actual [ MASK ] token .",related work,0,116,79,0,21
named-entity-recognition,8,The training data generator chooses 15 % of the token positions at random for prediction .,related work,0,117,80,0,16
named-entity-recognition,8,"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",related work,0,118,81,0,56
named-entity-recognition,8,"then ,",related work,0,119,82,0,2
named-entity-recognition,8,Ti will be used to predict the original token with cross entropy loss .,related work,0,120,83,0,14
named-entity-recognition,8,We compare variations of this procedure in Appendix C.2 .,related work,0,121,84,0,10
named-entity-recognition,8,"Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .",related work,0,122,85,0,47
named-entity-recognition,8,"In order to train a model that understands sentence relationships , we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus .",related work,0,123,86,0,30
named-entity-recognition,8,"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",related work,0,124,87,0,54
named-entity-recognition,8,"As we show in , C is used for next sentence prediction ( NSP ) .",related work,0,125,88,0,16
named-entity-recognition,8,"5 Despite its simplicity , we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI .",related work,0,126,89,0,24
named-entity-recognition,8,6,related work,0,127,90,0,1
named-entity-recognition,8,5 The final model achieves 97 % - 98 % accuracy on NSP .,related work,0,128,91,0,14
named-entity-recognition,8,"The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",related work,0,129,92,0,21
named-entity-recognition,8,he likes play ##ing my dog is cute Input,related work,0,130,93,0,9
named-entity-recognition,8,position,related work,0,131,94,0,1
named-entity-recognition,8,embeddings : bert input representation .,related work,0,132,95,0,6
named-entity-recognition,8,"The input embeddings are the sum of the token embeddings , the segmentation embeddings and the position embeddings .",related work,0,133,96,0,19
named-entity-recognition,8,The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee ( 2018 ) .,related work,0,134,97,1,22
named-entity-recognition,8,"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",related work,0,135,98,0,30
named-entity-recognition,8,pre-training data,related work,0,136,99,0,2
named-entity-recognition,8,The pre-training procedure largely follows the existing literature on language model pre-training .,related work,0,137,100,0,13
named-entity-recognition,8,"For the pre-training corpus we use the Books Corpus ( 800M words ) and English Wikipedia ( 2,500 M words ) .",related work,0,138,101,0,22
named-entity-recognition,8,"For Wikipedia we extract only the text passages and ignore lists , tables , and headers .",related work,0,139,102,0,17
named-entity-recognition,8,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,related work,0,140,103,0,32
named-entity-recognition,8,fine- tuning bert,related work,0,141,104,0,3
named-entity-recognition,8,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,related work,0,142,105,0,36
named-entity-recognition,8,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",related work,0,143,106,1,34
named-entity-recognition,8,"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",related work,0,144,107,0,33
named-entity-recognition,8,"For each task , we simply plugin the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .",related work,0,145,108,0,25
named-entity-recognition,8,"At the input , sentence A and sentence B from pre-training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text -?",related work,0,146,109,0,51
named-entity-recognition,8,pair in text classification or sequence tagging .,related work,0,147,110,0,8
named-entity-recognition,8,"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .",related work,0,148,111,0,47
named-entity-recognition,8,"Compared to pre-training , fine - tuning is relatively inexpensive .",related work,0,149,112,0,11
named-entity-recognition,8,"All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre-trained model .",related work,0,150,113,0,37
named-entity-recognition,8,We describe the task - specific details in the corresponding subsections of Section 4 .,related work,0,151,114,0,15
named-entity-recognition,8,More details can be found in Appendix A.5 .,related work,0,152,115,0,9
named-entity-recognition,8,experiments,experiment,0,153,1,0,1
named-entity-recognition,8,"In this section , we present BERT fine - tuning results on 11 NLP tasks .",experiment,0,154,2,0,16
named-entity-recognition,8,glue,experiment,1,155,3,0,1
named-entity-recognition,8,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",experiment,1,156,4,1,27
named-entity-recognition,8,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,experiment,0,157,5,0,11
named-entity-recognition,8,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ?",experiment,0,158,6,0,34
named-entity-recognition,8,R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .,experiment,0,159,7,0,18
named-entity-recognition,8,The only new parameters introduced during fine - tuning are classification layer weights W ?,experiment,0,160,8,0,15
named-entity-recognition,8,"R KH , where K is the number of labels .",experiment,0,161,9,0,11
named-entity-recognition,8,"We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) . :",experiment,0,162,10,0,23
named-entity-recognition,8,glue,experiment,0,163,11,0,1
named-entity-recognition,8,"Test results , scored by the evaluation server ( https://gluebenchmark.com/leaderboard ) .",experiment,0,164,12,0,12
named-entity-recognition,8,The number below each task denotes the number of training examples .,experiment,0,165,13,0,12
named-entity-recognition,8,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .",experiment,0,166,14,0,22
named-entity-recognition,8,"8 BERT and OpenAI GPT are singlemodel , single task .",experiment,0,167,15,0,11
named-entity-recognition,8,"F1 scores are reported for QQP and MRPC , Spearman correlations are reported for STS - B , and accuracy scores are reported for the other tasks .",experiment,0,168,16,0,28
named-entity-recognition,8,We exclude entries that use BERT as one of their components .,experiment,0,169,17,0,12
named-entity-recognition,8,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,experiment,1,170,18,0,22
named-entity-recognition,8,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",experiment,1,171,19,0,41
named-entity-recognition,8,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",experiment,1,172,20,0,32
named-entity-recognition,8,"With random restarts , we use the same pre-trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization .",experiment,0,173,21,0,23
named-entity-recognition,8,results are presented in .,result,0,174,1,0,5
named-entity-recognition,8,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",result,1,175,2,0,35
named-entity-recognition,8,Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .,result,0,176,3,0,21
named-entity-recognition,8,"For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .",result,0,177,4,0,21
named-entity-recognition,8,"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",result,0,178,5,0,30
named-entity-recognition,8,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",result,1,179,6,0,21
named-entity-recognition,8,The effect of model size is explored more thoroughly in Section 5.2 .,result,0,180,7,0,13
named-entity-recognition,8,squad v 1.1,result,1,181,8,0,3
named-entity-recognition,8,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,result,1,182,9,0,21
named-entity-recognition,8,"Given a question and a passage from The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",result,0,183,10,0,36
named-entity-recognition,8,10 https://gluebenchmark.com/leaderboard,result,0,184,11,0,2
named-entity-recognition,8,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",result,0,185,12,0,18
named-entity-recognition,8,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",result,0,186,13,0,38
named-entity-recognition,8,We only introduce a start vector S ?,result,0,187,14,0,8
named-entity-recognition,8,R H and an end vector E ?,result,0,188,15,0,8
named-entity-recognition,8,R H during fine - tuning .,result,0,189,16,0,7
named-entity-recognition,8,The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph : P i = e ST i j e ST j .,result,0,190,17,0,46
named-entity-recognition,8,The analogous formula is used for the end of the answer span .,result,0,191,18,0,13
named-entity-recognition,8,"The score of a candidate span from position i to position j is defined as ST i + ET j , and the maximum scoring span where j ?",result,0,192,19,0,29
named-entity-recognition,8,i is used as a prediction .,result,0,193,20,0,7
named-entity-recognition,8,The training objective is the sum of the log-likelihoods of the correct start and end positions .,result,0,194,21,0,17
named-entity-recognition,8,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,result,1,195,22,0,23
named-entity-recognition,8,shows top leaderboard entries as well as results from top published systems .,result,0,196,23,0,13
named-entity-recognition,8,"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",result,0,197,24,0,34
named-entity-recognition,8,"We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al. , 2017 ) befor fine - tuning on SQuAD .",result,0,198,25,1,30
named-entity-recognition,8,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,result,1,199,26,0,24
named-entity-recognition,8,"In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .",result,0,200,27,0,18
named-entity-recognition,8,"Without Trivia QA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .",result,0,201,28,0,26
named-entity-recognition,8,12,result,0,202,29,0,1
named-entity-recognition,8,squad v 2.0,result,1,203,30,0,3
named-entity-recognition,8,"The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .",result,0,204,31,0,31
named-entity-recognition,8,We use a simple approach to extend the SQuAD v1.1 BERT model for this task .,result,0,205,32,0,16
named-entity-recognition,8,We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .,result,0,206,33,0,25
named-entity-recognition,8,The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token .,result,0,207,34,0,24
named-entity-recognition,8,"For prediction , we compare the score of the no -answer span : s null = SC + EC to the score of the best non - null span The Trivia QA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",result,0,208,35,0,62
named-entity-recognition,8,We predict a non-null answer when ?,result,0,209,36,0,7
named-entity-recognition,8,"i , j > s null + ? , where the threshold ?",result,0,210,37,0,13
named-entity-recognition,8,is selected on the dev set to maximize F 1 .,result,0,211,38,0,11
named-entity-recognition,8,We did not use Trivia QA data for this model .,result,0,212,39,0,11
named-entity-recognition,8,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,result,1,213,40,0,23
named-entity-recognition,8,"The results compared to prior leaderboard entries and top published work are shown in , excluding systems that use BERT as one of their components .",result,0,214,41,0,26
named-entity-recognition,8,We observe a + 5.1 F1 improvement over the previous best system .,result,1,215,42,0,13
named-entity-recognition,8,swag,result,1,216,43,0,1
named-entity-recognition,8,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,result,1,217,44,0,23
named-entity-recognition,8,"Given a sentence , the task is to choose the most plausible continuation among four choices .",result,0,218,45,0,17
named-entity-recognition,8,"When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .",result,0,219,46,0,36
named-entity-recognition,8,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,result,0,220,47,0,35
named-entity-recognition,8,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,result,1,221,48,0,25
named-entity-recognition,8,results are presented in .,result,0,222,1,0,5
named-entity-recognition,8,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,result,1,223,2,0,22
named-entity-recognition,8,ablation studies,result,0,224,3,0,2
named-entity-recognition,8,"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .",result,0,225,4,0,24
named-entity-recognition,8,Additional : Ablation over the pre-training tasks using the BERT BASE architecture .,result,0,226,5,0,13
named-entity-recognition,8,""" No NSP "" is trained without the next sentence prediction task .",result,0,227,6,0,13
named-entity-recognition,8,""" LTR & No NSP "" is trained as a left - to - right LM without the next sentence prediction , like OpenAI GPT .",result,0,228,7,0,26
named-entity-recognition,8,""" + BiLSTM "" adds a randomly initialized BiLSTM on top of the "" LTR + No NSP "" model during fine - tuning .",result,0,229,8,0,25
named-entity-recognition,8,ablation studies can be found in Appendix C.,result,0,230,9,0,8
named-entity-recognition,8,Effect of Pre-training Tasks,result,0,231,10,0,4
named-entity-recognition,8,"We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE :",result,0,232,11,0,33
named-entity-recognition,8,no nsp :,result,0,233,12,0,3
named-entity-recognition,8,"A bidirectional model which is trained using the "" masked LM "" ( MLM ) but without the "" next sentence prediction "" ( NSP ) task .",result,0,234,13,0,28
named-entity-recognition,8,ltr & no nsp :,result,0,235,14,0,5
named-entity-recognition,8,"A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .",result,0,236,15,0,28
named-entity-recognition,8,"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre-train / fine - tune mismatch that degraded downstream performance .",result,0,237,16,0,29
named-entity-recognition,8,"Additionally , this model was pre-trained without the NSP task .",result,0,238,17,0,11
named-entity-recognition,8,"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",result,0,239,18,0,26
named-entity-recognition,8,We first examine the impact brought by the NSP task .,result,0,240,19,0,11
named-entity-recognition,8,"In , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQu AD 1.1 .",result,0,241,20,0,20
named-entity-recognition,8,"Next , we evaluate the impact of training bidirectional representations by comparing "" No NSP "" to "" LTR & No NSP "" .",result,0,242,21,0,24
named-entity-recognition,8,"The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .",result,0,243,22,0,21
named-entity-recognition,8,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",result,0,244,23,0,29
named-entity-recognition,8,"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",result,0,245,24,0,23
named-entity-recognition,8,"This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .",result,0,246,25,0,23
named-entity-recognition,8,The BiLSTM hurts performance on the GLUE tasks .,result,0,247,26,0,9
named-entity-recognition,8,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",result,0,248,27,0,31
named-entity-recognition,8,"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non-intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",result,0,249,28,0,71
named-entity-recognition,8,effect of model size,result,1,250,29,0,4
named-entity-recognition,8,"In this section , we explore the effect of model size on fine - tuning task accuracy .",result,0,251,30,0,18
named-entity-recognition,8,"We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .",result,0,252,31,0,34
named-entity-recognition,8,Results on selected GLUE tasks are shown in .,result,0,253,32,0,9
named-entity-recognition,8,"In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .",result,0,254,33,0,20
named-entity-recognition,8,"We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre-training tasks .",result,0,255,34,0,37
named-entity-recognition,8,It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature .,result,0,256,35,0,29
named-entity-recognition,8,"For example , the largest Transformer explored in is ( L=6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H = 512 , A=2 ) with 235M parameters .",result,0,257,36,0,51
named-entity-recognition,8,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .",result,0,258,37,0,15
named-entity-recognition,8,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",result,0,259,38,0,44
named-entity-recognition,8,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",result,1,260,39,0,39
named-entity-recognition,8,"presented mixed results on the downstream task impact of increasing the pre-trained bi - LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",result,0,261,40,0,47
named-entity-recognition,8,"Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre-trained representations even when downstream task data is very small .",result,0,262,41,0,60
named-entity-recognition,8,Feature - based Approach with BERT,result,1,263,42,0,6
named-entity-recognition,8,"All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre-trained model , and all parameters are jointly fine - tuned on a downstream task .",result,0,264,43,0,41
named-entity-recognition,8,"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",result,0,265,44,0,22
named-entity-recognition,8,"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",result,0,266,45,0,28
named-entity-recognition,8,"Second , there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation .",result,0,267,46,0,31
named-entity-recognition,8,"In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task .",result,0,268,47,0,25
named-entity-recognition,8,"In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .",result,0,269,48,0,27
named-entity-recognition,8,"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",result,0,270,49,0,22
named-entity-recognition,8,We use the representation of the first sub-token as the input to the token - level classifier over the NER label set .,result,0,271,50,0,23
named-entity-recognition,8,"To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .",result,0,272,51,0,33
named-entity-recognition,8,These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .,result,0,273,52,0,23
named-entity-recognition,8,results are presented in .,result,0,274,1,0,5
named-entity-recognition,8,BERT LARGE performs competitively with state - of - the - art methods .,result,1,275,2,0,14
named-entity-recognition,8,"The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .",result,0,276,3,0,32
named-entity-recognition,8,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,result,1,277,4,0,15
named-entity-recognition,8,conclusion,result,0,278,5,0,1
named-entity-recognition,8,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre-training is an integral part of many language understanding systems .",result,0,279,6,0,27
named-entity-recognition,8,"In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures .",result,0,280,7,0,18
named-entity-recognition,8,"Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre-trained model to successfully tackle a broad set of NLP tasks .",result,0,281,8,0,28
named-entity-recognition,8,We organize the appendix into three sections :,result,0,282,9,0,8
named-entity-recognition,8,Additional implementation details for BERT are presented in Appendix A ;,result,0,283,10,0,11
named-entity-recognition,8,Additional details for our experiments are presented in Appendix B ; and,result,0,284,11,0,12
named-entity-recognition,8,Additional ablation studies are presented in Appendix C.,result,0,285,12,0,8
named-entity-recognition,8,We present additional ablation studies for BERT including :,result,0,286,13,0,9
named-entity-recognition,8,- Effect of Number of Training Steps ; and - Ablation for Different Masking Procedures .,result,0,287,14,0,16
named-entity-recognition,8,A Additional Details for BERT,result,0,288,15,0,5
named-entity-recognition,8,A.1 Illustration of the Pre-training Tasks,result,0,289,16,0,6
named-entity-recognition,8,We provide examples of the pre-training tasks in the following .,result,0,290,17,0,11
named-entity-recognition,8,"Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by The purpose of this is to bias the representation towards the actual observed word .",result,0,291,18,0,59
named-entity-recognition,8,"The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predictor which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .",result,0,292,19,0,44
named-entity-recognition,8,"Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability .",result,0,293,20,0,36
named-entity-recognition,8,"In Section C.2 , we evaluate the impact this procedure .",result,0,294,21,0,11
named-entity-recognition,8,"Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre-training steps maybe required for the model to converge .",result,0,295,22,0,36
named-entity-recognition,8,"In Section C.1 we demonstrate that MLM does converge marginally slower than a leftto - right model ( which predicts every token ) , but the empirical improvements of the MLM model far outweigh the increased training cost .",result,0,296,23,0,39
named-entity-recognition,8,next sentence prediction,result,0,297,24,0,3
named-entity-recognition,8,The next sentence prediction task can be illustrated in the following examples .,result,0,298,25,0,13
named-entity-recognition,8,"To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as "" sentences "" even though they are typically much longer than single sentences ( but can be shorter also ) .",result,0,299,26,0,43
named-entity-recognition,8,The first sentence receives the A embedding and the second receives the B embedding .,result,0,300,27,0,15
named-entity-recognition,8,"50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the "" next sentence prediction "" task .",result,0,301,28,0,38
named-entity-recognition,8,They are sampled such that the combined length is ? 512 tokens .,result,0,302,29,0,13
named-entity-recognition,8,"The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .",result,0,303,30,0,27
named-entity-recognition,8,"We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .",result,0,304,31,0,36
named-entity-recognition,8,"We use Adam with learning rate of 1 e - 4 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",result,0,305,32,0,45
named-entity-recognition,8,We use a dropout probability of 0.1 on all layers .,result,0,306,33,0,11
named-entity-recognition,8,"We use a gelu activation rather than the standard relu , following OpenAI GPT .",result,0,307,34,0,15
named-entity-recognition,8,The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .,result,0,308,35,0,20
named-entity-recognition,8,Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .,result,0,309,36,0,20
named-entity-recognition,8,13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .,result,0,310,37,0,18
named-entity-recognition,8,Each pretraining took 4 days to complete .,result,0,311,38,0,8
named-entity-recognition,8,Longer sequences are disproportionately expensive because attention is quadratic to the sequence length .,result,0,312,39,0,14
named-entity-recognition,8,"To speedup pretraing in our experiments , we pre-train the model with sequence length of 128 for 90 % of the steps .",result,0,313,40,0,23
named-entity-recognition,8,"Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .",result,0,314,41,0,21
named-entity-recognition,8,A.3 Fine- tuning Procedure,result,0,315,42,0,4
named-entity-recognition,8,"For fine - tuning , most model hyperparameters are the same as in pre-training , with the exception of the batch size , learning rate , and number of training epochs .",result,0,316,43,0,32
named-entity-recognition,8,The dropout probability was always kept at 0.1 .,result,0,317,44,0,9
named-entity-recognition,8,"The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks :",result,0,318,45,0,25
named-entity-recognition,8,"batch size : 16 , 32",result,0,319,46,0,6
named-entity-recognition,8,"Learning rate ( Adam ) : 5 e - 5 , 3 e - 5 , 2 e - 5 Number of epochs : 2 , 3 , 4",result,0,320,47,0,29
named-entity-recognition,8,"We also observed that large data sets ( e.g. , 100 k + labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .",result,0,321,48,0,29
named-entity-recognition,8,"Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set .",result,0,322,49,0,34
named-entity-recognition,8,"A.4 Comparison of BERT , ELMo , and",result,0,323,50,0,8
named-entity-recognition,8,open ai gpt,result,0,324,51,0,3
named-entity-recognition,8,"Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .",result,0,325,52,0,19
named-entity-recognition,8,The comparisons between the model architectures are shown visually in .,result,0,326,53,0,11
named-entity-recognition,8,"Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .",result,0,327,54,0,26
named-entity-recognition,8,"The most comparable existing pre-training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .",result,0,328,55,0,28
named-entity-recognition,8,"In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .",result,0,329,56,0,32
named-entity-recognition,8,"The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained :",result,0,330,57,0,46
named-entity-recognition,8,"GPT is trained on the Books Corpus ( 800M words ) ; BERT is trained on the Books Corpus ( 800M words ) and Wikipedia ( 2,500 M words ) .",result,0,331,58,0,31
named-entity-recognition,8,"GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words .",result,0,332,59,0,30
named-entity-recognition,8,GPT used the same learning rate of 5 e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .,result,0,333,60,0,38
named-entity-recognition,8,"To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable .",result,0,334,61,0,38
named-entity-recognition,8,A.5 Illustrations of Fine - tuning on Different Tasks,result,0,335,62,0,9
named-entity-recognition,8,The illustration of fine - tuning BERT on different tasks can be seen in .,result,0,336,63,0,15
named-entity-recognition,8,"Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .",result,0,337,64,0,29
named-entity-recognition,8,"Among the tasks , ( a ) and MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task .",result,0,338,65,0,26
named-entity-recognition,8,"Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one .",result,0,339,66,0,30
named-entity-recognition,8,qqp,result,0,340,67,0,1
named-entity-recognition,8,quora question,result,0,341,68,0,2
named-entity-recognition,8,Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent .,result,0,342,69,0,22
named-entity-recognition,8,bert e [ cls ],result,0,343,70,0,5
named-entity-recognition,8,e 1 e ...,result,0,344,71,0,4
named-entity-recognition,8,...,result,0,345,72,0,1
named-entity-recognition,8,sst - 2,result,0,346,73,0,3
named-entity-recognition,8,The Stanford Sentiment Treebank is a binary single - sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment .,result,0,347,74,0,26
named-entity-recognition,8,cola,result,0,348,75,0,1
named-entity-recognition,8,"The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically "" acceptable "" or not .",result,0,349,76,0,32
named-entity-recognition,8,sts - b,result,0,350,77,0,3
named-entity-recognition,8,the semantic textual similarity,result,0,351,78,0,4
named-entity-recognition,8,Benchmark is a collection of sentence pairs drawn from news headlines and other sources .,result,0,352,79,0,15
named-entity-recognition,8,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,result,0,353,80,0,23
named-entity-recognition,8,mrpc,result,0,354,81,0,1
named-entity-recognition,8,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",result,0,355,82,0,29
named-entity-recognition,8,rte,result,0,356,83,0,1
named-entity-recognition,8,"Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ) .",result,0,357,84,0,20
named-entity-recognition,8,14 WNLI Winograd NLI is a small natural language inference dataset .,result,0,358,85,0,12
named-entity-recognition,8,"The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .",result,0,359,86,0,40
named-entity-recognition,8,We therefore exclude this set to be fair to OpenAI GPT .,result,0,360,87,0,12
named-entity-recognition,8,"For our GLUE submission , we always predicted the ma-jority class .",result,0,361,88,0,12
named-entity-recognition,8,c additional ablation studies,result,0,362,89,0,4
named-entity-recognition,8,C.1 Effect of Number of Training Steps presents MNLI,result,0,363,90,0,9
named-entity-recognition,8,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,result,0,364,91,0,14
named-entity-recognition,8,This allows us to answer the following questions :,result,0,365,92,0,9
named-entity-recognition,8,1 .,result,0,366,93,0,2
named-entity-recognition,8,question :,result,0,367,94,0,2
named-entity-recognition,8,"Does BERT really need such a large amount of pre-training ( 128,000 words / batch * 1,000,000 steps ) to achieve high fine - tuning accuracy ?",result,0,368,95,0,27
named-entity-recognition,8,"Answer : Yes , BERT BASE achieves almost 1.0 % additional accuracy on MNLI when trained on 1 M steps compared to 500 k steps .",result,0,369,96,0,26
named-entity-recognition,8,2 . question :,result,0,370,97,0,4
named-entity-recognition,8,"Does MLM pre-training converge slower than LTR pre-training , since only 15 % of words are predicted in each batch rather than every word ?",result,0,371,98,0,25
named-entity-recognition,8,Answer : The MLM model does converge slightly slower than the LTR model .,result,0,372,99,0,14
named-entity-recognition,8,"However , in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately .",result,0,373,100,0,19
named-entity-recognition,8,C.2 Ablation for Different Masking Procedures,result,0,374,101,0,6
named-entity-recognition,8,"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model ( MLM ) objective .",result,0,375,102,0,29
named-entity-recognition,8,The following is an ablation study to evaluate the effect of different masking strategies .,result,0,376,103,0,15
named-entity-recognition,8,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",result,0,377,104,0,35
named-entity-recognition,8,We report the Dev results for both MNLI and NER .,result,0,378,105,0,11
named-entity-recognition,8,"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",result,0,379,106,0,42
named-entity-recognition,8,The results are presented in .,result,0,380,107,0,6
named-entity-recognition,8,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",result,0,381,108,0,45
named-entity-recognition,8,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",result,0,382,109,0,33
named-entity-recognition,8,The right part of the paper represents the Dev set results .,result,0,383,110,0,12
named-entity-recognition,8,"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .",result,0,384,111,0,31
named-entity-recognition,8,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,result,0,385,112,0,19
named-entity-recognition,8,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",result,0,386,113,0,20
named-entity-recognition,8,"Interestingly , using only the RND strategy performs much worse than our strategy as well .",result,0,387,114,0,16
named-entity-recognition,9,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,1,2,1,0,16
named-entity-recognition,9,abstract,abstract,0,3,1,0,1
named-entity-recognition,9,motivation :,abstract,0,4,2,0,2
named-entity-recognition,9,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,0,5,3,0,16
named-entity-recognition,9,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,1,6,4,0,37
named-entity-recognition,9,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,1,7,5,0,30
named-entity-recognition,9,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,1,8,6,0,21
named-entity-recognition,9,results :,result,0,9,1,0,2
named-entity-recognition,9,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",result,0,10,2,0,32
named-entity-recognition,9,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",result,0,11,3,0,36
named-entity-recognition,9,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",result,0,12,4,0,65
named-entity-recognition,9,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,result,0,13,5,0,18
named-entity-recognition,9,availability and implementation :,result,0,14,6,0,4
named-entity-recognition,9,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",result,1,15,7,0,21
named-entity-recognition,9,introduction,introduction,0,16,1,0,1
named-entity-recognition,9,The volume of biomedical literature continues to rapidly increase .,introduction,0,17,2,0,10
named-entity-recognition,9,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",introduction,0,18,3,0,29
named-entity-recognition,9,PubMed alone has a total of 29M articles as of January 2019 .,introduction,0,19,4,0,13
named-entity-recognition,9,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,introduction,0,20,5,0,21
named-entity-recognition,9,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",introduction,0,21,6,0,20
named-entity-recognition,9,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,introduction,0,22,7,0,26
named-entity-recognition,9,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",introduction,0,23,8,0,36
named-entity-recognition,9,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,introduction,0,24,9,0,27
named-entity-recognition,9,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",method,0,25,1,0,20
named-entity-recognition,9,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",method,0,26,2,0,44
named-entity-recognition,9,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",method,1,27,3,0,26
named-entity-recognition,9,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",method,0,28,4,0,19
named-entity-recognition,9,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",method,0,29,5,0,37
named-entity-recognition,9,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions thatare usually not included in a general domain corpus .",method,0,30,6,0,39
named-entity-recognition,9,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",method,0,31,7,0,32
named-entity-recognition,9,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",method,0,32,8,0,34
named-entity-recognition,9,approach,method,0,33,9,0,1
named-entity-recognition,9,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",method,1,34,10,0,20
named-entity-recognition,9,The over all process of pre-training and fine - tuning BioBERT is illustrated in .,method,0,35,11,0,15
named-entity-recognition,9,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",method,1,36,12,0,25
named-entity-recognition,9,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",method,1,37,13,0,20
named-entity-recognition,9,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",method,1,38,14,0,27
named-entity-recognition,9,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",method,0,39,15,0,28
named-entity-recognition,9,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,method,0,40,16,0,20
named-entity-recognition,9,The contributions of our paper are as follows :,method,0,41,17,0,9
named-entity-recognition,9,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,method,0,42,18,0,23
named-entity-recognition,9,We show that pre-training BERT on biomedical corpora largely improves its performance .,method,0,43,19,0,13
named-entity-recognition,9,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",method,0,44,20,0,41
named-entity-recognition,9,"Compared with most previous biomedical text mining models thatare mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",method,0,45,21,0,47
named-entity-recognition,9,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",method,0,46,22,0,23
named-entity-recognition,9,materials and methods,method,0,47,1,0,3
named-entity-recognition,9,BioBERT basically has the same structure as BERT .,method,0,48,2,0,9
named-entity-recognition,9,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",method,0,49,3,0,24
named-entity-recognition,9,BERT : bidirectional encoder representations from transformers,method,0,50,4,0,7
named-entity-recognition,9,Learning word representations from a large amount of unannotated text is a long - established method .,method,0,51,5,0,17
named-entity-recognition,9,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",method,0,52,6,0,29
named-entity-recognition,9,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",method,0,53,7,0,22
named-entity-recognition,9,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,method,0,54,8,0,21
named-entity-recognition,9,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",method,0,55,9,0,41
named-entity-recognition,9,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",method,0,56,10,0,25
named-entity-recognition,9,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",method,0,57,11,0,26
named-entity-recognition,9,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",method,0,58,12,0,27
named-entity-recognition,9,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,method,0,59,13,0,26
named-entity-recognition,9,"Due to the space limitations , we refer readers to for a more detailed description of BERT .",method,0,60,14,0,18
named-entity-recognition,9,pre-training biobert,method,0,61,15,0,2
named-entity-recognition,9,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",method,0,62,1,0,18
named-entity-recognition,9,"However , biomedical domain texts contain a considerable number of domain - specific .",method,0,63,2,0,14
named-entity-recognition,9,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",method,0,64,3,0,37
named-entity-recognition,9,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",method,0,65,4,0,22
named-entity-recognition,9,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",method,0,66,5,0,24
named-entity-recognition,9,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",method,0,67,6,0,23
named-entity-recognition,9,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",method,0,68,7,0,25
named-entity-recognition,9,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,method,0,69,8,0,22
named-entity-recognition,9,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",method,0,70,9,0,18
named-entity-recognition,9,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",method,0,71,10,0,17
named-entity-recognition,9,I ##mm ##uno ##g ##lo # #bul # #in ) .,method,0,72,11,0,11
named-entity-recognition,9,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,method,0,73,12,0,19
named-entity-recognition,9,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",method,0,74,13,0,88
named-entity-recognition,9,fine-tuning biobert,method,0,75,14,0,2
named-entity-recognition,9,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",method,0,76,15,0,16
named-entity-recognition,9,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",method,0,77,16,0,21
named-entity-recognition,9,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",method,0,78,17,0,28
named-entity-recognition,9,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",method,0,79,18,0,24
named-entity-recognition,9,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,method,0,80,19,0,22
named-entity-recognition,9,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",method,0,81,20,0,31
named-entity-recognition,9,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",method,0,82,21,0,18
named-entity-recognition,9,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,method,0,83,22,0,16
named-entity-recognition,9,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",method,0,84,23,0,25
named-entity-recognition,9,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,method,0,85,24,0,20
named-entity-recognition,9,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,method,0,86,25,0,20
named-entity-recognition,9,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,method,0,87,26,0,39
named-entity-recognition,9,"The precision , recall and F 1 scores on the RE task are reported .",method,0,88,27,0,15
named-entity-recognition,9,Question answering is a task of answering questions posed in natural language given related passages .,method,0,89,28,0,16
named-entity-recognition,9,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",method,0,90,29,0,18
named-entity-recognition,9,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,method,0,91,30,0,16
named-entity-recognition,9,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,method,0,92,31,0,20
named-entity-recognition,9,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",method,0,93,32,0,32
named-entity-recognition,9,"Like , we excluded the samples with unanswerable questions from the training sets .",method,0,94,33,0,14
named-entity-recognition,9,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",method,0,95,34,0,26
named-entity-recognition,9,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",method,0,96,35,0,19
named-entity-recognition,9,results,result,0,97,1,0,1
named-entity-recognition,9,datasets,result,0,98,2,0,1
named-entity-recognition,9,The statistics of biomedical NER datasets are listed in .,result,0,99,3,0,10
named-entity-recognition,9,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",result,0,100,4,0,28
named-entity-recognition,9,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,result,0,101,5,0,24
named-entity-recognition,9,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,result,0,102,6,0,22
named-entity-recognition,9,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,result,0,103,7,0,20
named-entity-recognition,9,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",result,0,104,8,0,25
named-entity-recognition,9,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets thatare frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",result,0,105,9,0,39
named-entity-recognition,9,The RE datasets contain gene - disease relations and protein - chemical relations ) .,result,0,106,10,0,15
named-entity-recognition,9,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,result,0,107,11,0,14
named-entity-recognition,9,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",result,0,108,12,0,14
named-entity-recognition,9,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",result,0,109,13,0,21
named-entity-recognition,9,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,result,0,110,14,0,18
named-entity-recognition,9,We have made the pre-processed BioASQ datasets publicly available .,result,0,111,15,0,10
named-entity-recognition,9,"For all the datasets , we used the same dataset splits used in previous works ) for a fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",result,0,112,16,0,40
named-entity-recognition,9,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",result,0,113,17,0,31
named-entity-recognition,9,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,result,0,114,18,0,19
named-entity-recognition,9,Note that the state - of - the - art models each have a different architecture and training procedure .,result,0,115,19,0,20
named-entity-recognition,9,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",result,0,116,20,0,69
named-entity-recognition,9,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",result,0,117,21,0,27
named-entity-recognition,9,experimental setups,experiment,0,118,1,0,2
named-entity-recognition,9,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,experiment,1,119,2,0,18
named-entity-recognition,9,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,experiment,1,120,3,0,21
named-entity-recognition,9,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",experiment,0,121,4,0,26
named-entity-recognition,9,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",experiment,0,122,5,0,42
named-entity-recognition,9,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",experiment,0,123,6,0,33
named-entity-recognition,9,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,experiment,0,124,7,0,27
named-entity-recognition,9,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",experiment,0,125,8,0,29
named-entity-recognition,9,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,experiment,1,126,9,0,13
named-entity-recognition,9,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",experiment,1,127,10,0,25
named-entity-recognition,9,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,experiment,0,128,11,0,29
named-entity-recognition,9,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",experiment,0,129,12,0,23
named-entity-recognition,9,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,experiment,1,130,13,0,20
named-entity-recognition,9,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,experiment,0,131,14,0,15
named-entity-recognition,9,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",experiment,0,132,15,0,30
named-entity-recognition,9,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,experiment,0,133,16,0,33
named-entity-recognition,9,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",experiment,0,134,17,0,23
named-entity-recognition,9,experimental results,experiment,0,135,1,0,2
named-entity-recognition,9,The results of NER are shown in .,experiment,1,136,2,0,8
named-entity-recognition,9,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",experiment,0,137,3,0,48
named-entity-recognition,9,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",experiment,1,138,4,0,16
named-entity-recognition,9,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",experiment,1,139,5,0,45
named-entity-recognition,9,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",experiment,0,140,6,0,56
named-entity-recognition,9,The RE results of each model are shown in .,experiment,1,141,7,0,10
named-entity-recognition,9,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",experiment,0,142,8,0,26
named-entity-recognition,9,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",experiment,1,143,9,0,31
named-entity-recognition,9,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",experiment,1,144,10,0,17
named-entity-recognition,9,The QA results are shown in .,experiment,1,145,11,0,7
named-entity-recognition,9,We micro averaged the best scores of the state - of - the - art models from each batch .,experiment,0,146,12,0,20
named-entity-recognition,9,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,experiment,0,147,13,0,23
named-entity-recognition,9,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",experiment,1,148,14,0,55
named-entity-recognition,9,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",experiment,1,149,15,0,23
named-entity-recognition,9,discussion,experiment,0,150,16,0,1
named-entity-recognition,9,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,experiment,0,151,17,0,16
named-entity-recognition,9,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",experiment,0,152,18,0,25
named-entity-recognition,9,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",experiment,0,153,19,0,34
named-entity-recognition,9,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",experiment,0,154,20,0,22
named-entity-recognition,9,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,experiment,0,155,21,0,34
named-entity-recognition,9,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,experiment,0,156,22,0,26
named-entity-recognition,9,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,experiment,0,157,23,0,19
named-entity-recognition,9,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",experiment,0,158,24,0,22
named-entity-recognition,9,"F1 scores were used for NER / RE , and MRR scores were used for QA .",experiment,0,159,25,0,17
named-entity-recognition,9,BioBERT significantly improves performance on most of the datasets .,experiment,0,160,26,0,10
named-entity-recognition,9,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to see the effect of pre-training on downstream tasks .",experiment,0,161,27,0,26
named-entity-recognition,9,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,experiment,0,162,28,0,59
named-entity-recognition,9,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",experiment,0,163,29,0,30
named-entity-recognition,9,entities .,experiment,0,164,30,0,2
named-entity-recognition,9,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",experiment,0,165,31,0,19
named-entity-recognition,9,"Also , BioBERT can provide longer named entities as answers .",experiment,0,166,32,0,11
named-entity-recognition,9,conclusion,experiment,0,167,33,0,1
named-entity-recognition,9,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",experiment,0,168,34,0,20
named-entity-recognition,9,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,experiment,0,169,35,0,18
named-entity-recognition,9,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",experiment,0,170,36,0,25
named-entity-recognition,9,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",experiment,0,171,37,1,40
named-entity-recognition,9,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,experiment,0,172,38,0,53
named-entity-recognition,9,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",experiment,0,173,39,0,23
named-entity-recognition,9,"The best scores are in bold , and the second best scores are underlined .",experiment,0,174,40,0,15
named-entity-recognition,9,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",experiment,0,175,41,1,52
named-entity-recognition,9,"The best scores are in bold , and the second best scores are underlined .",experiment,0,176,42,0,15
named-entity-recognition,9,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",experiment,0,177,43,0,21
named-entity-recognition,9,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",experiment,0,178,44,0,27
named-entity-recognition,9,"The best scores are in bold , and the second best scores are underlined .",experiment,0,179,45,0,15
named-entity-recognition,9,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-are a.bioasq.org ) .,experiment,0,180,46,0,23
named-entity-recognition,9,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",experiment,0,181,47,0,19
named-entity-recognition,9,biobert,experiment,0,182,48,0,1
named-entity-recognition,9,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,experiment,0,183,49,0,15
named-entity-recognition,9,bc2gm,experiment,0,184,50,0,1
named-entity-recognition,9,bert,experiment,0,185,51,0,1
named-entity-recognition,9,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",experiment,0,186,52,0,25
named-entity-recognition,9,qa,experiment,0,187,53,0,1
named-entity-recognition,9,bioasq 6 b - factoid,experiment,0,188,54,0,5
named-entity-recognition,9,Q : Which type of urinary incontinence is diagnosed with the Q tip test ?,experiment,0,189,55,0,15
named-entity-recognition,9,bert,experiment,0,190,56,0,1
named-entity-recognition,9,A total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,experiment,0,191,57,0,17
named-entity-recognition,9,after undergoing ( . . .),experiment,0,192,58,0,6
named-entity-recognition,9,"q-tip test , . . .",experiment,0,193,59,0,6
named-entity-recognition,9,Q : Which bacteria causes erythrasma ?,experiment,0,194,60,0,7
named-entity-recognition,9,bert,experiment,0,195,61,0,1
named-entity-recognition,9,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,experiment,0,196,62,0,15
named-entity-recognition,9,note :,experiment,0,197,63,0,2
named-entity-recognition,9,Predicted named entities for NER and predicted answers for QA are in bold .,experiment,0,198,64,0,14
named-entity-recognition,9,funding,experiment,0,199,65,0,1
named-entity-recognition,1,Neural Architectures for Named Entity Recognition,title,1,2,1,0,6
named-entity-recognition,1,abstract,abstract,0,3,1,0,1
named-entity-recognition,1,"State - of - the - art named entity recognition systems rely heavily on hand - crafted features and domain - specific knowledge in order to learn effectively from the small , supervised training corpora thatare available .",abstract,0,4,2,0,38
named-entity-recognition,1,"In this paper , we introduce two new neural architectures - one based on bidirectional LSTMs and conditional random fields , and the other that constructs and labels segments using a transition - based approach inspired by shift - reduce parsers .",abstract,0,5,3,0,42
named-entity-recognition,1,Our models rely on two sources of information about words : character - based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora .,abstract,0,6,4,0,30
named-entity-recognition,1,Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,abstract,1,7,5,0,30
named-entity-recognition,1,1,abstract,0,8,6,0,1
named-entity-recognition,1,introduction,introduction,0,9,1,0,1
named-entity-recognition,1,Named entity recognition ( NER ) is a challenging learning problem .,introduction,0,10,2,0,12
named-entity-recognition,1,"One the one hand , in most languages and domains , there is only a very small amount of supervised training data available .",introduction,0,11,3,0,24
named-entity-recognition,1,"On the other , there are few constraints on the kinds of words that can be names , so generalizing from this small sample of data is difficult .",introduction,0,12,4,0,29
named-entity-recognition,1,"As a result , carefully constructed orthographic features and language - specific knowledge resources , such as gazetteers , are widely used for solving this task .",introduction,0,13,5,0,27
named-entity-recognition,1,"Unfortunately , languagespecific resources and features are costly to develop in new languages and new domains , making NER a challenge to adapt .",introduction,0,14,6,0,24
named-entity-recognition,1,Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision .,introduction,0,15,7,0,19
named-entity-recognition,1,"However , even systems that have relied extensively on unsupervised features have used these to augment , rather than replace , hand - engineered features ( e.g. , knowledge about capitalization patterns and character classes in a particular language ) and specialized knowledge resources ( e.g. , gazetteers ) .",introduction,0,16,8,0,50
named-entity-recognition,1,"In this paper , we present neural architectures for NER that use no language - specific resources or features beyond a small amount of supervised training data and unlabeled corpora .",introduction,0,17,9,0,31
named-entity-recognition,1,Our models are designed to capture two intuitions .,introduction,0,18,10,0,9
named-entity-recognition,1,"First , since names often consist of multiple tokens , reasoning jointly over tagging decisions for each token is important .",introduction,0,19,11,0,21
named-entity-recognition,1,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",introduction,1,20,12,0,66
named-entity-recognition,1,"Second , token - level evidence for "" being a name "" includes both orthographic evidence ( what does the word being tagged as a name look like ? ) and distributional evidence ( where does the word being tagged tend to occur in a corpus ? ) .",introduction,0,21,13,0,49
named-entity-recognition,1,"To capture orthographic sensitivity , we use character - based word representation model to capture distributional sensitivity , we combine these representations with distributional representations .",introduction,1,22,14,0,26
named-entity-recognition,1,"Our word representations combine both of these , and dropout training is used to encourage the model to learn to trust both sources of evidence ( 4 ) .",introduction,0,23,15,0,29
named-entity-recognition,1,"Experiments in English , Dutch , German , and Spanish show that we are able to obtain state - of - the - art NER performance with the LSTM - CRF model in Dutch , German , and Spanish , and very near the state - of - the - art in English without any hand - engineered features or gazetteers ( 5 ) .",introduction,0,24,16,0,65
named-entity-recognition,1,"The transition - based algorithm likewise surpasses the best previously published results in several languages , although it performs less well than the LSTM - CRF model .",introduction,0,25,17,0,28
named-entity-recognition,1,lstm - crf,introduction,0,26,18,0,3
named-entity-recognition,1,model,introduction,0,27,19,0,1
named-entity-recognition,1,"We provide a brief description of LSTMs and CRFs , and present a hybrid tagging architecture .",introduction,0,28,20,0,17
named-entity-recognition,1,This architecture is similar to the ones presented by .,introduction,0,29,21,0,10
named-entity-recognition,1,lstm,introduction,0,30,22,0,1
named-entity-recognition,1,Recurrent neural networks ( RNNs ) are a family of neural networks that operate on sequential data .,introduction,0,31,23,0,18
named-entity-recognition,1,"They take as input a sequence of vectors ( x 1 , x 2 , . . . , x n ) and return another sequence ( h 1 , h 2 , . . . , h n ) that represents some information about the sequence at every step in the input .",introduction,0,32,24,0,54
named-entity-recognition,1,"Although RNNs can , in theory , learn long dependencies , in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence .",introduction,0,33,25,0,32
named-entity-recognition,1,Long Short - term Memory Networks ( LSTMs ) have been designed to combat this issue by incorporating a memory - cell and have been shown to capture long - range dependencies .,introduction,0,34,26,0,33
named-entity-recognition,1,"They do so using several gates that control the proportion of the input to give to the memory cell , and the proportion from the previous state to forget .",introduction,0,35,27,0,30
named-entity-recognition,1,We use the following implementation :,introduction,0,36,28,0,6
named-entity-recognition,1,where ?,introduction,0,37,29,0,2
named-entity-recognition,1,"is the element - wise sigmoid function , and is the element - wise product .",introduction,0,38,30,0,16
named-entity-recognition,1,"For a given sentence ( x 1 , x 2 , . . . , x n ) containing n words , each represented as a d-dimensional vector , an LSTM computes a representation ? ?",introduction,0,39,31,0,36
named-entity-recognition,1,ht of the left context of the sentence at every word t.,introduction,0,40,32,0,12
named-entity-recognition,1,"Naturally , generating a representation of the right context ? ?",introduction,0,41,33,0,11
named-entity-recognition,1,ht as well should add useful information .,introduction,0,42,34,0,8
named-entity-recognition,1,This can be achieved using a second LSTM that reads the same sequence in reverse .,introduction,0,43,35,0,16
named-entity-recognition,1,We will refer to the former as the forward LSTM and the latter as the backward LSTM .,introduction,0,44,36,0,18
named-entity-recognition,1,These are two distinct networks with different parameters .,introduction,0,45,37,0,9
named-entity-recognition,1,This forward and backward LSTM pair is referred to as a bidirectional LSTM .,introduction,0,46,38,0,14
named-entity-recognition,1,"The representation of a word using this model is obtained by concatenating its left and right context representations ,",introduction,0,47,39,0,19
named-entity-recognition,1,"These representations effectively include a representation of a word in context , which is useful for numerous tagging applications .",introduction,0,48,40,0,20
named-entity-recognition,1,crf,introduction,0,49,41,0,1
named-entity-recognition,1,tagging models,introduction,0,50,42,0,2
named-entity-recognition,1,Avery simple - but surprisingly effective - tagging model is to use the ht 's as features to make independent tagging decisions for each output y t.,introduction,0,51,43,0,27
named-entity-recognition,1,"Despite this model 's success in simple problems like POS tagging , its independent classification decisions are limiting when there are strong dependencies across output labels .",introduction,0,52,44,0,27
named-entity-recognition,1,"NER is one such task , since the "" grammar "" that characterizes interpretable sequences of tags imposes several hard constraints ( e.g. , I - PER can not follow B - LOC ; see 2.4 for details ) that would be impossible to model with independence assumptions .",introduction,0,53,45,0,49
named-entity-recognition,1,"Therefore , instead of modeling tagging decisions independently , we model them jointly using a conditional random field .",introduction,0,54,46,0,19
named-entity-recognition,1,for an input sentence,introduction,0,55,47,0,4
named-entity-recognition,1,we consider P to be the matrix of scores output by the bidirectional LSTM network .,introduction,0,56,48,0,16
named-entity-recognition,1,"P is of size n k , where k is the number of distinct tags , and P i , j corresponds to the score of the j th tag of the i th word in a sentence .",introduction,0,57,49,0,39
named-entity-recognition,1,"For a sequence of predictions y = ( y 1 , y 2 , . . . , y n ) , we define it s score to be",introduction,0,58,50,0,29
named-entity-recognition,1,"where A is a matrix of transition scores such that A i , j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence , that we add to the set of possible tags .",introduction,0,59,51,0,52
named-entity-recognition,1,A is therefore a square matrix of size k + 2 .,introduction,0,60,52,0,12
named-entity-recognition,1,A softmax over all possible tag sequences yields a probability for the sequence y:,introduction,0,61,53,0,14
named-entity-recognition,1,"y ?Y Xe s ( X , y) .",introduction,0,62,54,0,9
named-entity-recognition,1,"During training , we maximize the log-probability of the correct tag sequence :",introduction,0,63,55,0,13
named-entity-recognition,1,where Y X represents all possible tag sequences ( even those that do not verify the IOB format ) for a sentence X .,introduction,0,64,56,0,24
named-entity-recognition,1,"From the formulation above , it is evident that we encourage our network to produce a valid sequence of output labels .",introduction,0,65,57,0,22
named-entity-recognition,1,"While decoding , we predict the output sequence that obtains the maximum score given by :",introduction,0,66,58,0,16
named-entity-recognition,1,"Since we are only modeling bigram interactions between outputs , both the summation in Eq. 1 and the maximum a posteriori sequence y * in Eq.",introduction,0,67,59,0,26
named-entity-recognition,1,2 can be computed using dynamic programming .,introduction,0,68,60,0,8
named-entity-recognition,1,parameterization and training,introduction,0,69,61,0,3
named-entity-recognition,1,"The scores associated with each tagging decision for each token ( i.e. , the P i , y 's ) are defined to be the dot product between the embedding of a wordin - context computed with a bidirectional LSTMexactly the same as the POS tagging model of and these are combined with bigram compatibility scores ( i.e. , the A y , y 's ) .",introduction,0,70,62,0,67
named-entity-recognition,1,This architecture is shown in figure,introduction,0,71,63,0,6
named-entity-recognition,1,"1 . Circles represent observed variables , diamonds are deterministic functions of their parents , and double circles are random variables .",introduction,0,72,64,0,22
named-entity-recognition,1,"The parameters of this model are thus the matrix of bigram compatibility scores A , and the parameters that give rise to the matrix P , namely the parameters of the bidirectional LSTM , the linear feature weights , and the word embeddings .",introduction,0,73,65,0,44
named-entity-recognition,1,"As in part 2.2 , let x i denote the sequence of word embeddings for every word in a sentence , and y i be their associated tags .",introduction,0,74,66,0,29
named-entity-recognition,1,We return to a discussion of how the embeddings xi are modeled in Section 4 .,introduction,0,75,67,0,16
named-entity-recognition,1,"The sequence of word embeddings is given as input to a bidirectional LSTM , which returns a representation of the left and right context for each word as explained in 2.1 .",introduction,0,76,68,0,32
named-entity-recognition,1,These representations are concatenated ( c i ) and linearly projected onto a layer whose size is equal to the number of distinct tags .,introduction,0,77,69,0,25
named-entity-recognition,1,"Instead of using the softmax output from this layer , we use a CRF as previously described to take into account neighboring tags , yielding the final predictions for every wordy i .",introduction,0,78,70,0,33
named-entity-recognition,1,"Additionally , we observed that adding a hidden layer between c i and the CRF layer marginally improved our results .",introduction,0,79,71,0,21
named-entity-recognition,1,All results reported with this model incorporate this extra-layer .,introduction,0,80,72,0,10
named-entity-recognition,1,"The parameters are trained to maximize Eq. 1 of observed sequences of NER tags in an annotated corpus , given the observed words .",introduction,0,81,73,0,24
named-entity-recognition,1,tagging schemes,introduction,0,82,74,0,2
named-entity-recognition,1,The task of named entity recognition is to assign a named entity label to every word in a sentence .,introduction,0,83,75,0,20
named-entity-recognition,1,A single named entity could span several tokens within a sentence .,introduction,0,84,76,0,12
named-entity-recognition,1,"Sentences are usually represented in the IOB format ( Inside , Outside , Beginning ) where every token is labeled as B- label if the token is the beginning of a named entity , I-label if it is inside a named entity but not the first token within the named entity , or O otherwise .",introduction,0,85,77,0,56
named-entity-recognition,1,"However , we decided to use the IOBES tagging scheme , a variant of IOB commonly used for named entity recognition , which encodes information about singleton entities ( S ) and explicitly marks the end of named entities ( E ) .",introduction,0,86,78,0,43
named-entity-recognition,1,"Using this scheme , tagging a word as I-label with high - confidence narrows down the choices for the subsequent word to I-label or E-label , however , the IOB scheme is only capable of determining that the subsequent word can not be the interior of another label .",introduction,0,87,79,0,49
named-entity-recognition,1,Ratinov and Roth and showed that using a more expressive tagging scheme like IOBES improves model performance marginally .,introduction,0,88,80,0,19
named-entity-recognition,1,"However , we did not observe a significant improvement over the IOB tagging scheme .",introduction,0,89,81,0,15
named-entity-recognition,1,transition - based chunking model,introduction,0,90,82,0,5
named-entity-recognition,1,"As an alternative to the LSTM - CRF discussed in the previous section , we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition - based dependency parsing .",introduction,0,91,83,0,38
named-entity-recognition,1,"This model directly constructs representations of the multi-token names ( e.g. , the name Mark Watney is composed into a single representation ) .",introduction,0,92,84,0,24
named-entity-recognition,1,This model relies on a stack data structure to incrementally construct chunks of the input .,introduction,0,93,85,0,16
named-entity-recognition,1,"To obtain representations of this stack used for predicting subsequent actions , we use the Stack - LSTM presented by , in which the LSTM is augmented with a "" stack pointer . """,introduction,0,94,86,0,34
named-entity-recognition,1,"While sequential LSTMs model sequences from left to right , stack LSTMs permit embedding of a stack of objects thatare both added to ( using a push operation ) and removed from ( using a pop operation ) .",introduction,0,95,87,0,39
named-entity-recognition,1,"This allows the Stack - LSTM to work like a stack that maintains a "" summary embedding "" of its contents .",introduction,0,96,88,0,22
named-entity-recognition,1,We refer to this model as Stack - LSTM or S - LSTM model for simplicity .,introduction,0,97,89,0,17
named-entity-recognition,1,"Finally , we refer interested readers to the original paper for details about the Stack - LSTM model since in this paper we merely use the same architecture through a new transition - based algorithm presented in the following Section .",introduction,0,98,90,0,41
named-entity-recognition,1,chunking algorithm,introduction,0,99,91,0,2
named-entity-recognition,1,"We designed a transition inventory which is given in that is inspired by transition - based parsers , in particular the arc-standard parser of .",introduction,0,100,92,0,25
named-entity-recognition,1,"In this algorithm , we make use of two stacks ( designated output and stack representing , respectively , completed chunks and scratch space ) and a buffer that contains the words that have yet to be processed .",introduction,0,101,93,0,39
named-entity-recognition,1,The transition inventory contains the following transitions :,introduction,0,102,94,0,8
named-entity-recognition,1,"The SHIFT transition moves a word from the buffer to the stack , the OUT transition moves a word from the buffer directly into the output stack while the REDUCE ( y ) transition pops all items from the top of the stack creating a "" chunk , "" labels this with label y , and pushes a representation of this chunk onto the output stack .",introduction,0,103,95,0,67
named-entity-recognition,1,The algorithm completes when the stack and buffer are both empty .,introduction,0,104,96,0,12
named-entity-recognition,1,"The algorithm is depicted in , which shows the sequence of operations required to process the sentence Mark Watney visited Mars .",introduction,0,105,97,0,22
named-entity-recognition,1,"The model is parameterized by defining a probability distribution over actions at each time step , given the current contents of the stack , buffer , and output , as well as the history of actions taken .",introduction,0,106,98,0,38
named-entity-recognition,1,"Following , we use stack LSTMs to compute a fixed dimensional embedding of each of these , and take a concatenation of these to obtain the full algorithm state .",introduction,0,107,99,0,30
named-entity-recognition,1,This representation is used to define a distribution over the possible actions that can betaken at each time step .,introduction,0,108,100,0,20
named-entity-recognition,1,The model is trained to maximize the conditional probability of sequences of reference actions ( extracted from a labeled training corpus ) given the input sentences .,introduction,0,109,101,0,27
named-entity-recognition,1,"To label a new input sequence at test time , the maximum probability action is chosen greedily until the algorithm reaches a termination state .",introduction,0,110,102,0,25
named-entity-recognition,1,"Although this is not guaranteed to find a global optimum , it is effective in practice .",introduction,0,111,103,0,17
named-entity-recognition,1,"Since each token is either moved directly to the output ( 1 action ) or first to the stack and then the output ( 2 actions ) , the total number of actions for a sequence of length n is maximally 2n .",introduction,0,112,104,0,43
named-entity-recognition,1,It is worth noting that the nature of this algorithm,introduction,0,113,105,0,10
named-entity-recognition,1,representing labeled chunks,introduction,0,114,106,0,3
named-entity-recognition,1,"When the REDUCE ( y ) operation is executed , the algorithm shifts a sequence of tokens ( together with their vector embeddings ) from the stack to the output buffer as a single completed chunk .",introduction,0,115,107,0,37
named-entity-recognition,1,"To compute an embedding of this sequence , we run a bidirectional LSTM over the embeddings of its constituent tokens together with a token representing the type of the chunk being identified ( i.e. , y ) .",introduction,0,116,108,0,38
named-entity-recognition,1,"This function is given as g ( u , . . . , v , r y ) , where r y is a learned embedding of a label type .",introduction,0,117,109,0,31
named-entity-recognition,1,"Thus , the output buffer contains a single vector representation for each labeled chunk that is generated , regardless of its length .",introduction,0,118,110,0,23
named-entity-recognition,1,input word embeddings,introduction,0,119,111,0,3
named-entity-recognition,1,The input layers to both of our models are vector representations of individual words .,introduction,0,120,112,0,15
named-entity-recognition,1,Learning independent representations for word types from the limited NER training data is a difficult problem : there are simply too many parameters to reliably estimate .,introduction,0,121,113,0,27
named-entity-recognition,1,"Since many languages have orthographic or morphological evidence that something is a name ( or not a name ) , we want representations thatare sensitive to the spelling of words .",introduction,0,122,114,0,31
named-entity-recognition,1,We therefore use a model that constructs representations of words from representations of the characters they are composed of ( 4.1 ) .,introduction,0,123,115,0,23
named-entity-recognition,1,"Our second intuition is that names , which may individually be quite varied , appear in regular contexts in large corpora .",introduction,0,124,116,0,22
named-entity-recognition,1,Therefore we use embed - dings learned from a large corpus thatare sensitive to word order ( 4.2 ) .,introduction,0,125,117,0,20
named-entity-recognition,1,"Finally , to prevent the models from depending on one representation or the other too strongly , we use dropout training and find this is crucial for good generalization performance ( 4.3 ) .",introduction,0,126,118,0,34
named-entity-recognition,1,Character - based models of words,introduction,0,127,119,0,6
named-entity-recognition,1,An important distinction of our work from most previous approaches is that we learn character - level features while training instead of hand - engineering prefix and suffix information about words .,introduction,0,128,120,0,32
named-entity-recognition,1,Learning character - level embeddings has the advantage of learning representations specific to the task and domain at hand .,introduction,0,129,121,0,20
named-entity-recognition,1,They have been found useful for morphologically rich languages and to handle the outof - vocabulary problem for tasks like part - of - speech tagging and language modeling or dependency parsing . describes our architecture to generate a word embedding for a word from its characters .,introduction,0,130,122,0,48
named-entity-recognition,1,A character lookup table initialized at random contains an embedding for every character .,introduction,0,131,123,0,14
named-entity-recognition,1,The character embeddings corresponding to every character in a word are given indirect and reverse order to a forward and a backward LSTM .,introduction,0,132,124,0,24
named-entity-recognition,1,The embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional LSTM .,introduction,0,133,125,0,23
named-entity-recognition,1,This character - level representation is then concatenated with a word - level representation from a word lookup - table .,introduction,0,134,126,0,21
named-entity-recognition,1,"During testing , words that do not have an embedding in the lookup table are mapped to a UNK embedding .",introduction,0,135,127,0,21
named-entity-recognition,1,"To train the UNK embedding , we replace singletons with the UNK embedding with a probability 0.5 .",introduction,0,136,128,0,18
named-entity-recognition,1,"In all our experiments , the hidden dimension of the forward and backward character LSTMs are 25 each , which results in our character - based representation of words being of dimension 50 .",introduction,0,137,129,0,34
named-entity-recognition,1,"Recurrent models like RNNs and LSTMs are capable of encoding very long sequences , however , they have a representation biased towards their most recent inputs .",introduction,0,138,130,0,27
named-entity-recognition,1,"As a result , we expect the final representation of the forward LSTM to bean accurate representation of the suffix of the word , and the final state of the backward LSTM to be a better representation of its prefix .",introduction,0,139,131,0,41
named-entity-recognition,1,Alternative approachesmost notably like convolutional networks - have been proposed to learn representations of words from their characters .,introduction,0,140,132,0,19
named-entity-recognition,1,"However , convnets are designed to discover position - invariant features of their inputs .",introduction,0,141,133,0,15
named-entity-recognition,1,"While this is appropriate for many problems , e.g. , image recognition ( a cat can appear anywhere in a picture ) , we argue that important information is position dependent ( e.g. , prefixes and suffixes encode different information than stems ) , making LSTMs an a priori better function class for modeling the relationship between words and their characters .",introduction,0,142,134,0,62
named-entity-recognition,1,pretrained embeddings,introduction,0,143,135,0,2
named-entity-recognition,1,"As in , we use pretrained word embeddings to initialize our lookup table .",introduction,0,144,136,0,14
named-entity-recognition,1,We observe significant improvements using pretrained word embeddings over randomly initialized ones .,introduction,0,145,137,0,13
named-entity-recognition,1,"Embeddings are pretrained using skip - n- gram ( Ling et al. , 2015 a ) , a variation of word2vec that accounts for word order .",introduction,0,146,138,1,27
named-entity-recognition,1,These embeddings are fine - tuned during training .,introduction,0,147,139,0,9
named-entity-recognition,1,"Word embeddings for Spanish , Dutch , German and English are trained using the Spanish Gigaword version 3 , the Leipzig corpora collection , the German monolingual training data from the 2010 Machine Translation Workshop and the English Gigaword version 4 ( with the LA Times and NY Times portions removed ) respectively .",introduction,0,148,140,0,54
named-entity-recognition,1,"We use an embedding dimension of 100 for English , 64 for other languages , a minimum word frequency cutoff of 4 , and a window size of 8 .",introduction,0,149,141,0,30
named-entity-recognition,1,dropout training,introduction,0,150,142,0,2
named-entity-recognition,1,Initial experiments showed that character - level embeddings did not improve our over all performance when used in conjunction with pretrained word representations .,introduction,0,151,143,0,24
named-entity-recognition,1,"To encourage the model to depend on both representations , we use dropout training , applying a dropout mask to the final embedding layer just before the input to the bidirectional LSTM in .",introduction,0,152,144,0,34
named-entity-recognition,1,We observe a significant improvement in our model 's performance after using dropout ( see ) .,introduction,0,153,145,0,17
named-entity-recognition,1,experiments,experiment,0,154,1,0,1
named-entity-recognition,1,"This section presents the methods we use to train our models , the results we obtained on various tasks and the impact of our networks ' configuration on model performance .",experiment,0,155,2,0,31
named-entity-recognition,1,training,experiment,0,156,3,0,1
named-entity-recognition,1,"For both models presented , we train our networks using the back - propagation algorithm updating our parameters on every training example , one at a time , using stochastic gradient descent ( SGD ) with a learning rate of 0.01 and a gradient clipping of 5.0 .",experiment,1,157,4,0,48
named-entity-recognition,1,"Several methods have been proposed to enhance the performance of SGD , such as Adadelta or Adam ( Kingma and Ba , 2014 ) .",experiment,0,158,5,1,25
named-entity-recognition,1,"Although we observe faster convergence using these methods , none of them perform as well as SGD with gradient clipping .",experiment,0,159,6,0,21
named-entity-recognition,1,Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,experiment,1,160,7,0,22
named-entity-recognition,1,Tuning this dimension did not significantly impact model performance .,experiment,0,161,8,0,10
named-entity-recognition,1,We set the dropout rate to 0.5 .,experiment,1,162,9,0,8
named-entity-recognition,1,"Using higher rates negatively impacted our results , while smaller rates led to longer training time .",experiment,0,163,10,0,17
named-entity-recognition,1,The stack - LSTM model uses two layers each of dimension 100 for each stack .,experiment,1,164,11,0,16
named-entity-recognition,1,"The embeddings of the actions used in the composition functions have 16 dimensions each , and the output embedding is of dimension 20 .",experiment,1,165,12,0,24
named-entity-recognition,1,We experimented with different dropout rates and reported the scores using the best dropout rate for each language .,experiment,0,166,13,0,19
named-entity-recognition,1,3,experiment,0,167,14,0,1
named-entity-recognition,1,"It is a greedy model that apply locally optimal actions until the entire sentence is processed , further improvements might be obtained with beam search or training with exploration .",experiment,0,168,15,0,30
named-entity-recognition,1,data sets,experiment,0,169,16,0,2
named-entity-recognition,1,We test our model on different datasets for named entity recognition .,experiment,0,170,17,0,12
named-entity-recognition,1,"To demonstrate our model 's ability to generalize to different languages , we present results on the ) that contain independent named entity labels for English , Spanish , German and Dutch .",experiment,0,171,18,0,33
named-entity-recognition,1,"All datasets contain four different types of named entities : locations , persons , organizations , and miscellaneous entities that do not belong in any of the three previous categories .",experiment,0,172,19,0,31
named-entity-recognition,1,"Although POS tags were made available for all datasets , we did not include them in our models .",experiment,0,173,20,0,19
named-entity-recognition,1,"We did not perform any dataset preprocessing , apart from replacing every digit with a zero in the English NER dataset .",experiment,0,174,21,0,22
named-entity-recognition,1,presents our comparisons with other models for named entity recognition in English .,experiment,0,175,22,0,13
named-entity-recognition,1,"To make the comparison between our model and others fair , we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases .",experiment,0,176,23,0,34
named-entity-recognition,1,Our models do not use gazetteers or any external labeled resources .,experiment,0,177,24,0,12
named-entity-recognition,1,The best score reported on this task is by .,experiment,0,178,25,0,10
named-entity-recognition,1,They obtained a F 1 of 91.2 by jointly modeling the NER and entity linking tasks .,experiment,0,179,26,0,17
named-entity-recognition,1,"Their model uses a lot of hand - engineered features including spelling features , WordNet clusters , Brown clusters , POS tags , chunks tags , as well as stemming and external knowledge bases like Freebase and Wikipedia .",experiment,0,180,27,0,39
named-entity-recognition,1,"Our LSTM - CRF model outperforms all other systems , including the ones using external labeled data like gazetteers .",experiment,1,181,28,0,20
named-entity-recognition,1,"Our Stack - LSTM model also outperforms all previous models that do not incorporate external features , apart from the one presented by .",experiment,1,182,29,0,24
named-entity-recognition,1,"and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .",experiment,1,183,30,0,20
named-entity-recognition,1,"On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .",experiment,1,184,31,0,24
named-entity-recognition,1,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .",experiment,1,185,32,0,22
named-entity-recognition,1,The Stack - LSTM also consistently presents statethe - art ( or close to ) results compared to systems that do not use external data .,experiment,1,186,33,0,26
named-entity-recognition,1,results,result,0,187,1,0,1
named-entity-recognition,1,"As we can see in the tables , the Stack - LSTM model is more dependent on character - based representations to achieve competitive performance ; we hypothesize that the LSTM - CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs ; however , the Stack - LSTM model consumes the words one by one and it just relies on the word representations when it chunks words .",result,0,188,2,0,76
named-entity-recognition,1,network architectures,result,0,189,3,0,2
named-entity-recognition,1,Our models had several components that we could tweak to understand their impact on the over all performance .,result,0,190,4,0,19
named-entity-recognition,1,"We explored the impact that the CRF , the character - level representations , pretraining of our presented a model similar to our LSTM - CRF , but using hand - crafted spelling features .",result,0,191,5,0,35
named-entity-recognition,1,also used a similar model and adapted it to the semantic role labeling task .,result,0,192,6,0,15
named-entity-recognition,1,"used a linear chain CRF with L 2 regularization , they added phrase cluster features extracted from the web data and spelling features .",result,0,193,7,0,24
named-entity-recognition,1,also used a linear chain CRF with spelling features and gazetteers .,result,0,194,8,0,12
named-entity-recognition,1,language independent,result,0,195,9,0,2
named-entity-recognition,1,NER models like ours have also been proposed in the past .,result,0,196,10,0,12
named-entity-recognition,1,present semi-supervised bootstrapping algorithms for named entity recognition by co-training character - level ( word - internal ) and token - level ( context ) features .,result,0,197,11,0,27
named-entity-recognition,1,use Bayesian nonparametrics to construct a data base of named entities in an almost unsupervised setting .,result,0,198,12,0,17
named-entity-recognition,1,Ratinov and Roth quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information .,result,0,199,13,0,25
named-entity-recognition,1,"Finally , there is currently a lot of interest in models for NER that use letter - based representations .",result,0,200,14,0,20
named-entity-recognition,1,model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character - based representations into their encoder model .,result,0,201,15,0,23
named-entity-recognition,1,"employ an architecture similar to ours , but instead use CNNs to learn character - level features , in a way similar to the work by .",result,0,202,16,0,27
named-entity-recognition,1,conclusion,result,0,203,17,0,1
named-entity-recognition,1,"This paper presents two neural architectures for sequence labeling that provide the best NER results ever reported in standard evaluation settings , even compared with models that use external resources , such as gazetteers .",result,0,204,18,0,35
named-entity-recognition,1,"A key aspect of our models are that they model output label dependencies , either via a simple CRF architecture , or using a transition - based algorithm to explicitly construct and label chunks of the input .",result,0,205,19,0,38
named-entity-recognition,1,"Word representations are also crucially important for success : we use both pre-trained word representations and "" character - based "" representations that capture morphological and orthographic information .",result,0,206,20,0,29
named-entity-recognition,1,"To prevent the learner from depending too heavily on one representation class , dropout is used .",result,0,207,21,0,17
named-entity-recognition,5,Sentence - State LSTM for Text Representation,title,1,2,1,0,7
named-entity-recognition,5,abstract,abstract,0,3,1,0,1
named-entity-recognition,5,bi-directional,abstract,0,4,2,0,1
named-entity-recognition,5,LSTMs are a powerful tool for text representation .,abstract,0,5,3,0,9
named-entity-recognition,5,"On the other hand , they have been shown to suffer various limitations due to their sequential nature .",abstract,0,6,4,0,19
named-entity-recognition,5,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .",abstract,1,7,5,0,20
named-entity-recognition,5,"Recurrent steps are used to perform local and global information exchange between words simultaneously , rather than incremental reading of a sequence of words .",abstract,0,8,6,0,25
named-entity-recognition,5,"Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power , giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers .",abstract,0,9,7,0,32
named-entity-recognition,5,introduction,introduction,0,10,1,0,1
named-entity-recognition,5,Neural models have become the dominant approach in the NLP literature .,introduction,0,11,2,0,12
named-entity-recognition,5,"Compared to handcrafted indicator features , neural sentence representations are less sparse , and more flexible in encoding intricate syntactic and semantic information .",introduction,0,12,3,0,24
named-entity-recognition,5,"Among various neural networks for encoding sentences , bi-directional LSTMs ( BiLSTM ) have been a dominant method , giving state - of - the - art results in language modelling , machine translation , syntactic parsing and question answering .",introduction,0,13,4,0,41
named-entity-recognition,5,"Despite their success , BiLSTMs have been shown to suffer several limitations .",introduction,0,14,5,0,13
named-entity-recognition,5,"For example , their inherently sequential nature endows computation non-parallel within the same sentence , which can lead to a computational bottleneck , hindering their use in the in - dustry .",introduction,0,15,6,0,32
named-entity-recognition,5,"In addition , local ngrams , which have been shown a highly useful source of contextual information for NLP , are not explicitly modelled .",introduction,0,16,7,0,25
named-entity-recognition,5,"Finally , sequential information flow leads to relatively weaker power in capturing longrange dependencies , which results in lower performance in encoding longer sentences .",introduction,0,17,8,0,25
named-entity-recognition,5,We investigate an alternative recurrent neural network structure for addressing these issues .,introduction,1,18,9,0,13
named-entity-recognition,5,"As shown in , the main idea is to model the hidden states of all words simultaneously at each recurrent step , rather than one word at a time .",introduction,1,19,10,0,30
named-entity-recognition,5,"In particular , we view the whole sentence as a single state , which consists of sub-states for individual words and an over all sentence - level state .",introduction,1,20,11,0,29
named-entity-recognition,5,"To capture local and non-local contexts , states are updated recurrently by exchanging information between each other .",introduction,1,21,12,0,18
named-entity-recognition,5,"Consequently , we refer to our model as sentence - state LSTM , or S - LSTM in short .",introduction,0,22,13,0,20
named-entity-recognition,5,"Empirically , S - LSTM can give effective sentence encoding after 3 - 6 recurrent steps .",introduction,0,23,14,0,17
named-entity-recognition,5,"In contrast , the number of recurrent steps necessary for BiLSTM scales with the size of the sentence .",introduction,0,24,15,0,19
named-entity-recognition,5,"At each recurrent step , information exchange is conducted between consecutive words in the sentence , and between the sentence - level state and each word .",introduction,1,25,16,0,27
named-entity-recognition,5,"In particular , each word receives information from its predecessor and successor simultaneously .",introduction,1,26,17,0,14
named-entity-recognition,5,"From an initial state without information exchange , each word - level state can obtain 3 - gram , 5 - gram and 7 - gram information after 1 , 2 and 3 recurrent steps , respectively .",introduction,0,27,18,0,38
named-entity-recognition,5,"Being connected with every word , the sentence - level state vector serves to exchange non-local information with each word .",introduction,0,28,19,0,21
named-entity-recognition,5,"In addition , it can also be used as a global sentence - level representation for classification tasks .",introduction,0,29,20,0,19
named-entity-recognition,5,"Results on both classification and sequence labelling show that S - LSTM gives better accuracies compared to BiLSTM using the same number of parameters , while being faster .",introduction,0,30,21,0,29
named-entity-recognition,5,"We release our code and models at https://github.com/ leuchine /S - LSTM , which include all baselines and the final model .",introduction,1,31,22,0,22
named-entity-recognition,5,related work,related work,0,32,1,0,2
named-entity-recognition,5,LSTM showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models .,related work,0,33,2,0,29
named-entity-recognition,5,"LSTM encoders have since been explored for other tasks , including syntactic parsing , text classification and machine reading .",related work,0,34,3,0,20
named-entity-recognition,5,Bidirectional extensions have become a standard configuration for achieving state - of - the - art accuracies among various tasks .,related work,0,35,4,0,21
named-entity-recognition,5,"S- LSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words , but different in the design of state transition .",related work,0,36,5,0,24
named-entity-recognition,5,"CNNs ) also allow better parallelis ation compared to LSTMs for sentence encoding , thanks to parallelism among convolution filters .",related work,0,37,6,0,21
named-entity-recognition,5,"On the other hand , convolution features embody only fix - sized local ngram information , whereas sentence - level feature aggregation via pooling can lead to loss of information .",related work,0,38,7,0,31
named-entity-recognition,5,"In contrast , S - LSTM uses a global sentence - level node to assemble and back - distribute local information in the recurrent state transition process , suffering less information loss compared to pooling .",related work,0,39,8,0,36
named-entity-recognition,5,"Attention has recently been explored as a standalone method for sentence encoding , giving competitive results compared to Bi - LSTM encoders for neural machine translation .",related work,0,40,9,0,27
named-entity-recognition,5,"The attention mechanism allows parallelis ation , and can play a similar role to the sentence - level state in S - LSTMs , which uses neural gates to integrate word - level information compared to hierarchical attention .",related work,0,41,10,0,39
named-entity-recognition,5,S - LSTM further allows local communication between neighbouring words .,related work,0,42,11,0,11
named-entity-recognition,5,Hierarchical stacking of CNN layers allows better interaction between non-local components in a sentence via incremental levels of abstraction .,related work,0,43,12,0,20
named-entity-recognition,5,"S - LSTM is similar to hierarchical attention and stacked CNN in this respect , incrementally refining sentence representations .",related work,0,44,13,0,20
named-entity-recognition,5,"However , S - LSTM models hierarchical encoding of sentence structure as a recurrent state transition process .",related work,0,45,14,0,18
named-entity-recognition,5,"In nature , our work belongs to the family of LSTM sentence representations .",related work,0,46,15,0,14
named-entity-recognition,5,S - LSTM is inspired by message passing over graphs ) .,related work,0,47,16,0,12
named-entity-recognition,5,Graph - structure neural models have been used for computer program verification and image object detection .,related work,0,48,17,0,17
named-entity-recognition,5,The closest previous work in NLP includes the use of convolutional neural networks and DAG LSTMs for modelling syntactic structures .,related work,0,49,18,0,21
named-entity-recognition,5,"Compared to our work , their motivations and network structures are highly different .",related work,0,50,19,0,14
named-entity-recognition,5,"In particular , the DAG LSTM of is a natural extension of tree LSTM , and is sequential rather than parallel in nature .",related work,0,51,20,0,24
named-entity-recognition,5,"To our knowledge , we are the first to investigate a graph RNN for encoding sentences , proposing parallel graph states for integrating word - level and sentence - level information .",related work,0,52,21,0,32
named-entity-recognition,5,"In this perspective , our contribution is similar to that of and in introducing a neural representation to the NLP literature .",related work,0,53,22,0,22
named-entity-recognition,5,model,related work,0,54,23,0,1
named-entity-recognition,5,Time 4 ) generally results in faster plateauing .,related work,0,55,24,0,9
named-entity-recognition,5,"This can be be explained by the intuition that information exchange between distant nodes can be achieved using more recurrent steps under a smaller window size , as can be achieved using fewer steps under a larger window size .",related work,0,56,25,0,40
named-entity-recognition,5,"Considering efficiency , we choose a window size of 1 for the remaining experiments , setting the number of recurrent steps to 9 according to .",related work,0,57,26,0,26
named-entity-recognition,5,"S- LSTM vs BiLSTM : As shown in , BiLSTM gives significantly better accuracies compared to uni-directional LSTM 2 , with the training time per epoch growing from 67 seconds to 106 seconds .",related work,0,58,27,0,34
named-entity-recognition,5,"Stacking 2 layers of BiLSTM gives further improvements to development results , with a larger time of 207 seconds .",related work,0,59,28,0,20
named-entity-recognition,5,3 layers of stacked BiLSTM does not further improve the results .,related work,0,60,29,0,12
named-entity-recognition,5,"In contrast , S - LSTM gives a development result of 82 . 64 % , which is significantly better compared to 2 - layer stacked BiLSTM , with a smaller number of model parameters and a shorter time of 65 seconds .",related work,0,61,30,0,43
named-entity-recognition,5,"We additionally make comparisons with stacked CNNs and hierarchical attention , shown in ( the CNN and Transformer rows ) , where N indicates the number of attention layers .",related work,0,62,31,0,30
named-entity-recognition,5,"CNN is the most efficient among all models compared , with the smallest model size .",related work,0,63,32,0,16
named-entity-recognition,5,"On the other hand , a 3 - layer stacked CNN gives an accuracy of 81 . 46 % , which is also the lowest compared with BiLSTM , hierarchical attention and S - LSTM .",related work,0,64,33,0,36
named-entity-recognition,5,The best performance of hierarchical attention is between single - layer and two - layer BiLSTMs in terms of both accuracy and efficiency .,related work,0,65,34,0,24
named-entity-recognition,5,S - LSTM gives significantly better accuracies compared with both CNN and hierarchical attention .,related work,0,66,35,0,15
named-entity-recognition,5,Influence of external attention mechanism .,related work,0,67,36,0,6
named-entity-recognition,5,additionally shows the results of BiLSTM and S - LSTM when external attention is used as described in Section 3.3 .,related work,0,68,37,0,21
named-entity-recognition,5,"Attention leads to improved accuracies for both BiLSTM and S - LSTM in classification , with S - LSTM still outperforming BiLSTM significantly .",related work,0,69,38,0,24
named-entity-recognition,5,"The result suggests that external techniques such as attention can play orthogonal roles compared with internal recurrent structures , therefore benefiting both BiLSTMs and S - LSTMs .",related work,0,70,39,0,28
named-entity-recognition,5,Similar observations are found using external CRF layers for sequence labelling .,related work,0,71,40,0,12
named-entity-recognition,5,baseline bilstm,related work,0,72,41,0,2
named-entity-recognition,5,"The baseline BiLSTM model consists of two LSTM components , which process the input in the forward left - to - right and the backward rightto - left directions , respectively .",related work,0,73,42,0,32
named-entity-recognition,5,"In each direction , the reading of input words is modelled as a recurrent process with a single hidden state .",related work,0,74,43,0,21
named-entity-recognition,5,"Given an initial value , the state changes its value recurrently , each time consuming an incoming word .",related work,0,75,44,0,19
named-entity-recognition,5,Take the forward LSTM component for example .,related work,0,76,45,0,8
named-entity-recognition,5,"Denoting the initial state as ? ? h 0 , which is a model parameter , the recurrent state transition step for calculating ? ? h 1 , . . . , ? ? h n+1 is defined as follows :",related work,0,77,46,0,41
named-entity-recognition,5,"where x t denotes the word representation of wt ; it , o t , ft and u t represent the values of an input gate , an output gate , a forget gate and an actual input at time step t , respectively , which controls the information flow for a recurrent cell ? ?",related work,0,78,47,0,56
named-entity-recognition,5,ct and the state vector,related work,0,79,48,0,5
named-entity-recognition,5,The backward LSTM component follows the same recurrent state transition process as described in Eq 1 .,related work,0,80,49,0,17
named-entity-recognition,5,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n ,",related work,0,81,50,0,23
named-entity-recognition,5,The BiLSTM model uses the concatenated value of ? ?,related work,0,82,51,0,10
named-entity-recognition,5,ht and ? ?,related work,0,83,52,0,4
named-entity-recognition,5,ht as the hidden vector for wt :,related work,0,84,53,0,8
named-entity-recognition,5,A single hidden vector representation g of the whole input sentence can be obtained using the final state values of the two LSTM components :,related work,0,85,54,0,25
named-entity-recognition,5,stacked bilstm,related work,0,86,55,0,2
named-entity-recognition,5,"Multiple layers of BiLTMs can be stacked for increased representation power , where the hidden vectors of a lower layer are used as inputs for an upper layer .",related work,0,87,56,0,29
named-entity-recognition,5,Different model parameters are used in each stacked BiLSTM layer .,related work,0,88,57,0,11
named-entity-recognition,5,sentence - state lstm,related work,0,89,58,0,4
named-entity-recognition,5,"Formally , an S - LSTM state at time step t can be denoted by :",related work,0,90,59,0,16
named-entity-recognition,5,which consists of a sub state ht i for each word w i and a sentence - level sub state gt .,related work,0,91,60,0,22
named-entity-recognition,5,"S - LSTM uses a recurrent state transition process to model information exchange between sub states , which enriches state representations incrementally .",related work,0,92,61,0,23
named-entity-recognition,5,"For the initial state H 0 , we set h 0",related work,0,93,62,0,11
named-entity-recognition,5,to ht i and from g t?1 tog t .,related work,0,94,63,0,10
named-entity-recognition,5,"We take an LSTM structure similar to the baseline BiLSTM for modelling state transition , using a recurrent cell ct i for each w i and a cell ct g for g.",related work,0,95,64,0,32
named-entity-recognition,5,"As shown in , the value of each ht i is computed based on the values of",related work,0,96,65,0,17
named-entity-recognition,5,"i + 1 and g t?1 , together with their corresponding cell values :",related work,0,97,66,0,14
named-entity-recognition,5,where ?,related work,0,98,67,0,2
named-entity-recognition,5,"ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ?",related work,0,99,68,0,35
named-entity-recognition,5,ti and xi to ct i .,related work,0,100,69,0,7
named-entity-recognition,5,"In particular , it i controls information from the input xi ; l ti , rt i , ft i and st i control information from the left context cell c t ?1 i ? 1 , the right context cell c t ?1 i + 1 , c t?1 i and the sentence context cell c t ? 1 g , respectively .",related work,0,101,70,0,64
named-entity-recognition,5,"The values of it i , l ti , rt i , ft i and st i are normalised such that they sum to 1 .",related work,0,102,71,0,26
named-entity-recognition,5,o ti is an output gate from the cell state ct i to the hidden state,related work,0,103,72,0,16
named-entity-recognition,5,The value of gt is computed based on the values,related work,0,104,73,0,10
named-entity-recognition,5,". . , ft n+1 and ft g are gates controlling information from c t ? 1 0 , . . . , c t?1 n+1 and c t ? 1 g , respectively , which are normalised .",related work,0,105,74,0,39
named-entity-recognition,5,o t is an output gate from the recurrent cell ct g tog t .,related work,0,106,75,0,15
named-entity-recognition,5,"W x , U x and bx ( x ? {g , f , o} ) are model parameters .",related work,0,107,76,0,20
named-entity-recognition,5,contrast with bilstm,related work,0,108,77,0,3
named-entity-recognition,5,The difference between S - LSTM and BiLSTM can be understood with respect to their recurrent states .,related work,0,109,78,0,18
named-entity-recognition,5,"While BiL - STM uses only one state in each direction to represent the subsequence from the beginning to a certain word , S - LSTM uses a structural state to represent the full sentence , which consists of a sentence - level sub state and n + 2 word - level sub states , simultaneously .",related work,0,110,79,0,57
named-entity-recognition,5,"Different from BiLSTMs , for which ht at different time steps are used to represent w 0 , . . . , w n + 1 , respectively , the word - level states ht i and sentence - level state gt of S - LSTMs directly correspond to the goal outputs hi and g , as introduced in the beginning of this section .",related work,0,111,80,0,65
named-entity-recognition,5,"As t increases from 0 , ht i and gt are enriched with increasingly deeper context information .",related work,0,112,81,0,18
named-entity-recognition,5,"From the perspective of information flow , BiL - STM passes information from one end of the sentence to the other .",related work,0,113,82,0,22
named-entity-recognition,5,"As a result , the number of time steps scales with the size of the input .",related work,0,114,83,0,17
named-entity-recognition,5,"In contrast , S - LSTM allows bi-directional information flow at each word simultaneously , and additionally between the sentence - level state and every wordlevel state .",related work,0,115,84,0,28
named-entity-recognition,5,"At each step , each hi captures an increasing larger ngram context , while additionally communicating globally to all other h j via g.",related work,0,116,85,0,24
named-entity-recognition,5,"The optimal number of recurrent steps is decided by the end - task performance , and does not necessarily scale with the sentence size .",related work,0,117,86,0,25
named-entity-recognition,5,"As a result , S - LSTM can potentially be both more efficient and more accurate compared with BiLSTMs .",related work,0,118,87,0,20
named-entity-recognition,5,increasing window size .,related work,0,119,88,0,4
named-entity-recognition,5,"By default S - LSTM exchanges information only between neighbouring words , which can be seen as adopting a 1 word window on each side .",related work,0,120,89,0,26
named-entity-recognition,5,"The window size can be extended to 2 , 3 or more words in order to allow more communication in a state transition , expediting information exchange .",related work,0,121,90,0,28
named-entity-recognition,5,"To this end , we modify Eq 2 , integrating additional context words to ? ti , with extended gates and cells .",related work,0,122,91,0,23
named-entity-recognition,5,"For example , with a window size of 2 ,",related work,0,123,92,0,10
named-entity-recognition,5,We study the effectiveness of window size in our experiments .,related work,0,124,93,0,11
named-entity-recognition,5,additional sentence - level nodes .,related work,0,125,94,0,6
named-entity-recognition,5,By default S - LSTM uses one sentence - level node .,related work,0,126,95,0,12
named-entity-recognition,5,"One way of enriching the parameter space is to add more sentence - level nodes , each communicating with word - level nodes in the same way as described by Eq 3 .",related work,0,127,96,0,33
named-entity-recognition,5,"In addition , different sentence - level nodes can communicate with each other during state transition .",related work,0,128,97,0,17
named-entity-recognition,5,"When one sentence - level node is used for classification outputs , the other sentencelevel node can serve as hidden memory units , or latent features .",related work,0,129,98,0,27
named-entity-recognition,5,We study the effectiveness of multiple sentence - level nodes empirically .,related work,0,130,99,0,12
named-entity-recognition,5,task settings,related work,0,131,100,0,2
named-entity-recognition,5,"We consider two task settings , namely classification and sequence labelling .",related work,0,132,101,0,12
named-entity-recognition,5,"For classification , g is fed to a softmax classification layer :",related work,0,133,102,0,12
named-entity-recognition,5,where y is the probability distribution of output class labels and W c and b care model parameters .,related work,0,134,103,0,19
named-entity-recognition,5,"For sequence labelling , each hi can be used as feature representation for a corresponding word w i .",related work,0,135,104,0,19
named-entity-recognition,5,external attention,related work,0,136,105,0,2
named-entity-recognition,5,It has been shown that summation of hidden states using attention give better accuracies compared to using the end states of BiLSTMs .,related work,0,137,106,0,23
named-entity-recognition,5,We study the influence of attention on both S - LSTM and BiLSTM for classification .,related work,0,138,107,0,16
named-entity-recognition,5,"In particular , additive attention ( Bahdanau",related work,0,139,108,0,7
named-entity-recognition,5,"Here W ? , u and b ?",related work,0,140,109,0,8
named-entity-recognition,5,are model parameters .,related work,0,141,110,0,4
named-entity-recognition,5,external crf,related work,0,142,111,0,2
named-entity-recognition,5,"For sequential labelling , we use a CRF layer on top of the hidden vectors h 1 , h 2 , . . . , h n for calculating the conditional probabilities of label sequences :",related work,0,143,112,0,36
named-entity-recognition,5,are parameters specific to two consecutive labels y i ?1 and y i .,related work,0,144,113,0,14
named-entity-recognition,5,"For training , standard log - likelihood loss is used with L 2 regularization given a set of gold - standard instances .",related work,0,145,114,0,23
named-entity-recognition,5,experiments,experiment,0,146,1,0,1
named-entity-recognition,5,We empirically compare S - LSTMs and BiLSTMs on different classification and sequence labelling tasks .,experiment,0,147,2,0,16
named-entity-recognition,5,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,experiment,1,148,3,0,15
named-entity-recognition,5,development experiments,experiment,0,149,1,0,2
named-entity-recognition,5,We use the movie review development data to investigate different configurations of S - LSTMs and BiLSTMs .,experiment,0,150,2,0,18
named-entity-recognition,5,"For S - LSTMs , the default configuration uses sand /s words for augmenting words Hyperparameters : shows the development results of various S - LSTM settings , where Time refers to training time per epoch .",experiment,0,151,3,0,37
named-entity-recognition,5,"Without the sentence - level node , the accuracy of S - LSTM drops to 81.76 % , demonstrating the necessity of global information exchange .",experiment,0,152,4,0,26
named-entity-recognition,5,"Adding one additional sentence - level node as described in Section 3.2 does not lead to accuracy improvements , although the number of parameters and decoding time increase accordingly .",experiment,0,153,5,0,30
named-entity-recognition,5,"As a result , we use only 1 sentence - level node for the remaining experiments .",experiment,0,154,6,0,17
named-entity-recognition,5,"The accuracies of S - LSTM increases as the hidden layer size for each node increases from 100 to 300 , but does not further increase when the size increases beyond 300 .",experiment,0,155,7,0,33
named-entity-recognition,5,We fix the hidden size to 300 accordingly .,experiment,0,156,8,0,9
named-entity-recognition,5,"Without using sand /s , the performance of S - LSTM drops from 82. 64 % to 82.36 % , showing the effectiveness of having these additional nodes .",experiment,0,157,9,0,29
named-entity-recognition,5,"Hyperparameters for BiLSTM models are also set according to the development data , which we omit here .",experiment,0,158,10,0,18
named-entity-recognition,5,state transition .,experiment,0,159,11,0,3
named-entity-recognition,5,"In , the number of recurrent state transition steps of S - LSTM is decided according to the best development performance .",experiment,0,160,12,0,22
named-entity-recognition,5,draws the development accuracies of S - LSTMs with various window sizes against the number of recurrent steps .,experiment,0,161,13,0,19
named-entity-recognition,5,"As can be seen from the figure , when the number of time steps increases from 1 to 11 , the accuracies generally increase , before reaching a maximum value .",experiment,0,162,14,0,31
named-entity-recognition,5,This shows the effectiveness of recurrent information exchange in S - LSTM state transition .,experiment,0,163,15,0,15
named-entity-recognition,5,"On the other hand , no significant differences are observed on the peak accuracies given by different window sizes , although a larger window size ( e.g.",experiment,0,164,16,0,27
named-entity-recognition,5,final results for classification,result,1,165,1,0,4
named-entity-recognition,5,"The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5 , respectively .",result,0,166,2,0,22
named-entity-recognition,5,"In addition to training time per epoch , test times are additionally reported .",result,0,167,3,0,14
named-entity-recognition,5,We use the best settings on the movie review development dataset for both S - LSTMs and BiLSTMs .,result,0,168,4,0,19
named-entity-recognition,5,The step number for S - LSTMs is set to 9 .,result,0,169,5,0,12
named-entity-recognition,5,"As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .",result,1,170,6,0,34
named-entity-recognition,5,Observations on CNN and hierarchical attention are consistent with the development results .,result,0,171,7,0,13
named-entity-recognition,5,S - LSTM also gives highly competitive results when compared with existing methods in the literature .,result,0,172,8,0,17
named-entity-recognition,5,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .",result,1,173,9,0,31
named-entity-recognition,5,"The average accuracy of S - LSTM is 85.6 % , significantly higher compared with 84.9 % by 2 - layer stacked BiLSTM .",result,1,174,10,0,24
named-entity-recognition,5,"3 - layer stacked BiL - STM gives an average accuracy of 84. 57 % , which is lower compared to a 2 - layer stacked BiLSTM , with a training time per epoch of 423.6 seconds .",result,0,175,11,0,38
named-entity-recognition,5,The relative speed advantage of S - LSTM over BiLSTM is larger on the 16 datasets as compared to the movie review test test .,result,0,176,12,0,25
named-entity-recognition,5,This is because the average length of inputs is larger on the 16 datasets ( see Section 4.5 ) .,result,0,177,13,0,20
named-entity-recognition,5,Final Results for Sequence Labelling,result,1,178,14,0,5
named-entity-recognition,5,bi-directional,result,0,179,15,0,1
named-entity-recognition,5,"RNN - CRF structures , and in particular BiLSTM - CRFs , have achieved the state of the art in the literature for sequence labelling tasks , including POS - tagging and NER .",result,0,180,16,0,34
named-entity-recognition,5,"We compare S - LSTM - CRF with BiLSTM - CRF for sequence labelling , using the same settings as decided on the movie review development experiments for both BiLSTMs and S - LSTMs .",result,0,181,17,0,35
named-entity-recognition,5,"For the latter , we decide the number of recurrent stepson the respective development sets for sequence labelling .",result,0,182,18,0,19
named-entity-recognition,5,"The POS accuracies and NER F1 - scores against the number of recurrent steps are shown in ( a ) and ( b ) , respectively .",result,0,183,19,0,27
named-entity-recognition,5,"For POS tagging , the best step number is set to 7 , with a development accuracy of 97.58 % .",result,0,184,20,0,21
named-entity-recognition,5,"For NER , the step number is set to 9 , with a development F1 - score of 94.98 % .",result,0,185,21,0,21
named-entity-recognition,5,As can be seen in with three layers of stacked LSTMs .,result,0,186,22,0,12
named-entity-recognition,5,"For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",result,1,187,23,0,28
named-entity-recognition,5,Stacking more layers of BiLSTMs leads to slightly better F1 - scores compared with a single - layer BiL - STM .,result,0,188,24,0,22
named-entity-recognition,5,"Our BiLSTM results are comparable to the results reported by and , who also use bidirectional RNN - CRF structures .",result,0,189,25,0,21
named-entity-recognition,5,"In contrast , S - LSTM gives the best reported results under the same settings .",result,0,190,26,0,16
named-entity-recognition,5,"In the second section of learning using additional language model objectives , obtaining an F-score of 86 . 26 % ; leverage character - level language models , obtaining an F- score of 91. 93 % , which is the current best result on the dataset .",result,0,191,27,0,47
named-entity-recognition,5,All the three models are based on BiLSTM - CRF .,result,0,192,28,0,11
named-entity-recognition,5,"On the other hand , these semi-supervised learning techniques are orthogonal to our work , and can potentially be used for S - LSTM also .",result,0,193,29,0,26
named-entity-recognition,5,analysis,result,0,194,30,0,1
named-entity-recognition,5,"Figure 4 ( a ) and ( b ) show the accuracies against the sentence length on the movie review and CoNLL datasets , respectively , where test samples are binned in batches of 80 .",result,0,195,31,0,36
named-entity-recognition,5,We find that the performances of both S - LSTM and BiLSTM decrease as the sentence length increases .,result,0,196,32,0,19
named-entity-recognition,5,"On the other hand , S - LSTM demonstrates relatively better robustness compared to BiLSTMs .",result,0,197,33,0,16
named-entity-recognition,5,This confirms our intuition that a sentence - level node can facilitate better non-local communication .,result,0,198,34,0,16
named-entity-recognition,5,"these comparisons , we mix all training instances , order them by the size , and put them into 10 equal groups , the medium sentence lengths of which are shown .",result,0,199,35,0,32
named-entity-recognition,5,"As can be seen from the figure , the speed advantage of S - LSTM is larger when the size of the input text increases , thanks to a fixed number of recurrent steps .",result,0,200,36,0,35
named-entity-recognition,5,"Similar to hierarchical attention , there is a relative dis advantage of S - LSTM in comparison with BiLSTM , which is that the memory consumption is relatively larger .",result,0,201,37,0,30
named-entity-recognition,5,"For example , over the movie review development set , the actual GPU memory consumption by S - LSTM , Bi LSTM , 2 - layer stacked BiLSTM and 4 - layer stacked BiLSTM are 252M , 89M , 146 M and 253M , respectively .",result,0,202,38,0,46
named-entity-recognition,5,This is due to the fact that computation is performed in parallel by S - LSTM and hierarchical attention .,result,0,203,39,0,20
named-entity-recognition,5,conclusion,result,0,204,40,0,1
named-entity-recognition,5,"We have investigated S - LSTM , a recurrent neural network for encoding sentences , which offers richer contextual information exchange with more parallelism compared to BiLSTMs .",result,0,205,41,0,28
named-entity-recognition,5,"Results on a range of classification and sequence labelling tasks show that S - LSTM outperforms BiLSTMs using the same number of parameters , demonstrating that S - LSTM can be a useful addition to the neural toolbox for encoding sentences .",result,0,206,42,0,42
named-entity-recognition,5,"The structural nature in S - LSTM states allows straightforward extension to tree structures , resulting in highly parallelis able tree LSTMs .",result,0,207,43,0,23
named-entity-recognition,5,We leave such investigation to future work .,result,0,208,44,0,8
named-entity-recognition,5,"Next directions also include the investigation of S - LSTM to more NLP tasks , such as machine translation .",result,0,209,45,0,20
named-entity-recognition,7,A Neural Transition - based Model for Nested Mention Recognition,title,1,2,1,0,10
named-entity-recognition,7,abstract,abstract,0,3,1,0,1
named-entity-recognition,7,It is common that entity mentions can contain other mentions recursively .,abstract,0,4,2,0,12
named-entity-recognition,7,This paper introduces a scalable transition - based method to model the nested structure of mentions .,abstract,1,5,3,0,17
named-entity-recognition,7,We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest .,abstract,0,6,4,0,23
named-entity-recognition,7,Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length .,abstract,0,7,5,0,35
named-entity-recognition,7,"Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns .",abstract,0,8,6,0,39
named-entity-recognition,7,"Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",abstract,0,9,7,0,22
named-entity-recognition,7,1,abstract,0,10,8,0,1
named-entity-recognition,7,introduction,introduction,0,11,1,0,1
named-entity-recognition,7,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",introduction,1,12,2,0,49
named-entity-recognition,7,"Practically , the mentions with nested structures frequently exist in news and biomedical documents .",introduction,0,13,3,0,15
named-entity-recognition,7,"For example in Traditional sequence labeling models such as conditional random fields ( CRF ) do not allow hierarchical structures between segments , making them incapable to handle such problems .",introduction,0,14,4,0,31
named-entity-recognition,7,presented a chart - based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree .,introduction,0,15,5,0,21
named-entity-recognition,7,The issue of using a chart - based parser is its cubic time complexity in the number of words in the sentence .,introduction,0,16,6,0,23
named-entity-recognition,7,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .",introduction,1,17,7,0,50
named-entity-recognition,7,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",introduction,1,18,8,0,25
named-entity-recognition,7,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,introduction,1,19,9,0,20
named-entity-recognition,7,shows an example of such a forest .,introduction,0,20,10,0,8
named-entity-recognition,7,"In contrast , the tree structure by further uses a root node to connect all tree elements .",introduction,0,21,11,0,18
named-entity-recognition,7,Our forest representation eliminates the root node so that the number of actions required to construct it can be reduced significantly .,introduction,0,22,12,0,22
named-entity-recognition,7,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .",introduction,1,23,13,0,33
named-entity-recognition,7,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,introduction,1,24,14,0,28
named-entity-recognition,7,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .",introduction,1,25,15,0,32
named-entity-recognition,7,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .",introduction,1,26,16,0,23
named-entity-recognition,7,We conduct experiments in three standard datasets .,introduction,0,27,17,0,8
named-entity-recognition,7,Our system achieves the state - of - the - art performance on ACE datasets and comparable performance in GENIA dataset .,introduction,0,28,18,0,22
named-entity-recognition,7,related work,related work,0,29,1,0,2
named-entity-recognition,7,Entity mention recognition with nested structures has been explored first with rule - based approaches where the authors first detected the innermost mentions and then relied on rule - based postprocessing methods to identify outer mentions .,related work,0,30,2,0,37
named-entity-recognition,7,proposed a structured multi-label model to represent overlapping segments in a sentence .,related work,0,31,3,0,13
named-entity-recognition,7,but it came with a cubic time complexity in the number of words .,related work,0,32,4,0,14
named-entity-recognition,7,proposed several ways to combine multiple conditional random fields ( CRF ) for such tasks .,related work,0,33,5,0,16
named-entity-recognition,7,Their best results were obtained by cascading several CRF models in a specific order while each model is responsible for detecting mentions of a particular type .,related work,0,34,6,0,27
named-entity-recognition,7,"However , such an approach can not model nested mentions of the same type , which frequently appear .",related work,0,35,7,0,19
named-entity-recognition,7,and proposed new representations of mention hypergraph and mention separator to model overlapping mentions .,related work,0,36,8,0,15
named-entity-recognition,7,"However , the nested structure is not guaranteed in such approaches since overlapping structures additionally include the crossing structures 3 , which rarely exist in practice .",related work,0,37,9,0,27
named-entity-recognition,7,"Also , their representations did not model the dependencies between nested mentions explicitly , which may limit their performance .",related work,0,38,10,0,20
named-entity-recognition,7,"In contrast , the chart - based parsing method can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities .",related work,0,39,11,0,32
named-entity-recognition,7,"However , their cubic time complexity makes them not scalable to large datasets .",related work,0,40,12,0,14
named-entity-recognition,7,"As neural network based approaches are proven effective in entity or mention recognition , recent efforts focus on incorporating neural components for recognizing nested mentions .",related work,0,41,13,0,26
named-entity-recognition,7,"dynamically stacked multiple LSTM - CRF layers , detecting mentions in an inside - out manner until no outer entities are extracted .",related work,0,42,14,0,23
named-entity-recognition,7,used recurrent neural networks to extract features for a hypergraph which encodes all nested mentions based on the BILOU tagging scheme .,related work,0,43,15,0,22
named-entity-recognition,7,model,related work,0,44,16,0,1
named-entity-recognition,7,"Specifically , given a sequence of words {x 0 , x 1 , . . . , x n } , the goal of our system is to output a set of mentions where nested structures are allowed .",related work,0,45,17,0,39
named-entity-recognition,7,"We use the forest structure to model the nested mentions scattered in a sentence , as shown in .",related work,0,46,18,0,19
named-entity-recognition,7,The mapping is straightforward : each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree .,related work,0,47,19,0,28
named-entity-recognition,7,4,related work,0,48,20,0,1
named-entity-recognition,7,shift - reduce system,related work,0,49,21,0,4
named-entity-recognition,7,"Our transition - based model is based on the shiftreduce parser for constituency parsing ( Watan - abe and Sumita , 2015 ) , which adopts .",related work,0,50,22,1,27
named-entity-recognition,7,"Generally , our system employs a stack to store ( partially ) processed nested elements .",related work,0,51,23,0,16
named-entity-recognition,7,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",related work,0,52,24,0,26
named-entity-recognition,7,in each step .,related work,0,53,25,0,4
named-entity-recognition,7,an action is applied to change the system 's state .,related work,0,54,26,0,11
named-entity-recognition,7,"Our system consists of three types of transition actions , which are also summarized in :",related work,0,55,27,0,16
named-entity-recognition,7,SHIFT pushes the next word from buffer to the stack .,related work,0,56,28,0,11
named-entity-recognition,7,REDUCE - X pops the top two items t 0 and t 1 from the tack and combines them as a new tree element { X ? t 0 t 1 } which is then pushed onto the stack .,related work,0,57,29,0,40
named-entity-recognition,7,UNARY - X pops the top item t 0 from the stack and constructs a new tree element { X ? t 0 } which is pushed back to the stack .,related work,0,58,30,0,32
named-entity-recognition,7,"Since the shift - reduce system assumes unary and binary branching , we binarize the trees in each forest in a left - branching manner .",related work,0,59,31,0,26
named-entity-recognition,7,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ?",related work,0,60,32,0,28
named-entity-recognition,7,{ p erson * ?,related work,0,61,33,0,5
named-entity-recognition,7,"A , B} , C} where P erson * is a temporary label for P erson .",related work,0,62,34,0,17
named-entity-recognition,7,"Hence , the X in reduce - actions will also include such temporary labels .",related work,0,63,35,0,15
named-entity-recognition,7,"Note that since most words are not contained in any mention , they are only shifted to the stack and not involved in any reduce - or unary - actions .",related work,0,64,36,0,31
named-entity-recognition,7,An example sequence of transitions can be found in .,related work,0,65,37,0,10
named-entity-recognition,7,Our shift - reduce system is different from previous parsers in terms of the terminal state .,related work,0,66,38,0,17
named-entity-recognition,7,1 ) It does not require the terminal stack to be a rooted tree .,related work,0,67,39,0,15
named-entity-recognition,7,"Instead , the final stack should be a forest consisting of multiple nested elements with tree structures .",related work,0,68,40,0,18
named-entity-recognition,7,"2 ) To conveniently determine the ending of our transition process , we add an auxiliary symbol $ to each sentence .",related work,0,69,41,0,22
named-entity-recognition,7,"Once it is pushed to the stack , it implies that all deductions of actual words are finished .",related work,0,70,42,0,19
named-entity-recognition,7,Since we do not allow unary rules between labels like X1 ?,related work,0,71,43,0,12
named-entity-recognition,7,"X2 , the length of maximal action sequence is 3 n .",related work,0,72,44,0,12
named-entity-recognition,7,5,related work,0,73,45,0,1
named-entity-recognition,7,action constraints,related work,0,74,46,0,2
named-entity-recognition,7,"To make sure that each action sequence is valid , we need to make some hard constraints on the ac - 5",related work,0,75,47,0,22
named-entity-recognition,7,"In this case , each word is shifted ( n ) and involved in a unary action ( n ) .",related work,0,76,48,0,21
named-entity-recognition,7,Then all elements are reduced to a single node ( n ? 1 ) .,related work,0,77,49,0,15
named-entity-recognition,7,The last action is to shift the symbol $. tion to take .,related work,0,78,50,0,13
named-entity-recognition,7,"For example , reduce - action can only be conducted when there are at least two elements in the stack .",related work,0,79,51,0,21
named-entity-recognition,7,Please see the Appendix for the full list of restrictions .,related work,0,80,52,0,11
named-entity-recognition,7,"Formally , we use V(S , i , A ) to denote the valid actions given the parser state .",related work,0,81,53,0,20
named-entity-recognition,7,Let us denote the feature vector for the parser state at time step k asp k .,related work,0,82,54,0,17
named-entity-recognition,7,The distribution of actions is computed as follows :,related work,0,83,55,0,9
named-entity-recognition,7,"( 1 ) where w z is a column weight vector for action z , and b z is a bias term .",related work,0,84,56,0,23
named-entity-recognition,7,neural transition - based model,related work,0,85,57,0,5
named-entity-recognition,7,"We use neural networks to learn the representation of the parser state , which is pk in ( 1 ) .",related work,0,86,58,0,21
named-entity-recognition,7,representation of words,related work,0,87,59,0,3
named-entity-recognition,7,Words are represented by concatenating three vectors :,related work,0,88,60,0,8
named-entity-recognition,7,where e w i and e pi denote the embeddings for i - th word and it s POS tag respectively .,related work,0,89,61,0,22
named-entity-recognition,7,cw i denotes the representation learned by a character - level model using a bidirectional LSTM .,related work,0,90,62,0,17
named-entity-recognition,7,"Specifically , for character sequence s 0 , s 1 , . . . , s n in the i - th word , we use the last hidden states of forward and backward LSTM as the character - based representation of this word , as shown below :",related work,0,91,63,0,49
named-entity-recognition,7,representation of parser states,related work,0,92,64,0,4
named-entity-recognition,7,"Generally , the buffer and action history are encoded using two vanilla LSTMs .",related work,0,93,65,0,14
named-entity-recognition,7,"For the stack that involves popping out top elements , we use the Stack - LSTM to efficiently encode it .",related work,0,94,66,0,21
named-entity-recognition,7,"Formally , if the unprocessed word sequence in the buffer is x i , x i +1 , . . . , x n and action history sequence is a 0 , a 1 , . . . , a k?1 , then we can compute buffer representation bk and action history representation a k at time step k as follows :",related work,0,95,67,0,62
named-entity-recognition,7,where each action is also mapped to a distributed representation ea .,related work,0,96,68,0,12
named-entity-recognition,7,"6 For the state of the stack , we also use an LSTM to encode a sequence of tree elements .",related work,0,97,69,0,21
named-entity-recognition,7,"However , the top elements of the stack are updated frequently .",related work,0,98,70,0,12
named-entity-recognition,7,Stack - LSTM provides an efficient implementation that incorporates a stackpointer .,related work,0,99,71,0,12
named-entity-recognition,7,7,related work,0,100,72,0,1
named-entity-recognition,7,"Formally , the state of the stack bk at time step k is computed as :",related work,0,101,73,0,16
named-entity-recognition,7,"where ht i denotes the representation of the i - th tree element from the top , which can be computed recursively similar to Recursive Neural Network as follows :",related work,0,102,74,0,30
named-entity-recognition,7,"where W u , l and W b , l denote the weight matrices for unary ( u ) and binary ( b ) composition with parent node being label ( l ) .",related work,0,103,75,0,34
named-entity-recognition,7,Note that the composition function is distinct for each label l .,related work,0,104,76,0,12
named-entity-recognition,7,Recall that the leaf nodes of each tree element are raw words .,related work,0,105,77,0,13
named-entity-recognition,7,"Instead of representing them with their original embeddings introduced in Section 3.3 , we found that 6 Note that LSTM b runs in a right - to - left order such that the output can represent the contextual information of x i.",related work,0,106,78,0,42
named-entity-recognition,7,7,related work,0,107,79,0,1
named-entity-recognition,7,Please refer to for details .,related work,0,108,80,0,6
named-entity-recognition,7,concatenating the buffer state in ( 5 ) are beneficial during our initial experiments .,related work,0,109,81,0,15
named-entity-recognition,7,"Formally , when a word xi is shifted to the stack at time step k , it s representation is computed as :",related work,0,110,82,0,23
named-entity-recognition,7,"Finally , the state of the system pk is the concatenation of the states of buffer , stack and action history :",related work,0,111,83,0,22
named-entity-recognition,7,training,related work,0,112,84,0,1
named-entity-recognition,7,We employ the greedy strategy to maximize the log -likelihood of the local action classifier in ( 1 ) .,related work,0,113,85,0,20
named-entity-recognition,7,"Specifically , let z ik denote the k - th action for the i - th sentence , the loss function with 2 norm is :",related work,0,114,86,0,26
named-entity-recognition,7,where ?,related work,0,115,87,0,2
named-entity-recognition,7,is the 2 coefficient .,related work,0,116,88,0,5
named-entity-recognition,7,experiments,experiment,0,117,1,0,1
named-entity-recognition,7,"We mainly evaluate our models on the standard ACE - 04 , , and GENIA datasets with the same splits used by previous research efforts .",experiment,0,118,2,0,26
named-entity-recognition,7,"In ACE datasets , more than 40 % of the mentions form nested structures with some other mention .",experiment,0,119,3,0,19
named-entity-recognition,7,"In GENIA , this number is 18 % .",experiment,0,120,4,0,9
named-entity-recognition,7,Please see for the full statistics .,experiment,0,121,5,0,7
named-entity-recognition,7,setup,experiment,0,122,6,0,1
named-entity-recognition,7,pre-trained embeddings,experiment,1,123,7,0,2
named-entity-recognition,7,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,experiment,1,124,8,0,17
named-entity-recognition,7,9 The embeddings of POS tags are initialized randomly with dimension 32 .,experiment,1,125,9,0,13
named-entity-recognition,7,The model is trained using Adam and a gradient clipping of 3.0 .,experiment,1,126,10,0,13
named-entity-recognition,7,Early stopping is used based on the performance of development sets .,experiment,0,127,11,0,12
named-entity-recognition,7,Dropout is used after the input layer .,experiment,0,128,12,0,8
named-entity-recognition,7,the 2 coefficient ?,experiment,0,129,13,0,4
named-entity-recognition,7,is also tuned during development process .,experiment,0,130,14,0,7
named-entity-recognition,7,results,result,0,131,1,0,1
named-entity-recognition,7,The main results are reported in .,result,0,132,2,0,7
named-entity-recognition,7,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,result,1,133,3,0,26
named-entity-recognition,7,We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets .,result,0,134,4,0,29
named-entity-recognition,7,"To verify this , we design an experiment to evaluate how well a system can recognize nested mentions .",result,0,135,5,0,19
named-entity-recognition,7,handling nested mentions,result,0,136,6,0,3
named-entity-recognition,7,The idea is that we split the test data into two portions : sentences with and without nested mentions .,result,0,137,7,0,20
named-entity-recognition,7,The results of GENIA are listed in .,result,0,138,8,0,8
named-entity-recognition,7,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .",result,1,139,9,0,28
named-entity-recognition,7,This observation helps explain why our model achieves greater improvement in ACE than in GENIA in since the former has much more nested structures than the latter .,result,0,140,10,0,28
named-entity-recognition,7,"Moreover , performs better when it comes to non-nested mentions possibly due to the CRF they used , which globally normalizes each stacked layer .",result,0,141,11,0,25
named-entity-recognition,7,decoding speed,result,0,142,12,0,2
named-entity-recognition,7,"Note that and also feature linear - time complexity , but with a greater constant factor .",result,0,143,13,0,17
named-entity-recognition,7,"To compare the decoding speed , we re-implemented their model with the same platform ( PyTorch ) and run them on the same machine ( CPU : Intel i5 2.7 GHz ) .",result,0,144,14,0,33
named-entity-recognition,7,"Our model turns out to be around 3 - 5 times faster than theirs , showing its scalability .",result,0,145,15,0,19
named-entity-recognition,7,We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable .,result,0,146,16,0,17
named-entity-recognition,7,ablation study,result,0,147,17,0,2
named-entity-recognition,7,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .",result,1,148,18,0,26
named-entity-recognition,7,The results are listed in .,result,0,149,19,0,6
named-entity-recognition,7,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .",result,1,150,20,0,24
named-entity-recognition,7,conclusion and future work,result,0,151,21,0,4
named-entity-recognition,7,"In this paper , we present a transition - based model for nested mention recognition using a forest representation .",result,0,152,22,0,20
named-entity-recognition,7,"Coupled with Stack - LSTM for representing the system 's state , our neural model can capture dependencies between nested mentions efficiently .",result,0,153,23,0,23
named-entity-recognition,7,"Moreover , the character - based component helps capture letter - level patterns in words .",result,0,154,24,0,16
named-entity-recognition,7,The system achieves the state - of - the - art performance in ACE datasets .,result,0,155,25,0,16
named-entity-recognition,7,One potential drawback of the system is the greedy training and decoding .,result,0,156,26,0,13
named-entity-recognition,7,We believe that alternatives like beam search and training with exploration could further boost the performance .,result,0,157,27,0,17
named-entity-recognition,7,Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans ( Muis and which frequently exist in the biomedical domain .,result,0,158,28,0,33
named-entity-recognition,4,Deep contextualized word representations,title,1,2,1,0,4
named-entity-recognition,4,abstract,abstract,0,3,1,0,1
named-entity-recognition,4,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",abstract,0,4,2,0,48
named-entity-recognition,4,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",abstract,0,5,3,0,29
named-entity-recognition,4,"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems , including question answering , textual entailment and sentiment analysis .",abstract,0,6,4,0,36
named-entity-recognition,4,"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial , allowing downstream models to mix different types of semi-supervision signals .",abstract,0,7,5,0,29
named-entity-recognition,4,introduction,introduction,0,8,1,0,1
named-entity-recognition,4,Pre-trained word representations are a key component in many neural language understanding models .,introduction,0,9,2,0,14
named-entity-recognition,4,"However , learning high quality representations can be challenging .",introduction,0,10,3,0,10
named-entity-recognition,4,"They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",introduction,0,11,4,0,40
named-entity-recognition,4,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",introduction,0,12,5,0,49
named-entity-recognition,4,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,introduction,1,13,6,0,26
named-entity-recognition,4,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,introduction,1,14,7,0,28
named-entity-recognition,4,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",introduction,1,15,8,0,16
named-entity-recognition,4,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",introduction,0,16,9,0,32
named-entity-recognition,4,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",introduction,1,17,10,0,33
named-entity-recognition,4,Combining the internal states in this manner allows for very rich word representations .,introduction,0,18,11,0,14
named-entity-recognition,4,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense dis ambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",introduction,0,19,12,0,65
named-entity-recognition,4,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision thatare most useful for each end task .",introduction,0,20,13,0,27
named-entity-recognition,4,Extensive experiments demonstrate that ELMo representations work extremely well in practice .,introduction,0,21,14,0,12
named-entity-recognition,4,"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems , including textual entailment , question answering and sentiment analysis .",introduction,0,22,15,0,31
named-entity-recognition,4,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .",introduction,0,23,16,0,26
named-entity-recognition,4,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",introduction,0,24,17,0,23
named-entity-recognition,4,"Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform ar Xiv : 1802.05365v2 [ cs. CL ] 22 Mar 2018 those derived from just the top layer of an LSTM .",introduction,0,25,18,0,36
named-entity-recognition,4,"Our trained models and code are publicly available , and we expect that ELMo will provide similar gains for many other NLP problems .",introduction,0,26,19,0,24
named-entity-recognition,4,1,introduction,0,27,20,0,1
named-entity-recognition,4,"2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",introduction,0,28,21,0,51
named-entity-recognition,4,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",introduction,0,29,22,0,18
named-entity-recognition,4,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",introduction,0,30,23,0,34
named-entity-recognition,4,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",introduction,0,31,24,0,32
named-entity-recognition,4,Other recent work has also focused on learning context - dependent representations .,introduction,0,32,25,0,13
named-entity-recognition,4,context2vec,introduction,0,33,26,0,1
named-entity-recognition,4,"( Melamud et al. , 2016 ) uses a bidirectional Long Short Term Memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) to encode the context around a pivot word .",introduction,0,34,27,1,32
named-entity-recognition,4,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,introduction,0,35,28,0,37
named-entity-recognition,4,"Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .",introduction,0,36,29,0,22
named-entity-recognition,4,"In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences .",introduction,0,37,30,0,28
named-entity-recognition,4,"We also generalize these approaches to deep contextual representations , which we show work well across a broad range of diverse NLP tasks .",introduction,0,38,31,0,24
named-entity-recognition,4,1 http://allennlp.org/elmo,introduction,0,39,32,0,2
named-entity-recognition,4,Previous work has also shown that different layers of deep biRNNs encode different types of information .,introduction,0,40,33,0,17
named-entity-recognition,4,"For example , introducing multi-task syntactic supervision ( e.g. , part - of - speech tags ) at the lower levels of a deep LSTM can improve over all performance of higher level tasks such as dependency parsing or CCG super tagging .",introduction,0,41,34,0,43
named-entity-recognition,4,"In an RNN - based encoder - decoder machine translation system , showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer .",introduction,0,42,35,0,37
named-entity-recognition,4,"Finally , the top layer of an LSTM for encoding word context has been shown to learn representations of word sense .",introduction,0,43,36,0,22
named-entity-recognition,4,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",introduction,0,44,37,0,39
named-entity-recognition,4,and pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .,introduction,0,45,38,0,21
named-entity-recognition,4,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",introduction,0,46,39,0,46
named-entity-recognition,4,dai and le,introduction,0,47,40,0,3
named-entity-recognition,4,bidirectional language models,introduction,0,48,41,0,3
named-entity-recognition,4,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k?1 ) :",introduction,0,49,42,0,53
named-entity-recognition,4,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,introduction,0,50,43,0,41
named-entity-recognition,4,"At each position k , each LSTM layer outputs a context - dependent representation ? ? h LM k , j where j = 1 , . . . , L. The top layer LSTM output , ? ? h LM k , L , is used to predict the next token t k +1 with a Softmax layer .",introduction,0,51,44,0,60
named-entity-recognition,4,"A backward LM is similar to a forward LM , except it runs over the sequence in reverse , predicting the previous token given the future context :",introduction,0,52,45,0,28
named-entity-recognition,4,"It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations ? ? h LM k , j oft k given ( t k+1 , . . . , t N ) .",introduction,0,53,46,0,49
named-entity-recognition,4,A bi LM combines both a forward and backward LM .,introduction,0,54,47,0,11
named-entity-recognition,4,Our formulation jointly maximizes the log likelihood of the forward and backward directions :,introduction,0,55,48,0,14
named-entity-recognition,4,We tie the parameters for both the token representation (? x ) and Softmax layer (? s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction .,introduction,0,56,49,0,35
named-entity-recognition,4,"Overall , this formulation is similar to the approach of Peters et al. , with the exception that we share some weights between directions instead of using completely independent parameters .",introduction,0,57,50,1,31
named-entity-recognition,4,"In the next section , we depart from previous work by introducing a new approach for learning word representations thatare a linear combination of the biLM layers .",introduction,0,58,51,0,28
named-entity-recognition,4,elmo,introduction,0,59,52,0,1
named-entity-recognition,4,ELMo is a task specific combination of the intermediate layer representations in the biLM .,introduction,0,60,53,0,15
named-entity-recognition,4,"For each token t k , a L-layer biLM computes a set of 2L + 1 representations",introduction,0,61,54,0,17
named-entity-recognition,4,"where h LM k ,0 is the token layer and h LM",introduction,0,62,55,0,12
named-entity-recognition,4,", for each bilstm layer .",introduction,0,63,56,0,6
named-entity-recognition,4,"For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",introduction,0,64,57,0,28
named-entity-recognition,4,"In the simplest case , ELMo just selects the top layer , E( R k ) = h LM k , L , as in Tag LM and CoVe .",introduction,0,65,58,0,30
named-entity-recognition,4,"More generally , we compute a task specific weighting of all biLM layers :",introduction,0,66,59,0,14
named-entity-recognition,4,"( 1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ?",introduction,0,67,60,0,20
named-entity-recognition,4,task allows the task model to scale the entire ELMo vector . ?,introduction,0,68,61,0,13
named-entity-recognition,4,is of practical importance to aid the optimization process ( see supplemental material for details ) .,introduction,0,69,62,0,17
named-entity-recognition,4,"Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization to each biLM layer before weighting .",introduction,0,70,63,0,30
named-entity-recognition,4,Using biLMs for supervised NLP tasks,introduction,0,71,64,0,6
named-entity-recognition,4,"Given a pre-trained biLM and a supervised architecture for a target NLP task , it is a simple process to use the biLM to improve the task model .",introduction,0,72,65,0,29
named-entity-recognition,4,We simply run the biLM and record all of the layer representations for each word .,introduction,0,73,66,0,16
named-entity-recognition,4,"Then , we let the end task model learn a linear combination of these representations , as described below .",introduction,0,74,67,0,20
named-entity-recognition,4,First consider the lowest layers of the supervised model without the biLM .,introduction,0,75,68,0,13
named-entity-recognition,4,"Most supervised NLP models share a common architecture at the lowest layers , allowing us to add ELMo in a consistent , unified manner .",introduction,0,76,69,0,25
named-entity-recognition,4,"Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre-trained word embeddings and optionally character - based representations .",introduction,0,77,70,0,45
named-entity-recognition,4,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .",introduction,0,78,71,0,26
named-entity-recognition,4,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",introduction,0,79,72,0,47
named-entity-recognition,4,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",introduction,0,80,73,0,49
named-entity-recognition,4,"As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",introduction,0,81,74,0,23
named-entity-recognition,4,"For example , see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs , or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs .",introduction,0,82,75,0,35
named-entity-recognition,4,"Finally , we found it beneficial to add a moderate amount of dropout to and in some cases to regularize the ELMo weights by adding ?",introduction,0,83,76,0,26
named-entity-recognition,4,w 2 2 to the loss .,introduction,0,84,77,0,7
named-entity-recognition,4,This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers .,introduction,0,85,78,0,20
named-entity-recognition,4,Pre-trained bidirectional language model architecture,introduction,0,86,79,0,5
named-entity-recognition,4,"The pre-trained biLMs in this paper are similar to the architectures in Jzefowicz et al. and , but modified to support joint training of both directions and add a residual connection between LSTM layers .",introduction,0,87,80,1,35
named-entity-recognition,4,"We focus on large scale biLMs in this work , as Peters et al .",introduction,0,88,81,1,15
named-entity-recognition,4,highlighted the importance of using biLMs over forward - only LMs and large scale training .,introduction,0,89,82,0,16
named-entity-recognition,4,"To balance over all language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character - based input representation , we halved all embedding and hidden dimensions from the single best model CNN - BIG - LSTM in Jzefowicz et al ..",introduction,0,90,83,1,48
named-entity-recognition,4,The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer .,introduction,0,91,84,0,27
named-entity-recognition,4,The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation .,introduction,0,92,85,0,26
named-entity-recognition,4,"As a result , the biLM provides three layers of representations for each input token , including those outside the training set due to the purely character input .",introduction,0,93,86,0,29
named-entity-recognition,4,"In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .",introduction,0,94,87,0,20
named-entity-recognition,4,"After training for 10 epochs on the 1B Word Benchmark , the average forward and backward perplexities is 39.7 , compared to 30.0 for the forward CNN - BIG - LSTM .",introduction,0,95,88,0,32
named-entity-recognition,4,"Generally , we found the forward and backward perplexities to be approximately equal , with the backward value slightly lower .",introduction,0,96,89,0,21
named-entity-recognition,4,"Once pretrained , the biLM can compute representations for any task .",introduction,0,97,90,0,12
named-entity-recognition,4,"In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .",introduction,0,98,91,0,26
named-entity-recognition,4,This can be seen as a type of domain transfer for the biLM .,introduction,0,99,92,0,14
named-entity-recognition,4,"As a result , in most cases we used a fine - tuned biLM in the downstream task .",introduction,0,100,93,0,19
named-entity-recognition,4,See supplemental material for details .,introduction,0,101,94,0,6
named-entity-recognition,4,shows the performance of ELMo across a diverse set of six benchmark NLP tasks .,introduction,0,102,95,0,15
named-entity-recognition,4,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",introduction,0,103,96,0,35
named-entity-recognition,4,This is a very general result across a diverse set model architectures and language understanding tasks .,introduction,0,104,97,0,17
named-entity-recognition,4,In the remainder of this section we provide high - level sketches of the individual task results ; see the supplemental material for full experimental details .,introduction,0,105,98,0,27
named-entity-recognition,4,evaluation,introduction,0,106,99,0,1
named-entity-recognition,4,question textual entailment,introduction,0,107,100,0,3
named-entity-recognition,4,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",introduction,1,108,101,0,21
named-entity-recognition,4,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,introduction,1,109,102,0,17
named-entity-recognition,4,"Our baseline , the ESIM sequence model from Chen et al. , uses a biL - STM to encode the premise and hypothesis , followed by a matrix attention layer , a local inference layer , another biLSTM inference composition layer , and finally a pooling operation before the output layer .",introduction,0,110,103,1,52
named-entity-recognition,4,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",introduction,1,111,104,0,21
named-entity-recognition,4,"A five member ensemble pushes the over all accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .",introduction,0,112,105,0,22
named-entity-recognition,4,semantic role labeling,introduction,0,113,106,0,3
named-entity-recognition,4,"A semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",introduction,0,114,107,0,32
named-entity-recognition,4,he et al .,introduction,0,115,108,1,4
named-entity-recognition,4,"( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",introduction,0,116,109,1,27
named-entity-recognition,4,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,introduction,1,117,110,0,25
named-entity-recognition,4,Our baseline model is the end - to - end span - based neural model of .,introduction,0,118,111,0,17
named-entity-recognition,4,It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .,introduction,0,119,112,0,25
named-entity-recognition,4,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",introduction,1,120,113,0,52
named-entity-recognition,4,analysis,introduction,0,121,114,0,1
named-entity-recognition,4,This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations .,introduction,0,122,115,0,21
named-entity-recognition,4,"Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer , regardless of whether they are produced from a biLM or MT encoder , and that ELMo representations provide the best over all performance .",introduction,0,123,116,0,47
named-entity-recognition,4,"Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers , consistent with MT encoders .",introduction,0,124,117,0,42
named-entity-recognition,4,It also shows that our biLM consistently provides richer representations then CoVe .,introduction,0,125,118,0,13
named-entity-recognition,4,"Additionally , we analyze the sensitivity to where ELMo is included in the task model ( Sec. 5.2 ) , training set size ( Sec. 5.4 ) , and visualize the ELMo learned weights across the tasks ( Sec. 5.5 ) .",introduction,0,126,119,0,42
named-entity-recognition,4,alternate layer weighting schemes,introduction,0,127,120,0,4
named-entity-recognition,4,There are many alternatives to Equation 1 for combining the biLM layers .,introduction,0,128,121,0,13
named-entity-recognition,4,"Previous work on contextual representations used only the last layer , whether it be from a biLM or an MT encoder ( CoVe ; .",introduction,0,129,122,0,25
named-entity-recognition,4,The choice of the regularization parameter ?,introduction,0,130,123,0,7
named-entity-recognition,4,"is also important , as large values such as ? = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , ? = 0.001 ) allow the layer weights to vary .",introduction,0,131,124,0,42
named-entity-recognition,4,"compares these alternatives for SQuAD , SNLI and SRL .",introduction,0,132,125,0,10
named-entity-recognition,4,"Including representations from all layers improves over all performance over just using the last layer , and including contextual representations from the last layer improves performance over the baseline .",introduction,0,133,126,0,30
named-entity-recognition,4,"For example , in the case of SQuAD , using just the last biLM layer improves development F 1 by 3.9 % over the baseline .",introduction,0,134,127,0,26
named-entity-recognition,4,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to ?= 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( ?= 1 vs. ?= 0.001 ) .",introduction,0,135,128,0,53
named-entity-recognition,4,a small ?,introduction,0,136,129,0,3
named-entity-recognition,4,"is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ?",introduction,0,137,130,0,26
named-entity-recognition,4,( not shown ) .,introduction,0,138,131,0,5
named-entity-recognition,4,The over all trend is similar with CoVe but with smaller increases over the baseline .,introduction,0,139,132,0,16
named-entity-recognition,4,"For SNLI , averaging all layers with ?= 1 improves development accuracy from 88.2 to 88.7 % over using just the last layer .",introduction,0,140,133,0,24
named-entity-recognition,4,SRL F 1 increased a marginal 0.1 % to 82.2 for the ?= 1 case compared to using the last layer only .,introduction,0,141,134,0,23
named-entity-recognition,4,where to include elmo ?,introduction,0,142,135,0,5
named-entity-recognition,4,All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,introduction,0,143,136,0,20
named-entity-recognition,4,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves over all results for some tasks .",introduction,0,144,137,0,26
named-entity-recognition,4,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .",introduction,0,145,138,0,48
named-entity-recognition,4,"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM 's internal representations .",introduction,0,146,139,0,40
named-entity-recognition,4,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement . the task - specific context representations are likely more important than those from the biLM .",introduction,0,147,140,0,53
named-entity-recognition,4,What information is captured by the biLM 's representations ?,introduction,0,148,141,0,10
named-entity-recognition,4,since adding,introduction,0,149,142,0,2
named-entity-recognition,4,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .",introduction,0,150,143,0,30
named-entity-recognition,4,"Intuitively , the biLM must be dis ambiguating the meaning of words using their context .",introduction,0,151,144,0,16
named-entity-recognition,4,"Consider "" play "" , a highly polysemous word .",introduction,0,152,145,0,10
named-entity-recognition,4,"The top of lists nearest neighbors to "" play "" using GloVe vectors .",introduction,0,153,146,0,14
named-entity-recognition,4,"They are spread across several parts of speech ( e.g. , "" played "" , "" playing "" as verbs , and "" player "" , "" game "" as nouns ) but concentrated in the sportsrelated senses of "" play "" .",introduction,0,154,147,0,43
named-entity-recognition,4,"In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM 's context representation of "" play "" in the source sentence .",introduction,0,155,148,0,34
named-entity-recognition,4,"In these cases , the biLM is able to dis ambiguate both the part of speech and word sense in the source sentence .",introduction,0,156,149,0,24
named-entity-recognition,4,These observations can be quantified using an intrinsic evaluation of the contextual representations similar to .,introduction,0,157,150,0,16
named-entity-recognition,4,"To isolate the information encoded by the biLM , the representations are used to directly make predictions for a fine grained word sense dis ambiguation ( WSD ) task and a POS tagging task .",introduction,0,158,151,0,35
named-entity-recognition,4,"Using this approach , it is also possible to compare to CoVe , and across each of the individual layers .",introduction,0,159,152,0,21
named-entity-recognition,4,"Word sense dis ambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach , similar to .",introduction,0,160,153,0,32
named-entity-recognition,4,"To do so , we first use the biLM to compute representations for all words in Sem - Cor 3.0 , our training corpus , and then take the average representation for each sense .",introduction,0,161,154,0,35
named-entity-recognition,4,"At test time , we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set , falling back to the first sense from WordNet for lemmas not observed during training .",introduction,0,162,155,0,43
named-entity-recognition,4,compares WSD results using the evaluation framework from across the same suite of four test sets in .,introduction,0,163,156,0,18
named-entity-recognition,4,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .",introduction,0,164,157,0,22
named-entity-recognition,4,This is competitive with a state - of - the - art WSD - specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse - grained semantic labels and POS tags .,introduction,0,165,158,0,41
named-entity-recognition,4,"The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher over all performance at the second layer compared to the first ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet first sense baseline .",introduction,0,166,159,0,45
named-entity-recognition,4,pos tagging,introduction,0,167,160,0,2
named-entity-recognition,4,"To examine whether the biLM captures basic syntax , we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) .",introduction,0,168,161,0,38
named-entity-recognition,4,"As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .",introduction,0,169,162,0,23
named-entity-recognition,4,"Similar to WSD , the biLM representations are competitive with carefully tuned , task specific biLSTMs .",introduction,0,170,163,0,17
named-entity-recognition,4,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .",introduction,0,171,164,0,32
named-entity-recognition,4,"CoVe POS tagging accuracies follow the same pattern as those from the biLM , and just like for WSD , the biLM achieves higher accuracies than the CoVe encoder .",introduction,0,172,165,0,30
named-entity-recognition,4,"Implications for supervised tasks Taken together , these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks .",introduction,0,173,166,0,37
named-entity-recognition,4,"In addition , the biLM 's representations are more transferable to WSD and POS tagging than those in CoVe , helping to illustrate why ELMo outperforms CoVe in downstream tasks .",introduction,0,174,167,0,31
named-entity-recognition,4,sample efficiency,introduction,0,175,168,0,2
named-entity-recognition,4,"Adding ELMo to a model increases the sample efficiency considerably , both in terms of number of parameter updates to reach state - of - the - art performance and the over all training set size .",introduction,0,176,169,0,37
named-entity-recognition,4,"For example , the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo .",introduction,0,177,170,0,20
named-entity-recognition,4,"After adding ELMo , the model exceeds the baseline maximum at epoch 10 , a 98 % relative decrease in the number of updates needed to reach In addition , ELMo - enhanced models use smaller training sets more efficiently than models without ELMo.",introduction,0,178,171,0,44
named-entity-recognition,4,compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1 % to 100 % .,introduction,0,179,172,0,27
named-entity-recognition,4,Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance .,introduction,0,180,173,0,26
named-entity-recognition,4,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .",introduction,0,181,174,0,33
named-entity-recognition,4,visualizes the softmax - normalized learned layer weights .,introduction,0,182,175,0,9
named-entity-recognition,4,"At the input layer , the task model favors the first biLSTM layer .",introduction,0,183,176,0,14
named-entity-recognition,4,"For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks .",introduction,0,184,177,0,22
named-entity-recognition,4,"The output layer weights are relatively balanced , with a slight preference for the lower layers .",introduction,0,185,178,0,17
named-entity-recognition,4,visualization of learned weights,introduction,0,186,179,0,4
named-entity-recognition,4,"We have introduced a general approach for learning high - quality deep context - dependent representations from biLMs , and shown large improvements when applying ELMo to a broad range of NLP tasks .",introduction,0,187,180,0,34
named-entity-recognition,4,"Through ablations and other controlled experiments , we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin - context , and that using all layers improves over all task performance .",introduction,0,188,181,0,40
named-entity-recognition,4,A Supplemental Material to accompany Deep contextualized word representations,introduction,0,189,182,0,9
named-entity-recognition,4,"This supplement contains details of the model architectures , training routines and hyper - parameter choices for the state - of - the - art models in Section 4 .",introduction,0,190,183,0,30
named-entity-recognition,4,All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs .,introduction,0,191,184,0,38
named-entity-recognition,4,a.1,introduction,0,192,185,0,1
named-entity-recognition,4,fine tuning bilm,introduction,0,193,186,0,3
named-entity-recognition,4,"As noted in Sec. 3.4 , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .",introduction,0,194,187,0,22
named-entity-recognition,4,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",introduction,0,195,188,0,33
named-entity-recognition,4,"Once fine tuned , the biLM weights were fixed during task training .",introduction,0,196,189,0,13
named-entity-recognition,4,lists the development set perplexities for the considered tasks .,introduction,0,197,190,0,10
named-entity-recognition,4,"In every case except CoNLL 2012 , fine tuning results in a large improvement in perplexity , e.g. , from 72.1 to 16.8 for SNLI .",introduction,0,198,191,0,26
named-entity-recognition,4,The impact of fine tuning on supervised performance is task dependent .,introduction,0,199,192,0,12
named-entity-recognition,4,"In the case of SNLI , fine tuning the biLM increased development accuracy 0.6 % from 88.9 % to 89.5 % for our single best model .",introduction,0,200,193,0,27
named-entity-recognition,4,"However , for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used .",introduction,0,201,194,0,21
named-entity-recognition,4,A.2 Importance of ? in Eqn .,introduction,0,202,195,0,7
named-entity-recognition,4,"The ? parameter in Eqn. ( 1 ) was of practical importance to aid optimization , due to the different distributions between the biLM internal representations and the task specific representations .",introduction,0,203,196,0,32
named-entity-recognition,4,It is especially important in the last - only casein Sec. 5.1 .,introduction,0,204,197,0,13
named-entity-recognition,4,"Without this parameter , the last - only case performed poorly ( well below the baseline ) for SNLI and training failed completely for SRL .",introduction,0,205,198,0,26
named-entity-recognition,4,a.3 textual entailment,introduction,0,206,199,0,3
named-entity-recognition,4,Our baseline SNLI model is the ESIM sequence model from .,introduction,0,207,200,0,11
named-entity-recognition,4,"Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training .",introduction,0,208,201,0,28
named-entity-recognition,4,"For regularization , we added 50 % variational dropout to the input of each LSTM layer and 50 % dropout at the input to the final two fully connected layers .",introduction,0,209,202,0,31
named-entity-recognition,4,All feed forward layers use ReLU activations .,introduction,0,210,203,0,8
named-entity-recognition,4,"Parameters were optimized using Adam ( Kingma and with gradient norms clipped at 5.0 and initial learning rate 0.0004 , decreasing by half each time accuracy on the development set did not increase in subsequent epochs .",introduction,0,211,204,0,37
named-entity-recognition,4,The batch size was 32 .,introduction,0,212,205,0,6
named-entity-recognition,4,"The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM , using ( 1 ) with layer normalization and ? = 0.001 .",introduction,0,213,206,0,31
named-entity-recognition,4,"Due to the increased number of parameters in the ELMo model , we added 2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50 % dropout after the attention layer .",introduction,0,214,207,0,37
named-entity-recognition,4,compares test set accuracy of our system to previously published systems .,introduction,0,215,208,0,12
named-entity-recognition,4,"Overall , adding ELMo to the ESIM model improved accuracy by 0.7 % establishing a new single model state - of - the - art of 88.7 % , and a five member ensemble pushes the over all accuracy to 89.3 % .",introduction,0,216,209,0,43
named-entity-recognition,4,a.4 question answering,introduction,0,217,210,0,3
named-entity-recognition,4,Our QA model is a simplified version of the model from .,introduction,0,218,211,0,12
named-entity-recognition,4,It embeds tokens by concatenating each token 's case - sensitive 300 dimensional Glo Ve word vector with a character - derived embedding produced using a convolutional neural network followed by max - pooling on learned character embeddings .,introduction,0,219,212,0,39
named-entity-recognition,4,"The token embeddings are passed through a shared bi-directional GRU , and then the bi-directional attention mechanism from .",introduction,0,220,213,0,19
named-entity-recognition,4,the augmented con-,introduction,0,221,214,0,3
named-entity-recognition,4,model,introduction,0,222,215,0,1
named-entity-recognition,4,acc.,introduction,0,223,216,0,1
named-entity-recognition,4,"Feature based 78.2 DIIN 88.0 BCN+Char+CoVe 88.0 ESIM + TreeLSTM 88.6 ESIM+ELMo 88.7 0.17 DIIN ensemble 88.9 ESIM + ELMo ensemble 89.3 text vectors are then passed through a linear layer with ReLU activations , a residual self - attention layer that uses a GRU followed by the same attention mechanism applied context - to - context , and another linear layer with ReLU activations .",introduction,0,224,217,0,66
named-entity-recognition,4,"Finally , the results are fed through linear layers to predict the start and end token of the answer .",introduction,0,225,218,0,20
named-entity-recognition,4,Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2 .,introduction,0,226,219,0,20
named-entity-recognition,4,"A dimensionality of 90 is used for the GRUs , and 180 for the linear layers .",introduction,0,227,220,0,17
named-entity-recognition,4,We optimize the model using Adadelta with a batch size of 45 .,introduction,0,228,221,0,13
named-entity-recognition,4,At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17 .,introduction,0,229,222,0,25
named-entity-recognition,4,We do not update the word vectors during training .,introduction,0,230,223,0,10
named-entity-recognition,4,Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized (? = 0 ) .,introduction,0,231,224,0,31
named-entity-recognition,4,"compares test set results from the SQuAD leaderboard as of November 17 , 2017 when we submitted our system .",introduction,0,232,225,0,20
named-entity-recognition,4,"Overall , our submission had the highest single model and ensemble results , improving the previous single model result ( SAN ) by 1.4 % F 1 and our baseline by 4.2 % .",introduction,0,233,226,0,34
named-entity-recognition,4,"A 11 member ensemble pushes F 1 to 87.4 % , 1.0 % increase over the previous ensemble best .",introduction,0,234,227,0,20
named-entity-recognition,4,A.5 Semantic Role Labeling,introduction,0,235,228,0,4
named-entity-recognition,4,Our baseline SRL model is an exact reimplementation of .,introduction,0,236,229,0,10
named-entity-recognition,4,"Words are represented using a concatenation of 100 dimensional vector representations , initialized using GloVe and a binary , per-word predicate feature , represented using an 100 dimensional em-bedding .",introduction,0,237,230,0,30
named-entity-recognition,4,"This 200 dimensional token representation is then passed through an 8 layer "" interleaved "" biLSTM with a 300 dimensional hidden size , in which the directions of the LSTM layers alternate per layer .",introduction,0,238,231,0,35
named-entity-recognition,4,This deep LSTM uses Highway connections between layers and variational recurrent dropout .,introduction,0,239,232,0,13
named-entity-recognition,4,This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags .,introduction,0,240,233,0,25
named-entity-recognition,4,Labels consist of semantic roles from PropBank augmented with a BIO labeling scheme to represent argument spans .,introduction,0,241,234,0,18
named-entity-recognition,4,"During training , we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ? = 0.95 .",introduction,0,242,235,0,26
named-entity-recognition,4,"At test time , we perform Viterbi decoding to enforce valid spans using BIO constraints .",introduction,0,243,236,0,16
named-entity-recognition,4,Variational dropout of 10 % is added to all LSTM hidden layers .,introduction,0,244,237,0,13
named-entity-recognition,4,Gradients are clipped if their value exceeds 1.0 .,introduction,0,245,238,0,9
named-entity-recognition,4,"Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs , whichever is sooner .",introduction,0,246,239,0,21
named-entity-recognition,4,The pretrained Glo Ve vectors are fine - tuned during training .,introduction,0,247,240,0,12
named-entity-recognition,4,The final dense layer and all cells of all LSTMs are initialized to be orthogonal .,introduction,0,248,241,0,16
named-entity-recognition,4,"The forget gate bias is initialized to 1 for all LSTMs , with all other gates initialized to 0 , as per . perparameters exactly following the original implementation .",introduction,0,249,242,0,30
named-entity-recognition,4,The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using ( 1 ) without any regularization ( ? = 0 ) or layer normalization .,introduction,0,250,243,0,34
named-entity-recognition,4,50 % dropout was added to the ELMo representations .,introduction,0,251,244,0,10
named-entity-recognition,4,compares our results with previously published results .,introduction,0,252,245,0,8
named-entity-recognition,4,"Overall , we improve the single model state - of - the - art by 3.2 % average F 1 , and our single model result improves the previous ensemble best by 1.6 % F 1 .",introduction,0,253,246,0,37
named-entity-recognition,4,Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F 1 by approximately 0.7 % ( not shown ) .,introduction,0,254,247,0,26
named-entity-recognition,4,A.7 Named Entity Recognition,introduction,0,255,248,0,4
named-entity-recognition,4,Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors with a CNN character based representation .,introduction,0,256,249,0,17
named-entity-recognition,4,"The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters , a ReLU activation and by max pooling .",introduction,0,257,250,0,25
named-entity-recognition,4,"The token representation is passed through two biLSTM layers , the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer .",introduction,0,258,251,0,32
named-entity-recognition,4,"During training , we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid .",introduction,0,259,252,0,28
named-entity-recognition,4,Variational dropout is added to the input of both biLSTM layers .,introduction,0,260,253,0,12
named-entity-recognition,4,During training the gradients are rescaled if their 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001 .,introduction,0,261,254,0,24
named-entity-recognition,4,The pre-trained Senna embeddings are fine tuned during training .,introduction,0,262,255,0,10
named-entity-recognition,4,We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds .,introduction,0,263,256,0,23
named-entity-recognition,4,ELMo was added to the input of the lowest layer task biLSTM .,introduction,0,264,257,0,13
named-entity-recognition,4,"As the CoNLL 2003 NER data set is relatively small , we found the best performance by constraining the trainable layer weights to be effectively constant by setting ? = 0.1 with ( 1 ) .",introduction,0,265,258,0,36
named-entity-recognition,4,from all layers of the biLM provides a modest improvement .,introduction,0,266,259,0,11
named-entity-recognition,4,a.8 sentiment classification,introduction,0,267,260,0,3
named-entity-recognition,4,"We use almost the same biattention classification network architecture described in , with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout .",introduction,0,268,261,0,34
named-entity-recognition,4,"A BCN model with a batch - normalized maxout network reached significantly lower validation accuracies in our experiments , although there maybe discrepancies between our implementation and that of .",introduction,0,269,262,0,30
named-entity-recognition,4,"To match the CoVe training setup , we only train on phrases that contain four or more tokens .",introduction,0,270,263,0,19
named-entity-recognition,4,We use 300 -d hidden states for the biLSTM and optimize the model parameters with Adam ( Kingma and using a learning rate of 0.0001 .,introduction,0,271,264,0,26
named-entity-recognition,4,"The trainable biLM layer weights are regularized by ? = 0.001 , and we add ELMo to both the input and output of the biLSTM ; the output ELMo vectors are computed with a second biLSTM and concatenated to the input .",introduction,0,272,265,0,42
named-entity-recognition,6,Robust Lexical Features for Improved Neural Network Named - Entity Recognition,title,1,2,1,0,11
named-entity-recognition,6,abstract,abstract,0,3,1,0,1
named-entity-recognition,6,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,abstract,1,4,2,0,16
named-entity-recognition,6,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",abstract,0,5,3,0,28
named-entity-recognition,6,"In this work , we show that this is unfair : lexical features are actually quite useful .",abstract,0,6,4,0,18
named-entity-recognition,6,We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia .,abstract,0,7,5,0,26
named-entity-recognition,6,"From this , we compute - offline - a feature vector representing each word .",abstract,0,8,6,0,15
named-entity-recognition,6,"When used with a vanilla recurrent neural network model , this representation yields substantial improvements .",abstract,0,9,7,0,16
named-entity-recognition,6,"We establish a new state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset .",abstract,0,10,8,0,46
named-entity-recognition,6,This work is licensed under a Creative Commons Attribution 4.0 International License .,abstract,0,11,9,0,13
named-entity-recognition,6,license details :,abstract,0,12,10,0,3
named-entity-recognition,6,introduction,introduction,0,13,1,0,1
named-entity-recognition,6,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,introduction,1,14,2,0,24
named-entity-recognition,6,"Various approaches have been proposed to tackle the task , from hand - crafted feature - based machine learning models like conditional random fields and perceptron , to deep neural models .",introduction,0,15,3,0,32
named-entity-recognition,6,"Word representations , also known as word embeddings , are a key element for multiple NLP tasks including NER .",introduction,0,16,4,0,20
named-entity-recognition,6,"Due to the small amount of named - entity annotated data , embeddings are used to extend , rather than replace , hand - crafted features in order to obtain state - of - the - art performance .",introduction,0,17,5,0,39
named-entity-recognition,6,Recent studies have explored methods for supplying deep sequential taggers with complementary features to standard embeddings .,introduction,0,18,6,0,17
named-entity-recognition,6,and tested special embeddings extracted from a neural language model ( LM ) trained on a large corpus .,introduction,0,19,7,0,19
named-entity-recognition,6,LM embeddings capture context - dependent aspects of word meaning using future ( forward LM ) and previous ( backward LM ) context words .,introduction,0,20,8,0,25
named-entity-recognition,6,"When this information is added to standard features , it leads to significant improvements in NER .",introduction,0,21,9,0,17
named-entity-recognition,6,"Also , showed that external knowledge resources ( namely gazetteers ) are crucial to NER performance .",introduction,0,22,10,0,17
named-entity-recognition,6,Gazetteer features encode the presence of word n-grams in predefined lists of NEs .,introduction,0,23,11,0,14
named-entity-recognition,6,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .",introduction,1,24,12,0,34
named-entity-recognition,6,"In a nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .",introduction,1,25,13,0,32
named-entity-recognition,6,"From this vector space , we compute for each word a 120 - dimensional vector , where each dimension encodes the similarity of the word with an entity type .",introduction,0,26,14,0,30
named-entity-recognition,6,"We call this vector an LS representation , for Lexical Similarity .",introduction,0,27,15,0,12
named-entity-recognition,6,"When included in a vanilla LSTM - CRF NER model , LS representations lead to significant gains .",introduction,0,28,16,0,18
named-entity-recognition,6,"We establish a new state - of - the - art F 1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance on the over - studied In the rest of this paper , we motivate our work in Section 2 .",introduction,0,29,17,0,50
named-entity-recognition,6,We describe how we compute LS vectors in Section 3 .,introduction,0,30,18,0,11
named-entity-recognition,6,"We present our system in Section 4 and report results in Section 5 . In Section 6 , we discuss related works before concluding in Section 7 .",introduction,0,31,19,0,28
named-entity-recognition,6,motivation,introduction,0,32,20,0,1
named-entity-recognition,6,Gazetteers are lists of entities thatare associated with specific NE categories .,introduction,0,33,21,0,12
named-entity-recognition,6,"They are widely used as a feature source in NER , and have been successfully included in feature - based models .",introduction,0,34,22,0,22
named-entity-recognition,6,"Typically , lists of entities are compiled from structured data sources such as DBpedia or Freebase .",introduction,0,35,23,0,17
named-entity-recognition,6,"The surface form of the title of a Wikipedia article , as well as aliases and redirects are mapped to an entity type using the object type attribute of the related DBpedia ( or Freebase ) page .",introduction,0,36,24,0,38
named-entity-recognition,6,"use this methodology to compile 30 lists of fine - grained entity types extracted from Wikipedia , while Chiu and Nichols ( 2016 ) create 4 gazetteers that map to CoNLL categories ( PER , LOC , ORG and MISC ) .",introduction,0,37,25,1,42
named-entity-recognition,6,"Despite their importance , gazetteer - based features suffer from a number of limitations .",introduction,0,38,26,0,15
named-entity-recognition,6,binary representation .,introduction,0,39,27,0,3
named-entity-recognition,6,Gazetteer features encode only the presence of an n-gram in each list and omit its relative frequency .,introduction,0,40,28,0,18
named-entity-recognition,6,"For example , the word "" France "" can be used as a person , an organization , or a location , while it likely refers to the country most of the time .",introduction,0,41,29,0,34
named-entity-recognition,6,Binary features can not capture this preference .,introduction,0,42,30,0,8
named-entity-recognition,6,generation .,introduction,0,43,31,0,2
named-entity-recognition,6,"At test time , we need to match every n-gram ( up to the length of the longest lexicon entry ) in a sentence against entries in the lexicons , which is time consuming .",introduction,0,44,32,0,35
named-entity-recognition,6,"In their work , Chiu and Nichols ( 2016 ) use 4 lists that count over 2.3 M entries .",introduction,0,45,33,1,20
named-entity-recognition,6,non-entity words .,introduction,0,46,34,0,3
named-entity-recognition,6,"Gazetteer features do not capture signal from non-entity words , while earlier feature - based models strived to encode that some words ( or n-grams ) trigger specific entity types .",introduction,0,47,35,0,31
named-entity-recognition,6,"For instance , words such as "" eat "" , "" directed "" or "" born "" are words that typically appear after a mention of type PER .",introduction,0,48,36,0,29
named-entity-recognition,6,"To overcome those limitations , we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words .",introduction,0,49,37,0,32
named-entity-recognition,6,This vector compactly and efficiently encodes both gazetteer and lexical information .,introduction,0,50,38,0,12
named-entity-recognition,6,"Note that at test time , we only have to feed our model with this feature vector , which is efficient .",introduction,0,51,39,0,22
named-entity-recognition,6,our method,method,0,52,1,0,2
named-entity-recognition,6,Embedding Words and Entity Types,method,0,53,2,0,5
named-entity-recognition,6,Turning Wikipedia into a corpus of named - entities annotated with types is a task that received continuous attention over the years .,method,0,54,3,0,23
named-entity-recognition,6,It consists mainly in exploiting the hyperlink structure of Wikipedia in order to detect entity mentions .,method,0,55,4,0,17
named-entity-recognition,6,"Then , structured data from a knowledge base ( for instance Freebase ) are used to map hyperlinks to entity types .",method,0,56,5,0,22
named-entity-recognition,6,"Because the number of anchored strings in Wikipedia is no more than 3 % of the text tokens , proposed to augment Wikipedia articles with mentions unmarked in Wikipedia , thanks to a mix of heuristics that benefit the Wikipedia structure , as well as a coreference resolution system adapted specifically to Wikipedia .",method,0,57,6,0,54
named-entity-recognition,6,"The authors applied their approach on English Wikipedia and produce coarse ( 4 classes ) and finegrained ( 120 labels ) named- entity annotations , leading to WiNER and WiFiNE .",method,0,58,7,0,31
named-entity-recognition,6,"In this work , we adopt WiFiNE which is publicly available at http://rali.iro.umontreal.ca/rali/en/wifiner-wikipedia-for-et as our source of annotations .",method,0,59,8,0,19
named-entity-recognition,6,Each entity mention is mapped ( via it s Freebase object type attribute ) to a pre-defined set of 120 entity types .,method,0,60,9,0,23
named-entity-recognition,6,Types are stored in a 2 - level hierarchical structure ( e.g. / person and / person / musician ) .,method,0,61,10,0,21
named-entity-recognition,6,"The corpus consist of 3.2 M Wikipedia articles , comprising 1.3G tokens that we annotated with 157.4 M named - entity mentions and their types .",method,0,62,11,0,26
named-entity-recognition,6,We used this very large quantity of automatically annotated data for jointly embedding words and entity types into the same low - dimensional space .,method,0,63,12,0,25
named-entity-recognition,6,The key idea consists in learning an embedding for each entity type using its surrounding words .,method,0,64,13,0,17
named-entity-recognition,6,"For instance , the embedding for / product / software will be trained using context words that surround all entities that were ( automatically ) labelled as / product / software in Wikipedia .",method,0,65,14,0,34
named-entity-recognition,6,"In practice , we found that simply concatenating a sentence ( v1 ) with its annotated version ( v 2 ) , as illustrated in , offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them .",method,0,66,15,0,48
named-entity-recognition,6,We use the FastText toolkit to learn the uncased embeddings for both words and entity types .,method,0,67,16,0,17
named-entity-recognition,6,"We train a skipgram model to learn 100 - dimensional vectors with a minimum word frequency cutoff of 5 , and a window size of 5 .",method,0,68,17,0,27
named-entity-recognition,6,This configuration ( recommended by the authors ) performs the best in the experiments described in Section 5 .,method,0,69,18,0,19
named-entity-recognition,6,"Since FastText learns representations of character ngrams , it has the ability to produce vectors for unknown words .",method,0,70,19,0,19
named-entity-recognition,6,"For visualization proposes , we only plot single - word mentions that were annotated in WiFiNE with one of those 6 types .",method,0,71,20,0,23
named-entity-recognition,6,Words were randomly and proportionally sampled according to the frequency of each entity type .,method,0,72,21,0,15
named-entity-recognition,6,"In addition , words have the color associated with the most frequent type they were annotated within WiFiNE .",method,0,73,22,0,19
named-entity-recognition,6,We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type .,method,0,74,23,0,21
named-entity-recognition,6,"For instance , "" firefox "" is close to the type / product / software , while "" enzyme "" is close to the / biology entity type .",method,0,75,24,0,29
named-entity-recognition,6,We also notice that words thatare labelled with different types tend to appear between types they were annotated with .,method,0,76,25,0,20
named-entity-recognition,6,"For instance , "" gpx2 "" , which is used both as a software and as a gene , has it s embedding in between / product / software and / biology .",method,0,77,26,0,33
named-entity-recognition,6,"We inspected some of the words plotted in , and found that "" jrun "" and "" xp "" are incorrectly labelled as / product / weapon in WiFiNE .",method,0,78,27,0,30
named-entity-recognition,6,"But since these words are seen in a software context , their embeddings are closer to the / product / software embedding than the / product / weapon one .",method,0,79,28,0,30
named-entity-recognition,6,"We feel this tolerance to noise is a desirable feature , one that hopefully allows a more efficient use of distant supervision .",method,0,80,29,0,23
named-entity-recognition,6,"Last , we also observe the tendency of rare words to cluster around their entity type .",method,0,81,30,0,17
named-entity-recognition,6,"For instance , "" iota "" and "" x.org "" are embedded near their respective types , despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE .",method,0,82,31,0,37
named-entity-recognition,6,ls representation,method,0,83,32,0,2
named-entity-recognition,6,"This joint vector space only serves the purpose of associating to each word a LS representation , that is , a 120 - dimensional vector where the ith coefficient is a value in the [ ? 1 , + 1 ] interval , equal to the cosine similarity 1 between the word embedding and the embedding of the ith entity type ( we have 120 types ) .",method,0,84,33,0,68
named-entity-recognition,6,word,method,0,85,34,0,1
named-entity-recognition,6,entity .,method,0,86,35,0,2
named-entity-recognition,6,shows the topmost similar entity types for proper names ( left column ) and common words ( right column ) .,method,0,87,36,0,21
named-entity-recognition,6,We observe that ambiguous mentions ( those annotated with several types ) are adequately handled .,method,0,88,37,0,16
named-entity-recognition,6,"For instance , the LS representation of the word "" hilton "" encodes that it more often refers to a hotel or a restaurant than to an actress .",method,0,89,38,0,29
named-entity-recognition,6,"Also , we observe that entity words thatare either not or rarely annotated in WiFiNE are still adequately associated with their right type .",method,0,90,39,0,24
named-entity-recognition,6,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .",method,0,91,40,0,39
named-entity-recognition,6,"Interestingly , this mention does not have its page in English Wikipedia .",method,0,92,41,0,13
named-entity-recognition,6,"Furthermore , we observe that non-entity context words have a strong similarity to types they precede or succeed .",method,0,93,42,0,19
named-entity-recognition,6,"For instance the verb "" directed "" is very close to / person / director , an entity type that usually precedes it , and to / art / film , that usually follows it .",method,0,94,43,0,36
named-entity-recognition,6,"Likewise , the preposition "" in "" is near / date and / location / city , which frequently follow "" in "" .",method,0,95,44,0,24
named-entity-recognition,6,Strength of the LS Representation,method,0,96,45,0,5
named-entity-recognition,6,"To summarize , we propose a compact lexical representation which is computed offline , therefore incurring no computation burden at test time",method,0,97,46,0,22
named-entity-recognition,6,"This representation encodes the preference of an entity - mention word for a given type , an information out of reach of binary gazetteer features .",method,0,98,47,0,26
named-entity-recognition,6,It also lends itself nicely to the inclusion of lexical features that have been successfully used in earlier feature - based systems .,method,0,99,48,0,23
named-entity-recognition,6,"Also , because entity types are well represented in WiFiNE , their embeddings are robust :",method,0,100,49,0,16
named-entity-recognition,6,Our representation does accommodate unfrequent words and seems tolerant to the inherent noise of distant supervision .,method,0,101,50,0,17
named-entity-recognition,6,our ner system,method,0,102,51,0,3
named-entity-recognition,6,"In order to test the efficiency of our lexical feature representation , we implemented a state - of - the - art NER system we now describe .",method,0,103,52,0,28
named-entity-recognition,6,bi-lstm- crf,method,0,104,53,0,2
named-entity-recognition,6,model,method,0,105,54,0,1
named-entity-recognition,6,"We adopt the popular Bi - LSTM - CRF architecture , a de facto baseline in many sequential tagging tasks .",method,0,106,55,0,21
named-entity-recognition,6,features,method,0,107,56,0,1
named-entity-recognition,6,"In addition to the LS vector , we incorporate publicly available pre-trained embeddings , as well as character - level , and capitalization features .",method,0,108,57,0,25
named-entity-recognition,6,Those features have been shown to be crucial for stateof - the - art performance .,method,0,109,58,0,16
named-entity-recognition,6,word embeddings,method,0,110,59,0,2
named-entity-recognition,6,"We experimented with several publicly available word embeddings , such as Senna , Word2 Vec , GloVe , and SSKIP .",method,0,111,60,0,21
named-entity-recognition,6,We find that the latter performs the best in our experiments .,method,0,112,61,0,12
named-entity-recognition,6,SSKIP embeddings are 100 - dimensional case sensitive vectors that where trained using a n-skip - gram model on 42B tokens .,method,0,113,62,0,22
named-entity-recognition,6,"These embeddings were previously used by , who report good performance on CONLL , and state - of - the - art results on ONTONOTES respectively .",method,0,114,63,0,27
named-entity-recognition,6,Note that these pre-trained embeddings are adjusted during training .,method,0,115,64,0,10
named-entity-recognition,6,character embeddings,method,0,116,65,0,2
named-entity-recognition,6,"Following , we use a forward and a backward LSTM to derive a representation of each word from its characters ( right part of .",method,0,117,66,0,25
named-entity-recognition,6,"A character lookup table is randomly initialized , then trained at the same time as the Bi - LSTM model sketched in Section 4.1 .",method,0,118,67,0,25
named-entity-recognition,6,capitalization features,method,0,119,68,0,2
named-entity-recognition,6,"Similarly to previous works , we use capitalization features for characterizing certain categories of capitalization patterns : all Upper , allLower , upperFirst , upperNotFirst , numeric or noAlphaNum .",method,0,120,69,0,30
named-entity-recognition,6,"We define a random lookup table for these features , and learn its parameters during training .",method,0,121,70,0,17
named-entity-recognition,6,ls vectors,method,0,122,71,0,2
named-entity-recognition,6,"Contrarily to previous features , lexical vectors are computed offline and are not adjusted during training .",method,0,123,72,0,17
named-entity-recognition,6,"We found useful in practice to apply a MinMax scaler in the range [ ? 1 , + 1 ] to each LS vector we computed ; thus , [.. , 0.095 , .. , 0.20 , .. , 0.76 , .. ] becomes [.. , ? 1 , .. , ? 0.67 , .. , 1 , ..].",method,0,124,73,0,59
named-entity-recognition,6,experiments,experiment,0,125,1,0,1
named-entity-recognition,6,data and evaluation,experiment,0,126,2,0,3
named-entity-recognition,6,We consider two well - established NER benchmarks :,experiment,0,127,3,0,9
named-entity-recognition,6,CONLL-2003 and ONTONOTES 5.0 . provides an overview of the two datasets .,experiment,0,128,4,0,13
named-entity-recognition,6,"As we can see , ONTONOTES is much larger .",experiment,0,129,5,0,10
named-entity-recognition,6,"For both datasets , we convert the IOB encoding to BILOU , since found the latter to perform better .",experiment,0,130,6,0,20
named-entity-recognition,6,"In keeping with others , we report mention - level F 1 score using the conlleval script 2 .",experiment,0,131,7,0,19
named-entity-recognition,6,The ) is a well known collection of Reuters newswire articles that contains a large portion of sports news .,experiment,0,132,8,0,20
named-entity-recognition,6,"It is annotated with four entity types : Person ( PER ) , Location ( LOC ) , Organization ( ORG ) and Miscellaneous ( MISC ) .",experiment,0,133,9,0,28
named-entity-recognition,6,"The four entity types are fairly evenly distributed , and the train / dev / test datasets present a similar type distribution. , magazine ( 120 k ) , newswire ( 625 k ) , and web data ( 300 k ) .",experiment,0,134,10,0,43
named-entity-recognition,6,"This dataset is annotated with 18 entity types , and is much larger than CONLL .",experiment,0,135,11,0,16
named-entity-recognition,6,"Following previous researches , we use the official train / dev / test split of the CoNLL - 2012 shared task .",experiment,0,136,12,0,22
named-entity-recognition,6,"Also , we exclude ( both during training and testing ) the New Testaments portion as it does not contain gold NE annotations .",experiment,0,137,13,0,24
named-entity-recognition,6,training and implementation,experiment,0,138,14,0,3
named-entity-recognition,6,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,experiment,1,139,15,0,24
named-entity-recognition,6,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .",experiment,1,140,16,0,21
named-entity-recognition,6,"More sophisticated optimization algorithms such as AdaDelta or Adam ( Kingma and Ba , 2014 ) converge faster , but none outperformed SGD with exponential learning rate decay in our experiments .",experiment,0,141,17,1,32
named-entity-recognition,6,Our system uses a single Bi - LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively .,experiment,0,142,18,0,28
named-entity-recognition,6,"For both models , the character embedding size was set to 25 , and the hidden dimension of the forward and backward character LSTMs are set to 50 .",experiment,0,143,19,0,29
named-entity-recognition,6,"To mitigate overfitting , we apply a dropout mask with a probability of 0.5 on the input and output vectors of the Bi - LSTM layer .",experiment,0,144,20,0,27
named-entity-recognition,6,"For both datasets , we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs .",experiment,0,145,21,0,22
named-entity-recognition,6,"We tuned the hyper - parameters by grid search , and used early stopping based on the performance on the development set .",experiment,0,146,22,0,23
named-entity-recognition,6,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .",experiment,1,147,23,0,44
named-entity-recognition,6,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .",experiment,1,148,24,0,21
named-entity-recognition,6,Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES .,experiment,0,149,25,0,13
named-entity-recognition,6,shows the development set performance of our final models on each dataset compared to the work of .,experiment,0,150,26,0,18
named-entity-recognition,6,"The authors use an architecture similar to ours , but use a binary gazetteer feature set , while we use our LS representation .",experiment,0,151,27,0,24
named-entity-recognition,6,"Since our systems involve random initialization , we report the mean as well as the standard deviation over five runs .",experiment,0,152,28,0,21
named-entity-recognition,6,"The improvements yielded by our model on the CONLL dataset are significant although modest , while those observed on ONTONOTES are more substantial .",experiment,0,153,29,0,24
named-entity-recognition,6,We also observe a lower variance of our system over the 5 runs .,experiment,0,154,30,0,14
named-entity-recognition,6,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .",experiment,1,155,31,0,51
named-entity-recognition,6,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .",experiment,1,156,32,0,31
named-entity-recognition,6,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .",experiment,1,157,33,0,26
named-entity-recognition,6,use three layers of stacked residual RNN ( Bi - LSTM ) with bias decoding .,experiment,0,158,34,0,16
named-entity-recognition,6,Our model is much simpler and faster .,experiment,0,159,35,0,8
named-entity-recognition,6,They report a performance of 90.43 when using an architecture similar to ours .,experiment,0,160,36,0,14
named-entity-recognition,6,The two systems that have slightly higher F 1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark .,experiment,0,161,37,0,34
named-entity-recognition,6,"They report gains between 0.8 and 1.2 points by using such LM embeddings , which suggests that LS vectors are indeed efficient .",experiment,0,162,38,0,23
named-entity-recognition,6,"Unfortunately , due to time and resource constraints , 4 we were notable to measure whether both features complement each other .",experiment,0,163,39,0,22
named-entity-recognition,6,This is left for future investigations .,experiment,0,164,40,0,7
named-entity-recognition,6,reports the F 1 score of our system compared to the performance reported by others on the ONTONOTES test set .,experiment,0,165,41,0,21
named-entity-recognition,6,"To the best of our knowledge , we surpass previously reported F 1 scores on this dataset .",experiment,0,166,42,0,18
named-entity-recognition,6,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .",experiment,1,167,43,1,36
named-entity-recognition,6,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .",experiment,1,168,44,0,40
named-entity-recognition,6,Results on the Development Set,experiment,0,169,45,0,5
named-entity-recognition,6,conll,experiment,0,170,46,0,1
named-entity-recognition,6,results on conll,result,1,171,1,0,3
named-entity-recognition,6,results on ontonotes,result,1,172,1,0,3
named-entity-recognition,6,model,result,0,173,2,0,1
named-entity-recognition,6,We also observe that models that use both feature sets significantly outperform other configurations .,result,1,174,3,0,15
named-entity-recognition,6,"To confirm that the gains came from our feature vector and not from increasing the number of hidden units , we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors .",result,0,175,4,0,47
named-entity-recognition,6,"We observed a degradation of performance on both datasets , mostly due to overfitting on the training set .",result,0,176,5,0,19
named-entity-recognition,6,"From those results , we conclude that our lexical representation and the SSKIP one are complementary .",result,0,177,6,0,17
named-entity-recognition,6,ablation results,result,0,178,1,0,2
named-entity-recognition,6,"In this experiment , we directly compare the LS representation with the SSKIP word - embedding feature set .",result,0,179,2,0,19
named-entity-recognition,6,"In order to maintain a high level of performance , both character and capitalization features are used in all configurations .",result,0,180,3,0,21
named-entity-recognition,6,"We want to point out that LS vectors are not adapted during training , contrarily to the SSKIP embeddings .",result,0,181,4,0,20
named-entity-recognition,6,"Similarly to Section 5.3 , we report in , for each feature configuration , the average F 1 score as well as the standard deviation over five runs .",result,0,182,5,0,29
named-entity-recognition,6,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .",result,1,183,6,0,24
named-entity-recognition,6,"The difference is not has high as we first expected , especially since the SSKIP model is adjusted during training , while our representation is not .",result,0,184,7,0,27
named-entity-recognition,6,"Still , LS vectors seem to encode a large portion of the information needed to model the NER task .",result,0,185,8,0,20
named-entity-recognition,6,"Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",result,0,186,9,0,20
named-entity-recognition,6,related works,related work,0,187,1,0,2
named-entity-recognition,6,"Traditional approaches to NER , like CRF - based and Perceptron - based systems ( Ratinov and Roth , 2009 ) have dominated the field for over a decade .",related work,0,188,2,1,30
named-entity-recognition,6,They rely heavily on hand - engineered features and external resources such as gazetteers .,related work,0,189,3,0,15
named-entity-recognition,6,One major drawback of such an approach is its weak generalization power .,related work,0,190,4,0,13
named-entity-recognition,6,"Current state - of - the art systems use a combination of Convolutional Neural Networks ( CNNs ) , Bi - LSTMs , along with a CRF decoder .",related work,0,191,5,0,29
named-entity-recognition,6,"CNNs are used to encode character - level features ( prefix and suffix ) , while LSTM is used to encode word - level features .",related work,0,192,6,0,26
named-entity-recognition,6,"Finally , a CRF is placed on top of those models in order to decode the best tag sequence .",related work,0,193,7,0,20
named-entity-recognition,6,Pre-trained embeddings obtained by unsupervised learning are core features of those models .,related work,0,194,8,0,13
named-entity-recognition,6,"In this work , we show that deep NN architectures can also benefit from lexical features , at least when encoded in the compact form we propose .",related work,0,195,9,0,28
named-entity-recognition,6,and propose an alternative approach different from ours .,related work,0,196,10,0,9
named-entity-recognition,6,They incorporate LM embeddings that were pre-trained on a large unlabelled corpus as features for NER .,related work,0,197,11,0,17
named-entity-recognition,6,These embeddings allow to generate a representation for a word depending on its context .,related work,0,198,12,0,15
named-entity-recognition,6,"For instance , the LM embeddings of the word France in "" France is a developed country "" is different than that in "" Anatole France began his literary career "" .",related work,0,199,13,0,32
named-entity-recognition,6,Such embeddings are trained on very large amount of texts .,related work,0,200,14,0,11
named-entity-recognition,6,"Our feature set is crafted from distant supervision applied to Wikipedia , a much less time - consuming process which we showed to be nevertheless adapted to rare words .",related work,0,201,15,0,30
named-entity-recognition,6,Chiu and Nichols ( 2016 ) used gazetteer features in order to establish state - of - the - art performance on both CONLL and ONTONOTES .,related work,0,202,16,1,27
named-entity-recognition,6,They mined DBPedia in order to compile 4 lists of named - entities that contain over 2.3 M entries .,related work,0,203,17,0,20
named-entity-recognition,6,We show that LS representations outperform their gazetteer features .,related work,0,204,18,0,10
named-entity-recognition,6,conclusion and future work,related work,0,205,19,0,4
named-entity-recognition,6,We have explored the idea of generating lexical features for NER out of Wikipedia data automatically annotated with fine - grained entity types .,related work,0,206,20,0,24
named-entity-recognition,6,"We used WiFiNE , a Wikipedia dump annotated with fine entity type mentions , for training a vector space that jointly embeds words and named - entities .",related work,0,207,21,0,28
named-entity-recognition,6,"This vector space is used to compute a 120 dimensional vector per word , which encodes the similarity of the word to each of the entity types .",related work,0,208,22,0,28
named-entity-recognition,6,"Our results show that our proposed lexical representation , even though it is not adjusted at training time , matches state - of - the - art results compared to more complex approaches on the well - studied CONLL dataset , and delivers a new state - of - the - art F 1 score of 87.95 on the more diversified ONTONOTES dataset .",related work,0,209,23,0,64
named-entity-recognition,6,We further observe larger gains on collections with more unfrequent words .,related work,0,210,24,0,12
named-entity-recognition,6,"The source code and the data we used in this work are publicly available at http://rali.iro. umontreal.ca/rali/en/wikipedia-lex-sim , with the hope that other researchers will report gains , when using our lexical representation .",related work,0,211,25,0,34
named-entity-recognition,6,"As a future work , we want to investigate the usefulness of our LS feature representation on other NER tasks , including NER in tweets where out - of - vocabulary and low - frequency words represent a challenge ; as well as finer - grained NER which suffers from the lack of manually annotated training data .",related work,0,212,26,0,58
named-entity-recognition,2,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,title,1,2,1,0,9
named-entity-recognition,2,abstract,abstract,0,3,1,0,1
named-entity-recognition,2,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs .",abstract,0,4,2,0,28
named-entity-recognition,2,Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .,abstract,0,5,3,0,44
named-entity-recognition,2,"Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency .",abstract,0,6,4,0,19
named-entity-recognition,2,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .",abstract,1,7,5,0,38
named-entity-recognition,2,"Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents .",abstract,0,8,6,0,37
named-entity-recognition,2,"We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF .",abstract,0,9,7,0,34
named-entity-recognition,2,"Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",abstract,0,10,8,0,26
named-entity-recognition,2,introduction,introduction,0,11,1,0,1
named-entity-recognition,2,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .",introduction,1,12,2,0,45
named-entity-recognition,2,Speed is not sufficient of course : they must also be expressive enough to tolerate the tremendous lexical variation in input data .,introduction,0,13,3,0,23
named-entity-recognition,2,The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling .,introduction,0,14,4,0,22
named-entity-recognition,2,"While these models are expressive and accurate , they fail to fully exploit the parallelism opportunities of a GPU , and thus their speed is limited .",introduction,0,15,5,0,27
named-entity-recognition,2,"Specifically , they employ either recurrent neural networks ( RNNs ) for feature extraction , or Viterbi inference in a structured output model , both of which require sequential computation across the length of the input .",introduction,0,16,6,0,37
named-entity-recognition,2,"Instead , parallelized runtime independent of the length of the sequence saves time and energy costs , maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models .",introduction,0,17,7,0,35
named-entity-recognition,2,Convolutional neural networks ( CNNs ) provide exactly this property .,introduction,0,18,8,0,11
named-entity-recognition,2,"Rather than composing representations incrementally over each token in a sequence , they apply filters in parallel across the entire sequence at once .",introduction,0,19,9,0,24
named-entity-recognition,2,"Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .",introduction,0,20,10,0,27
named-entity-recognition,2,"This provides , for example , audio generation models that can be trained in parallel ( van den .",introduction,0,21,11,0,19
named-entity-recognition,2,"Despite the clear computational advantages of CNNs , RNNs have become the standard method for composing deep representations of text .",introduction,0,22,12,0,21
named-entity-recognition,2,"This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence , but the CNN 's representation is limited by the effective input width 1 of the network : the size of the input context which is observed , directly or indirectly , by the representation of a token at a given layer in the network .",introduction,0,23,13,0,64
named-entity-recognition,2,"Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w ?",introduction,0,24,14,0,45
named-entity-recognition,2,1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .,introduction,0,25,15,0,25
named-entity-recognition,2,"To avoid this scaling , one could pool representations across the sequence , but this is not appropriate for sequence labeling , since it reduces the output resolution of the representation .",introduction,0,26,16,0,32
named-entity-recognition,2,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .",introduction,1,27,17,0,16
named-entity-recognition,2,"For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .",introduction,0,28,18,0,33
named-entity-recognition,2,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",introduction,1,29,19,0,41
named-entity-recognition,2,"By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers :",introduction,1,30,20,0,36
named-entity-recognition,2,The size of the effective input width for a token at layer l is now given by 2 l +1 ?1 .,introduction,0,31,21,0,22
named-entity-recognition,2,"More concretely , just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .",introduction,0,32,22,0,37
named-entity-recognition,2,Our over all iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,introduction,1,33,23,0,26
named-entity-recognition,2,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,introduction,1,34,24,0,19
named-entity-recognition,2,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .",introduction,1,35,25,0,43
named-entity-recognition,2,"In experiments on CoNLL 2003 and OntoNotes 1 What we call effective input width here is known as the receptive field in the vision literature , drawing an analogy to the visual receptive field of a neuron in the retina . :",introduction,0,36,26,0,42
named-entity-recognition,2,A dilated CNN block with maximum dilation width 4 and filter width 3 .,introduction,0,37,27,0,14
named-entity-recognition,2,Neurons contributing to a single highlighted neuron in the last layer are also highlighted .,introduction,0,38,28,0,15
named-entity-recognition,2,"5.0 English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance .",introduction,0,39,29,0,25
named-entity-recognition,2,"When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .",introduction,0,40,30,0,46
named-entity-recognition,2,"As an extractor of per-token logits for a CRF , our model out - performs the Bi - LSTM - CRF .",introduction,0,41,31,0,22
named-entity-recognition,2,"We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8 faster .",introduction,0,42,32,0,30
named-entity-recognition,2,The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .,introduction,0,43,33,0,38
named-entity-recognition,2,2 2 background,introduction,0,44,34,0,3
named-entity-recognition,2,conditional probability,introduction,0,45,35,0,2
named-entity-recognition,2,models for tagging,introduction,0,46,36,0,3
named-entity-recognition,2,"Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per-token output tags .",introduction,0,47,37,0,37
named-entity-recognition,2,Let D be the domain size of each y i .,introduction,0,48,38,0,11
named-entity-recognition,2,"We predict the most likely y , given a conditional model P ( y|x ) .",introduction,0,49,39,0,16
named-entity-recognition,2,This paper considers two factorizations of the conditional distribution .,introduction,0,50,40,0,10
named-entity-recognition,2,"first , we have",introduction,0,51,41,0,4
named-entity-recognition,2,where the tags are conditionally independent given some features for x .,introduction,0,52,42,0,12
named-entity-recognition,2,"Given these features , O ( D ) prediction is simple and parallelizable across the length of the sequence .",introduction,0,53,43,0,20
named-entity-recognition,2,"However , feature extraction may not necessarily be parallelizable .",introduction,0,54,44,0,10
named-entity-recognition,2,"For example , RNN - based features require iterative passes along the length of x .",introduction,0,55,45,0,16
named-entity-recognition,2,We also consider a linear - chain CRF model that couples all of y together :,introduction,0,56,46,0,16
named-entity-recognition,2,where ?,introduction,0,57,47,0,2
named-entity-recognition,2,"t is a local factor , ?",introduction,0,58,48,0,7
named-entity-recognition,2,"p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",introduction,0,59,49,0,18
named-entity-recognition,2,"to avoid overfitting , ?",introduction,0,60,50,0,5
named-entity-recognition,2,p does not depend on the timestep tor the input x in our experiments .,introduction,0,61,51,0,15
named-entity-recognition,2,Prediction in this model requires global search using the O ( D 2 T ),introduction,0,62,52,0,15
named-entity-recognition,2,viterbi algorithm .,introduction,0,63,53,0,3
named-entity-recognition,2,"CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step .",introduction,0,64,54,0,26
named-entity-recognition,2,The suitability of such compilation depends on the properties and quantity of the data .,introduction,0,65,55,0,15
named-entity-recognition,2,"While CRF prediction requires non-trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging , will always be satisfied .",introduction,0,66,56,0,29
named-entity-recognition,2,"It may also have better sample complexity , as it imposes more prior knowledge about the structure of the interactions among the tags .",introduction,0,67,57,0,24
named-entity-recognition,2,"However , it has worse computational complexity than independent prediction .",introduction,0,68,58,0,11
named-entity-recognition,2,dilated convolutions,introduction,0,69,59,0,2
named-entity-recognition,2,"CNNs in NLP are typically one - dimensional , applied to a sequence of vectors representing tokens rather than to a two -dimensional grid of vectors representing pixels .",introduction,0,70,60,0,29
named-entity-recognition,2,"In this setting , a convolutional neural network layer is equivalent to applying an affine transformation , W c to a sliding window of width r tokens on either side of each token in the sequence .",introduction,0,71,61,0,37
named-entity-recognition,2,"Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations .",introduction,0,72,62,0,19
named-entity-recognition,2,The convolutional operator applied to each token x t with output ct is defined as :,introduction,0,73,63,0,16
named-entity-recognition,2,where ?,introduction,0,74,64,0,2
named-entity-recognition,2,is vector concatenation .,introduction,0,75,65,0,4
named-entity-recognition,2,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ?",introduction,0,76,66,0,30
named-entity-recognition,2,"inputs at a time , where ?",introduction,0,77,67,0,7
named-entity-recognition,2,is the dilation width .,introduction,0,78,68,0,5
named-entity-recognition,2,We define the dilated convolution operator :,introduction,0,79,69,0,7
named-entity-recognition,2,A dilated convolution of width 1 is equivalent to a simple convolution .,introduction,0,80,70,0,13
named-entity-recognition,2,"Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the ? > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .",introduction,0,81,71,0,44
named-entity-recognition,2,multi - scale context aggregation,introduction,0,82,72,0,5
named-entity-recognition,2,We can leverage the ability of dilated convolutions to incorporate global context without losing important local information by stacking dilated convolutions of increasing width .,introduction,0,83,73,0,25
named-entity-recognition,2,"First described for pixel classification in computer vision , achieve state - of - the - art results on image segmentation benchmarks by stacking dilated convolutions with exponentially increasing rates of dilation , a technique they refer to as multiscale context aggregation .",introduction,0,84,74,0,43
named-entity-recognition,2,"By feeding the outputs of each dilated convolution as the input to the next , increasingly non-local information is incorporated into each pixel 's representation .",introduction,0,85,75,0,26
named-entity-recognition,2,Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .,introduction,0,86,76,0,25
named-entity-recognition,2,"By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .",introduction,0,87,77,0,46
named-entity-recognition,2,"Unfortunately , simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments .",introduction,0,88,78,0,17
named-entity-recognition,2,"In response , we present Iterated Dilated CNNs ( ID - CNNs ) , which instead apply the same small stack of dilated convolutions multiple times , each iterate taking as input the result of the last application .",introduction,0,89,79,0,39
named-entity-recognition,2,Repeatedly employing the same parameters in a recurrent fashion provides both broad effective input width and desirable generalization capabilities .,introduction,0,90,80,0,20
named-entity-recognition,2,"We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .",introduction,0,91,81,0,31
named-entity-recognition,2,model architecture,introduction,0,92,82,0,2
named-entity-recognition,2,"The network takes as input a sequence of T vectors x t , and outputs a sequence of per-class scores ht , which serve either as the local conditional distributions of Eqn. ( 1 ) or the local factors ?",introduction,0,93,83,0,40
named-entity-recognition,2,t of eqn..,introduction,0,94,84,0,3
named-entity-recognition,2,We denote the jth dilated convolutional layer of dilation width ? as D ( j ) ? .,introduction,0,95,85,0,18
named-entity-recognition,2,The first layer in the network is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation it :,introduction,0,96,86,0,26
named-entity-recognition,2,"Next , L c layers of dilated convolutions of exponentially increasing dilation width are applied to it , folding in increasingly broader context into the embedded representation of x tat each layer .",introduction,0,97,87,0,33
named-entity-recognition,2,Let r ( ) denote the ReLU activation function .,introduction,0,98,88,0,10
named-entity-recognition,2,Beginning with ct ( 0 ) = it we define the stack of layers with the following recurrence :,introduction,0,99,89,0,19
named-entity-recognition,2,and add a final dilation - 1 layer to the stack :,introduction,0,100,90,0,12
named-entity-recognition,2,"We refer to this stack of dilated convolutions as a block B ( ) , which has output resolution equal to its input resolution .",introduction,0,101,91,0,25
named-entity-recognition,2,"To incorporate even broader context without over - fitting , we avoid making B deeper , and instead iteratively apply B L b times , introducing no extra parameters .",introduction,0,102,92,0,30
named-entity-recognition,2,starting,introduction,0,103,93,0,1
named-entity-recognition,2,We apply a simple affine transformation W o to this final representation to obtain per-class scores for each token x t :,introduction,0,104,94,0,22
named-entity-recognition,2,training,introduction,0,105,95,0,1
named-entity-recognition,2,"Our main focus is to apply the ID - CNN an encoder to produce per-token logits for the first conditional model described in Sec. 2.1 , where tags are conditionally independent given deep features , since this will enable prediction that is parallelizable across the length of the input sequence .",introduction,0,106,96,0,51
named-entity-recognition,2,"Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn. :",introduction,0,107,97,0,33
named-entity-recognition,2,"We can also use the ID - CNN as logits for the CRF model ( Eqn. ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .",introduction,0,108,98,0,37
named-entity-recognition,2,We next present an alternative training method that helps bridge the gap between these two techniques .,introduction,0,109,99,0,17
named-entity-recognition,2,Sec. 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .,introduction,0,110,100,0,23
named-entity-recognition,2,"In response , we compile some of this reasoning in output space into ID - CNN feature extraction .",introduction,0,111,101,0,19
named-entity-recognition,2,"Instead of explicit reasoning over output labels during inference , we train the network such that each block is predictive of output labels .",introduction,0,112,102,0,24
named-entity-recognition,2,"Subsequent blocks learn to correct dependency violations of their predecessors , refining the final sequence prediction .",introduction,0,113,103,0,17
named-entity-recognition,2,"To do so , we first define predictions of the model after each of the L b applications of the block .",introduction,0,114,104,0,22
named-entity-recognition,2,"Let ht ( k ) be the result of applying the matrix W o from ( 9 ) to b t ( k ) , the output of block k.",introduction,0,115,105,0,30
named-entity-recognition,2,We minimize the average of the losses for each application of the block :,introduction,0,116,106,0,14
named-entity-recognition,2,"By rewarding accurate predictions after each application of the block , we learn a model where later blocks are used to refine initial predictions .",introduction,0,117,107,0,25
named-entity-recognition,2,The loss also helps reduce the vanishing gradient problem for deep architectures .,introduction,0,118,108,0,13
named-entity-recognition,2,"Such an approach has been applied in a variety of contexts for training very deep networks in computer vision , but not to our knowledge in NLP .",introduction,0,119,109,0,28
named-entity-recognition,2,We apply dropout to the raw inputs x t and to each block 's output b t ( b ) to help prevent overfitting .,introduction,0,120,110,0,25
named-entity-recognition,2,The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used at test time .,introduction,0,121,111,0,30
named-entity-recognition,2,"present dropout with expectationlinear regularization , which explicitly regularizes these two predictors to behave similarly .",introduction,0,122,112,0,16
named-entity-recognition,2,All of our best reported results include such regularization .,introduction,0,123,113,0,10
named-entity-recognition,2,"This is the first investigation of the technique 's effectiveness for NLP , including for RNNs .",introduction,0,124,114,0,17
named-entity-recognition,2,We encourage its further application .,introduction,0,125,115,0,6
named-entity-recognition,2,related work,related work,0,126,1,0,2
named-entity-recognition,2,"The state - of - the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain - structured graphical model , or approximates this search with a beam .",related work,0,127,2,0,39
named-entity-recognition,2,"These outperform similar systems that use the same features , but independent local predictions .",related work,0,128,3,0,15
named-entity-recognition,2,"On the other hand , the greedy sequential prediction ) approach of , which employs lexicalized features , gazetteers , and word clusters , outperforms CRFs with similar features .",related work,0,129,4,0,30
named-entity-recognition,2,LSTMs were used for NER as early as the CoNLL shared task in 2003 .,related work,0,130,5,0,15
named-entity-recognition,2,"More recently , a wide variety of neural network architectures for NER have been proposed .",related work,0,131,6,0,16
named-entity-recognition,2,"employ a one - layer CNN with pre-trained word embeddings , capitalization and lexicon features , and CRF - based prediction .",related work,0,132,7,0,22
named-entity-recognition,2,"achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF. proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .",related work,0,133,8,0,66
named-entity-recognition,2,Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .,related work,0,134,9,0,25
named-entity-recognition,2,"use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - of the - art performance on part - of - speech tagging and CoNLL NER without lexicons .",related work,0,135,10,0,37
named-entity-recognition,2,"Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .",related work,0,136,11,1,28
named-entity-recognition,2,use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .,related work,0,137,12,0,21
named-entity-recognition,2,"In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre-training of distributed word representations .",related work,0,138,13,0,27
named-entity-recognition,2,"Though our models would also likely benefit from additional features such as character representations and lexicons , we focus on simpler models which use word - embeddings alone , leaving more elaborate input representations to future work .",related work,0,139,14,0,38
named-entity-recognition,2,"In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .",related work,0,140,15,0,20
named-entity-recognition,2,"Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim Our work draws on the use of dilated convolutions for image segmentation in the computer vision community .",related work,0,141,16,0,49
named-entity-recognition,2,"Similar to our block , Yu and Koltun ( 2016 ) employ a context - module of stacked dilated convolutions of exponentially increasing dilation width .",related work,0,142,17,1,26
named-entity-recognition,2,"Dilated convolutions were recently applied to the task of speech generation , and concurrent with this work , posted a pre-print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .",related work,0,143,18,0,40
named-entity-recognition,2,"Our basic model architecture is similar to that of the ByteNet encoder , except that the inputs to our model are tokens and not bytes .",related work,0,144,19,0,26
named-entity-recognition,2,"Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .",related work,0,145,20,0,24
named-entity-recognition,2,We are the first to use dilated convolutions for sequence labeling .,related work,0,146,21,0,12
named-entity-recognition,2,The broad effective input width of the ID - CNN helps aggregate document - level context .,related work,0,147,22,0,17
named-entity-recognition,2,"incorporate document context in their greedy model by adding features based on tagged entities within a large , fixed window of tokens .",related work,0,148,23,0,23
named-entity-recognition,2,Prior work has also posed a structured model that couples predictions across the whole document .,related work,0,149,24,0,16
named-entity-recognition,2,experimental results,experiment,0,150,1,0,2
named-entity-recognition,2,We describe experiments on two benchmark English named entity recognition datasets .,experiment,0,151,2,0,12
named-entity-recognition,2,"On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per-token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .",experiment,0,152,3,0,61
named-entity-recognition,2,"We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .",experiment,0,153,4,0,55
named-entity-recognition,2,data and evaluation,experiment,0,154,5,0,3
named-entity-recognition,2,We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong and OntoNotes 5.0 .,experiment,0,155,6,0,18
named-entity-recognition,2,"Following previous work , we use the same OntoNotes data split used for co-reference resolution in the CoNLL - 2012 shared task .",experiment,0,156,7,0,23
named-entity-recognition,2,"For both datasets , we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance .",experiment,0,157,8,0,24
named-entity-recognition,2,As in previous work we evaluate the performance of our models using segment - level micro -averaged F1 score .,experiment,0,158,9,0,20
named-entity-recognition,2,Hyperparameters that resulted in the best performance on the validation set were selected via grid search .,experiment,0,159,10,0,17
named-entity-recognition,2,"A more detailed description of the data , evaluation , optimization and data pre-processing can be found in the Appendix .",experiment,0,160,11,0,21
named-entity-recognition,2,baselines,experiment,0,161,12,0,1
named-entity-recognition,2,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .",experiment,1,162,13,0,34
named-entity-recognition,2,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,experiment,1,163,14,0,73
named-entity-recognition,2,"We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .",experiment,0,164,15,0,49
named-entity-recognition,2,We do not compare with deeper or more elaborate CNN architectures for a number of reasons :,experiment,0,165,16,0,17
named-entity-recognition,2,"1 ) Fast train and test performance are highly desirable for NLP practitioners , and deeper models require more computation time 2 ) more complicated models tend to over - fit on this relatively small dataset and 3 ) most accurate deep CNN architectures repeatedly up - sample and down - sample the inputs .",experiment,0,166,17,0,55
named-entity-recognition,2,We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .,experiment,0,167,18,0,24
named-entity-recognition,2,"Since our task is sequence labeling , we desire a model that maintains the token - level resolution of the input , making dilated convolutions an elegant solution .",experiment,0,168,19,0,29
named-entity-recognition,2,6.3 CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,experiment,1,169,20,0,28
named-entity-recognition,2,"For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .",experiment,0,170,21,0,20
named-entity-recognition,2,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .",experiment,1,171,22,0,68
named-entity-recognition,2,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .",experiment,1,172,23,0,51
named-entity-recognition,2,"All CNN models out - perform the Bi-Model F1 86.96 90.33 Bi-LSTM 89.34 0.28 4 - layer CNN 89.97 0.20 5 - layer CNN 90.23 0.16 ID- CNN 90.32 0.26 88.67 90.05 90.20 Bi-LSTM-CRF ( re-impl ) 90.43 0.12 ID-CNN- CRF 90.54 0.18 LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .",experiment,0,173,24,0,66
named-entity-recognition,2,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .",experiment,1,174,25,0,35
named-entity-recognition,2,Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .,experiment,0,175,26,0,22
named-entity-recognition,2,"lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .",experiment,0,176,27,0,19
named-entity-recognition,2,We report decoding times using the fastest batch size for each method .,experiment,0,177,28,0,13
named-entity-recognition,2,The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .,experiment,0,178,29,0,16
named-entity-recognition,2,"With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .",experiment,0,179,30,0,32
named-entity-recognition,2,"The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF at test time , with comparable accuracy .",experiment,0,180,31,0,46
named-entity-recognition,2,"The 5 - layer CNN , which observes the same effective input width as the ID - CNN but with more parameters , performs at about the same speed as the ID - CNN in our experiments .",experiment,0,181,32,0,38
named-entity-recognition,2,"With a better implementation of dilated convolutions than currently included in TensorFlow , we would expect the ID - CNN to be notably faster than the 5 - layer CNN .",experiment,0,182,33,0,31
named-entity-recognition,2,"We emphasize the importance of the dropout regularizer of in , where we observe increased F1 for every model trained with expectation - linear dropout regularization .",experiment,0,183,34,0,27
named-entity-recognition,2,"Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as .",experiment,0,184,35,0,21
named-entity-recognition,2,We recommend this regularizer as a simple and helpful tool for practitioners training neural networks for NLP .,experiment,0,185,36,0,18
named-entity-recognition,2,document - level prediction,experiment,1,186,37,0,4
named-entity-recognition,2,In we show that adding document - level context improves every model on CoNLL - 2003 .,experiment,1,187,38,0,17
named-entity-recognition,2,"Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 .",experiment,0,188,39,0,19
named-entity-recognition,2,"We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1,000 tokens long such as entire documents .",experiment,0,189,40,0,79
named-entity-recognition,2,We also note that our combination of training objective ( Eqn. 11 ) and tied parameters ( Eqn. : Comparing ID - CNNs with 1 ) backpropagating loss only from the final layer ( 1 - loss ) and 2 ) untied parameters across blocks ( noshare ) 8 ) more effectively learns to aggregate this broad context than a vanilla cross - entropy loss or deep CNN back - propagated from the final neural network layer .,experiment,0,190,41,0,78
named-entity-recognition,2,compares models trained to incorporate entire document context using the document baselines described in Section 6.2 .,experiment,0,191,42,0,17
named-entity-recognition,2,"In we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .",experiment,0,192,43,0,43
named-entity-recognition,2,"On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .",experiment,0,193,44,0,46
named-entity-recognition,2,OntoNotes 5.0 English NER,experiment,1,194,45,0,4
named-entity-recognition,2,We observe similar patterns on OntoNotes as we do on CoNLL. lists over all F 1 scores of our models compared to those in the existing literature .,experiment,0,195,46,0,28
named-entity-recognition,2,The greedy Bi - LSTM out - performs the lex -,experiment,0,196,47,0,11
named-entity-recognition,2,model,experiment,0,197,48,0,1
named-entity-recognition,2,Speed Bi-LSTM-CRF 1 Bi-LSTM 4.60 ID- CNN 7.96 85.76 0.13 21.19 ID-CNN 85.27 0.24 13.21 ID - CNN ( 1 block ) 84.28 0.10 26.01 : F1 score of sentence and document models on OntoNotes .,experiment,0,198,49,0,36
named-entity-recognition,2,"icalized greedy model of , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co-reference .",experiment,0,199,50,0,47
named-entity-recognition,2,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .",experiment,1,200,51,1,46
named-entity-recognition,2,The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL .,experiment,0,201,52,0,17
named-entity-recognition,2,"We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi-token named entity segment in CoNLL is about one token shorter than in OntoNotes .",experiment,0,202,53,0,43
named-entity-recognition,2,These long entities benefit more from explicit structured constraints enforced in Viterbi decoding .,experiment,0,203,54,0,14
named-entity-recognition,2,"Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .",experiment,0,204,55,0,25
named-entity-recognition,2,"Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .",experiment,0,205,56,0,25
named-entity-recognition,2,"In , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .",experiment,0,206,57,0,27
named-entity-recognition,2,"For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .",experiment,0,207,58,0,50
named-entity-recognition,2,"We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with dis ambiguation .",experiment,0,208,59,0,47
named-entity-recognition,2,"In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .",experiment,0,209,60,0,43
named-entity-recognition,2,conclusion,experiment,0,210,61,0,1
named-entity-recognition,2,"We present iterated dilated convolutional neural networks , fast token encoders that efficiently aggregate broad context without losing resolution .",experiment,0,211,62,0,20
named-entity-recognition,2,"These provide impressive speed improvements for sequence labeling , particularly when processing entire documents at a time .",experiment,0,212,63,0,18
named-entity-recognition,2,"In the future we hope to extend this work to NLP tasks with richer structured output , such as parsing .",experiment,0,213,64,0,21
named-entity-recognition,3,Semi-supervised sequence tagging with bidirectional language models,title,1,2,1,0,7
named-entity-recognition,3,abstract,abstract,0,3,1,0,1
named-entity-recognition,3,Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks .,abstract,0,4,2,0,20
named-entity-recognition,3,"However , in most cases , the recurrent network that operates on word - level representations to produce context sensitive representations is trained on relatively little labeled data .",abstract,0,5,3,0,29
named-entity-recognition,3,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",abstract,1,6,4,0,30
named-entity-recognition,3,"We evaluate our model on two standard datasets for named entity recognition ( NER ) and chunking , and in both cases achieve state of the art results , surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers .",abstract,0,7,5,0,50
named-entity-recognition,3,introduction,introduction,0,8,1,0,1
named-entity-recognition,3,"Due to their simplicity and efficacy , pre-trained word embedding have become ubiquitous in NLP systems .",introduction,0,9,2,0,17
named-entity-recognition,3,Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks .,introduction,0,10,3,0,33
named-entity-recognition,3,"However , in many NLP tasks it is essential to represent not just the meaning of a word , but also the word in context .",introduction,0,11,4,0,26
named-entity-recognition,3,"For example , in the two phrases "" A Central Bank spokesman "" and "" The Central African Republic "" , the word ' Central ' is used as part of both an Organization and Location .",introduction,0,12,5,0,37
named-entity-recognition,3,"Accordingly , current state of the art sequence tagging models typically include a bidirectional re-current neural network ( RNN ) that encodes token sequences into a context sensitive representation before making token specific predictions .",introduction,0,13,6,0,35
named-entity-recognition,3,"Although the token representation is initialized with pre-trained embeddings , the parameters of the bidirectional RNN are typically learned only on labeled data .",introduction,0,14,7,0,24
named-entity-recognition,3,"Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks ( e.g. , .",introduction,0,15,8,0,22
named-entity-recognition,3,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .",introduction,1,16,9,0,18
named-entity-recognition,3,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .",introduction,1,17,10,0,46
named-entity-recognition,3,"Since the LM embeddings are used to compute the probability of future words in a neural LM , they are likely to encode both the semantic and syntactic roles of words in context .",introduction,0,18,11,0,34
named-entity-recognition,3,Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting .,introduction,0,19,12,0,25
named-entity-recognition,3,"When we include the LM embeddings in our system over all performance increases from 90. 87 % to 91.93 % F 1 for the CoNLL 2003 NER task , a more then 1 % absolute F1 increase , and a substantial improvement over the previous state of the art .",introduction,0,20,13,0,50
named-entity-recognition,3,We also establish a new state of the art result ( 96.37 % F 1 ) for the CoNLL 2000 Chunking task .,introduction,0,21,14,0,23
named-entity-recognition,3,"As a secondary contribution , we show that using both forward and backward LM embeddings boosts performance over a forward only LM .",introduction,0,22,15,0,23
named-entity-recognition,3,We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers .,introduction,0,23,16,0,23
named-entity-recognition,3,The main components in our language - modelaugmented sequence tagger ( TagLM ) are illustrated in .,introduction,0,24,17,0,17
named-entity-recognition,3,"After pre-training word embeddings and a neural LM on large , unlabeled corpora ( Step 1 ) , we extract the word and LM embeddings for every token in a given input sequence Step 2 ) and use them in the supervised sequence tagging model (",introduction,0,25,18,0,46
named-entity-recognition,3,step 3 ) .,introduction,0,26,19,0,4
named-entity-recognition,3,baseline sequence tagging model,introduction,0,27,20,0,4
named-entity-recognition,3,"Our baseline sequence tagging model is a hierarchical neural tagging model , closely following a number of recent studies ) ( left side of ) .",introduction,0,28,21,0,26
named-entity-recognition,3,"Given a sentence of tokens ( t 1 , t 2 , . . . , t N ) it first forms a representation , x k , for each token by concatenating a character based representation ck with a token embedding wk :",introduction,0,29,22,0,44
named-entity-recognition,3,The character representation ck captures morphological information and is either a convolutional neural network ( CNN ) or RNN .,introduction,0,30,23,0,20
named-entity-recognition,3,"It is parameterized by C ( , ? c ) with parameters ? c .",introduction,0,31,24,0,15
named-entity-recognition,3,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",introduction,0,32,25,0,30
named-entity-recognition,3,"To learn a context sensitive representation , we employ multiple layers of bidirectional RNNs .",introduction,0,33,26,0,15
named-entity-recognition,3,"For each token position , k , the hidden state h k , i of RNN layer i is formed by concatenating the hidden states from the forward ( ? ? h k , i ) and backward ( ? ? h k , i ) RNNs .",introduction,0,34,27,0,48
named-entity-recognition,3,"As a result , the bidirectional RNN is able to use both past and future information to make a prediction at token k.",introduction,0,35,28,0,23
named-entity-recognition,3,"More formally , for the first RNN layer that operates on x k to output h k,1 :",introduction,0,36,29,0,18
named-entity-recognition,3,Step 2 : Prepare word embedding and LM embedding for each token in the input sequence .,introduction,0,37,30,0,17
named-entity-recognition,3,"Two representations of the word "" York """,introduction,0,38,31,0,8
named-entity-recognition,3,Step 3 : Use both word embeddings and LM embeddings in the sequence tagging model .,introduction,0,39,32,0,16
named-entity-recognition,3,new york is located :,introduction,0,40,33,0,5
named-entity-recognition,3,"The main components in TagLM , our language - model - augmented sequence tagging system .",introduction,0,41,34,0,16
named-entity-recognition,3,The language model component ( in orange ) is used to augment the input token representation in a traditional sequence tagging models ( in grey ) .,introduction,0,42,35,0,27
named-entity-recognition,3,"The second RNN layer is similar and uses h k , 1 to output h k ,2 .",introduction,0,43,36,0,18
named-entity-recognition,3,"In this paper , we use L = 2 layers of RNNs in all experiments and parameterize R i as either Gated Recurrent Units ( GRU ) or Long Short - Term Memory units ( LSTM ) depending on the task .",introduction,0,44,37,0,42
named-entity-recognition,3,"Finally , the output of the final RNN layer h k,L is used to predict a score for each possible tag using a single dense layer .",introduction,0,45,38,0,27
named-entity-recognition,3,"Due to the dependencies between successive tags in our sequence labeling tasks ( e.g. using the BIOES labeling scheme , it is not possible for I - PER to follow B - LOC ) , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token .",introduction,0,46,39,0,55
named-entity-recognition,3,"Accordingly , we add another layer with parameters for each label bigram , computing the sentence conditional random field ( CRF ) loss using the forward - backward algorithm at training time , and using the Viterbi algorithm to find the most likely tag sequence at test time , similar to Collobert et al .",introduction,0,47,40,1,55
named-entity-recognition,3,bidirectional lm,introduction,0,48,41,0,2
named-entity-recognition,3,"A language model computes the probability of a token sequence ( t 1 , t 2 , . . . , t N )",introduction,0,49,42,0,24
named-entity-recognition,3,"Recent state of the art neural language models ) use a similar architecture to our baseline sequence tagger where they pass a token representation ( either from a CNN over characters or as token embeddings ) through multiple layers of LSTMs to embed the history ( t 1 , t 2 , . . . , t k ) into a fixed dimensional vector ? ? h LM k .",introduction,0,50,43,0,70
named-entity-recognition,3,This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model .,introduction,0,51,44,0,26
named-entity-recognition,3,"Finally , the language model predicts the probability of token t k + 1 using a softmax layer over words in the vocabulary .",introduction,0,52,45,0,24
named-entity-recognition,3,The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM .,introduction,0,53,46,0,28
named-entity-recognition,3,A backward LM predicts the previous token given the future context .,introduction,0,54,47,0,12
named-entity-recognition,3,"Given a sentence with N tokens , it computes",introduction,0,55,48,0,9
named-entity-recognition,3,A backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ?,introduction,0,56,49,0,22
named-entity-recognition,3,"h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",introduction,0,57,50,0,31
named-entity-recognition,3,"In our final system , after pre-training the forward and backward LMs separately , we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings , i.e. , h LM",introduction,0,58,51,0,38
named-entity-recognition,3,"Note that in our formulation , the forward and backward LMs are independent , without any shared parameters .",introduction,0,59,52,0,19
named-entity-recognition,3,Combining LM with sequence model,introduction,0,60,53,0,5
named-entity-recognition,3,"Our combined system , TagLM , uses the LM embeddings as additional inputs to the sequence tagging model .",introduction,0,61,54,0,19
named-entity-recognition,3,"In particular , we concatenate the LM embeddings h LM with the output from one of the bidirectional RNN layers in the sequence model .",introduction,0,62,55,0,25
named-entity-recognition,3,"In our experiments , we found that introducing the LM embeddings at the output of the first layer performed the best .",introduction,0,63,56,0,22
named-entity-recognition,3,"More formally , we simply replace ( 2 ) with",introduction,0,64,57,0,10
named-entity-recognition,3,There are alternate possibilities for adding the LM embeddings to the sequence model .,introduction,0,65,58,0,14
named-entity-recognition,3,One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,introduction,0,66,59,0,18
named-entity-recognition,3,where f is a non-linear function ) .,introduction,0,67,60,0,8
named-entity-recognition,3,Another possibility introduces an attention - like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model .,introduction,0,68,61,0,25
named-entity-recognition,3,"Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study , preferring to leave them for future work .",introduction,0,69,62,0,28
named-entity-recognition,3,experiments,experiment,0,70,1,0,1
named-entity-recognition,3,"We evaluate our approach on two well benchmarked sequence tagging tasks , the CoNLL 2003 NER task and the CoNLL 2000 Chunking task .",experiment,0,71,2,0,24
named-entity-recognition,3,We report the official evaluation metric ( micro - averaged F 1 ) .,experiment,0,72,3,0,14
named-entity-recognition,3,"In both cases , we use the BIOES labeling scheme for the output tags , following previous work which showed it outperforms other options ( e.g. , ) .",experiment,0,73,4,0,29
named-entity-recognition,3,"Following , we use the Senna word embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0 .",experiment,0,74,5,0,23
named-entity-recognition,3,conll 2003 ner .,experiment,1,75,6,0,4
named-entity-recognition,3,"The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",experiment,0,76,7,0,29
named-entity-recognition,3,"It includes standard train , development and test sets .",experiment,0,77,8,0,10
named-entity-recognition,3,Following previous work we trained on both the train and development sets after tuning hyperparameters on the development set .,experiment,0,78,9,0,20
named-entity-recognition,3,The hyperparameters for our baseline model are similar to .,experiment,0,79,10,0,10
named-entity-recognition,3,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,experiment,1,80,11,0,20
named-entity-recognition,3,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,experiment,1,81,12,0,13
named-entity-recognition,3,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .",experiment,1,82,13,0,22
named-entity-recognition,3,conll 2000 chunking .,experiment,1,83,14,0,4
named-entity-recognition,3,The CoNLL 2000 chunking task uses sections 15 - 18 from the Wall Street Journal corpus for training and section 20 for testing .,experiment,0,84,15,0,24
named-entity-recognition,3,"It defines 11 syntactic chunk types ( e.g. , NP , VP , ADJP ) in addition to other .",experiment,0,85,16,0,20
named-entity-recognition,3,We randomly sampled 1000 sentences from the training set as a held - out development set .,experiment,0,86,17,0,17
named-entity-recognition,3,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,experiment,1,87,18,0,30
named-entity-recognition,3,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,experiment,1,88,19,0,12
named-entity-recognition,3,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",experiment,1,89,20,0,33
named-entity-recognition,3,pre-trained language models .,experiment,0,90,21,0,4
named-entity-recognition,3,"The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark , a publicly available benchmark for largescale language modeling .",experiment,0,91,22,0,26
named-entity-recognition,3,"The training split has approximately 800 million tokens , about a 4000X increase over the number training tokens in the CoNLL datasets .",experiment,0,92,23,0,23
named-entity-recognition,3,explored several model architectures and released their best single model and training recipes .,experiment,0,93,24,0,14
named-entity-recognition,3,"Following , they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state .",experiment,0,94,25,0,27
named-entity-recognition,3,Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity .,experiment,0,95,26,0,18
named-entity-recognition,3,"It uses a character CNN with 4096 filters for input , followed by two stacked LSTMs , each with 8192 hidden units and a 1024 dimensional projection layer .",experiment,0,96,27,0,29
named-entity-recognition,3,We use CNN - BIG - LSTM to refer to this language model in our results .,experiment,0,97,28,0,17
named-entity-recognition,3,"In addition to CNN - BIG - LSTM from , 1 we used the same corpus to train two additional language models with fewer parameters : forward LSTM - 2048-512 and backward LSTM - 2048-512 .",experiment,0,98,29,0,36
named-entity-recognition,3,Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer .,experiment,0,99,30,0,23
named-entity-recognition,3,"We closely followed the procedure outlined in , except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs .",experiment,0,100,31,0,31
named-entity-recognition,3,"The test set perplexities for our forward and backward LSTM - 2048 - 512 language models are 47.7 and 47.3 , respectively .",experiment,0,101,32,0,23
named-entity-recognition,3,2,experiment,0,102,33,0,1
named-entity-recognition,3,Model F 1 std 90.91 0.20 90.94 91.37 Our baseline without LM 90.87 0.13 TagLM 91.93 0.19 Training .,experiment,0,103,34,0,19
named-entity-recognition,3,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .",experiment,1,104,35,1,20
named-entity-recognition,3,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .",experiment,1,105,36,0,22
named-entity-recognition,3,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .",experiment,1,106,37,0,27
named-entity-recognition,3,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,experiment,1,107,38,0,25
named-entity-recognition,3,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ?",experiment,0,108,39,0,22
named-entity-recognition,3,"an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ?",experiment,0,109,40,0,19
named-entity-recognition,3,"an order of magnitude again , train for five more epochs and stop .",experiment,0,110,41,0,14
named-entity-recognition,3,"Following , we train each final model configuration ten times with different random seeds and report the mean and standard deviation F 1 .",experiment,0,111,42,0,24
named-entity-recognition,3,It is important to estimate the variance of model performance since the test data sets are relatively small .,experiment,0,112,43,0,19
named-entity-recognition,3,overall system results,result,0,113,1,0,3
named-entity-recognition,3,Tables 1 and 2 compare results from Tag LM with previously published state of the art results without additional labeled data or task specific gazetteers .,result,0,114,2,0,26
named-entity-recognition,3,compare results of,result,0,115,1,0,3
named-entity-recognition,3,Tag LM to other systems that include additional labeled data or gazetteers .,result,0,116,2,0,13
named-entity-recognition,3,"In both tasks , Tag LM establishes a new state of the art using bidirectional LMs ( the forward CNN - BIG - LSTM and the backward LSTM - 2048 - 512 ) .",result,0,117,3,1,34
named-entity-recognition,3,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .",result,1,118,4,0,50
named-entity-recognition,3,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .",result,1,119,5,0,33
named-entity-recognition,3,The improvement over the previous best result of 95.77 in that jointly trains with Penn Treebank ( PTB ) POS tags is statistically significant at 95 % ( p < 0.001 assuming standard deviation of 0.1 ) .,result,0,120,6,0,38
named-entity-recognition,3,"Importantly , the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F 1 in the NER and Chunking tasks , respectively .",result,0,121,7,0,26
named-entity-recognition,3,adding external resources .,result,0,122,8,0,4
named-entity-recognition,3,"Although we do not use external labeled data or gazetteers , we found that TagLM outperforms previous state of the art results in both tasks when external resources ( labeled data or task specific gazetteers ) are available .",result,0,123,9,0,39
named-entity-recognition,3,"Furthermore , show that , in most cases , the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning .",result,0,124,10,0,34
named-entity-recognition,3,"For example , noted an improvement of only 0.06 F 1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and reported an increase of 0.71 F 1 when adding gazetteers to their baseline .",result,0,125,11,0,42
named-entity-recognition,3,"In the Chunking task , previous work has reported from 0.28 to 0.75 improvement in F 1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities .",result,0,126,12,0,31
named-entity-recognition,3,analysis,result,0,127,13,0,1
named-entity-recognition,3,"To elucidate the characteristics of our LM augmented sequence tagger , we ran a number of additional experiments on the CoNLL 2003 NER task .",result,0,128,14,0,25
named-entity-recognition,3,How to use LM embeddings ?,result,0,129,15,0,6
named-entity-recognition,3,"In this experiment , we concatenate the LM embeddings at dif - shows that the second alternative performs best .",result,0,130,16,0,20
named-entity-recognition,3,We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves over all system performance .,result,0,131,17,0,47
named-entity-recognition,3,These results are consistent with who found that chunking performance was sensitive to the level at which additional POS supervision was added .,result,0,132,18,0,23
named-entity-recognition,3,Does it matter which language model to use ?,result,0,133,19,0,9
named-entity-recognition,3,"In this experiment , we compare six different configurations of the forward and backward language models ( including the baseline model which does not use any language models ) .",result,0,134,20,0,30
named-entity-recognition,3,The results are reported in .,result,0,135,21,0,6
named-entity-recognition,3,"We find that adding backward LM embeddings consistently outperforms forward - only LM embeddings , with F 1 improvements between 0.22 and 0.27 % , even with the relatively small backward LSTM - 2048-512 LM .",result,0,136,22,0,36
named-entity-recognition,3,"LM size is important , and replacing the forward LSTM - 2048 - 512 with CNN - BIG - LSTM ( test perplexities of 47.7 to 30.0 on 1B Word Benchmark ) improves F 1 by 0.26 - 0.31 % , about as much as adding backward LM .",result,0,137,23,0,49
named-entity-recognition,3,"Accordingly , we hypothesize ( but have not tested ) that replacing the backward LSTM - 2048 - 512 with a backward LM analogous to the CNN - BIG - LSTM would further improve performance .",result,0,138,24,0,36
named-entity-recognition,3,"To highlight the importance of including language models trained on a large scale data , we also experimented with training a language model on just the CoNLL 2003 training and development data .",result,0,139,25,0,33
named-entity-recognition,3,Due to the much smaller size of this data Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM .,result,0,140,26,0,27
named-entity-recognition,3,"This result supports the hypothesis that adding language models help because they learn composition functions ( i.e. , the RNN parameters in the language model ) from much larger data compared to the composition functions in the baseline tagger , which are only learned from labeled data .",result,0,141,27,0,48
named-entity-recognition,3,Importance of task specific RNN .,result,0,142,28,0,6
named-entity-recognition,3,To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags .,result,0,143,29,0,39
named-entity-recognition,3,"In this setup , performance was very low , 88.17 F 1 , well below our baseline .",result,0,144,30,0,18
named-entity-recognition,3,This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings .,result,0,145,31,0,22
named-entity-recognition,3,Does the LM transfer across domains ?,result,0,146,32,0,7
named-entity-recognition,3,One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,result,0,147,33,0,40
named-entity-recognition,3,"To test the sensitivity to the LM training domain , we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 , Science IE .",result,0,148,34,0,33
named-entity-recognition,3,"5 Scien - ce IE requires end - to - end joint entity and relationship extraction from scientific publications across three diverse fields ( computer science , material sciences , and physics ) and defines three broad entity types ( Task , Material and Process ) .",result,0,149,35,0,47
named-entity-recognition,3,"For this task , Tag LM increased F 1 on the development set by 4.12 % ( from 49.93 to to 54.05 % ) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to Science IE , Scenario 1 .",result,0,150,36,0,50
named-entity-recognition,3,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain .,result,0,151,37,0,23
named-entity-recognition,3,related work,related work,0,152,1,0,2
named-entity-recognition,3,unlabeled data .,related work,0,153,2,0,3
named-entity-recognition,3,Tag LM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models .,related work,0,154,3,0,18
named-entity-recognition,3,"Besides pre-trained word embeddings , our method is most closely related to .",related work,0,155,4,0,13
named-entity-recognition,3,"Instead of using a LM , uses a probabilistic generative model to infer contextsensitive latent variables for each token , which are then used as extra features in a supervised CRF tagger .",related work,0,156,5,0,33
named-entity-recognition,3,"Other semisupervised learning methods for structured prediction problems include co-training , expectation maximization , structural learning ( Ando and Zhang , 2005 ) and maximum discriminant functions .",related work,0,157,6,1,28
named-entity-recognition,3,It is easy to combine Tag LM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model ( except for expectation maximization ) .,related work,0,158,7,0,34
named-entity-recognition,3,A detailed discussion of semisupervised learning methods in NLP can be found in .,related work,0,159,8,0,14
named-entity-recognition,3,"learned a context encoder from unlabeled data with an objective function similar to a bi-directional LM and applied it to several NLP tasks closely related to the unlabeled objective function : sentence completion , lexical substitution and word sense dis ambiguation .",related work,0,160,9,0,42
named-entity-recognition,3,"LM embeddings are related to a class of methods ( e.g. , for learning sentence and document encoders from unlabeled data , which can be used for text classification and textual entailment among other tasks .",related work,0,161,10,0,36
named-entity-recognition,3,Dai and Le pre-trained LSTMs using language models and sequence autoencoders then fine tuned the weights for classification tasks .,related work,0,162,11,0,20
named-entity-recognition,3,"In contrast to our method that uses unlabeled data to learn token - in - context embeddings , all of these methods use unlabeled data to learn an encoder for an entire text sequence ( sentence or document ) .",related work,0,163,12,0,40
named-entity-recognition,3,neural language models .,method,0,164,1,0,4
named-entity-recognition,3,LMs have always been a critical component in statistical machine translation systems .,method,0,165,2,0,13
named-entity-recognition,3,"Recently , neural LMs have also been integrated in neural machine translation systems ( e.g. , to score candidate translations .",method,0,166,3,0,21
named-entity-recognition,3,"In contrast , Tag LM uses neural LMs to encode words in the input sequence .",method,0,167,4,0,16
named-entity-recognition,3,"Unlike forward LMs , bidirectional LMs have received little prior attention .",method,0,168,5,0,12
named-entity-recognition,3,"Most similar to our formulation , used a bidirectional neural LM in a statistical machine translation system for instance selection .",method,0,169,6,0,21
named-entity-recognition,3,"They tied the input token embeddings and softmax weights in the forward and backward directions , unlike our approach which uses two distinct models without any shared parameters .",method,0,170,7,0,29
named-entity-recognition,3,Frinken et al. ( 2012 ) also used a bidirectional n-gram LM for handwriting recognition .,method,0,171,8,1,16
named-entity-recognition,3,interpreting rnn states .,method,0,172,9,0,4
named-entity-recognition,3,"Recently , there has been some interest in interpreting the activations of RNNs .",method,0,173,10,0,14
named-entity-recognition,3,showed that single LSTM units can learn to predict singular - plural distinctions .,method,0,174,11,0,14
named-entity-recognition,3,"visualized character level LSTM states and showed that individual cells capture long - range dependencies such as line lengths , quotes and brackets .",method,0,175,12,0,24
named-entity-recognition,3,Our work complements these studies by showing that LM states are useful for downstream tasks as away of interpreting what they learn .,method,0,176,13,0,23
named-entity-recognition,3,other sequence tagging models .,method,0,177,14,0,5
named-entity-recognition,3,Current state of the art results in sequence tagging problems are based on bidirectional RNN models .,method,0,178,15,0,17
named-entity-recognition,3,"However , many other sequence tagging models have been proposed in the literature for this class of problems ( e.g. , .",method,0,179,16,0,22
named-entity-recognition,3,"LM embeddings could also be used as additional features in other models , although it is not clear whether the model complexity would be sufficient to effectively make use of them .",method,0,180,17,0,32
named-entity-recognition,3,conclusion,method,0,181,18,0,1
named-entity-recognition,3,"In this paper , we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models .",method,0,182,19,0,26
named-entity-recognition,3,Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking .,method,0,183,20,0,19
named-entity-recognition,3,Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance .,method,0,184,21,0,18
named-entity-recognition,3,"The proposed method is robust even when the LM is trained on unlabeled data from a different domain , or when the baseline model is trained on a large number of labeled examples .",method,0,185,22,0,34
named-entity-recognition,0,"0,000 + Times Accelerated Robust Subset Selection ( ARSS )",title,1,2,1,0,10
named-entity-recognition,0,abstract,abstract,0,3,1,0,1
named-entity-recognition,0,Subset selection from massive data with noised information is increasingly popular for various applications .,abstract,1,4,2,0,15
named-entity-recognition,0,This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers .,abstract,0,5,3,0,19
named-entity-recognition,0,"To address the above two issues , we propose an accelerated robust subset selection ( ARSS ) method .",abstract,0,6,4,0,19
named-entity-recognition,0,"Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .",abstract,1,7,5,0,40
named-entity-recognition,0,"As a result , the robustness against outlier elements is greatly enhanced .",abstract,0,8,6,0,13
named-entity-recognition,0,"Actually , data size is generally much larger than feature length , i.e. N L. Based on this observation , we propose a speedup solver ( via ALM and equivalent derivations ) to highly reduce the computational cost , theoretically from ON 4 to ON 2 L .",abstract,0,9,7,0,48
named-entity-recognition,0,"Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods , but also runs 10,000 + times faster than the most related method .",abstract,0,10,8,0,32
named-entity-recognition,0,introduction,introduction,0,11,1,0,1
named-entity-recognition,0,"Due to the explosive growth of data , subset selection methods are increasingly popular for a wide range of machine learning and computer vision applications .",introduction,0,12,2,0,26
named-entity-recognition,0,This kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset .,introduction,0,13,3,0,22
named-entity-recognition,0,"By analyzing a few , we can roughly know all .",introduction,0,14,4,0,11
named-entity-recognition,0,"Such case is very important to summarize and visualize huge datasets of texts , images and videos etc . .",introduction,0,15,5,0,20
named-entity-recognition,0,"Besides , by only using the selected exemplars for succeeding tasks , the cost of memories and computational time will be greatly reduced .",introduction,0,16,6,0,24
named-entity-recognition,0,"Additionally , as outliers are generally less representative , the side effect of outliers will be reduced , thus boosting the performance of subsequent applications .",introduction,0,17,7,0,26
named-entity-recognition,0,There have been several subset selection methods .,introduction,0,18,8,0,8
named-entity-recognition,0,The most intuitional method is to randomly select a fixed number of samples .,introduction,0,19,9,0,14
named-entity-recognition,0,"Although highly efficient , there is no guarantee for an effective selection .",introduction,0,20,10,0,13
named-entity-recognition,0,"For the other methods , depending on the mechanism of representative exemplars , there are mainly three categories of selection methods .",introduction,0,21,11,0,22
named-entity-recognition,0,One category Data size ( N ) Selection Time,introduction,0,22,12,0,9
named-entity-recognition,0,classifiers,introduction,0,23,13,0,1
named-entity-recognition,0,Classification Accuracy Performance TED RRSSNie RRSSour ARSSour : Comparisons of four algorithms on Optdigit .,introduction,0,24,14,0,15
named-entity-recognition,0,Two conclusions can be drawn .,introduction,0,25,15,0,6
named-entity-recognition,0,"First , our method ( ARSSour ) is highly faster than all others ; with the help of an elegant new theorem , RRSSour is significantly faster than the authorial algorithm RRSSNie.",introduction,0,26,16,0,32
named-entity-recognition,0,"Second , ARSSour achieves highly promising prediction accuracies .",introduction,0,27,17,0,9
named-entity-recognition,0,relies on the assumption that the data points lie in one or multiple low - dimensional subspaces .,introduction,0,28,18,0,18
named-entity-recognition,0,"Specifically , the Rank Revealing QR ( RRQR ) selects the subsets that give the best conditional sub-matrix .",introduction,0,29,19,0,19
named-entity-recognition,0,"Unfortunately , this method has suboptimal properties , as it is not assured to find the globally optimum in polynomial time .",introduction,0,30,20,0,22
named-entity-recognition,0,Another category assumes that the samples are distributed around centers .,introduction,0,31,21,0,11
named-entity-recognition,0,The center or its nearest neighbour are selected as exemplars .,introduction,0,32,22,0,11
named-entity-recognition,0,"Perhaps , Kmeans and Kmedoids are the most typical methods ( Kmedoids is a variant of Kmeans ) .",introduction,0,33,23,0,19
named-entity-recognition,0,Both methods employ an EM - like algorithm .,introduction,0,34,24,0,9
named-entity-recognition,0,"Thus , the results depend tightly on the initialization , and they are highly unstable for large K ( i.e. the number of centers or selected samples ) .",introduction,0,35,25,0,29
named-entity-recognition,0,"Recently , there are a few methods that assume exemplars are the samples that can best represent the whole dataset .",introduction,0,36,26,0,21
named-entity-recognition,0,"However , for , the optimization is a combinatorial problem ( NP - hard ) , which is computationally intractable to solve .",introduction,0,37,27,0,23
named-entity-recognition,0,"Besides , the representation loss is measured by the least square measure , which is sensitive to outliers in data .",introduction,0,38,28,0,21
named-entity-recognition,0,"Then improves by employing a robust loss via the 2 , 1 - norm ; the 1 - norm is applied to samples , and the 2 - norm is used for features .",introduction,0,39,29,0,34
named-entity-recognition,0,"In this way , the side effect of outlier samples is relieved .",introduction,0,40,30,0,13
named-entity-recognition,0,The solver of ) is theoretically perfect due to its ability of convergence to global optima .,introduction,0,41,31,0,17
named-entity-recognition,0,"Unfortunately , in terms of computational costs , the solver is highly complex .",introduction,0,42,32,0,14
named-entity-recognition,0,It takes ON 4 for one iteration as shown in .,introduction,0,43,33,0,11
named-entity-recognition,0,This is infeasible for the case of large N ( e.g. it takes 2000 + hours for a case of N = 13000 ) .,introduction,0,44,34,1,25
named-entity-recognition,0,"Moreover , the representation loss is only robust against outlier samples .",introduction,0,45,35,0,12
named-entity-recognition,0,"Such case is worth improvement , as there may exist outlier elements in real data .",introduction,0,46,36,0,16
named-entity-recognition,0,contributions .,introduction,0,47,37,0,2
named-entity-recognition,0,"In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .",introduction,1,48,38,0,31
named-entity-recognition,0,"To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .",introduction,1,49,39,0,33
named-entity-recognition,0,"As a result , the robustness against outliers is greatly boosted .",introduction,0,50,40,0,12
named-entity-recognition,0,"Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .",introduction,1,51,41,0,27
named-entity-recognition,0,The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .,introduction,1,52,42,0,18
named-entity-recognition,0,"Via them , we reduce the computational complexity from ON 4 to ON 2 L .",introduction,0,53,43,0,16
named-entity-recognition,0,"Extensive results on ten benchmark datasets demonstrate that in average , our method is 10,000 + times faster than Nie 's method .",introduction,0,54,44,0,23
named-entity-recognition,0,The selection quality is highly encouraging as shown in .,introduction,0,55,45,0,10
named-entity-recognition,0,"Additionally , via another equivalent derivation , we give an accelerated solver for Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 as listed in , empirically obtaining a 500 + times speedup compared with the authorial solver .",introduction,0,56,46,0,50
named-entity-recognition,0,notations .,introduction,0,57,47,0,2
named-entity-recognition,0,We use boldface uppercase letters to denote matrices and boldface lowercase letters to represent vectors .,introduction,0,58,48,0,16
named-entity-recognition,0,For a matrix Y = [ Y ln ] ?,introduction,0,59,49,0,10
named-entity-recognition,0,"R LN , we denote it s l throw and n th column as y land y n respectively .",introduction,0,60,50,0,20
named-entity-recognition,0,"The 2 ,1 - norm of a matrix is defined as",introduction,0,61,51,0,11
named-entity-recognition,0,Subset Selection via Self - Representation,introduction,0,62,52,0,6
named-entity-recognition,0,"In the problem of subset selection , we are often given a set of N unlabelled points X = x 1 , x 2 , , x N | x n ?",introduction,0,63,53,0,32
named-entity-recognition,0,"R L , where L is the feature length .",introduction,0,64,54,0,10
named-entity-recognition,0,The goal is to select the top K ( K N ) most representative and informative samples ( i.e. exemplars ) to effectively describe the entire dataset X .,introduction,0,65,55,0,29
named-entity-recognition,0,"By solely using these K exemplars for subsequent tasks , we could greatly reduce the computational costs and largely alleviate the side effects of outlier elements in data .",introduction,0,66,56,0,29
named-entity-recognition,0,Such a motivation could be formulated as the Transductive Experimental Design ( TED ) model :,introduction,0,67,57,0,16
named-entity-recognition,0,where q ?,introduction,0,68,58,0,3
named-entity-recognition,0,"R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ?",introduction,0,69,59,0,20
named-entity-recognition,0,"x , ?k ?",introduction,0,70,60,0,4
named-entity-recognition,0,"{ 1 , , K} ; A = [ a 1 , , a N ] ?",introduction,0,71,61,0,17
named-entity-recognition,0,R KN is the corresponding linear combination coefficients .,introduction,0,72,62,0,9
named-entity-recognition,0,"By minimizing ( 1 ) , TED could select the highly informative and representative samples , as they have to well represent all the samples in X .",introduction,0,73,63,0,28
named-entity-recognition,0,"Although TED ( 1 ) is well modeled - very accurate and intuitive , there are two bottlenecks .",introduction,0,74,64,0,19
named-entity-recognition,0,"First , the objective is a combinatorial optimization problem .",introduction,0,75,65,0,10
named-entity-recognition,0,It is NP - hard to exhaustively search the optimal subset Q from X .,introduction,0,76,66,0,15
named-entity-recognition,0,"For this reason , the author approximate ( 1 ) via a sequential optimization problem , which is solved by an inefficient greedy optimization algorithm .",introduction,0,77,67,0,26
named-entity-recognition,0,"Second , similar to the existing least square loss based models in machine learning and statistics , ( 1 ) is sensitive to the presence of outliers",introduction,0,78,68,0,27
named-entity-recognition,0,where ?,introduction,0,79,69,0,2
named-entity-recognition,0,"is a nonnegative parameter ; A is constrained to be row - sparse , and thus to select the most representative and informative samples .",introduction,0,80,70,0,25
named-entity-recognition,0,"As the representation loss is accumulated via the 1 - norm among samples , compared with ( 1 ) , the robustness against outlier samples is enhanced .",introduction,0,81,71,0,28
named-entity-recognition,0,"Equivalently , ( 2 ) is rewritten in the matrix format :",introduction,0,82,72,0,12
named-entity-recognition,0,"Since the objective is convex in A , the global minimum maybe found by differentiating ( 3 ) and setting the derivative to zero , resulting in a linear system 1",introduction,0,83,73,0,31
named-entity-recognition,0,where v ?,introduction,0,84,74,0,3
named-entity-recognition,0,rn,introduction,0,85,75,0,1
named-entity-recognition,0,N is a diagonal matrix with then th diagonal entry as V nn = 1 2 an 2 and U nn = 1 2 xn ?,introduction,0,86,76,0,26
named-entity-recognition,0,xan 2 .,introduction,0,87,77,0,3
named-entity-recognition,0,"It seems perfect to use ( 4 ) to solve the objective ( 3 ) , because ( 4 ) looks simple and the global optimum is theoretically guaranteed .",introduction,0,88,78,0,30
named-entity-recognition,0,"Unfortunately , in terms of speed , ( 4 ) is usually infeasible due to the incredible computational demand in the case of large N ( the number of samples ) .",introduction,0,89,79,0,32
named-entity-recognition,0,"At each iteration , the computational complexity of ( 4 ) is up to ON 4 , as analyzed in Remark 1 .",introduction,0,90,80,0,23
named-entity-recognition,0,"According to our experiments , the time cost is up to 2088 hours ( i.e. 87 days ) for a subset selection problem of 13000 samples .",introduction,0,91,81,0,27
named-entity-recognition,0,remark,introduction,0,92,82,0,1
named-entity-recognition,0,1 . Since U nn X T X +? V ?,introduction,0,93,83,0,11
named-entity-recognition,0,"RN N , the major computational cost of ( 4 ) focuses on a N N linear system .",introduction,0,94,84,0,19
named-entity-recognition,0,"If solved by the Cholesky factorization method , it costs 1 3 N 3 for factorization as well as 2N 2 for forward and backward substitution .",introduction,0,95,85,0,27
named-entity-recognition,0,This amounts to ON 3 in total .,introduction,0,96,86,0,8
named-entity-recognition,0,"By now , we only solve an .",introduction,0,97,87,0,8
named-entity-recognition,0,"Once solving all the set of {a n } N n= 1 , the total complexity amounts to ON 4 for one iteration step .",introduction,0,98,88,0,25
named-entity-recognition,0,Accelerated Robust Subset Selection ( ARSS ),method,0,99,1,0,7
named-entity-recognition,0,"Due to the huge computational costs , Nie 's method is infeasible for the case of large N - the computational time is up to 2088 hours for a case of 13000 samples .",method,0,100,2,0,34
named-entity-recognition,0,"Besides , Nie 's model imposes the 2 - norm among features , which is prone to outliers in features .",method,0,101,3,0,21
named-entity-recognition,0,"To tackle the above two issues , we propose a more robust model in the p ( 0 < p ? 1 ) norm .",method,0,102,4,0,25
named-entity-recognition,0,"Although the resulted objective is challenging to solve , a speedup algorithm is proposed to dramatically save the computational costs .",method,0,103,5,0,21
named-entity-recognition,0,"For the same task of N = 13000 , it costs our method 1.8 minutes , achieving a 68429 times acceleration compared with the speed of Nie 's method .",method,0,104,6,0,30
named-entity-recognition,0,modeling .,method,0,105,7,0,2
named-entity-recognition,0,"To boost the robustness against outliers in both samples and features , we formulate the discrepancy between X and XA via the p ( 0 < p < 1 ) - norm .",method,0,106,8,0,33
named-entity-recognition,0,"There are theoretical and empirical evidences to verify that compared with 2 or 1 norms , the p - norm is more able to prevent outlier elements from dominating the objective , enhancing the robustness ) .",method,0,107,9,0,37
named-entity-recognition,0,"Thus , we have the following objective min",method,0,108,10,0,8
named-entity-recognition,0,where ?,method,0,109,11,0,2
named-entity-recognition,0,"is a balancing parameter ; A is a row sparse matrix , used to select the most informative and representative samples .",method,0,110,12,0,22
named-entity-recognition,0,"By minimizing the energy of ( 5 ) , we could capture the most essential properties of the dataset X.",method,0,111,13,0,20
named-entity-recognition,0,"After obtaining the optimal A , the row indexes are sorted by the row - sum value of the absolute A in decreasing order .",method,0,112,14,0,25
named-entity-recognition,0,The samples specified by the top K indexes are selected as exemplars .,method,0,113,15,0,13
named-entity-recognition,0,Note that the model ( 5 ) could be applied to the unsupervised feature selection problem by only transposing the data matrix X .,method,0,114,16,0,24
named-entity-recognition,0,"In this case , A is a L L row sparse matrix , used to select the most representative features .",method,0,115,17,0,21
named-entity-recognition,0,"Accelerated Solver for the ARSS Objective in Although objective ( 5 ) is challenging to solve , we propose an effective and highly efficient solver .",method,0,116,18,0,26
named-entity-recognition,0,The acceleration owes to the ALM and an equivalent derivation .,method,0,117,19,0,11
named-entity-recognition,0,alm,method,0,118,20,0,1
named-entity-recognition,0,"The most intractable challenge of ( 5 ) is that , the p ( 0 < p ? 1 ) - norm is non-convex , non-smooth and notdifferentiable at the zero point .",method,0,119,21,0,33
named-entity-recognition,0,"Therefore , it is beneficial to use the Augmented Lagrangian Method ( ALM ) to solve ( 5 ) , resulting in several easily tackled unconstrained subproblems .",method,0,120,22,0,28
named-entity-recognition,0,"By solving them iteratively , the solutions of subproblems could eventually converge to a minimum .",method,0,121,23,0,16
named-entity-recognition,0,"Specifically , we introduce an auxiliary variable E = X ? XA ?",method,0,122,24,0,13
named-entity-recognition,0,r ln .,method,0,123,25,0,3
named-entity-recognition,0,"Thus , the objective ( 5 ) becomes : min",method,0,124,26,0,10
named-entity-recognition,0,"To deal with the equality constraint in , the most convenient method is to add a penalty , resulting in",method,0,125,27,0,20
named-entity-recognition,0,where is a penalty parameter .,method,0,126,28,0,6
named-entity-recognition,0,"To guarantee the equality constraint , it requires approaching infinity , which may cause bad numerical conditions .",method,0,127,29,0,18
named-entity-recognition,0,"Instead , once introducing a Lagrangian multiplier , it is no longer requiring ? ?.",method,0,128,30,0,15
named-entity-recognition,0,"Thus , we rewrite into the standard ALM formulation as :",method,0,129,31,0,11
named-entity-recognition,0,where ?,method,0,130,32,0,2
named-entity-recognition,0,consists of L N Lagrangian multipliers .,method,0,131,33,0,7
named-entity-recognition,0,"In the following , a highly efficient solver will be given .",method,0,132,34,0,12
named-entity-recognition,0,the updating rule for ?,method,0,133,35,0,5
named-entity-recognition,0,"Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",method,0,134,36,0,28
named-entity-recognition,0,where is a monotonically increasing parameter over iteration steps .,method,0,135,37,0,10
named-entity-recognition,0,"For example , ? ? , where 1 < ? < 2 is a predefined parameter .",method,0,136,38,0,17
named-entity-recognition,0,"Efficient solver for E Removing irrelevant terms with E from ( 8 ) , we have",method,0,137,39,0,16
named-entity-recognition,0,where h = x ? xa ? ? ?,method,0,138,40,0,9
named-entity-recognition,0,r ln .,method,0,139,41,0,3
named-entity-recognition,0,"According to the definition of the p - norm and the Frobenius - norm , ( 10 ) could be decoupled into L N independent and unconstrained subproblems .",method,0,140,42,0,29
named-entity-recognition,0,The standard form of these subproblems is,method,0,141,43,0,7
named-entity-recognition,0,"where ? = 1 is a given positive parameter , y is the scalar variable need to deal with , c is a known scalar constant .",method,0,142,44,0,27
named-entity-recognition,0,Zuo et al. ) has recently proposed a generalized iterative shrinkage algorithm to solve ( 11 ) .,method,0,143,45,1,18
named-entity-recognition,0,This algorithm is easy to implement and able to achieve more accurate solutions than current methods .,method,0,144,46,0,17
named-entity-recognition,0,"Thus , we use it for our problem as :",method,0,145,47,0,10
named-entity-recognition,0,is obtained by solving the following equation :,method,0,146,48,0,8
named-entity-recognition,0,which could be solved efficiently via an iterative algorithm .,method,0,147,49,0,10
named-entity-recognition,0,"In this manner , ( 10 ) could be sovled extremely fast .",method,0,148,50,0,13
named-entity-recognition,0,"Accelerated solver for A The main acceleration focuses on the solver of A. Removing irrelevant terms with A from ( 8 ) , we have",method,0,149,51,0,25
named-entity-recognition,0,where ? = ?,method,0,150,52,0,4
named-entity-recognition,0,"is a nonnegative parameter , P = X ? E ? ? ?",method,0,151,53,0,13
named-entity-recognition,0,"R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .",method,0,152,54,0,29
named-entity-recognition,0,This amounts to tackling the following linear system 2 :,method,0,153,55,0,10
named-entity-recognition,0,as v + ? x tx ?,method,0,154,56,0,7
named-entity-recognition,0,"RN N , is mainly a N N linear system .",method,0,155,57,0,11
named-entity-recognition,0,"Once solved by the Cholesky factorization , the computational complexity is highly up to ON 3 .",method,0,156,58,0,17
named-entity-recognition,0,This is by no means a good choice for real applications with large N .,method,0,157,59,0,15
named-entity-recognition,0,"In the following , an equivalent derivation of ( 14 ) will be proposed to significantly save the computational complexity .",method,0,158,60,0,21
named-entity-recognition,0,theorem,method,0,159,61,0,1
named-entity-recognition,0,2 . The N N linear system is equivalent to the following L L linear system :,method,0,160,62,0,17
named-entity-recognition,0,where IL is a L L identity matrix .,method,0,161,63,0,9
named-entity-recognition,0,proof .,method,0,162,64,0,2
named-entity-recognition,0,"Note that V is a N N diagonal and positive - definite matrix , the exponent of V is efficient to achieve , i.e. V ? = { V ? nn } N n=1 , ?? ?",method,0,163,65,0,37
named-entity-recognition,0,r.,method,0,164,66,0,1
named-entity-recognition,0,We have the following equations,method,0,165,67,0,5
named-entity-recognition,0,"where Z = XV ? 1 2 , IN is a N N identity matrix .",method,0,166,68,0,16
named-entity-recognition,0,The following equation holds for any conditions,method,0,167,69,0,7
named-entity-recognition,0,"Multiplying with IN + ?Z T Z ?1 on the left and IL + ? ZZ T ? 1 on the right of both sides of the equal - sign , we have the equation as :",method,0,168,70,0,37
named-entity-recognition,0,"Therefore , substituting ( 18 ) and Z = XV ? 1 2 into ( 16 ) , we have the simplified updating rule as :",method,0,169,71,0,26
named-entity-recognition,0,"When N L , the most complex operation is the matrix multiplications , not the L L linear system .",method,0,170,72,0,20
named-entity-recognition,0,corollary,method,0,171,73,0,1
named-entity-recognition,0,3 . We have two equivalent updating rules and ( 15 ) for the objective ( 13 ) .,method,0,172,74,0,19
named-entity-recognition,0,If using ( 14 ) when N ?,method,0,173,75,0,8
named-entity-recognition,0,"L , and otherwise using ( 15 ) as shown in Algorithm 1 , the computational complexity of solvers for ( 13 ) is ON 2 L .",method,0,174,76,0,28
named-entity-recognition,0,"Due to N L , we have highly reduced the complexity from ON 4 to ON 2 L compared with Nie 's method .",method,0,175,77,0,24
named-entity-recognition,0,"Algorithm 1 for ( 13 ) : A * = ARSS A ( X , V , P , IL , ?)",method,0,176,78,0,22
named-entity-recognition,0,"Input : X , V , P , IL , ? 1 : if N ?",method,0,177,79,0,16
named-entity-recognition,0,l then 2 :,method,0,178,80,0,4
named-entity-recognition,0,"update A via the updating rule , that is 3 : update ? by the updating rule ( 9 ) , ? ?.",method,0,179,81,0,23
named-entity-recognition,0,8 : until convergence,method,0,180,82,0,4
named-entity-recognition,0,Output : A The solver to update A is given in Algorithm,method,0,181,83,0,12
named-entity-recognition,0,1 . The over all solver for our model ( 5 ) is summarized in Algorithm,method,0,182,84,0,16
named-entity-recognition,0,2 .,method,0,183,85,0,2
named-entity-recognition,0,"According to Theorem 2 and Corollary 3 , the solver for our model ( 13 ) is highly simplified , as feature length is generally much smaller than data size , i.e L N .",method,0,184,86,0,35
named-entity-recognition,0,"similarly ,",method,0,185,87,0,2
named-entity-recognition,0,"Nie 's method could be highly accelerated by Theorem 4 , obtaining 500 + times speedup , as shown in and .",method,0,186,88,0,22
named-entity-recognition,0,theorem,method,0,187,89,0,1
named-entity-recognition,0,4 . Nie 's N N solver ( 20 ) ) is equivalent to the following L L linear system ( 21 ) an = U nn U nn X TX + ? V,method,0,188,90,0,34
named-entity-recognition,0,?1 X T x n ( 20 ),method,0,189,91,0,8
named-entity-recognition,0,"?n ? { 1 , 2 , , N } , where IL is a L L identity matrix .",method,0,190,92,0,20
named-entity-recognition,0,proof .,method,0,191,93,0,2
named-entity-recognition,0,"Based on ( 20 ) , we have the following equalities :",method,0,192,94,0,12
named-entity-recognition,0,The derivations are equivalent ; their results are equal .,method,0,193,95,0,10
named-entity-recognition,0,2 v ?,method,0,194,96,0,3
named-entity-recognition,0,rn,method,0,195,97,0,1
named-entity-recognition,0,"N is a positive and diagonal matrix with then th diagonal entry as Vnn = 1 ? an 2 2 + > 0 , where is a small value to avoid singular failures Corollary 5 .",method,0,196,98,0,36
named-entity-recognition,0,"Since feature length is generally much smaller than data size , i.e. L N , our accelerated solver ( 20 ) for Nie 's model ( 3 ) is highly faster than the authorial solver ( 21 ) .",method,0,197,99,0,39
named-entity-recognition,0,"Theoretically , we reduce the computational complexity from ON 4 to ON 2 L + N L 3 , while maintaining the same solution .",method,0,198,100,0,25
named-entity-recognition,0,"That is , like Nie 's solver ( 20 ) , our speedup solver ( 21 ) can reach the global optimum .",method,0,199,101,0,23
named-entity-recognition,0,Extensive empirical results will verify the huge acceleration,method,0,200,102,0,8
named-entity-recognition,0,experiments experimental settings,experiment,0,201,1,0,3
named-entity-recognition,0,"In this part , the experimental settings are introduced .",experiment,0,202,2,0,10
named-entity-recognition,0,"All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .",experiment,1,203,3,0,30
named-entity-recognition,0,"Brief descriptions often benchmark datasets are summarized in , where ' Total ( N * ) ' denotes the total set of samples in each data .",experiment,0,204,4,0,27
named-entity-recognition,0,"Due to the high computational complexity , other methods can only handle small datasets ( while our method can handle the total set ) .",experiment,0,205,5,0,25
named-entity-recognition,0,"Thus , we randomly choose the candidate set from the total set to reduce the sample size , i.e. N < N * ( cf. ' Total ( N * ) ' and ' candid . ( N ) ' in ) .",experiment,0,206,6,0,43
named-entity-recognition,0,The remainder ( except candidate set ) are used for test .,experiment,0,207,7,0,12
named-entity-recognition,0,"Specifically , to simulate the varying quality of samples , ten percentage of candidate samples from each class are randomly selected and arbitrarily added one of the following three kinds of noise : "" Gaussian "" , "" Laplace "" and "" Salt & pepper "" respectively .",experiment,0,208,8,0,48
named-entity-recognition,0,"In a word , all experiment settings are same and fair for all the methods .",experiment,0,209,9,0,16
named-entity-recognition,0,speed comparisons,experiment,1,210,10,0,2
named-entity-recognition,0,There are two parts of speed comparisons .,experiment,0,211,11,0,8
named-entity-recognition,0,"First , how speed varies with increasing N is illustrated in .",experiment,0,212,12,0,12
named-entity-recognition,0,Then the comparison of specific speed is summarized in .,experiment,0,213,13,0,10
named-entity-recognition,0,Note that TED and RRSS Nie denote the authorial solver ( via authorial codes ) ; RRSS our is our accelerated solver for Nie 's model via Theorem 4 ; ARSS is the proposed method .,experiment,0,214,14,0,36
named-entity-recognition,0,"Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .",experiment,1,215,15,0,30
named-entity-recognition,0,"The results are illustrated in , where there are three sub-figures showing the speed of four methods on the benchmark datasets of Letter , MNIST and Waveform respectively .",experiment,0,216,16,0,29
named-entity-recognition,0,"As we shall see , both selection time of TED and RRSS Nie increases dramatically as N increases .",experiment,0,217,17,0,19
named-entity-recognition,0,"No surprisingly , RRSS Nie is incredibly time - consuming as N grows the order of curves looks higher than quadratic .",experiment,0,218,18,0,22
named-entity-recognition,0,"Actually , the theoretical complexity of RRSS Nie is highly up to ON 4 as analyzed in Remark",experiment,0,219,19,0,18
named-entity-recognition,0,1 .,experiment,0,220,20,0,2
named-entity-recognition,0,"Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .",experiment,1,221,21,0,33
named-entity-recognition,0,This is owing to the speedup techniques of ALM and equivalent derivations .,experiment,0,222,22,0,13
named-entity-recognition,0,"Via them , we reduce the computational cost from ON 4 to ON 2 L , as analyzed in Theorem 2 and Corollary",experiment,0,223,23,0,23
named-entity-recognition,0,3 .,experiment,0,224,24,0,2
named-entity-recognition,0,"Moreover , with the help of Theorem 4 , RRSS our is the second faster algorithm that is significantly accelerated compared with the authorial algorithm RRSS Nie .",experiment,0,225,25,0,28
named-entity-recognition,0,"Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .",experiment,1,226,26,0,31
named-entity-recognition,0,Four conclusions can be drawn from .,experiment,0,227,27,0,7
named-entity-recognition,0,"First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .",experiment,1,228,28,0,17
named-entity-recognition,0,"Second , with the help of Theorem 4 , RRSS our is highly faster than RRSS Nie , averagely obtaining a 559 times acceleration .",experiment,0,229,29,0,25
named-entity-recognition,0,"Third , ARSS is dramatically faster than :",experiment,0,230,30,0,8
named-entity-recognition,0,"Performances of TED , RRSS and ARSS : ( left - a ) speed in seconds , prediction accuracies .",experiment,0,231,31,0,20
named-entity-recognition,0,"In terms of speed , with the help of Theorem 4 , RRSSour is averagely 559 + times faster than the authorial algorithm , i.e. RRSSNie ; ARSS achieves surprisingly 23275 + times acceleration compared with RRSSNie .",experiment,0,232,32,0,38
named-entity-recognition,0,"Due to the more robust loss in the p -norm , the prediction accuracy of ARSS is highly encouraging .",experiment,0,233,33,0,20
named-entity-recognition,0,datasets,experiment,0,234,34,0,1
named-entity-recognition,0,"Speed 1 ' ARSS ( N * ) ' means the task of selecting samples from the whole dataset ( with N * samples as shown in the 2 nd column in ) , while ' TED ' to ' ARSS ' indicate the problem of dealing with the candidate sample sets ( with N samples as shown in the 3 rd column in ) .",experiment,0,235,35,0,66
named-entity-recognition,0,verify an average acceleration of 23275 times faster than RRSS Nie and 281 times faster than TED .,experiment,0,236,36,0,18
named-entity-recognition,0,"This means that for example if it takes RRSS Nie 100 years to do a subset selection task , it only takes our method 1.6 days to address the same problem .",experiment,0,237,37,0,32
named-entity-recognition,0,"Finally , we apply ARSS to the whole sample set of each data .",experiment,0,238,38,0,14
named-entity-recognition,0,RRSS Nie and TED ; the results in,experiment,0,239,39,0,8
named-entity-recognition,0,"The results are displayed in the 6 th column in , showing its capability to process very large datasets .",experiment,0,240,40,0,20
named-entity-recognition,0,prediction accuracy,experiment,1,241,41,0,2
named-entity-recognition,0,accuracy comparison,experiment,1,242,42,0,2
named-entity-recognition,0,We conduct experiments on ten benchmark datasets .,experiment,0,243,43,0,8
named-entity-recognition,0,"For each dataset , the top 200 representative samples are selected for training .",experiment,0,244,44,0,14
named-entity-recognition,0,"The prediction accuracies are reported in , including the results of two popular classifiers .",experiment,0,245,45,0,15
named-entity-recognition,0,Three observations can be drawn from this table .,experiment,0,246,46,0,9
named-entity-recognition,0,"First , Linear SVM generally outperforms KNN .",experiment,0,247,47,0,8
named-entity-recognition,0,"Second , in general , our method performs the best ; for a few cases , our method achieves comparable results with the best performances .",experiment,0,248,48,0,26
named-entity-recognition,0,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .",experiment,1,249,49,0,15
named-entity-recognition,0,The above analyses are better illustrated in the last row of .,experiment,0,250,50,0,12
named-entity-recognition,0,These results demonstrate that the p loss in our model is well suited to select exemplars from the sample sets of various quality .,experiment,0,251,51,0,24
named-entity-recognition,0,"Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .",experiment,1,252,52,0,27
named-entity-recognition,0,There are two rows and four columns of sub-figures .,experiment,0,253,53,0,10
named-entity-recognition,0,"The top row shows the results of KNN , and the bottom one shows results of SVM .",experiment,0,254,54,0,18
named-entity-recognition,0,Each column gives the result on one dataset .,experiment,0,255,55,0,9
named-entity-recognition,0,"As we shall see , the prediction accuracies generally increase as K increases .",experiment,1,256,56,0,14
named-entity-recognition,0,Such case is consistent with the common view that more training data will boost the prediction accuracy .,experiment,0,257,57,0,18
named-entity-recognition,0,"For each sub-figure , ARSS is generally among the best .",experiment,1,258,58,0,11
named-entity-recognition,0,This case implies that our robust objective ( 5 ) via the p - norm is feasible to select subsets from the data of varying qualities .,experiment,0,259,59,0,27
named-entity-recognition,0,conclusion,experiment,0,260,60,0,1
named-entity-recognition,0,"To deal with tremendous data of varying quality , we propose an accelerated robust subset selection ( ARSS ) method .",experiment,0,261,61,0,21
named-entity-recognition,0,The p - norm is exploited to enhance the robustness against both outlier samples and outlier features .,experiment,0,262,62,0,18
named-entity-recognition,0,"Although the resulted objective is complex to solve , we propose a highly efficient solver via two techniques : ALM and equivalent derivations .",experiment,0,263,63,0,24
named-entity-recognition,0,"Via them , we greatly reduce the computational complexity from ON 4 to ON 2 L .",experiment,0,264,64,0,17
named-entity-recognition,0,"Here feature length L is much smaller than data size N , i.e. L N .",experiment,0,265,65,0,16
named-entity-recognition,0,"Extensive results on ten benchmark datasets verify that our method not only runs 10,000 + times faster than the most related method , but also outperforms state of the art methods .",experiment,0,266,66,0,32
named-entity-recognition,0,"Moreover , we propose an accelerated solver to highly speedup Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 .",experiment,0,267,67,0,31
named-entity-recognition,0,"Empirically , our accelerated solver could achieve equal results and 500 + times acceleration compared with the authorial solver .",experiment,0,268,68,0,20
named-entity-recognition,0,limitation .,experiment,0,269,69,0,2
named-entity-recognition,0,"Our efficient algorithm build on the observation that the number of samples is generally larger than feature length , i.e. N > L. For the case of N ?",experiment,0,270,70,0,29
named-entity-recognition,0,"L , the acceleration will be inapparent .",experiment,0,271,71,0,8
